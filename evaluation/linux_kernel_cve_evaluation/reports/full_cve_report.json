{
    "summary": {
        "total_cves_tested": 17,
        "total_versions_tested": 50,
        "total_failed_patches": 2,
        "total_unique_downstream_versions_tested": 50,
        "total_unique_downstream_failed_patches": 2,
        "cves_with_all_failures": 0,
        "cves_with_partial_failures": 1,
        "cves_with_all_successful_patches": 16,
        "cves_skipped": 0
    },
    "cves_with_all_failures": [],
    "cves_with_partial_failures": [
        {
            "cve_id": "CVE-2025-21639",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21639.json",
            "patch_attempts": [
                {
                    "upstream_commit": "ea62dd1383913b5999f3d16ae99d411f41b528d4",
                    "upstream_commit_date": "2025-01-09 08:53:34 -0800",
                    "upstream_patch": "9fc17b76fc70763780aa78b38fcf4742384044a5",
                    "total_versions_tested": 6,
                    "successful_patches": 4,
                    "failed_patches": 2,
                    "patch_results": [
                        {
                            "downstream_patch": "4059507e34aa5fe0fa9fd5b2b5f0c8b26ab2d482",
                            "downstream_commit": "3cd0659deb9c03535fd61839e91d4d4d3e51ac71",
                            "commit_date": "2025-01-17 13:34:41 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 426 with fuzz 1 (offset -7 lines).\nHunk #2 succeeded at 454 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 4059507e34aa5fe0fa9fd5b2b5f0c8b26ab2d482\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:33 2025 +0100\n\n    sctp: sysctl: rto_min/max: avoid using current->nsproxy\n    \n    commit 9fc17b76fc70763780aa78b38fcf4742384044a5 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n    \n    Fixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 01fe23faf20d..0dd5da971689 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -426,7 +426,7 @@ static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -454,7 +454,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "dc9d0e3cfd16f66fbf0862857c6b391c8613ca9f",
                            "downstream_commit": "ad673e514b2793b8d5902f6ba6ab7e890dea23d5",
                            "commit_date": "2025-01-17 13:36:17 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 437 with fuzz 1 (offset 4 lines).\nHunk #2 succeeded at 465 with fuzz 1 (offset 4 lines).",
                            "downstream_patch_content": "commit dc9d0e3cfd16f66fbf0862857c6b391c8613ca9f\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:33 2025 +0100\n\n    sctp: sysctl: rto_min/max: avoid using current->nsproxy\n    \n    commit 9fc17b76fc70763780aa78b38fcf4742384044a5 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n    \n    Fixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 680ee80055f7..2df0b5fa22f8 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -437,7 +437,7 @@ static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -465,7 +465,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table,\n\t\t\t\t\t\t\t ARRAY_SIZE(sctp_net_table));\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "c87f1f6ade56c711f8736901e330685b453e420e",
                            "downstream_commit": "f0bb3935470684306e4e04793a20ac4c4b08de0b",
                            "commit_date": "2025-01-17 13:40:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c",
                            "downstream_patch_content": "commit c87f1f6ade56c711f8736901e330685b453e420e\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:33 2025 +0100\n\n    sctp: sysctl: rto_min/max: avoid using current->nsproxy\n    \n    commit 9fc17b76fc70763780aa78b38fcf4742384044a5 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n    \n    Fixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 9848d19630a4..a5285815264d 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -433,7 +433,7 @@ static int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -461,7 +461,7 @@ static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n};\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tsize_t table_size = ARRAY_SIZE(sctp_net_table);\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < table_size; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table, table_size);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tconst struct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "0f78f09466744589e420935e646ae78212a38290",
                            "downstream_commit": "86ddf8118123cb58a0fb8724cad6979c4069065b",
                            "commit_date": "2025-01-23 17:15:51 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 426 with fuzz 1 (offset -7 lines).\nHunk #2 succeeded at 454 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 0f78f09466744589e420935e646ae78212a38290\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:33 2025 +0100\n\n    sctp: sysctl: rto_min/max: avoid using current->nsproxy\n    \n    commit 9fc17b76fc70763780aa78b38fcf4742384044a5 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n    \n    Fixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 01fe23faf20d..0dd5da971689 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -426,7 +426,7 @@ static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -454,7 +454,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "c8d179f3b1c1d60bf4484f50aa67b4c70f91bff9",
                            "downstream_commit": "58f9e20e2a7602e1dd649a1ec4790077c251cb6c",
                            "commit_date": "2025-02-01 18:18:49 +0100",
                            "result": "failure",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 401 with fuzz 2 (offset -32 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej",
                            "downstream_patch_content": "commit c8d179f3b1c1d60bf4484f50aa67b4c70f91bff9\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:33 2025 +0100\n\n    sctp: sysctl: rto_min/max: avoid using current->nsproxy\n    \n    [ Upstream commit 9fc17b76fc70763780aa78b38fcf4742384044a5 ]\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n    \n    Fixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 4513d8d45e55..7777c0096a38 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -372,7 +372,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n \t\t\t\tvoid __user *buffer, size_t *lenp,\n \t\t\t\tloff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -401,7 +401,7 @@ static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n \t\t\t\tvoid __user *buffer, size_t *lenp,\n \t\t\t\tloff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t\t   loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic struct ctl_table sctp_net_table[] = {\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t{\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t\t   loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            },
                            "error": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 401 with fuzz 2 (offset -32 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej",
                            "total_hunks": 2,
                            "total_failed_hunks": 1,
                            "failed_hunks": [
                                2
                            ],
                            "file_conflicts": [
                                {
                                    "file_name": "net/sctp/sysctl.c",
                                    "rej_file_content": "```diff\n--- net/sctp/sysctl.c\n+++ net/sctp/sysctl.c\n@@ -461,7 +461,7 @@ static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n```",
                                    "inline_merge_conflicts": [
                                        {
                                            "hunk_number": 2,
                                            "merge_conflict": "<<<<<<< DOWNSTREAM (version c8d179f3b1c1d60bf4484f50aa67b4c70f91bff9)\n\n=======\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n>>>>>>> UPSTREAM PATCH (commit ea62dd1383913b5999f3d16ae99d411f41b528d4)"
                                        }
                                    ],
                                    "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 401 with fuzz 2 (offset -32 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej"
                                }
                            ]
                        },
                        {
                            "downstream_patch": "246428bfb9e7db15c5cd08e1d0eca41b65af2b06",
                            "downstream_commit": "acec80d9f126cd3fa764bbe3d96bc0cb5cd2b087",
                            "commit_date": "2025-02-01 18:22:26 +0100",
                            "result": "failure",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 424 with fuzz 1 (offset -9 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej",
                            "downstream_patch_content": "commit 246428bfb9e7db15c5cd08e1d0eca41b65af2b06\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:33 2025 +0100\n\n    sctp: sysctl: rto_min/max: avoid using current->nsproxy\n    \n    [ Upstream commit 9fc17b76fc70763780aa78b38fcf4742384044a5 ]\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n    \n    Fixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 8be80096fbb6..82b736843c9d 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -396,7 +396,7 @@ static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -424,7 +424,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            },
                            "error": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 424 with fuzz 1 (offset -9 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej",
                            "total_hunks": 2,
                            "total_failed_hunks": 1,
                            "failed_hunks": [
                                2
                            ],
                            "file_conflicts": [
                                {
                                    "file_name": "net/sctp/sysctl.c",
                                    "rej_file_content": "```diff\n--- net/sctp/sysctl.c\n+++ net/sctp/sysctl.c\n@@ -461,7 +461,7 @@ static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n```",
                                    "inline_merge_conflicts": [
                                        {
                                            "hunk_number": 2,
                                            "merge_conflict": "<<<<<<< DOWNSTREAM (version 246428bfb9e7db15c5cd08e1d0eca41b65af2b06)\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n=======\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n>>>>>>> UPSTREAM PATCH (commit ea62dd1383913b5999f3d16ae99d411f41b528d4)"
                                        }
                                    ],
                                    "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 424 with fuzz 1 (offset -9 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej"
                                }
                            ]
                        }
                    ],
                    "upstream_patch_content": "From 9fc17b76fc70763780aa78b38fcf4742384044a5 Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:33 +0100\nSubject: [PATCH] sctp: sysctl: rto_min/max: avoid using current->nsproxy\n\nAs mentioned in a previous commit of this series, using the 'net'\nstructure via 'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe 'net' structure can be obtained from the table->data using\ncontainer_of().\n\nNote that table->data could also be used directly, as this is the only\nmember needed from the 'net' structure, but that would increase the size\nof this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.\n\nFixes: 4f3fdf3bc59c (\"sctp: add check rto_min and rto_max in sysctl\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/sctp/sysctl.c | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 9848d19630a4..a5285815264d 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -433,7 +433,7 @@ static int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n@@ -461,7 +461,7 @@ static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n \t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n \tunsigned int min = *(unsigned int *) ctl->extra1;\n \tunsigned int max = *(unsigned int *) ctl->extra2;\n \tstruct ctl_table tbl;\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                    }
                }
            ]
        }
    ],
    "cves_with_all_successful_patches": [
        {
            "cve_id": "CVE-2025-21629",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21629.json",
            "patch_attempts": [
                {
                    "upstream_commit": "a8620de72e5676993ec3a3b975f7c10908f5f60f",
                    "upstream_commit_date": "2025-01-02 17:13:44 -0800",
                    "upstream_patch": "68e068cabd2c6c533ef934c2e5151609cf6ecc6d",
                    "total_versions_tested": 3,
                    "successful_patches": 3,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "ac9cfef69565021c9e1022a493a9c40b03e2caf9",
                            "downstream_commit": "32e1e748a85bd52b20b3857d80fd166d22fa455a",
                            "commit_date": "2025-01-09 13:30:02 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/core/dev.c\nHunk #1 succeeded at 3679 (offset 37 lines).",
                            "downstream_patch_content": "commit ac9cfef69565021c9e1022a493a9c40b03e2caf9\nAuthor: Willem de Bruijn <willemb@google.com>\nDate:   Wed Jan 1 11:47:40 2025 -0500\n\n    net: reenable NETIF_F_IPV6_CSUM offload for BIG TCP packets\n    \n    [ Upstream commit 68e068cabd2c6c533ef934c2e5151609cf6ecc6d ]\n    \n    The blamed commit disabled hardware offoad of IPv6 packets with\n    extension headers on devices that advertise NETIF_F_IPV6_CSUM,\n    based on the definition of that feature in skbuff.h:\n    \n     *   * - %NETIF_F_IPV6_CSUM\n     *     - Driver (device) is only able to checksum plain\n     *       TCP or UDP packets over IPv6. These are specifically\n     *       unencapsulated packets of the form IPv6|TCP or\n     *       IPv6|UDP where the Next Header field in the IPv6\n     *       header is either TCP or UDP. IPv6 extension headers\n     *       are not supported with this feature. This feature\n     *       cannot be set in features for a device with\n     *       NETIF_F_HW_CSUM also set. This feature is being\n     *       DEPRECATED (see below).\n    \n    The change causes skb_warn_bad_offload to fire for BIG TCP\n    packets.\n    \n    [  496.310233] WARNING: CPU: 13 PID: 23472 at net/core/dev.c:3129 skb_warn_bad_offload+0xc4/0xe0\n    \n    [  496.310297]  ? skb_warn_bad_offload+0xc4/0xe0\n    [  496.310300]  skb_checksum_help+0x129/0x1f0\n    [  496.310303]  skb_csum_hwoffload_help+0x150/0x1b0\n    [  496.310306]  validate_xmit_skb+0x159/0x270\n    [  496.310309]  validate_xmit_skb_list+0x41/0x70\n    [  496.310312]  sch_direct_xmit+0x5c/0x250\n    [  496.310317]  __qdisc_run+0x388/0x620\n    \n    BIG TCP introduced an IPV6_TLV_JUMBO IPv6 extension header to\n    communicate packet length, as this is an IPv6 jumbogram. But, the\n    feature is only enabled on devices that support BIG TCP TSO. The\n    header is only present for PF_PACKET taps like tcpdump, and not\n    transmitted by physical devices.\n    \n    For this specific case of extension headers that are not\n    transmitted, return to the situation before the blamed commit\n    and support hardware offload.\n    \n    ipv6_has_hopopt_jumbo() tests not only whether this header is present,\n    but also that it is the only extension header before a terminal (L4)\n    header.\n    \n    Fixes: 04c20a9356f2 (\"net: skip offload for NETIF_F_IPV6_CSUM if ipv6 header contains extension\")\n    Reported-by: syzbot <syzkaller@googlegroups.com>\n    Reported-by: Eric Dumazet <edumazet@google.com>\n    Closes: https://lore.kernel.org/netdev/CANn89iK1hdC3Nt8KPhOtTF8vCPc1AHDCtse_BTNki1pWxAByTQ@mail.gmail.com/\n    Signed-off-by: Willem de Bruijn <willemb@google.com>\n    Reviewed-by: Eric Dumazet <edumazet@google.com>\n    Link: https://patch.msgid.link/20250101164909.1331680-1-willemdebruijn.kernel@gmail.com\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/net/core/dev.c b/net/core/dev.c\nindex 2ee1a535b3cb..90559cb66803 100644\n--- a/net/core/dev.c\n+++ b/net/core/dev.c\n@@ -3679,8 +3679,10 @@ int skb_csum_hwoffload_help(struct sk_buff *skb,\n \n \tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n \t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n-\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n+\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr) &&\n+\t\t    !ipv6_has_hopopt_jumbo(skb))\n \t\t\tgoto sw_checksum;\n+\n \t\tswitch (skb->csum_offset) {\n \t\tcase offsetof(struct tcphdr, check):\n \t\tcase offsetof(struct udphdr, check):\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "95ccf006bbc8b59044313b8c309dcf29c546abd4",
                            "downstream_commit": "9eea3703c882876e5713071d51a510fecd3471d5",
                            "commit_date": "2025-01-09 13:32:02 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/core/dev.c\nHunk #1 succeeded at 3628 (offset -14 lines).",
                            "downstream_patch_content": "commit 95ccf006bbc8b59044313b8c309dcf29c546abd4\nAuthor: Willem de Bruijn <willemb@google.com>\nDate:   Wed Jan 1 11:47:40 2025 -0500\n\n    net: reenable NETIF_F_IPV6_CSUM offload for BIG TCP packets\n    \n    [ Upstream commit 68e068cabd2c6c533ef934c2e5151609cf6ecc6d ]\n    \n    The blamed commit disabled hardware offoad of IPv6 packets with\n    extension headers on devices that advertise NETIF_F_IPV6_CSUM,\n    based on the definition of that feature in skbuff.h:\n    \n     *   * - %NETIF_F_IPV6_CSUM\n     *     - Driver (device) is only able to checksum plain\n     *       TCP or UDP packets over IPv6. These are specifically\n     *       unencapsulated packets of the form IPv6|TCP or\n     *       IPv6|UDP where the Next Header field in the IPv6\n     *       header is either TCP or UDP. IPv6 extension headers\n     *       are not supported with this feature. This feature\n     *       cannot be set in features for a device with\n     *       NETIF_F_HW_CSUM also set. This feature is being\n     *       DEPRECATED (see below).\n    \n    The change causes skb_warn_bad_offload to fire for BIG TCP\n    packets.\n    \n    [  496.310233] WARNING: CPU: 13 PID: 23472 at net/core/dev.c:3129 skb_warn_bad_offload+0xc4/0xe0\n    \n    [  496.310297]  ? skb_warn_bad_offload+0xc4/0xe0\n    [  496.310300]  skb_checksum_help+0x129/0x1f0\n    [  496.310303]  skb_csum_hwoffload_help+0x150/0x1b0\n    [  496.310306]  validate_xmit_skb+0x159/0x270\n    [  496.310309]  validate_xmit_skb_list+0x41/0x70\n    [  496.310312]  sch_direct_xmit+0x5c/0x250\n    [  496.310317]  __qdisc_run+0x388/0x620\n    \n    BIG TCP introduced an IPV6_TLV_JUMBO IPv6 extension header to\n    communicate packet length, as this is an IPv6 jumbogram. But, the\n    feature is only enabled on devices that support BIG TCP TSO. The\n    header is only present for PF_PACKET taps like tcpdump, and not\n    transmitted by physical devices.\n    \n    For this specific case of extension headers that are not\n    transmitted, return to the situation before the blamed commit\n    and support hardware offload.\n    \n    ipv6_has_hopopt_jumbo() tests not only whether this header is present,\n    but also that it is the only extension header before a terminal (L4)\n    header.\n    \n    Fixes: 04c20a9356f2 (\"net: skip offload for NETIF_F_IPV6_CSUM if ipv6 header contains extension\")\n    Reported-by: syzbot <syzkaller@googlegroups.com>\n    Reported-by: Eric Dumazet <edumazet@google.com>\n    Closes: https://lore.kernel.org/netdev/CANn89iK1hdC3Nt8KPhOtTF8vCPc1AHDCtse_BTNki1pWxAByTQ@mail.gmail.com/\n    Signed-off-by: Willem de Bruijn <willemb@google.com>\n    Reviewed-by: Eric Dumazet <edumazet@google.com>\n    Link: https://patch.msgid.link/20250101164909.1331680-1-willemdebruijn.kernel@gmail.com\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/net/core/dev.c b/net/core/dev.c\nindex 4beb9acf2c18..69da7b009f8b 100644\n--- a/net/core/dev.c\n+++ b/net/core/dev.c\n@@ -3628,8 +3628,10 @@ int skb_csum_hwoffload_help(struct sk_buff *skb,\n \n \tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n \t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n-\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n+\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr) &&\n+\t\t    !ipv6_has_hopopt_jumbo(skb))\n \t\t\tgoto sw_checksum;\n+\n \t\tswitch (skb->csum_offset) {\n \t\tcase offsetof(struct tcphdr, check):\n \t\tcase offsetof(struct udphdr, check):\n",
                            "downstream_file_content": {
                                "net/core/dev.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *      NET3    Protocol independent device support routines.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitmap.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/skbuff.h>\n#include <linux/kthread.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dsa.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/gro.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <net/tcx.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <trace/events/qdisc.h>\n#include <trace/events/xdp.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_netdev.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n#include <linux/net_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/devlink.h>\n#include <linux/pm_runtime.h>\n#include <linux/prandom.h>\n#include <linux/once_lite.h>\n#include <net/netdev_rx_queue.h>\n\n#include \"dev.h\"\n#include \"net-sysfs.h\"\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic DEFINE_MUTEX(ifalias_mutex);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic DECLARE_RWSEM(devnet_rename_sem);\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock_irqsave(struct softnet_data *sd,\n\t\t\t\t    unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_lock_irqsave(&sd->input_pkt_queue.lock, *flags);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_save(*flags);\n}\n\nstatic inline void rps_lock_irq_disable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_lock_irq(&sd->input_pkt_queue.lock);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_disable();\n}\n\nstatic inline void rps_unlock_irq_restore(struct softnet_data *sd,\n\t\t\t\t\t  unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_unlock_irqrestore(&sd->input_pkt_queue.lock, *flags);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_restore(*flags);\n}\n\nstatic inline void rps_unlock_irq_enable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_unlock_irq(&sd->input_pkt_queue.lock);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_enable();\n}\n\nstatic struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,\n\t\t\t\t\t\t       const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = kmalloc(sizeof(*name_node), GFP_KERNEL);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_HLIST_NODE(&name_node->hlist);\n\tname_node->dev = dev;\n\tname_node->name = name;\n\treturn name_node;\n}\n\nstatic struct netdev_name_node *\nnetdev_name_node_head_alloc(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = netdev_name_node_alloc(dev, dev->name);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&name_node->list);\n\treturn name_node;\n}\n\nstatic void netdev_name_node_free(struct netdev_name_node *name_node)\n{\n\tkfree(name_node);\n}\n\nstatic void netdev_name_node_add(struct net *net,\n\t\t\t\t struct netdev_name_node *name_node)\n{\n\thlist_add_head_rcu(&name_node->hlist,\n\t\t\t   dev_name_hash(net, name_node->name));\n}\n\nstatic void netdev_name_node_del(struct netdev_name_node *name_node)\n{\n\thlist_del_rcu(&name_node->hlist);\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup(struct net *net,\n\t\t\t\t\t\t\tconst char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,\n\t\t\t\t\t\t\t    const char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry_rcu(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nbool netdev_name_in_use(struct net *net, const char *name)\n{\n\treturn netdev_name_node_lookup(net, name);\n}\nEXPORT_SYMBOL(netdev_name_in_use);\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (name_node)\n\t\treturn -EEXIST;\n\tname_node = netdev_name_node_alloc(dev, name);\n\tif (!name_node)\n\t\treturn -ENOMEM;\n\tnetdev_name_node_add(net, name_node);\n\t/* The node that holds dev->name acts as a head of per-device list. */\n\tlist_add_tail(&name_node->list, &dev->name_node->list);\n\n\treturn 0;\n}\n\nstatic void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)\n{\n\tlist_del(&name_node->list);\n\tkfree(name_node->name);\n\tnetdev_name_node_free(name_node);\n}\n\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (!name_node)\n\t\treturn -ENOENT;\n\t/* lookup might have found our primary name or a name belonging\n\t * to another device.\n\t */\n\tif (name_node == dev->name_node || name_node->dev != dev)\n\t\treturn -EINVAL;\n\n\tnetdev_name_node_del(name_node);\n\tsynchronize_rcu();\n\t__netdev_name_node_alt_destroy(name_node);\n\n\treturn 0;\n}\n\nstatic void netdev_name_node_alt_flush(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\n\tlist_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list)\n\t\t__netdev_name_node_alt_destroy(name_node);\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\tnetdev_name_node_add(net, dev->name_node);\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock(&dev_base_lock);\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_add(net, name_node);\n\n\t/* We reserved the ifindex, this can't fail */\n\tWARN_ON(xa_store(&net->dev_by_index, dev->ifindex, dev, GFP_KERNEL));\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev, bool lock)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\txa_erase(&net->dev_by_index, dev->ifindex);\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_del(name_node);\n\n\t/* Unlink dev from the device chain */\n\tif (lock)\n\t\twrite_lock(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\tnetdev_name_node_del(dev->name_node);\n\thlist_del_rcu(&dev->index_hlist);\n\tif (lock)\n\t\twrite_unlock(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\nstatic struct net_device_path *dev_fwd_path(struct net_device_path_stack *stack)\n{\n\tint k = stack->num_paths++;\n\n\tif (WARN_ON_ONCE(k >= NET_DEVICE_PATH_STACK_MAX))\n\t\treturn NULL;\n\n\treturn &stack->path[k];\n}\n\nint dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,\n\t\t\t  struct net_device_path_stack *stack)\n{\n\tconst struct net_device *last_dev;\n\tstruct net_device_path_ctx ctx = {\n\t\t.dev\t= dev,\n\t};\n\tstruct net_device_path *path;\n\tint ret = 0;\n\n\tmemcpy(ctx.daddr, daddr, sizeof(ctx.daddr));\n\tstack->num_paths = 0;\n\twhile (ctx.dev && ctx.dev->netdev_ops->ndo_fill_forward_path) {\n\t\tlast_dev = ctx.dev;\n\t\tpath = dev_fwd_path(stack);\n\t\tif (!path)\n\t\t\treturn -1;\n\n\t\tmemset(path, 0, sizeof(struct net_device_path));\n\t\tret = ctx.dev->netdev_ops->ndo_fill_forward_path(&ctx, path);\n\t\tif (ret < 0)\n\t\t\treturn -1;\n\n\t\tif (WARN_ON_ONCE(last_dev == ctx.dev))\n\t\t\treturn -1;\n\t}\n\n\tif (!ctx.dev)\n\t\treturn ret;\n\n\tpath = dev_fwd_path(stack);\n\tif (!path)\n\t\treturn -1;\n\tpath->type = DEV_PATH_ETHERNET;\n\tpath->dev = ctx.dev;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_fill_forward_path);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup_rcu(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/* Deprecated for new users, call netdev_get_by_name() instead */\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\tnetdev_get_by_name() - find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\t@tracker: tracking object for the acquired reference\n *\t@gfp: allocation flags for the tracker\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use netdev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\nstruct net_device *netdev_get_by_name(struct net *net, const char *name,\n\t\t\t\t      netdevice_tracker *tracker, gfp_t gfp)\n{\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_name(net, name);\n\tif (dev)\n\t\tnetdev_tracker_alloc(dev, tracker, gfp);\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n/* Deprecated for new users, call netdev_get_by_index() instead */\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tnetdev_get_by_index() - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\t@tracker: tracking object for the acquired reference\n *\t@gfp: allocation flags for the tracker\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tnetdev_put() to indicate they have finished with it.\n */\nstruct net_device *netdev_get_by_index(struct net *net, int ifindex,\n\t\t\t\t       netdevice_tracker *tracker, gfp_t gfp)\n{\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_index(net, ifindex);\n\tif (dev)\n\t\tnetdev_tracker_alloc(dev, tracker, gfp);\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tint ret;\n\n\tdown_read(&devnet_rename_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tstrcpy(name, dev->name);\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\tup_read(&devnet_rename_sem);\n\treturn ret;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tallow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strnlen(name, IFNAMSIZ) == IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tp = strchr(name, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = bitmap_zalloc(max_netdevices, GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tstruct netdev_name_node *name_node;\n\n\t\t\tnetdev_for_each_altname(d, name_node) {\n\t\t\t\tif (!sscanf(name_node->name, name, &i))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\t\tif (!strncmp(buf, name_node->name, IFNAMSIZ))\n\t\t\t\t\t__set_bit(i, inuse);\n\t\t\t}\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\t__set_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tbitmap_free(inuse);\n\t}\n\n\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!netdev_name_in_use(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\nstatic int dev_prep_valid_name(struct net *net, struct net_device *dev,\n\t\t\t       const char *want_name, char *out_name)\n{\n\tint ret;\n\n\tif (!dev_valid_name(want_name))\n\t\treturn -EINVAL;\n\n\tif (strchr(want_name, '%')) {\n\t\tret = __dev_alloc_name(net, want_name, out_name);\n\t\treturn ret < 0 ? ret : 0;\n\t} else if (netdev_name_in_use(net, want_name)) {\n\t\treturn -EEXIST;\n\t} else if (out_name != want_name) {\n\t\tstrscpy(out_name, want_name, IFNAMSIZ);\n\t}\n\n\treturn 0;\n}\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tBUG_ON(!net);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrscpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\treturn dev_alloc_name_ns(dev_net(dev), dev, name);\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = dev_prep_valid_name(net, dev, name, buf);\n\tif (ret >= 0)\n\t\tstrscpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\n\tdown_write(&devnet_rename_sem);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s%s\\n\", oldname,\n\t\t\t    dev->flags & IFF_UP ? \" (while UP)\" : \"\");\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\tup_write(&devnet_rename_sem);\n\t\treturn ret;\n\t}\n\n\tup_write(&devnet_rename_sem);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock(&dev_base_lock);\n\tnetdev_name_node_del(dev->name_node);\n\twrite_unlock(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock(&dev_base_lock);\n\tnetdev_name_node_add(net, dev->name_node);\n\twrite_unlock(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tdown_write(&devnet_rename_sem);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tnetdev_err(dev, \"name change rollback failed: %d\\n\",\n\t\t\t\t   ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tstruct dev_ifalias *new_alias = NULL;\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (len) {\n\t\tnew_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);\n\t\tif (!new_alias)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(new_alias->ifalias, alias, len);\n\t\tnew_alias->ifalias[len] = 0;\n\t}\n\n\tmutex_lock(&ifalias_mutex);\n\tnew_alias = rcu_replace_pointer(dev->ifalias, new_alias,\n\t\t\t\t\tmutex_is_locked(&ifalias_mutex));\n\tmutex_unlock(&ifalias_mutex);\n\n\tif (new_alias)\n\t\tkfree_rcu(new_alias, rcuhead);\n\n\treturn len;\n}\nEXPORT_SYMBOL(dev_set_alias);\n\n/**\n *\tdev_get_alias - get ifalias of a device\n *\t@dev: device\n *\t@name: buffer to store name of ifalias\n *\t@len: size of buffer\n *\n *\tget ifalias for a device.  Caller must make sure dev cannot go\n *\taway,  e.g. rcu read lock or own a reference count to device.\n */\nint dev_get_alias(const struct net_device *dev, char *name, size_t len)\n{\n\tconst struct dev_ifalias *alias;\n\tint ret = 0;\n\n\trcu_read_lock();\n\talias = rcu_dereference(dev->ifalias);\n\tif (alias)\n\t\tret = snprintf(name, len, \"%s\", alias->ifalias);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info.dev = dev,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL, 0, NULL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * __netdev_notify_peers - notify network peers about existence of @dev,\n * to be called when rtnl lock is already held.\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid __netdev_notify_peers(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n}\nEXPORT_SYMBOL(__netdev_notify_peers);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\t__netdev_notify_peers(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int napi_threaded_poll(void *data);\n\nstatic int napi_kthread_create(struct napi_struct *n)\n{\n\tint err = 0;\n\n\t/* Create and wake up the kthread once to put it in\n\t * TASK_INTERRUPTIBLE mode to avoid the blocked task\n\t * warning and work with loadavg.\n\t */\n\tn->thread = kthread_run(napi_threaded_poll, n, \"napi/%s-%d\",\n\t\t\t\tn->dev->name, n->napi_id);\n\tif (IS_ERR(n->thread)) {\n\t\terr = PTR_ERR(n->thread);\n\t\tpr_err(\"kthread_run failed with err %d\\n\", err);\n\t\tn->thread = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\tdev_addr_check(dev);\n\n\tif (!netif_device_present(dev)) {\n\t\t/* may be detached because parent is runtime-suspended */\n\t\tif (dev->dev.parent)\n\t\t\tpm_runtime_resume(dev->dev.parent);\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t}\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev: device to open\n *\t@extack: netlink extended ack\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n/**\n *\tdev_disable_gro_hw - disable HW Generic Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable HW Generic Receive Offload (GRO_HW) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if Generic XDP is installed on\n *\tthe device.\n */\nstatic void dev_disable_gro_hw(struct net_device *dev)\n{\n\tdev->wanted_features &= ~NETIF_F_GRO_HW;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_GRO_HW))\n\t\tnetdev_WARN(dev, \"failed to disable GRO_HW!\\n\");\n}\n\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd)\n{\n#define N(val) \t\t\t\t\t\t\\\n\tcase NETDEV_##val:\t\t\t\t\\\n\t\treturn \"NETDEV_\" __stringify(val);\n\tswitch (cmd) {\n\tN(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)\n\tN(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)\n\tN(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)\n\tN(POST_INIT) N(PRE_UNINIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN)\n\tN(CHANGEUPPER) N(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA)\n\tN(BONDING_INFO) N(PRECHANGEUPPER) N(CHANGELOWERSTATE)\n\tN(UDP_TUNNEL_PUSH_INFO) N(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)\n\tN(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)\n\tN(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)\n\tN(PRE_CHANGEADDR) N(OFFLOAD_XSTATS_ENABLE) N(OFFLOAD_XSTATS_DISABLE)\n\tN(OFFLOAD_XSTATS_REPORT_USED) N(OFFLOAD_XSTATS_REPORT_DELTA)\n\tN(XDP_FEAT_CHANGE)\n\t}\n#undef N\n\treturn \"UNKNOWN_NETDEV_EVENT\";\n}\nEXPORT_SYMBOL_GPL(netdev_cmd_to_name);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t};\n\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int call_netdevice_register_notifiers(struct notifier_block *nb,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tint err;\n\n\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\treturn 0;\n}\n\nstatic void call_netdevice_unregister_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\tstruct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\tdev);\n\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t}\n\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n}\n\nstatic int call_netdevice_register_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t struct net *net)\n{\n\tstruct net_device *dev;\n\tint err;\n\n\tfor_each_netdev(net, dev) {\n\t\terr = call_netdevice_register_notifiers(nb, dev);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\treturn 0;\n\nrollback:\n\tfor_each_netdev_continue_reverse(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n\treturn err;\n}\n\nstatic void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t    struct net *net)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\terr = call_netdevice_register_net_notifiers(nb, net);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n\nrollback:\n\tfor_each_net_continue_reverse(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\nstatic int __register_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t     struct notifier_block *nb,\n\t\t\t\t\t     bool ignore_call_fail)\n{\n\tint err;\n\n\terr = raw_notifier_chain_register(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\tif (dev_boot_phase)\n\t\treturn 0;\n\n\terr = call_netdevice_register_net_notifiers(nb, net);\n\tif (err && !ignore_call_fail)\n\t\tgoto chain_unregister;\n\n\treturn 0;\n\nchain_unregister:\n\traw_notifier_chain_unregister(&net->netdev_chain, nb);\n\treturn err;\n}\n\nstatic int __unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t       struct notifier_block *nb)\n{\n\tint err;\n\n\terr = raw_notifier_chain_unregister(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\n\tcall_netdevice_unregister_net_notifiers(nb, net);\n\treturn 0;\n}\n\n/**\n * register_netdevice_notifier_net - register a per-netns network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(net, nb, false);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_net);\n\n/**\n * unregister_netdevice_notifier_net - unregister a per-netns\n *                                     network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier_net(). The notifier is unlinked from the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __unregister_netdevice_notifier_net(net, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_net);\n\nstatic void __move_netdevice_notifier_net(struct net *src_net,\n\t\t\t\t\t  struct net *dst_net,\n\t\t\t\t\t  struct notifier_block *nb)\n{\n\t__unregister_netdevice_notifier_net(src_net, nb);\n\t__register_netdevice_notifier_net(dst_net, nb, true);\n}\n\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(dev_net(dev), nb, false);\n\tif (!err) {\n\t\tnn->nb = nb;\n\t\tlist_add(&nn->list, &dev->net_notifier_list);\n\t}\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_dev_net);\n\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\tlist_del(&nn->list);\n\terr = __unregister_netdevice_notifier_net(dev_net(dev), nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);\n\nstatic void move_netdevice_notifiers_dev_net(struct net_device *dev,\n\t\t\t\t\t     struct net *net)\n{\n\tstruct netdev_net_notifier *nn;\n\n\tlist_for_each_entry(nn, &dev->net_notifier_list, list)\n\t\t__move_netdevice_notifier_net(dev_net(dev), net, nn->nb);\n}\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t  struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/* Run per-netns notifier block chain first, then run the global one.\n\t * Hopefully, one day, the global one is going to be removed after\n\t * all notifier block registrators get converted to be per-netns.\n\t */\n\tret = raw_notifier_call_chain(&net->netdev_chain, val, info);\n\tif (ret & NOTIFY_STOP_MASK)\n\t\treturn ret;\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers_info_robust - call per-netns notifier blocks\n *\t                                       for and rollback on error\n *\t@val_up: value passed unmodified to notifier function\n *\t@val_down: value passed unmodified to the notifier function when\n *\t           recovering from an error on @val_up\n *\t@info: notifier information data\n *\n *\tCall all per-netns network notifier blocks, but not notifier blocks on\n *\tthe global notifier chain. Parameters and return value are as for\n *\traw_notifier_call_chain_robust().\n */\n\nstatic int\ncall_netdevice_notifiers_info_robust(unsigned long val_up,\n\t\t\t\t     unsigned long val_down,\n\t\t\t\t     struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\n\tASSERT_RTNL();\n\n\treturn raw_notifier_call_chain_robust(&net->netdev_chain,\n\t\t\t\t\t      val_up, val_down, info);\n}\n\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t\t.extack = extack,\n\t};\n\n\treturn call_netdevice_notifiers_info(val, &info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\treturn call_netdevice_notifiers_extack(val, dev, NULL);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n/**\n *\tcall_netdevice_notifiers_mtu - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@arg: additional u32 argument passed to the notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\nstatic int call_netdevice_notifiers_mtu(unsigned long val,\n\t\t\t\t\tstruct net_device *dev, u32 arg)\n{\n\tstruct netdev_notifier_info_ext info = {\n\t\t.info.dev = dev,\n\t\t.ext.mtu = arg,\n\t};\n\n\tBUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);\n\n\treturn call_netdevice_notifiers_info(val, &info.info);\n}\n\n#ifdef CONFIG_NET_INGRESS\nstatic DEFINE_STATIC_KEY_FALSE(ingress_needed_key);\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_branch_inc(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_branch_dec(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic DEFINE_STATIC_KEY_FALSE(egress_needed_key);\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_branch_inc(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_branch_dec(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nDEFINE_STATIC_KEY_FALSE(netstamp_needed_key);\nEXPORT_SYMBOL(netstamp_needed_key);\n#ifdef CONFIG_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_branch_enable(&netstamp_needed_key);\n\telse\n\t\tstatic_branch_disable(&netstamp_needed_key);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted = atomic_read(&netstamp_wanted);\n\n\twhile (wanted > 0) {\n\t\tif (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted + 1))\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_inc(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted = atomic_read(&netstamp_wanted);\n\n\twhile (wanted > 1) {\n\t\tif (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted - 1))\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_dec(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tskb->mono_delivery_time = 0;\n\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\tskb->tstamp = ktime_get_real();\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\t\\\n\tif (static_branch_unlikely(&netstamp_needed_key)) {\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\t\t\\\n\t\t\t(SKB)->tstamp = ktime_get_real();\t\\\n\t}\t\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\treturn __is_skb_forwardable(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nstatic int __dev_forward_skb2(struct net_device *dev, struct sk_buff *skb,\n\t\t\t      bool check_mtu)\n{\n\tint ret = ____dev_forward_skb(dev, skb, check_mtu);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, false) ?: netif_rx_internal(skb);\n}\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * dev_nit_active - return true if any network interface taps are in use\n *\n * @dev: network device to check for the presence of taps\n */\nbool dev_nit_active(struct net_device *dev)\n{\n\treturn !list_empty(&ptype_all) || !list_empty(&dev->ptype_all);\n}\nEXPORT_SYMBOL_GPL(dev_nit_active);\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (READ_ONCE(ptype->ignore_outgoing))\n\t\t\tcontinue;\n\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tnetdev_warn(dev, \"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tnetdev_warn(dev, \"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\t    i, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\t/* walk through the TCs and see if it falls into any of them */\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\t/* didn't find it, just return -1 to indicate no match */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_txq_to_tc);\n\n#ifdef CONFIG_XPS\nstatic struct static_key xps_needed __read_mostly;\nstatic struct static_key xps_rxqs_needed __read_mostly;\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     struct xps_dev_maps *old_maps, int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tif (old_maps)\n\t\t\tRCU_INIT_POINTER(old_maps->attr_map[tci], NULL);\n\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev_maps->num_tc;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, NULL, tci, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void reset_xps_maps(struct net_device *dev,\n\t\t\t   struct xps_dev_maps *dev_maps,\n\t\t\t   enum xps_map_type type)\n{\n\tstatic_key_slow_dec_cpuslocked(&xps_needed);\n\tif (type == XPS_RXQS)\n\t\tstatic_key_slow_dec_cpuslocked(&xps_rxqs_needed);\n\n\tRCU_INIT_POINTER(dev->xps_maps[type], NULL);\n\n\tkfree_rcu(dev_maps, rcu);\n}\n\nstatic void clean_xps_maps(struct net_device *dev, enum xps_map_type type,\n\t\t\t   u16 offset, u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tbool active = false;\n\tint i, j;\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (!dev_maps)\n\t\treturn;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, j, offset, count);\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\n\tif (type == XPS_CPUS) {\n\t\tfor (i = offset + (count - 1); count--; i--)\n\t\t\tnetdev_queue_numa_node_write(\n\t\t\t\tnetdev_get_tx_queue(dev, i), NUMA_NO_NODE);\n\t}\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tif (!static_key_false(&xps_needed))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&xps_map_mutex);\n\n\tif (static_key_false(&xps_rxqs_needed))\n\t\tclean_xps_maps(dev, XPS_RXQS, offset, count);\n\n\tclean_xps_maps(dev, XPS_CPUS, offset, count);\n\n\tmutex_unlock(&xps_map_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,\n\t\t\t\t      u16 index, bool is_rxqs_map)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add tx-queue to this CPU's/rx-queue's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store tx-queue on this CPU's/rx-queue's\n\t *  map\n\t */\n\tif (is_rxqs_map)\n\t\tnew_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);\n\telse\n\t\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t\t       cpu_to_node(attr_index));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\n/* Copy xps maps at a given index */\nstatic void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,\n\t\t\t      struct xps_dev_maps *new_dev_maps, int index,\n\t\t\t      int tc, bool skip_tc)\n{\n\tint i, tci = index * dev_maps->num_tc;\n\tstruct xps_map *map;\n\n\t/* copy maps belonging to foreign traffic classes */\n\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\tif (i == tc && skip_tc)\n\t\t\tcontinue;\n\n\t\t/* fill in the new device map from the old device map */\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n}\n\n/* Must be called under cpus_read_lock */\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL, *old_dev_maps = NULL;\n\tconst unsigned long *online_mask = NULL;\n\tbool active = false, copy = false;\n\tint i, j, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tunsigned int nr_ids;\n\n\tWARN_ON_ONCE(index >= dev->num_tx_queues);\n\n\tif (dev->num_tc) {\n\t\t/* Do not allow XPS on subordinate device directly */\n\t\tnum_tc = dev->num_tc;\n\t\tif (num_tc < 0)\n\t\t\treturn -EINVAL;\n\n\t\t/* If queue belongs to subordinate dev use its map */\n\t\tdev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;\n\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (type == XPS_RXQS) {\n\t\tmaps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);\n\t\tnr_ids = dev->num_rx_queues;\n\t} else {\n\t\tmaps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);\n\t\tif (num_possible_cpus() > 1)\n\t\t\tonline_mask = cpumask_bits(cpu_online_mask);\n\t\tnr_ids = nr_cpu_ids;\n\t}\n\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\t/* The old dev_maps could be larger or smaller than the one we're\n\t * setting up now, as dev->num_tc or nr_ids could have been updated in\n\t * between. We could try to be smart, but let's be safe instead and only\n\t * copy foreign traffic classes if the two map sizes match.\n\t */\n\tif (dev_maps &&\n\t    dev_maps->num_tc == num_tc && dev_maps->nr_ids == nr_ids)\n\t\tcopy = true;\n\n\t/* allocate memory for queue storage */\n\tfor (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tif (!new_dev_maps) {\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\t\tif (!new_dev_maps) {\n\t\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tnew_dev_maps->nr_ids = nr_ids;\n\t\t\tnew_dev_maps->num_tc = num_tc;\n\t\t}\n\n\t\ttci = j * num_tc + tc;\n\t\tmap = copy ? xmap_dereference(dev_maps->attr_map[tci]) : NULL;\n\n\t\tmap = expand_xps_map(map, j, index, type == XPS_RXQS);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tif (!dev_maps) {\n\t\t/* Increment static keys at most once per type */\n\t\tstatic_key_slow_inc_cpuslocked(&xps_needed);\n\t\tif (type == XPS_RXQS)\n\t\t\tstatic_key_slow_inc_cpuslocked(&xps_rxqs_needed);\n\t}\n\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tbool skip_tc = false;\n\n\t\ttci = j * num_tc + tc;\n\t\tif (netif_attr_test_mask(j, mask, nr_ids) &&\n\t\t    netif_attr_test_online(j, online_mask, nr_ids)) {\n\t\t\t/* add tx-queue to CPU/rx-queue maps */\n\t\t\tint pos = 0;\n\n\t\t\tskip_tc = true;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (type == XPS_CPUS) {\n\t\t\t\tif (numa_node_id == -2)\n\t\t\t\t\tnuma_node_id = cpu_to_node(j);\n\t\t\t\telse if (numa_node_id != cpu_to_node(j))\n\t\t\t\t\tnuma_node_id = -1;\n\t\t\t}\n#endif\n\t\t}\n\n\t\tif (copy)\n\t\t\txps_copy_dev_maps(dev_maps, new_dev_maps, j, tc,\n\t\t\t\t\t  skip_tc);\n\t}\n\n\trcu_assign_pointer(dev->xps_maps[type], new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * dev_maps->num_tc; i--; tci++) {\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tif (!map)\n\t\t\t\tcontinue;\n\n\t\t\tif (copy) {\n\t\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\t\tif (map == new_map)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\told_dev_maps = dev_maps;\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\tif (type == XPS_CPUS)\n\t\t/* update Tx queue numa node */\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t\t     (numa_node_id >= 0) ?\n\t\t\t\t\t     numa_node_id : NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes tx-queue from unused CPUs/rx-queues */\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\ttci = j * dev_maps->num_tc;\n\n\t\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\t\tif (i == tc &&\n\t\t\t    netif_attr_test_mask(j, mask, dev_maps->nr_ids) &&\n\t\t\t    netif_attr_test_online(j, online_mask, dev_maps->nr_ids))\n\t\t\t\tcontinue;\n\n\t\t\tactive |= remove_xps_queue(dev_maps,\n\t\t\t\t\t\t   copy ? old_dev_maps : NULL,\n\t\t\t\t\t\t   tci, index);\n\t\t}\n\t}\n\n\tif (old_dev_maps)\n\t\tkfree_rcu(old_dev_maps, rcu);\n\n\t/* free map if not active */\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = copy ?\n\t\t\t      xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(__netif_set_xps_queue);\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nstatic void netdev_unbind_all_sb_channels(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n\t/* Unbind any subordinate channels */\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev)\n\t\t\tnetdev_unbind_sb_channel(dev, txq->sb_dev);\n\t}\n}\n\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\t/* Reset TC configuration of device */\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(sb_dev, 0);\n#endif\n\tmemset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));\n\tmemset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));\n\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev == sb_dev)\n\t\t\ttxq->sb_dev = NULL;\n\t}\n}\nEXPORT_SYMBOL(netdev_unbind_sb_channel);\n\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset)\n{\n\t/* Make certain the sb_dev and dev are already configured */\n\tif (sb_dev->num_tc >= 0 || tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\t/* We cannot hand out queues we don't have */\n\tif ((offset + count) > dev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\t/* Record the mapping */\n\tsb_dev->tc_to_txq[tc].count = count;\n\tsb_dev->tc_to_txq[tc].offset = offset;\n\n\t/* Provide a way for Tx queue to find the tc_to_txq map or\n\t * XPS map for itself.\n\t */\n\twhile (count--)\n\t\tnetdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_bind_sb_channel_queue);\n\nint netdev_set_sb_channel(struct net_device *dev, u16 channel)\n{\n\t/* Do not use a multiqueue device to represent a subordinate channel */\n\tif (netif_is_multiqueue(dev))\n\t\treturn -ENODEV;\n\n\t/* We allow channels 1 - 32767 to be used for subordinate channels.\n\t * Channel 0 is meant to be \"native\" mode and used only to represent\n\t * the main root device. We allow writing 0 to reset the device back\n\t * to normal mode after being used as a subordinate channel.\n\t */\n\tif (channel > S16_MAX)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = -channel;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_sb_channel);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater than real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tbool disabling;\n\tint rc;\n\n\tdisabling = txq < dev->real_num_tx_queues;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tdev_qdisc_change_real_num_tx(dev, txq);\n\n\t\tdev->real_num_tx_queues = txq;\n\n\t\tif (disabling) {\n\t\t\tsynchronize_net();\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t} else {\n\t\tdev->real_num_tx_queues = txq;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n *\tnetif_set_real_num_queues - set actual number of RX and TX queues used\n *\t@dev: Network device\n *\t@txq: Actual number of TX queues\n *\t@rxq: Actual number of RX queues\n *\n *\tSet the real number of both TX and RX queues.\n *\tDoes nothing if the number of queues is already correct.\n */\nint netif_set_real_num_queues(struct net_device *dev,\n\t\t\t      unsigned int txq, unsigned int rxq)\n{\n\tunsigned int old_rxq = dev->real_num_rx_queues;\n\tint err;\n\n\tif (txq < 1 || txq > dev->num_tx_queues ||\n\t    rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\t/* Start from increases, so the error path only does decreases -\n\t * decreases can't fail.\n\t */\n\tif (rxq > dev->real_num_rx_queues) {\n\t\terr = netif_set_real_num_rx_queues(dev, rxq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (txq > dev->real_num_tx_queues) {\n\t\terr = netif_set_real_num_tx_queues(dev, txq);\n\t\tif (err)\n\t\t\tgoto undo_rx;\n\t}\n\tif (rxq < dev->real_num_rx_queues)\n\t\tWARN_ON(netif_set_real_num_rx_queues(dev, rxq));\n\tif (txq < dev->real_num_tx_queues)\n\t\tWARN_ON(netif_set_real_num_tx_queues(dev, txq));\n\n\treturn 0;\nundo_rx:\n\tWARN_ON(netif_set_real_num_rx_queues(dev, old_rxq));\n\treturn err;\n}\nEXPORT_SYMBOL(netif_set_real_num_queues);\n\n/**\n * netif_set_tso_max_size() - set the max size of TSO frames supported\n * @dev:\tnetdev to update\n * @size:\tmax skb->len of a TSO frame\n *\n * Set the limit on the size of TSO super-frames the device can handle.\n * Unless explicitly set the stack will assume the value of\n * %GSO_LEGACY_MAX_SIZE.\n */\nvoid netif_set_tso_max_size(struct net_device *dev, unsigned int size)\n{\n\tdev->tso_max_size = min(GSO_MAX_SIZE, size);\n\tif (size < READ_ONCE(dev->gso_max_size))\n\t\tnetif_set_gso_max_size(dev, size);\n\tif (size < READ_ONCE(dev->gso_ipv4_max_size))\n\t\tnetif_set_gso_ipv4_max_size(dev, size);\n}\nEXPORT_SYMBOL(netif_set_tso_max_size);\n\n/**\n * netif_set_tso_max_segs() - set the max number of segs supported for TSO\n * @dev:\tnetdev to update\n * @segs:\tmax number of TCP segments\n *\n * Set the limit on the number of TCP segments the device can generate from\n * a single TSO super-frame.\n * Unless explicitly set the stack will assume the value of %GSO_MAX_SEGS.\n */\nvoid netif_set_tso_max_segs(struct net_device *dev, unsigned int segs)\n{\n\tdev->tso_max_segs = segs;\n\tif (segs < READ_ONCE(dev->gso_max_segs))\n\t\tnetif_set_gso_max_segs(dev, segs);\n}\nEXPORT_SYMBOL(netif_set_tso_max_segs);\n\n/**\n * netif_inherit_tso_max() - copy all TSO limits from a lower device to an upper\n * @to:\t\tnetdev to update\n * @from:\tnetdev from which to copy the limits\n */\nvoid netif_inherit_tso_max(struct net_device *to, const struct net_device *from)\n{\n\tnetif_set_tso_max_size(to, from->tso_max_size);\n\tnetif_set_tso_max_segs(to, from->tso_max_segs);\n}\nEXPORT_SYMBOL(netif_inherit_tso_max);\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * Default value is the number of physical cores if there are only 1 or 2, or\n * divided by 2 if there are more.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\tcpumask_var_t cpus;\n\tint cpu, count = 0;\n\n\tif (unlikely(is_kdump_kernel() || !zalloc_cpumask_var(&cpus, GFP_KERNEL)))\n\t\treturn 1;\n\n\tcpumask_copy(cpus, cpu_online_mask);\n\tfor_each_cpu(cpu, cpus) {\n\t\t++count;\n\t\tcpumask_andnot(cpus, cpus, topology_sibling_cpumask(cpu));\n\t}\n\tfree_cpumask_var(cpus);\n\n\treturn count > 2 ? DIV_ROUND_UP(count, 2) : count;\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_drop_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!netif_xmit_stopped(txq)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid dev_kfree_skb_irq_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(dev_kfree_skb_irq_reason);\n\nvoid dev_kfree_skb_any_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tif (in_hardirq() || irqs_disabled())\n\t\tdev_kfree_skb_irq_reason(skb, reason);\n\telse\n\t\tkfree_skb_reason(skb, reason);\n}\nEXPORT_SYMBOL(dev_kfree_skb_any_reason);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nstatic u16 skb_tx_hash(const struct net_device *dev,\n\t\t       const struct net_device *sb_dev,\n\t\t       struct sk_buff *skb)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = dev->real_num_tx_queues;\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = sb_dev->tc_to_txq[tc].offset;\n\t\tqcount = sb_dev->tc_to_txq[tc].count;\n\t\tif (unlikely(!qcount)) {\n\t\t\tnet_warn_ratelimited(\"%s: invalid qcount, qoffset %u for tc %u\\n\",\n\t\t\t\t\t     sb_dev->name, qoffset, tc);\n\t\t\tqoffset = 0;\n\t\t\tqcount = dev->real_num_tx_queues;\n\t\t}\n\t}\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tDEBUG_NET_WARN_ON_ONCE(qcount == 0);\n\t\thash = skb_get_rx_queue(skb);\n\t\tif (hash >= qoffset)\n\t\t\thash -= qoffset;\n\t\twhile (unlikely(hash >= qcount))\n\t\t\thash -= qcount;\n\t\treturn hash + qoffset;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\n\nvoid skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tskb_dump(KERN_WARNING, skb, false);\n\tWARN(1, \"%s: caps=(%pNF, %pNF)\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_is_gso(skb))) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tret = -EINVAL;\n\tif (unlikely(offset >= skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset (%d) >= skb_headlen() (%u)\\n\",\n\t\t\t  offset, skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tif (unlikely(offset + sizeof(__sum16) > skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset+2 (%zu) > skb_headlen() (%u)\\n\",\n\t\t\t  offset + sizeof(__sum16), skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tret = skb_ensure_writable(skb, offset + sizeof(__sum16));\n\tif (ret)\n\t\tgoto out;\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__le32));\n\tif (ret)\n\t\tgoto out;\n\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb_reset_csum_not_inet(skb);\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb->data;\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn vlan_get_protocol_and_depth(skb, type, depth);\n}\n\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nstatic void do_netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tnetdev_err(dev, \"hw csum failure\\n\");\n\tskb_dump(KERN_ERR, skb, true);\n\tdump_stack();\n}\n\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tDO_ONCE_LITE(do_netdev_rx_csum_fault, dev, skb);\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* XXX: check that highmem exists at all on the given machine. */\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, NULL);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > READ_ONCE(dev->gso_max_segs))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (unlikely(skb->len >= netif_get_gso_max_size(dev, skb)))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (!skb_shinfo(skb)->gso_type) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\t}\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (dev_nit_active(dev))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb_mark_not_on_list(skb);\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_tx_queue_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb_csum_is_sctp(skb)))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\tif (features & NETIF_F_HW_CSUM)\n\t\treturn 0;\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n\t\t\tgoto sw_checksum;\n\t\tswitch (skb->csum_offset) {\n\t\tcase offsetof(struct tcphdr, check):\n\t\tcase offsetof(struct udphdr, check):\n\t\t\treturn 0;\n\t\t}\n\t}\n\nsw_checksum:\n\treturn skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tskb = sk_validate_xmit_skb(skb, dev);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\tskb = validate_xmit_xfrm(skb, features, again);\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tdev_core_stats_tx_dropped_inc(dev);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev, again);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size && skb_transport_header_was_set(skb)) {\n\t\tu16 gso_segs = shinfo->gso_segs;\n\t\tunsigned int hdr_len;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_offset(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\t\tconst struct tcphdr *th;\n\t\t\tstruct tcphdr _tcphdr;\n\n\t\t\tth = skb_header_pointer(skb, hdr_len,\n\t\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\t\tif (likely(th))\n\t\t\t\thdr_len += __tcp_hdrlen(th);\n\t\t} else if (shinfo->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tstruct udphdr _udphdr;\n\n\t\t\tif (skb_header_pointer(skb, hdr_len,\n\t\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\t\thdr_len += sizeof(struct udphdr);\n\t\t}\n\n\t\tif (unlikely(shinfo->gso_type & SKB_GSO_DODGY)) {\n\t\t\tint payload = skb->len - hdr_len;\n\n\t\t\t/* Malicious packet. */\n\t\t\tif (payload <= 0)\n\t\t\t\treturn;\n\t\t\tgso_segs = DIV_ROUND_UP(payload, shinfo->gso_size);\n\t\t}\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic int dev_qdisc_enqueue(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t     struct sk_buff **to_free,\n\t\t\t     struct netdev_queue *txq)\n{\n\tint rc;\n\n\trc = q->enqueue(skb, q, to_free) & NET_XMIT_MASK;\n\tif (rc == NET_XMIT_SUCCESS)\n\t\ttrace_qdisc_enqueue(q, txq, skb);\n\treturn rc;\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tif (q->flags & TCQ_F_CAN_BYPASS && nolock_qdisc_is_empty(q) &&\n\t\t    qdisc_run_begin(q)) {\n\t\t\t/* Retest nolock_qdisc_is_empty() within the protection\n\t\t\t * of q->seqlock to protect from racing with requeuing.\n\t\t\t */\n\t\t\tif (unlikely(!nolock_qdisc_is_empty(q))) {\n\t\t\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\t\t\t__qdisc_run(q);\n\t\t\t\tqdisc_run_end(q);\n\n\t\t\t\tgoto no_lock_out;\n\t\t\t}\n\n\t\t\tqdisc_bstats_cpu_update(q, skb);\n\t\t\tif (sch_direct_xmit(skb, q, dev, txq, NULL, true) &&\n\t\t\t    !nolock_qdisc_is_empty(q))\n\t\t\t\t__qdisc_run(q);\n\n\t\t\tqdisc_run_end(q);\n\t\t\treturn NET_XMIT_SUCCESS;\n\t\t}\n\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tqdisc_run(q);\n\nno_lock_out:\n\t\tif (unlikely(to_free))\n\t\t\tkfree_skb_list_reason(to_free,\n\t\t\t\t\t      SKB_DROP_REASON_QDISC_DROP);\n\t\treturn rc;\n\t}\n\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t * On PREEMPT_RT it is possible to preempt the qdisc owner during xmit\n\t * and then other tasks will only enqueue packets. The packets will be\n\t * sent after the qdisc owner is scheduled again. To prevent this\n\t * scenario the task always serialize on the lock.\n\t */\n\tcontended = qdisc_is_running(q) || IS_ENABLED(CONFIG_PREEMPT_RT);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\n\t\tqdisc_run_end(q);\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t\tqdisc_run_end(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list_reason(to_free, SKB_DROP_REASON_QDISC_DROP);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tconst struct netprio_map *map;\n\tconst struct sock *sk;\n\tunsigned int prioidx;\n\n\tif (skb->priority)\n\t\treturn;\n\tmap = rcu_dereference_bh(skb->dev->priomap);\n\tif (!map)\n\t\treturn;\n\tsk = skb_to_full_sk(skb);\n\tif (!sk)\n\t\treturn;\n\n\tprioidx = sock_cgroup_prioidx(&sk->sk_cgrp_data);\n\n\tif (prioidx < map->priomap_len)\n\t\tskb->priority = map->priomap[prioidx];\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tDEBUG_NET_WARN_ON_ONCE(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct netdev_queue *\nnetdev_tx_queue_mapping(struct net_device *dev, struct sk_buff *skb)\n{\n\tint qm = skb_get_queue_mapping(skb);\n\n\treturn netdev_get_tx_queue(dev, netdev_cap_txqueue(dev, qm));\n}\n\nstatic bool netdev_xmit_txqueue_skipped(void)\n{\n\treturn __this_cpu_read(softnet_data.xmit.skip_txqueue);\n}\n\nvoid netdev_xmit_skip_txqueue(bool skip)\n{\n\t__this_cpu_write(softnet_data.xmit.skip_txqueue, skip);\n}\nEXPORT_SYMBOL_GPL(netdev_xmit_skip_txqueue);\n#endif /* CONFIG_NET_EGRESS */\n\n#ifdef CONFIG_NET_XGRESS\nstatic int tc_run(struct tcx_entry *entry, struct sk_buff *skb)\n{\n\tint ret = TC_ACT_UNSPEC;\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(entry->miniq);\n\tstruct tcf_result res;\n\n\tif (!miniq)\n\t\treturn ret;\n\n\ttc_skb_cb(skb)->mru = 0;\n\ttc_skb_cb(skb)->post_ct = false;\n\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\tret = tcf_classify(skb, miniq->block, miniq->filter_list, &res, false);\n\t/* Only tcf related quirks below. */\n\tswitch (ret) {\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\tbreak;\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(res.classid);\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn ret;\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(tcx_needed_key);\n\nvoid tcx_inc(void)\n{\n\tstatic_branch_inc(&tcx_needed_key);\n}\n\nvoid tcx_dec(void)\n{\n\tstatic_branch_dec(&tcx_needed_key);\n}\n\nstatic __always_inline enum tcx_action_base\ntcx_run(const struct bpf_mprog_entry *entry, struct sk_buff *skb,\n\tconst bool needs_mac)\n{\n\tconst struct bpf_mprog_fp *fp;\n\tconst struct bpf_prog *prog;\n\tint ret = TCX_NEXT;\n\n\tif (needs_mac)\n\t\t__skb_push(skb, skb->mac_len);\n\tbpf_mprog_foreach_prog(entry, fp, prog) {\n\t\tbpf_compute_data_pointers(skb);\n\t\tret = bpf_prog_run(prog, skb);\n\t\tif (ret != TCX_NEXT)\n\t\t\tbreak;\n\t}\n\tif (needs_mac)\n\t\t__skb_pull(skb, skb->mac_len);\n\treturn tcx_action_code(skb, ret);\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n\tstruct bpf_mprog_entry *entry = rcu_dereference_bh(skb->dev->tcx_ingress);\n\tint sch_ret;\n\n\tif (!entry)\n\t\treturn skb;\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\ttcx_set_ingress(skb, true);\n\n\tif (static_branch_unlikely(&tcx_needed_key)) {\n\t\tsch_ret = tcx_run(entry, skb, true);\n\t\tif (sch_ret != TC_ACT_UNSPEC)\n\t\t\tgoto ingress_verdict;\n\t}\n\tsch_ret = tc_run(tcx_entry(entry), skb);\ningress_verdict:\n\tswitch (sch_ret) {\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by BPF, so we can safely\n\t\t * push the L2 header back before redirecting to another\n\t\t * netdev.\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tif (skb_do_redirect(skb) == -EAGAIN) {\n\t\t\t__skb_pull(skb, skb->mac_len);\n\t\t\t*another = true;\n\t\t\tbreak;\n\t\t}\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\tcase TC_ACT_SHOT:\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_INGRESS);\n\t\t*ret = NET_RX_DROP;\n\t\treturn NULL;\n\t/* used by tc_run */\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\tfallthrough;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\t}\n\n\treturn skb;\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct bpf_mprog_entry *entry = rcu_dereference_bh(dev->tcx_egress);\n\tint sch_ret;\n\n\tif (!entry)\n\t\treturn skb;\n\n\t/* qdisc_skb_cb(skb)->pkt_len & tcx_set_ingress() was\n\t * already set by the caller.\n\t */\n\tif (static_branch_unlikely(&tcx_needed_key)) {\n\t\tsch_ret = tcx_run(entry, skb, false);\n\t\tif (sch_ret != TC_ACT_UNSPEC)\n\t\t\tgoto egress_verdict;\n\t}\n\tsch_ret = tc_run(tcx_entry(entry), skb);\negress_verdict:\n\tswitch (sch_ret) {\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tcase TC_ACT_SHOT:\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_EGRESS);\n\t\t*ret = NET_XMIT_DROP;\n\t\treturn NULL;\n\t/* used by tc_run */\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\tfallthrough;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\t}\n\n\treturn skb;\n}\n#else\nstatic __always_inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n\treturn skb;\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\treturn skb;\n}\n#endif /* CONFIG_NET_XGRESS */\n\n#ifdef CONFIG_XPS\nstatic int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct xps_dev_maps *dev_maps, unsigned int tci)\n{\n\tint tc = netdev_get_prio_tc_map(dev, skb->priority);\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\tif (tc >= dev_maps->num_tc || tci >= dev_maps->nr_ids)\n\t\treturn queue_index;\n\n\ttci *= dev_maps->num_tc;\n\ttci += tc;\n\n\tmap = rcu_dereference(dev_maps->attr_map[tci]);\n\tif (map) {\n\t\tif (map->len == 1)\n\t\t\tqueue_index = map->queues[0];\n\t\telse\n\t\t\tqueue_index = map->queues[reciprocal_scale(\n\t\t\t\t\t\tskb_get_hash(skb), map->len)];\n\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\tqueue_index = -1;\n\t}\n\treturn queue_index;\n}\n#endif\n\nstatic int get_xps_queue(struct net_device *dev, struct net_device *sb_dev,\n\t\t\t struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct sock *sk = skb->sk;\n\tint queue_index = -1;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn -1;\n\n\trcu_read_lock();\n\tif (!static_key_false(&xps_rxqs_needed))\n\t\tgoto get_cpus_map;\n\n\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_RXQS]);\n\tif (dev_maps) {\n\t\tint tci = sk_rx_queue_get(sk);\n\n\t\tif (tci >= 0)\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t}\n\nget_cpus_map:\n\tif (queue_index < 0) {\n\t\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_CPUS]);\n\t\tif (dev_maps) {\n\t\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_pick_tx_zero);\n\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev)\n{\n\treturn (u16)raw_smp_processor_id() % dev->real_num_tx_queues;\n}\nEXPORT_SYMBOL(dev_pick_tx_cpu_id);\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tsb_dev = sb_dev ? : dev;\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, sb_dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, sb_dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\nEXPORT_SYMBOL(netdev_pick_tx);\n\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, sb_dev);\n\t\telse\n\t\t\tqueue_index = netdev_pick_tx(dev, skb, sb_dev);\n\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n * __dev_queue_xmit() - transmit a buffer\n * @skb:\tbuffer to transmit\n * @sb_dev:\tsuboordinate device used for L2 forwarding offload\n *\n * Queue a buffer for transmission to a network device. The caller must\n * have set the device and priority and built the buffer before calling\n * this function. The function can be called from an interrupt.\n *\n * When calling this method, interrupts MUST be enabled. This is because\n * the BH enable code must have IRQs enabled so that it will not deadlock.\n *\n * Regardless of the return value, the skb is consumed, so it is currently\n * difficult to retry a send to this method. (You can bump the ref count\n * before sending to hold a reference for retry if you are careful.)\n *\n * Return:\n * * 0\t\t\t\t- buffer successfully transmitted\n * * positive qdisc return code\t- NET_XMIT_DROP etc.\n * * negative errno\t\t- other errors\n */\nint __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq = NULL;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\tbool again = false;\n\n\tskb_reset_mac_header(skb);\n\tskb_assert_len(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n\ttcx_set_ingress(skb, false);\n#ifdef CONFIG_NET_EGRESS\n\tif (static_branch_unlikely(&egress_needed_key)) {\n\t\tif (nf_hook_egress_active()) {\n\t\t\tskb = nf_hook_egress(skb, &rc, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tnetdev_xmit_skip_txqueue(false);\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tnf_skip_egress(skb, false);\n\n\t\tif (netdev_xmit_txqueue_skipped())\n\t\t\ttxq = netdev_tx_queue_mapping(dev, skb);\n\t}\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\tif (!txq)\n\t\ttxq = netdev_core_pick_tx(dev, skb, sb_dev);\n\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\t/* Other cpus might concurrently change txq->xmit_lock_owner\n\t\t * to -1 or to their cpu id, but not to our id.\n\t\t */\n\t\tif (READ_ONCE(txq->xmit_lock_owner) != cpu) {\n\t\t\tif (dev_xmit_recursion())\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev, &again);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\tdev_xmit_recursion_inc();\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\tdev_xmit_recursion_dec();\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\nEXPORT_SYMBOL(__dev_queue_xmit);\n\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev, &again);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tskb_set_queue_mapping(skb, queue_id);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\tlocal_bh_disable();\n\n\tdev_xmit_recursion_inc();\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\tdev_xmit_recursion_dec();\n\n\tlocal_bh_enable();\n\treturn ret;\ndrop:\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\nEXPORT_SYMBOL(__dev_direct_xmit);\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nunsigned int sysctl_skb_defer_max __read_mostly = 64;\nint netdev_budget __read_mostly = 300;\n/* Must be at least 2 jiffes to guarantee 1 jiffy timeout */\nunsigned int __read_mostly netdev_budget_usecs = 2 * USEC_PER_SEC / HZ;\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tstruct task_struct *thread;\n\n\tlockdep_assert_irqs_disabled();\n\n\tif (test_bit(NAPI_STATE_THREADED, &napi->state)) {\n\t\t/* Paired with smp_mb__before_atomic() in\n\t\t * napi_enable()/dev_set_threaded().\n\t\t * Use READ_ONCE() to guarantee a complete\n\t\t * read on napi->thread. Only call\n\t\t * wake_up_process() when it's not NULL.\n\t\t */\n\t\tthread = READ_ONCE(napi->thread);\n\t\tif (thread) {\n\t\t\t/* Avoid doing set_bit() if the thread is in\n\t\t\t * INTERRUPTIBLE state, cause napi_thread_wait()\n\t\t\t * makes sure to proceed with napi polling\n\t\t\t * if the thread is explicitly woken from here.\n\t\t\t */\n\t\t\tif (READ_ONCE(thread->__state) != TASK_INTERRUPTIBLE)\n\t\t\t\tset_bit(NAPI_STATE_SCHED_THREADED, &napi->state);\n\t\t\twake_up_process(thread);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\tWRITE_ONCE(napi->list_owner, smp_processor_id());\n\t/* If not called from net_rx_action()\n\t * we have to raise NET_RX_SOFTIRQ.\n\t */\n\tif (!sd->in_net_rx_action)\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key_false rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key_false rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match.\n\t\t * This READ_ONCE() pairs with WRITE_ONCE() from rps_record_sock_flow().\n\t\t */\n\t\tident = READ_ONCE(sock_flow_table->ents[hash & sock_flow_table->mask]);\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = READ_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/* Called from hardirq (IPI) context */\nstatic void trigger_rx_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\tsmp_store_release(&sd->defer_ipi_scheduled, 0);\n}\n\n/*\n * After we queued a packet into sd->input_pkt_queue,\n * we need to make sure this queue is serviced soon.\n *\n * - If this is another cpu queue, link it to our rps_ipi_list,\n *   and make sure we will process rps_ipi_list from net_rx_action().\n *\n * - If this is our own queue, NAPI schedule our backlog.\n *   Note that this also raises NET_RX_SOFTIRQ.\n */\nstatic void napi_schedule_rps(struct softnet_data *sd)\n{\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n#ifdef CONFIG_RPS\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t/* If not called from net_rx_action() or napi_threaded_poll()\n\t\t * we have to raise NET_RX_SOFTIRQ.\n\t\t */\n\t\tif (!mysd->in_net_rx_action && !mysd->in_napi_threaded_poll)\n\t\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn;\n\t}\n#endif /* CONFIG_RPS */\n\t__napi_schedule_irqoff(&mysd->backlog);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (READ_ONCE(netdev_max_backlog) >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tenum skb_drop_reason reason;\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\treason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tsd = &per_cpu(softnet_data, cpu);\n\n\trps_lock_irqsave(sd, &flags);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= READ_ONCE(netdev_max_backlog) && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock_irq_restore(sd, &flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state))\n\t\t\tnapi_schedule_rps(sd);\n\t\tgoto enqueue;\n\t}\n\treason = SKB_DROP_REASON_CPU_BACKLOG;\n\ndrop:\n\tsd->dropped++;\n\trps_unlock_irq_restore(sd, &flags);\n\n\tdev_core_stats_rx_dropped_inc(skb->dev);\n\tkfree_skb_reason(skb, reason);\n\treturn NET_RX_DROP;\n}\n\nstatic struct netdev_rx_queue *netif_get_rxqueue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_rx_queue *rxqueue;\n\n\trxqueue = dev->_rx;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\n\t\t\treturn rxqueue; /* Return first rxqueue */\n\t\t}\n\t\trxqueue += index;\n\t}\n\treturn rxqueue;\n}\n\nu32 bpf_prog_run_generic_xdp(struct sk_buff *skb, struct xdp_buff *xdp,\n\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tvoid *orig_data, *orig_data_end, *hard_start;\n\tstruct netdev_rx_queue *rxqueue;\n\tbool orig_bcast, orig_host;\n\tu32 mac_len, frame_sz;\n\t__be16 orig_eth_type;\n\tstruct ethhdr *eth;\n\tu32 metalen, act;\n\tint off;\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thard_start = skb->data - skb_headroom(skb);\n\n\t/* SKB \"head\" area always have tailroom for skb_shared_info */\n\tframe_sz = (void *)skb_end_pointer(skb) - hard_start;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\trxqueue = netif_get_rxqueue(skb);\n\txdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,\n\t\t\t skb_headlen(skb) + mac_len, true);\n\n\torig_data_end = xdp->data_end;\n\torig_data = xdp->data;\n\teth = (struct ethhdr *)xdp->data;\n\torig_host = ether_addr_equal_64bits(eth->h_dest, skb->dev->dev_addr);\n\torig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);\n\torig_eth_type = eth->h_proto;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t/* check if bpf_xdp_adjust_head was used */\n\toff = xdp->data - orig_data;\n\tif (off) {\n\t\tif (off > 0)\n\t\t\t__skb_pull(skb, off);\n\t\telse if (off < 0)\n\t\t\t__skb_push(skb, -off);\n\n\t\tskb->mac_header += off;\n\t\tskb_reset_network_header(skb);\n\t}\n\n\t/* check if bpf_xdp_adjust_tail was used */\n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0) {\n\t\tskb_set_tail_pointer(skb, xdp->data_end - xdp->data);\n\t\tskb->len += off; /* positive on grow, negative on shrink */\n\t}\n\n\t/* check if XDP changed eth hdr such SKB needs update */\n\teth = (struct ethhdr *)xdp->data;\n\tif ((orig_eth_type != eth->h_proto) ||\n\t    (orig_host != ether_addr_equal_64bits(eth->h_dest,\n\t\t\t\t\t\t  skb->dev->dev_addr)) ||\n\t    (orig_bcast != is_multicast_ether_addr_64bits(eth->h_dest))) {\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->pkt_type = PACKET_HOST;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t}\n\n\t/* Redirect/Tx gives L2 packet, code that will reuse skb must __skb_pull\n\t * before calling us again on redirect path. We do not call do_redirect\n\t * as we leave that up to the caller.\n\t *\n\t * Caller is responsible for managing lifetime of skb (i.e. calling\n\t * kfree_skb in response to actions it cannot handle/XDP_DROP).\n\t */\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\tbreak;\n\tcase XDP_PASS:\n\t\tmetalen = xdp->data - xdp->data_meta;\n\t\tif (metalen)\n\t\t\tskb_metadata_set(skb, metalen);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tu32 act = XDP_DROP;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_is_redirected(skb))\n\t\treturn XDP_PASS;\n\n\t/* XDP packets must be linear and must have sufficient headroom\n\t * of XDP_PACKET_HEADROOM bytes. This is the guarantee that also\n\t * native XDP provides, thus we need to do it here as well.\n\t */\n\tif (skb_cloned(skb) || skb_is_nonlinear(skb) ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tint hroom = XDP_PACKET_HEADROOM - skb_headroom(skb);\n\t\tint troom = skb->tail + skb->data_len - skb->end;\n\n\t\t/* In case we have to go down the path and also linearize,\n\t\t * then lets do the pskb_expand_head() work just once here.\n\t\t */\n\t\tif (pskb_expand_head(skb,\n\t\t\t\t     hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,\n\t\t\t\t     troom > 0 ? troom + 128 : 0, GFP_ATOMIC))\n\t\t\tgoto do_drop;\n\t\tif (skb_linearize(skb))\n\t\t\tgoto do_drop;\n\t}\n\n\tact = bpf_prog_run_generic_xdp(skb, xdp, xdp_prog);\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\tcase XDP_PASS:\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior. This also means\n * that XDP packets are able to starve other packets going through a qdisc,\n * and DDOS attacks will be more effective. In-driver-XDP use dedicated TX\n * queues, so they do not have this starvation issue.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_core_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_frozen_or_drv_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tdev_core_stats_tx_dropped_inc(dev);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(generic_xdp_needed_key);\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tstruct xdp_buff xdp;\n\t\tu32 act;\n\t\tint err;\n\n\t\tact = netif_receive_generic_xdp(skb, &xdp, xdp_prog);\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      &xdp, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t\tbreak;\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_XDP);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\ttrace_netif_rx(skb);\n\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, smp_processor_id(), &qtail);\n\t}\n\treturn ret;\n}\n\n/**\n *\t__netif_rx\t-\tSlightly optimized version of netif_rx\n *\t@skb: buffer to post\n *\n *\tThis behaves as netif_rx except that it does not disable bottom halves.\n *\tAs a result this function may only be invoked from the interrupt context\n *\t(either hard or soft interrupt).\n */\nint __netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\tlockdep_assert_once(hardirq_count() | softirq_count());\n\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netif_rx);\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process via the backlog NAPI device. It\n *\talways succeeds. The buffer may be dropped during processing for\n *\tcongestion control or by the protocol layers.\n *\tThe network buffer is passed via the backlog NAPI device. Modern NIC\n *\tdriver should use NAPI and GRO.\n *\tThis function can used from interrupt and from process context. The\n *\tcaller from process context must not disable interrupts before invoking\n *\tthis function.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\nint netif_rx(struct sk_buff *skb)\n{\n\tbool need_bh_off = !(hardirq_count() | softirq_count());\n\tint ret;\n\n\tif (need_bh_off)\n\t\tlocal_bh_disable();\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\tif (need_bh_off)\n\t\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb, net_tx_action);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action,\n\t\t\t\t\t\tget_kfree_skb_cb(skb)->reason);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__napi_kfree_skb(skb,\n\t\t\t\t\t\t get_kfree_skb_cb(skb)->reason);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\trcu_read_lock();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock = NULL;\n\n\t\t\thead = head->next_sched;\n\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\n\t\t\tif (!(q->flags & TCQ_F_NOLOCK)) {\n\t\t\t\troot_lock = qdisc_lock(q);\n\t\t\t\tspin_lock(root_lock);\n\t\t\t} else if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t\t     &q->state))) {\n\t\t\t\t/* There is a synchronize_net() between\n\t\t\t\t * STATE_DEACTIVATED flag being set and\n\t\t\t\t * qdisc_reset()/some_qdisc_is_busy() in\n\t\t\t\t * dev_deactivate(), so we can safely bail out\n\t\t\t\t * early here to avoid data race between\n\t\t\t\t * qdisc_deactivate() and some_qdisc_is_busy()\n\t\t\t\t * for lockless qdisc.\n\t\t\t\t */\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tif (root_lock)\n\t\t\t\tspin_unlock(root_lock);\n\t\t}\n\n\t\trcu_read_unlock();\n\t}\n\n\txfrm_dev_backlog(sd);\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\tif (dev->priv_flags & IFF_NO_RX_HANDLER)\n\t\treturn -EINVAL;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,\n\t\t\t\t    struct packet_type **ppt_prev)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (static_branch_unlikely(&generic_xdp_needed_key)) {\n\t\tint ret2;\n\n\t\tmigrate_disable();\n\t\tret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\tmigrate_enable();\n\n\t\tif (ret2 != XDP_PASS) {\n\t\t\tret = NET_RX_DROP;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (eth_type_vlan(skb->protocol)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_branch_unlikely(&ingress_needed_key)) {\n\t\tbool another = false;\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev,\n\t\t\t\t\t &another);\n\t\tif (another)\n\t\t\tgoto another_round;\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tnf_skip_egress(skb, false);\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_redirect(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\t\tbreak;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {\ncheck_vlan_id:\n\t\tif (skb_vlan_tag_get_id(skb)) {\n\t\t\t/* Vlan id is non 0 and vlan_do_receive() above couldn't\n\t\t\t * find vlan device.\n\t\t\t */\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t} else if (eth_type_vlan(skb->protocol)) {\n\t\t\t/* Outer header is 802.1P with vlan 0, inner header is\n\t\t\t * 802.1Q or 802.1AD and vlan_do_receive() above could\n\t\t\t * not find vlan dev for vlan id 0.\n\t\t\t */\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb = skb_vlan_untag(skb);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\tif (vlan_do_receive(&skb))\n\t\t\t\t/* After stripping off 802.1P header with vlan 0\n\t\t\t\t * vlan dev is found for inner header.\n\t\t\t\t */\n\t\t\t\tgoto another_round;\n\t\t\telse if (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\telse\n\t\t\t\t/* We have stripped outer 802.1P vlan 0 header.\n\t\t\t\t * But could not find vlan dev.\n\t\t\t\t * check again for vlan id to set OTHERHOST.\n\t\t\t\t */\n\t\t\t\tgoto check_vlan_id;\n\t\t}\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\t*ppt_prev = pt_prev;\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tdev_core_stats_rx_dropped_inc(skb->dev);\n\t\telse\n\t\t\tdev_core_stats_rx_nohandler_inc(skb->dev);\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\t/* The invariant here is that if *ppt_prev is not NULL\n\t * then skb should also be non-NULL.\n\t *\n\t * Apparently *ppt_prev assignment above holds this invariant due to\n\t * skb dereferencing near it.\n\t */\n\t*pskb = skb;\n\treturn ret;\n}\n\nstatic int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct net_device *orig_dev = skb->dev;\n\tstruct packet_type *pt_prev = NULL;\n\tint ret;\n\n\tret = __netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\tif (pt_prev)\n\t\tret = INDIRECT_CALL_INET(pt_prev->func, ipv6_rcv, ip_rcv, skb,\n\t\t\t\t\t skb->dev, pt_prev, orig_dev);\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb_core - special purpose version of netif_receive_skb\n *\t@skb: buffer to process\n *\n *\tMore direct receive version of netif_receive_skb().  It should\n *\tonly be used by callers that have a need to skip RPS and Generic XDP.\n *\tCaller must also take care of handling if ``(page_is_)pfmemalloc``.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb_core(struct sk_buff *skb)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __netif_receive_skb_one_core(skb, false);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb_core);\n\nstatic inline void __netif_receive_skb_list_ptype(struct list_head *head,\n\t\t\t\t\t\t  struct packet_type *pt_prev,\n\t\t\t\t\t\t  struct net_device *orig_dev)\n{\n\tstruct sk_buff *skb, *next;\n\n\tif (!pt_prev)\n\t\treturn;\n\tif (list_empty(head))\n\t\treturn;\n\tif (pt_prev->list_func != NULL)\n\t\tINDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,\n\t\t\t\t   ip_list_rcv, head, pt_prev, orig_dev);\n\telse\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tskb_list_del_init(skb);\n\t\t\tpt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t\t}\n}\n\nstatic void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)\n{\n\t/* Fast-path assumptions:\n\t * - There is no RX handler.\n\t * - Only one packet_type matches.\n\t * If either of these fails, we will end up doing some per-packet\n\t * processing in-line, then handling the 'last ptype' for the whole\n\t * sublist.  This can't cause out-of-order delivery to any single ptype,\n\t * because the 'last ptype' must be constant across the sublist, and all\n\t * other ptypes are handled per-packet.\n\t */\n\t/* Current (common) ptype of sublist */\n\tstruct packet_type *pt_curr = NULL;\n\t/* Current (common) orig_dev of sublist */\n\tstruct net_device *od_curr = NULL;\n\tstruct list_head sublist;\n\tstruct sk_buff *skb, *next;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tstruct net_device *orig_dev = skb->dev;\n\t\tstruct packet_type *pt_prev = NULL;\n\n\t\tskb_list_del_init(skb);\n\t\t__netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\t\tif (!pt_prev)\n\t\t\tcontinue;\n\t\tif (pt_curr != pt_prev || od_curr != orig_dev) {\n\t\t\t/* dispatch old sublist */\n\t\t\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n\t\t\t/* start new sublist */\n\t\t\tINIT_LIST_HEAD(&sublist);\n\t\t\tpt_curr = pt_prev;\n\t\t\tod_curr = orig_dev;\n\t\t}\n\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\n\t/* dispatch final sublist */\n\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_one_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_one_core(skb, false);\n\n\treturn ret;\n}\n\nstatic void __netif_receive_skb_list(struct list_head *head)\n{\n\tunsigned long noreclaim_flag = 0;\n\tstruct sk_buff *skb, *next;\n\tbool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tif ((sk_memalloc_socks() && skb_pfmemalloc(skb)) != pfmemalloc) {\n\t\t\tstruct list_head sublist;\n\n\t\t\t/* Handle the previous sublist */\n\t\t\tlist_cut_before(&sublist, head, &skb->list);\n\t\t\tif (!list_empty(&sublist))\n\t\t\t\t__netif_receive_skb_list_core(&sublist, pfmemalloc);\n\t\t\tpfmemalloc = !pfmemalloc;\n\t\t\t/* See comments in __netif_receive_skb */\n\t\t\tif (pfmemalloc)\n\t\t\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\t\telse\n\t\t\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t\t}\n\t}\n\t/* Handle the remaining sublist */\n\tif (!list_empty(head))\n\t\t__netif_receive_skb_list_core(head, pfmemalloc);\n\t/* Restore pflags */\n\tif (pfmemalloc)\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_branch_dec(&generic_xdp_needed_key);\n\t\t} else if (new && !old) {\n\t\t\tstatic_branch_inc(&generic_xdp_needed_key);\n\t\t\tdev_disable_lro(dev);\n\t\t\tdev_disable_gro_hw(dev);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\nvoid netif_receive_skb_list_internal(struct list_head *head)\n{\n\tstruct sk_buff *skb, *next;\n\tstruct list_head sublist;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\t\tskb_list_del_init(skb);\n\t\tif (!skb_defer_rx_timestamp(skb))\n\t\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\tlist_splice_init(&sublist, head);\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\t\tif (cpu >= 0) {\n\t\t\t\t/* Will be handled, remove from list */\n\t\t\t\tskb_list_del_init(skb);\n\t\t\t\tenqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\t__netif_receive_skb_list(head);\n\trcu_read_unlock();\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_receive_skb_entry(skb);\n\n\tret = netif_receive_skb_internal(skb);\n\ttrace_netif_receive_skb_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/**\n *\tnetif_receive_skb_list - process many receive buffers from network\n *\t@head: list of skbs to process.\n *\n *\tSince return value of netif_receive_skb() is normally ignored, and\n *\twouldn't be meaningful for a list, this function returns void.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n */\nvoid netif_receive_skb_list(struct list_head *head)\n{\n\tstruct sk_buff *skb;\n\n\tif (list_empty(head))\n\t\treturn;\n\tif (trace_netif_receive_skb_list_entry_enabled()) {\n\t\tlist_for_each_entry(skb, head, list)\n\t\t\ttrace_netif_receive_skb_list_entry(skb);\n\t}\n\tnetif_receive_skb_list_internal(head);\n\ttrace_netif_receive_skb_list_exit(0);\n}\nEXPORT_SYMBOL(netif_receive_skb_list);\n\nstatic DEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trps_lock_irq_disable(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock_irq_enable(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic bool flush_required(int cpu)\n{\n#if IS_ENABLED(CONFIG_RPS)\n\tstruct softnet_data *sd = &per_cpu(softnet_data, cpu);\n\tbool do_flush;\n\n\trps_lock_irq_disable(sd);\n\n\t/* as insertion into process_queue happens with the rps lock held,\n\t * process_queue access may race only with dequeue\n\t */\n\tdo_flush = !skb_queue_empty(&sd->input_pkt_queue) ||\n\t\t   !skb_queue_empty_lockless(&sd->process_queue);\n\trps_unlock_irq_enable(sd);\n\n\treturn do_flush;\n#endif\n\t/* without RPS we can't safely check input_pkt_queue: during a\n\t * concurrent remote skb_queue_splice() we can detect as empty both\n\t * input_pkt_queue and process_queue even if the latter could end-up\n\t * containing a lot of packets.\n\t */\n\treturn true;\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tstatic cpumask_t flush_cpus;\n\tunsigned int cpu;\n\n\t/* since we are under rtnl lock protection we can use static data\n\t * for the cpumask and avoid allocating on stack the possibly\n\t * large mask\n\t */\n\tASSERT_RTNL();\n\n\tcpus_read_lock();\n\n\tcpumask_clear(&flush_cpus);\n\tfor_each_online_cpu(cpu) {\n\t\tif (flush_required(cpu)) {\n\t\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\t\t\tcpumask_set_cpu(cpu, &flush_cpus);\n\t\t}\n\t}\n\n\t/* we can have in flight packet[s] on the cpus we are not flushing,\n\t * synchronize_net() in unregister_netdevice_many() will take care of\n\t * them\n\t */\n\tfor_each_cpu(cpu, &flush_cpus)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tcpus_read_unlock();\n}\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = READ_ONCE(dev_rx_weight);\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\trps_lock_irq_disable(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock_irq_enable(sd);\n\t}\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable to\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long new, val = READ_ONCE(n->state);\n\n\tdo {\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked.\n *\n * On PREEMPT_RT enabled kernels this maps to __napi_schedule()\n * because the interrupt disabled assumption might not be true\n * due to force-threaded interrupts and spinlock substitution.\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\telse\n\t\t__napi_schedule(n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new, timeout = 0;\n\tbool ret = true;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (work_done) {\n\t\tif (n->gro_bitmask)\n\t\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tn->defer_hard_irqs_count = READ_ONCE(n->dev->napi_defer_hard_irqs);\n\t}\n\tif (n->defer_hard_irqs_count > 0) {\n\t\tn->defer_hard_irqs_count--;\n\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tif (timeout)\n\t\t\tret = false;\n\t}\n\tif (n->gro_bitmask) {\n\t\t/* When the NAPI instance uses a timeout and keeps postponing\n\t\t * it, we need to bound somehow the time packets are kept in\n\t\t * the GRO layer\n\t\t */\n\t\tnapi_gro_flush(n, !!timeout);\n\t}\n\n\tgro_normal_list(n);\n\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\tWRITE_ONCE(n->list_owner, -1);\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |\n\t\t\t      NAPIF_STATE_SCHED_THREADED |\n\t\t\t      NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\tif (timeout)\n\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\nstatic void __busy_poll_stop(struct napi_struct *napi, bool skip_schedule)\n{\n\tif (!skip_schedule) {\n\t\tgro_normal_list(napi);\n\t\t__napi_schedule(napi);\n\t\treturn;\n\t}\n\n\tif (napi->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(napi, HZ >= 1000);\n\t}\n\n\tgro_normal_list(napi);\n\tclear_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock, bool prefer_busy_poll,\n\t\t\t   u16 budget)\n{\n\tbool skip_schedule = false;\n\tunsigned long timeout;\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\tif (prefer_busy_poll) {\n\t\tnapi->defer_hard_irqs_count = READ_ONCE(napi->dev->napi_defer_hard_irqs);\n\t\ttimeout = READ_ONCE(napi->dev->gro_flush_timeout);\n\t\tif (napi->defer_hard_irqs_count && timeout) {\n\t\t\thrtimer_start(&napi->timer, ns_to_ktime(timeout), HRTIMER_MODE_REL_PINNED);\n\t\t\tskip_schedule = true;\n\t\t}\n\t}\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, budget);\n\t/* We can't gro_normal_list() here, because napi->poll() might have\n\t * rearmed the napi (napi_complete_done()) in which case it could\n\t * already be running on another CPU.\n\t */\n\ttrace_napi_poll(napi, rc, budget);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == budget)\n\t\t__busy_poll_stop(napi, skip_schedule);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL)) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, budget);\n\t\ttrace_napi_poll(napi, work, budget);\n\t\tgro_normal_list(napi);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\t\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nstatic void napi_hash_del(struct napi_struct *napi)\n{\n\tspin_lock(&napi_hash_lock);\n\n\thlist_del_init_rcu(&napi->napi_hash_node);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (!napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t__napi_schedule_irqoff(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void init_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tINIT_LIST_HEAD(&napi->gro_hash[i].list);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n\tnapi->gro_bitmask = 0;\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded)\n{\n\tstruct napi_struct *napi;\n\tint err = 0;\n\n\tif (dev->threaded == threaded)\n\t\treturn 0;\n\n\tif (threaded) {\n\t\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\t\tif (!napi->thread) {\n\t\t\t\terr = napi_kthread_create(napi);\n\t\t\t\tif (err) {\n\t\t\t\t\tthreaded = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdev->threaded = threaded;\n\n\t/* Make sure kthread is created before THREADED bit\n\t * is set.\n\t */\n\tsmp_mb__before_atomic();\n\n\t/* Setting/unsetting threaded mode on a napi might not immediately\n\t * take effect, if the current napi instance is actively being\n\t * polled. In this case, the switch between threaded mode and\n\t * softirq mode will happen in the next round of napi_schedule().\n\t * This should not cause hiccups/stalls to the live traffic.\n\t */\n\tlist_for_each_entry(napi, &dev->napi_list, dev_list)\n\t\tassign_bit(NAPI_STATE_THREADED, &napi->state, threaded);\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_threaded);\n\nvoid netif_napi_add_weight(struct net_device *dev, struct napi_struct *napi,\n\t\t\t   int (*poll)(struct napi_struct *, int), int weight)\n{\n\tif (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tINIT_HLIST_NODE(&napi->napi_hash_node);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tinit_gro_hash(napi);\n\tnapi->skb = NULL;\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tnetdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,\n\t\t\t\tweight);\n\tnapi->weight = weight;\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tnapi->list_owner = -1;\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tset_bit(NAPI_STATE_NPSVC, &napi->state);\n\tlist_add_rcu(&napi->dev_list, &dev->napi_list);\n\tnapi_hash_add(napi);\n\tnapi_get_frags_check(napi);\n\t/* Create kthread for this napi if dev->threaded is set.\n\t * Clear dev->threaded if kthread creation failed so that\n\t * threaded mode will not be enabled in napi_enable().\n\t */\n\tif (dev->threaded && napi_kthread_create(napi))\n\t\tdev->threaded = 0;\n}\nEXPORT_SYMBOL(netif_napi_add_weight);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\twhile (val & (NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC)) {\n\t\t\tusleep_range(20, 200);\n\t\t\tval = READ_ONCE(n->state);\n\t\t}\n\n\t\tnew = val | NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC;\n\t\tnew &= ~(NAPIF_STATE_THREADED | NAPIF_STATE_PREFER_BUSY_POLL);\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: NAPI context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nvoid napi_enable(struct napi_struct *n)\n{\n\tunsigned long new, val = READ_ONCE(n->state);\n\n\tdo {\n\t\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &val));\n\n\t\tnew = val & ~(NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC);\n\t\tif (n->dev->threaded && n->thread)\n\t\t\tnew |= NAPIF_STATE_THREADED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n}\nEXPORT_SYMBOL(napi_enable);\n\nstatic void flush_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tstruct sk_buff *skb, *n;\n\n\t\tlist_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)\n\t\t\tkfree_skb(skb);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n}\n\n/* Must be called in process context */\nvoid __netif_napi_del(struct napi_struct *napi)\n{\n\tif (!test_and_clear_bit(NAPI_STATE_LISTED, &napi->state))\n\t\treturn;\n\n\tnapi_hash_del(napi);\n\tlist_del_rcu(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tflush_gro_hash(napi);\n\tnapi->gro_bitmask = 0;\n\n\tif (napi->thread) {\n\t\tkthread_stop(napi->thread);\n\t\tnapi->thread = NULL;\n\t}\n}\nEXPORT_SYMBOL(__netif_napi_del);\n\nstatic int __napi_poll(struct napi_struct *n, bool *repoll)\n{\n\tint work, weight;\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tif (unlikely(work > weight))\n\t\tnetdev_err_once(n->dev, \"NAPI poll function %pS returned %d, exceeding its budget of %d.\\n\",\n\t\t\t\tn->poll, work, weight);\n\n\tif (likely(work < weight))\n\t\treturn work;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\treturn work;\n\t}\n\n\t/* The NAPI context has more processing work, but busy-polling\n\t * is preferred. Exit early.\n\t */\n\tif (napi_prefer_busy_poll(n)) {\n\t\tif (napi_complete_done(n, work)) {\n\t\t\t/* If timeout is not set, we need to make sure\n\t\t\t * that the NAPI is re-scheduled.\n\t\t\t */\n\t\t\tnapi_schedule(n);\n\t\t}\n\t\treturn work;\n\t}\n\n\tif (n->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\tgro_normal_list(n);\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\treturn work;\n\t}\n\n\t*repoll = true;\n\n\treturn work;\n}\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tbool do_repoll = false;\n\tvoid *have;\n\tint work;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\twork = __napi_poll(n, &do_repoll);\n\n\tif (do_repoll)\n\t\tlist_add_tail(&n->poll_list, repoll);\n\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic int napi_thread_wait(struct napi_struct *napi)\n{\n\tbool woken = false;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\twhile (!kthread_should_stop()) {\n\t\t/* Testing SCHED_THREADED bit here to make sure the current\n\t\t * kthread owns this napi and could poll on this napi.\n\t\t * Testing SCHED bit is not enough because SCHED bit might be\n\t\t * set by some other busy poll thread or by napi_disable().\n\t\t */\n\t\tif (test_bit(NAPI_STATE_SCHED_THREADED, &napi->state) || woken) {\n\t\t\tWARN_ON(!list_empty(&napi->poll_list));\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn 0;\n\t\t}\n\n\t\tschedule();\n\t\t/* woken being true indicates this thread owns this napi. */\n\t\twoken = true;\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn -1;\n}\n\nstatic void skb_defer_free_flush(struct softnet_data *sd)\n{\n\tstruct sk_buff *skb, *next;\n\n\t/* Paired with WRITE_ONCE() in skb_attempt_defer_free() */\n\tif (!READ_ONCE(sd->defer_list))\n\t\treturn;\n\n\tspin_lock(&sd->defer_lock);\n\tskb = sd->defer_list;\n\tsd->defer_list = NULL;\n\tsd->defer_count = 0;\n\tspin_unlock(&sd->defer_lock);\n\n\twhile (skb != NULL) {\n\t\tnext = skb->next;\n\t\tnapi_consume_skb(skb, 1);\n\t\tskb = next;\n\t}\n}\n\nstatic int napi_threaded_poll(void *data)\n{\n\tstruct napi_struct *napi = data;\n\tstruct softnet_data *sd;\n\tvoid *have;\n\n\twhile (!napi_thread_wait(napi)) {\n\t\tunsigned long last_qs = jiffies;\n\n\t\tfor (;;) {\n\t\t\tbool repoll = false;\n\n\t\t\tlocal_bh_disable();\n\t\t\tsd = this_cpu_ptr(&softnet_data);\n\t\t\tsd->in_napi_threaded_poll = true;\n\n\t\t\thave = netpoll_poll_lock(napi);\n\t\t\t__napi_poll(napi, &repoll);\n\t\t\tnetpoll_poll_unlock(have);\n\n\t\t\tsd->in_napi_threaded_poll = false;\n\t\t\tbarrier();\n\n\t\t\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\t\t\tlocal_irq_disable();\n\t\t\t\tnet_rps_action_and_irq_enable(sd);\n\t\t\t}\n\t\t\tskb_defer_free_flush(sd);\n\t\t\tlocal_bh_enable();\n\n\t\t\tif (!repoll)\n\t\t\t\tbreak;\n\n\t\t\trcu_softirq_qs_periodic(last_qs);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(READ_ONCE(netdev_budget_usecs));\n\tint budget = READ_ONCE(netdev_budget);\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\nstart:\n\tsd->in_net_rx_action = true;\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tskb_defer_free_flush(sd);\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (list_empty(&repoll)) {\n\t\t\t\tsd->in_net_rx_action = false;\n\t\t\t\tbarrier();\n\t\t\t\t/* We need to check if ____napi_schedule()\n\t\t\t\t * had refilled poll_list while\n\t\t\t\t * sd->in_net_rx_action was true.\n\t\t\t\t */\n\t\t\t\tif (!list_empty(&sd->poll_list))\n\t\t\t\t\tgoto start;\n\t\t\t\tif (!sd_has_rps_ipi_waiting(sd))\n\t\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\telse\n\t\tsd->in_net_rx_action = false;\n\n\tnet_rps_action_and_irq_enable(sd);\nend:;\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\tnetdevice_tracker dev_tracker;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* lookup ignore flag */\n\tbool ignore;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int ____netdev_has_upper_dev(struct net_device *upper_dev,\n\t\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct net_device *dev = (struct net_device *)priv->data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t     &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all_rcu - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t       &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nstatic struct net_device *__netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master) && !upper->ignore)\n\t\treturn upper->dev;\n\treturn NULL;\n}\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *__netdev_next_upper_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\t*ignore = upper->ignore;\n\n\treturn upper->dev;\n}\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nstatic int __netdev_walk_all_upper_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = __netdev_next_upper_dev(now, &iter, &ignore);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = netdev_next_upper_dev_rcu(now, &iter);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\nstatic bool __netdev_has_upper_dev(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn __netdev_walk_all_upper_dev(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t   &priv);\n}\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nstatic struct net_device *__netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\t*ignore = lower->ignore;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic int __netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = __netdev_next_lower_dev(now, &iter, &ignore);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_next_lower_dev_rcu);\n\nstatic u8 __netdev_upper_depth(struct net_device *dev)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore);\n\t     udev;\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < udev->upper_level)\n\t\t\tmax_depth = udev->upper_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic u8 __netdev_lower_depth(struct net_device *dev)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore);\n\t     ldev;\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < ldev->lower_level)\n\t\t\tmax_depth = ldev->lower_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic int __netdev_update_upper_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *__unused)\n{\n\tdev->upper_level = __netdev_upper_depth(dev) + 1;\n\treturn 0;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic LIST_HEAD(net_unlink_list);\n\nstatic void net_unlink_todo(struct net_device *dev)\n{\n\tif (list_empty(&dev->unlink_list))\n\t\tlist_add_tail(&dev->unlink_list, &net_unlink_list);\n}\n#endif\n\nstatic int __netdev_update_lower_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tdev->lower_level = __netdev_lower_depth(dev) + 1;\n\n#ifdef CONFIG_LOCKDEP\n\tif (!priv)\n\t\treturn 0;\n\n\tif (priv->flags & NESTED_SYNC_IMM)\n\t\tdev->nested_level = dev->lower_level - 1;\n\tif (priv->flags & NESTED_SYNC_TODO)\n\t\tnet_unlink_todo(dev);\n#endif\n\treturn 0;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev_rcu(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tadj->ignore = false;\n\tnetdev_hold(adj_dev, &adj->dev_tracker, GFP_KERNEL);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree(adj);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info,\n\t\t\t\t   struct netdev_nested_priv *priv,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t\t.extack = extack,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.master = master,\n\t\t.linking = true,\n\t\t.upper_info = upper_info,\n\t};\n\tstruct net_device *master_dev;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)\n\t\treturn -EMLINK;\n\n\tif (!master) {\n\t\tif (__netdev_has_upper_dev(dev, upper_dev))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tmaster_dev = __netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\treturn master_dev == upper_dev ? -EEXIST : -EBUSY;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, false,\n\t\t\t\t       NULL, NULL, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\nstatic void __netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev,\n\t\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.linking = false,\n\t};\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n}\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\t__netdev_upper_dev_unlink(dev, upper_dev, &priv);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\nstatic void __netdev_adjacent_dev_set(struct net_device *upper_dev,\n\t\t\t\t      struct net_device *lower_dev,\n\t\t\t\t      bool val)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(lower_dev, &upper_dev->adj_list.lower);\n\tif (adj)\n\t\tadj->ignore = val;\n\n\tadj = __netdev_find_adj(upper_dev, &lower_dev->adj_list.upper);\n\tif (adj)\n\t\tadj->ignore = val;\n}\n\nstatic void netdev_adjacent_dev_disable(struct net_device *upper_dev,\n\t\t\t\t\tstruct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, true);\n}\n\nstatic void netdev_adjacent_dev_enable(struct net_device *upper_dev,\n\t\t\t\t       struct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, false);\n}\n\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\tint err;\n\n\tif (!new_dev)\n\t\treturn 0;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_disable(dev, old_dev);\n\terr = __netdev_upper_dev_link(new_dev, dev, false, NULL, NULL, &priv,\n\t\t\t\t      extack);\n\tif (err) {\n\t\tif (old_dev && new_dev != old_dev)\n\t\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_adjacent_change_prepare);\n\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev || !old_dev)\n\t\treturn;\n\n\tif (new_dev == old_dev)\n\t\treturn;\n\n\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t__netdev_upper_dev_unlink(old_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_commit);\n\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev)\n\t\treturn;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\n\t__netdev_upper_dev_unlink(new_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_abort);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info info = {\n\t\t.info.dev = dev,\n\t};\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic int netdev_offload_xstats_enable_l3(struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\tint err;\n\tint rc;\n\n\tdev->offload_xstats_l3 = kzalloc(sizeof(*dev->offload_xstats_l3),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!dev->offload_xstats_l3)\n\t\treturn -ENOMEM;\n\n\trc = call_netdevice_notifiers_info_robust(NETDEV_OFFLOAD_XSTATS_ENABLE,\n\t\t\t\t\t\t  NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t\t\t  &info.info);\n\terr = notifier_to_errno(rc);\n\tif (err)\n\t\tgoto free_stats;\n\n\treturn 0;\n\nfree_stats:\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n\treturn err;\n}\n\nint netdev_offload_xstats_enable(struct net_device *dev,\n\t\t\t\t enum netdev_offload_xstats_type type,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn netdev_offload_xstats_enable_l3(dev, extack);\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enable);\n\nstatic void netdev_offload_xstats_disable_l3(struct net_device *dev)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\n\tcall_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t      &info.info);\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n}\n\nint netdev_offload_xstats_disable(struct net_device *dev,\n\t\t\t\t  enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\tif (!netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\tnetdev_offload_xstats_disable_l3(dev);\n\t\treturn 0;\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_disable);\n\nstatic void netdev_offload_xstats_disable_all(struct net_device *dev)\n{\n\tnetdev_offload_xstats_disable(dev, NETDEV_OFFLOAD_XSTATS_TYPE_L3);\n}\n\nstatic struct rtnl_hw_stats64 *\nnetdev_offload_xstats_get_ptr(const struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type)\n{\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn dev->offload_xstats_l3;\n\t}\n\n\tWARN_ON(1);\n\treturn NULL;\n}\n\nbool netdev_offload_xstats_enabled(const struct net_device *dev,\n\t\t\t\t   enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\treturn netdev_offload_xstats_get_ptr(dev, type);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enabled);\n\nstruct netdev_notifier_offload_xstats_ru {\n\tbool used;\n};\n\nstruct netdev_notifier_offload_xstats_rd {\n\tstruct rtnl_hw_stats64 stats;\n\tbool used;\n};\n\nstatic void netdev_hw_stats64_add(struct rtnl_hw_stats64 *dest,\n\t\t\t\t  const struct rtnl_hw_stats64 *src)\n{\n\tdest->rx_packets\t  += src->rx_packets;\n\tdest->tx_packets\t  += src->tx_packets;\n\tdest->rx_bytes\t\t  += src->rx_bytes;\n\tdest->tx_bytes\t\t  += src->tx_bytes;\n\tdest->rx_errors\t\t  += src->rx_errors;\n\tdest->tx_errors\t\t  += src->tx_errors;\n\tdest->rx_dropped\t  += src->rx_dropped;\n\tdest->tx_dropped\t  += src->tx_dropped;\n\tdest->multicast\t\t  += src->multicast;\n}\n\nstatic int netdev_offload_xstats_get_used(struct net_device *dev,\n\t\t\t\t\t  enum netdev_offload_xstats_type type,\n\t\t\t\t\t  bool *p_used,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_ru report_used = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_used = &report_used,\n\t};\n\tint rc;\n\n\tWARN_ON(!netdev_offload_xstats_enabled(dev, type));\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_USED,\n\t\t\t\t\t   &info.info);\n\t*p_used = report_used.used;\n\treturn notifier_to_errno(rc);\n}\n\nstatic int netdev_offload_xstats_get_stats(struct net_device *dev,\n\t\t\t\t\t   enum netdev_offload_xstats_type type,\n\t\t\t\t\t   struct rtnl_hw_stats64 *p_stats,\n\t\t\t\t\t   bool *p_used,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_rd report_delta = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_delta = &report_delta,\n\t};\n\tstruct rtnl_hw_stats64 *stats;\n\tint rc;\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn -EINVAL;\n\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_DELTA,\n\t\t\t\t\t   &info.info);\n\n\t/* Cache whatever we got, even if there was an error, otherwise the\n\t * successful stats retrievals would get lost.\n\t */\n\tnetdev_hw_stats64_add(stats, &report_delta.stats);\n\n\tif (p_stats)\n\t\t*p_stats = *stats;\n\t*p_used = report_delta.used;\n\n\treturn notifier_to_errno(rc);\n}\n\nint netdev_offload_xstats_get(struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t      struct rtnl_hw_stats64 *p_stats, bool *p_used,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (p_stats)\n\t\treturn netdev_offload_xstats_get_stats(dev, type, p_stats,\n\t\t\t\t\t\t       p_used, extack);\n\telse\n\t\treturn netdev_offload_xstats_get_used(dev, type, p_used,\n\t\t\t\t\t\t      extack);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_get);\n\nvoid\nnetdev_offload_xstats_report_delta(struct netdev_notifier_offload_xstats_rd *report_delta,\n\t\t\t\t   const struct rtnl_hw_stats64 *stats)\n{\n\treport_delta->used = true;\n\tnetdev_hw_stats64_add(&report_delta->stats, stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_delta);\n\nvoid\nnetdev_offload_xstats_report_used(struct netdev_notifier_offload_xstats_ru *report_used)\n{\n\treport_used->used = true;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_used);\n\nvoid netdev_offload_xstats_push_delta(struct net_device *dev,\n\t\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t\t      const struct rtnl_hw_stats64 *p_stats)\n{\n\tstruct rtnl_hw_stats64 *stats;\n\n\tASSERT_RTNL();\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn;\n\n\tnetdev_hw_stats64_add(stats, p_stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_push_delta);\n\n/**\n * netdev_get_xmit_slave - Get the xmit slave of master device\n * @dev: device\n * @skb: The packet\n * @all_slaves: assume all the slaves are active\n *\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n * %NULL is returned if no slave is found.\n */\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_xmit_slave)\n\t\treturn NULL;\n\treturn ops->ndo_get_xmit_slave(dev, skb, all_slaves);\n}\nEXPORT_SYMBOL(netdev_get_xmit_slave);\n\nstatic struct net_device *netdev_sk_get_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_sk_get_lower_dev)\n\t\treturn NULL;\n\treturn ops->ndo_sk_get_lower_dev(dev, sk);\n}\n\n/**\n * netdev_sk_get_lowest_dev - Get the lowest device in chain given device and socket\n * @dev: device\n * @sk: the socket\n *\n * %NULL is returned if no lower device is found.\n */\n\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct net_device *lower;\n\n\tlower = netdev_sk_get_lower_dev(dev, sk);\n\twhile (lower) {\n\t\tdev = lower;\n\t\tlower = netdev_sk_get_lower_dev(dev, sk);\n\t}\n\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_sk_get_lowest_dev);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\n/**\n * netdev_lower_state_changed - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info = {\n\t\t.info.dev = lower_dev,\n\t};\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tnetdev_warn(dev, \"promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tnetdev_info(dev, \"%s promiscuous mode\\n\",\n\t\t\t    dev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(audit_context(), GFP_ATOMIC,\n\t\t\t\t  AUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t  \"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\t  dev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t  (old_flags & IFF_PROMISC),\n\t\t\t\t  from_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\t  from_kuid(&init_user_ns, uid),\n\t\t\t\t  from_kgid(&init_user_ns, gid),\n\t\t\t\t  audit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC, 0, NULL);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tnetdev_warn(dev, \"allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tnetdev_info(dev, \"%s allmulticast mode\\n\",\n\t\t\t    dev->flags & IFF_ALLMULTI ? \"entered\" : \"left\");\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags, 0, NULL);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev, extack);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges, u32 portid,\n\t\t\tconst struct nlmsghdr *nlh)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC, portid, nlh);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info = {\n\t\t\t\t.dev = dev,\n\t\t\t},\n\t\t\t.flags_changed = changes,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\t@extack: netlink extended ack\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes, 0, NULL);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\t/* Pairs with all the lockless reads of dev->mtu in the stack */\n\tWRITE_ONCE(dev->mtu, new_mtu);\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\nint dev_validate_mtu(struct net_device *dev, int new_mtu,\n\t\t     struct netlink_ext_ack *extack)\n{\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu less than device minimum\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu greater than device maximum\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu_ext - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\t@extack: netlink extended ack\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu_ext(struct net_device *dev, int new_mtu,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\terr = dev_validate_mtu(dev, new_mtu, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t   orig_mtu);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t     new_mtu);\n\t\t}\n\t}\n\treturn err;\n}\n\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct netlink_ext_ack extack;\n\tint err;\n\n\tmemset(&extack, 0, sizeof(extack));\n\terr = dev_set_mtu_ext(dev, new_mtu, &extack);\n\tif (err && extack._msg)\n\t\tnet_err_ratelimited(\"%s: %s\\n\", dev->name, extack._msg);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_change_tx_queue_len - Change TX queue length of a netdevice\n *\t@dev: device\n *\t@new_len: new tx queue length\n */\nint dev_change_tx_queue_len(struct net_device *dev, unsigned long new_len)\n{\n\tunsigned int orig_len = dev->tx_queue_len;\n\tint res;\n\n\tif (new_len != (unsigned int)new_len)\n\t\treturn -ERANGE;\n\n\tif (new_len != orig_len) {\n\t\tdev->tx_queue_len = new_len;\n\t\tres = call_netdevice_notifiers(NETDEV_CHANGE_TX_QUEUE_LEN, dev);\n\t\tres = notifier_to_errno(res);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t\tres = dev_qdisc_change_tx_queue_len(dev);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t}\n\n\treturn 0;\n\nerr_rollback:\n\tnetdev_err(dev, \"refused to change device tx_queue_len\\n\");\n\tdev->tx_queue_len = orig_len;\n\treturn res;\n}\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\n\n/**\n *\tdev_pre_changeaddr_notify - Call NETDEV_PRE_CHANGEADDR.\n *\t@dev: device\n *\t@addr: new address\n *\t@extack: netlink extended ack\n */\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_pre_changeaddr_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.dev_addr = addr,\n\t};\n\tint rc;\n\n\trc = call_netdevice_notifiers_info(NETDEV_PRE_CHANGEADDR, &info.info);\n\treturn notifier_to_errno(rc);\n}\nEXPORT_SYMBOL(dev_pre_changeaddr_notify);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\t@extack: netlink extended ack\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = dev_pre_changeaddr_notify(dev, sa->sa_data, extack);\n\tif (err)\n\t\treturn err;\n\tif (memcmp(dev->dev_addr, sa->sa_data, dev->addr_len)) {\n\t\terr = ops->ndo_set_mac_address(dev, sa);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\nstatic DECLARE_RWSEM(dev_addr_sem);\n\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tdown_write(&dev_addr_sem);\n\tret = dev_set_mac_address(dev, sa, extack);\n\tup_write(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_set_mac_address_user);\n\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name)\n{\n\tsize_t size = sizeof(sa->sa_data_min);\n\tstruct net_device *dev;\n\tint ret = 0;\n\n\tdown_read(&dev_addr_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_name_rcu(net, dev_name);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!dev->addr_len)\n\t\tmemset(sa->sa_data, 0, size);\n\telse\n\t\tmemcpy(sa->sa_data, dev->dev_addr,\n\t\t       min_t(size_t, size, dev->addr_len));\n\tsa->sa_family = dev->type;\n\nunlock:\n\trcu_read_unlock();\n\tup_read(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (ops->ndo_get_phys_port_name) {\n\t\terr = ops->ndo_get_phys_port_name(dev, name, len);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\treturn devlink_compat_phys_port_name_get(dev, name, len);\n}\n\n/**\n *\tdev_get_port_parent_id - Get the device's port parent identifier\n *\t@dev: network device\n *\t@ppid: pointer to a storage for the port's parent identifier\n *\t@recurse: allow/disallow recursion to lower devices\n *\n *\tGet the devices's port parent identifier\n */\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid,\n\t\t\t   bool recurse)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct netdev_phys_item_id first = { };\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\tint err;\n\n\tif (ops->ndo_get_port_parent_id) {\n\t\terr = ops->ndo_get_port_parent_id(dev, ppid);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\n\terr = devlink_compat_switch_id_get(dev, ppid);\n\tif (!recurse || err != -EOPNOTSUPP)\n\t\treturn err;\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter) {\n\t\terr = dev_get_port_parent_id(lower_dev, ppid, true);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!first.id_len)\n\t\t\tfirst = *ppid;\n\t\telse if (memcmp(&first, ppid, sizeof(*ppid)))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_get_port_parent_id);\n\n/**\n *\tnetdev_port_same_parent_id - Indicate if two network devices have\n *\tthe same port parent identifier\n *\t@a: first network device\n *\t@b: second network device\n */\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)\n{\n\tstruct netdev_phys_item_id a_id = { };\n\tstruct netdev_phys_item_id b_id = { };\n\n\tif (dev_get_port_parent_id(a, &a_id, true) ||\n\t    dev_get_port_parent_id(b, &b_id, true))\n\t\treturn false;\n\n\treturn netdev_phys_item_id_same(&a_id, &b_id);\n}\nEXPORT_SYMBOL(netdev_port_same_parent_id);\n\n/**\n *\tdev_change_proto_down - set carrier according to proto_down.\n *\n *\t@dev: device\n *\t@proto_down: new value\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tif (!(dev->priv_flags & IFF_CHANGE_PROTO_DOWN))\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\tif (proto_down)\n\t\tnetif_carrier_off(dev);\n\telse\n\t\tnetif_carrier_on(dev);\n\tdev->proto_down = proto_down;\n\treturn 0;\n}\n\n/**\n *\tdev_change_proto_down_reason - proto down reason\n *\n *\t@dev: device\n *\t@mask: proto down mask\n *\t@value: proto down value\n */\nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value)\n{\n\tint b;\n\n\tif (!mask) {\n\t\tdev->proto_down_reason = value;\n\t} else {\n\t\tfor_each_set_bit(b, &mask, 32) {\n\t\t\tif (value & (1 << b))\n\t\t\t\tdev->proto_down_reason |= BIT(b);\n\t\t\telse\n\t\t\t\tdev->proto_down_reason &= ~BIT(b);\n\t\t}\n\t}\n}\n\nstruct bpf_xdp_link {\n\tstruct bpf_link link;\n\tstruct net_device *dev; /* protected by rtnl_lock, no refcnt held */\n\tint flags;\n};\n\nstatic enum bpf_xdp_mode dev_xdp_mode(struct net_device *dev, u32 flags)\n{\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\treturn XDP_MODE_HW;\n\tif (flags & XDP_FLAGS_DRV_MODE)\n\t\treturn XDP_MODE_DRV;\n\tif (flags & XDP_FLAGS_SKB_MODE)\n\t\treturn XDP_MODE_SKB;\n\treturn dev->netdev_ops->ndo_bpf ? XDP_MODE_DRV : XDP_MODE_SKB;\n}\n\nstatic bpf_op_t dev_xdp_bpf_op(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tswitch (mode) {\n\tcase XDP_MODE_SKB:\n\t\treturn generic_xdp_install;\n\tcase XDP_MODE_DRV:\n\tcase XDP_MODE_HW:\n\t\treturn dev->netdev_ops->ndo_bpf;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_xdp_link *dev_xdp_link(struct net_device *dev,\n\t\t\t\t\t enum bpf_xdp_mode mode)\n{\n\treturn dev->xdp_state[mode].link;\n}\n\nstatic struct bpf_prog *dev_xdp_prog(struct net_device *dev,\n\t\t\t\t     enum bpf_xdp_mode mode)\n{\n\tstruct bpf_xdp_link *link = dev_xdp_link(dev, mode);\n\n\tif (link)\n\t\treturn link->link.prog;\n\treturn dev->xdp_state[mode].prog;\n}\n\nu8 dev_xdp_prog_count(struct net_device *dev)\n{\n\tu8 count = 0;\n\tint i;\n\n\tfor (i = 0; i < __MAX_XDP_MODE; i++)\n\t\tif (dev->xdp_state[i].prog || dev->xdp_state[i].link)\n\t\t\tcount++;\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(dev_xdp_prog_count);\n\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tstruct bpf_prog *prog = dev_xdp_prog(dev, mode);\n\n\treturn prog ? prog->aux->id : 0;\n}\n\nstatic void dev_xdp_set_link(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_xdp_link *link)\n{\n\tdev->xdp_state[mode].link = link;\n\tdev->xdp_state[mode].prog = NULL;\n}\n\nstatic void dev_xdp_set_prog(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_prog *prog)\n{\n\tdev->xdp_state[mode].link = NULL;\n\tdev->xdp_state[mode].prog = prog;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t   bpf_op_t bpf_op, struct netlink_ext_ack *extack,\n\t\t\t   u32 flags, struct bpf_prog *prog)\n{\n\tstruct netdev_bpf xdp;\n\tint err;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = mode == XDP_MODE_HW ? XDP_SETUP_PROG_HW : XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\t/* Drivers assume refcnt is already incremented (i.e, prog pointer is\n\t * \"moved\" into driver), so they don't increment it on their own, but\n\t * they do decrement refcnt when program is detached or replaced.\n\t * Given net_device also owns link/prog, we need to bump refcnt here\n\t * to prevent drivers from underflowing it.\n\t */\n\tif (prog)\n\t\tbpf_prog_inc(prog);\n\terr = bpf_op(dev, &xdp);\n\tif (err) {\n\t\tif (prog)\n\t\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\tif (mode != XDP_MODE_HW)\n\t\tbpf_prog_change_xdp(dev_xdp_prog(dev, mode), prog);\n\n\treturn 0;\n}\n\nstatic void dev_xdp_uninstall(struct net_device *dev)\n{\n\tstruct bpf_xdp_link *link;\n\tstruct bpf_prog *prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tfor (mode = XDP_MODE_SKB; mode < __MAX_XDP_MODE; mode++) {\n\t\tprog = dev_xdp_prog(dev, mode);\n\t\tif (!prog)\n\t\t\tcontinue;\n\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op)\n\t\t\tcontinue;\n\n\t\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\n\t\t/* auto-detach link from net device */\n\t\tlink = dev_xdp_link(dev, mode);\n\t\tif (link)\n\t\t\tlink->dev = NULL;\n\t\telse\n\t\t\tbpf_prog_put(prog);\n\n\t\tdev_xdp_set_link(dev, mode, NULL);\n\t}\n}\n\nstatic int dev_xdp_attach(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t\t  struct bpf_xdp_link *link, struct bpf_prog *new_prog,\n\t\t\t  struct bpf_prog *old_prog, u32 flags)\n{\n\tunsigned int num_modes = hweight32(flags & XDP_FLAGS_MODES);\n\tstruct bpf_prog *cur_prog;\n\tstruct net_device *upper;\n\tstruct list_head *iter;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* either link or prog attachment, never both */\n\tif (link && (new_prog || old_prog))\n\t\treturn -EINVAL;\n\t/* link supports only XDP mode flags */\n\tif (link && (flags & ~XDP_FLAGS_MODES)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid XDP flags for BPF link attachment\");\n\t\treturn -EINVAL;\n\t}\n\t/* just one XDP mode bit should be set, zero defaults to drv/skb mode */\n\tif (num_modes > 1) {\n\t\tNL_SET_ERR_MSG(extack, \"Only one XDP mode flag can be set\");\n\t\treturn -EINVAL;\n\t}\n\t/* avoid ambiguity if offload + drv/skb mode progs are both loaded */\n\tif (!num_modes && dev_xdp_prog_count(dev) > 1) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"More than one program loaded, unset mode is ambiguous\");\n\t\treturn -EINVAL;\n\t}\n\t/* old_prog != NULL implies XDP_FLAGS_REPLACE is set */\n\tif (old_prog && !(flags & XDP_FLAGS_REPLACE)) {\n\t\tNL_SET_ERR_MSG(extack, \"XDP_FLAGS_REPLACE is not specified\");\n\t\treturn -EINVAL;\n\t}\n\n\tmode = dev_xdp_mode(dev, flags);\n\t/* can't replace attached link */\n\tif (dev_xdp_link(dev, mode)) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active BPF XDP link\");\n\t\treturn -EBUSY;\n\t}\n\n\t/* don't allow if an upper device already has a program */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter) {\n\t\tif (dev_xdp_prog_count(upper) > 0) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot attach when an upper device already has a program\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\tcur_prog = dev_xdp_prog(dev, mode);\n\t/* can't replace attached prog with link */\n\tif (link && cur_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active XDP program with BPF link\");\n\t\treturn -EBUSY;\n\t}\n\tif ((flags & XDP_FLAGS_REPLACE) && cur_prog != old_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Active program does not match expected\");\n\t\treturn -EEXIST;\n\t}\n\n\t/* put effective new program into new_prog */\n\tif (link)\n\t\tnew_prog = link->link.prog;\n\n\tif (new_prog) {\n\t\tbool offload = mode == XDP_MODE_HW;\n\t\tenum bpf_xdp_mode other_mode = mode == XDP_MODE_SKB\n\t\t\t\t\t       ? XDP_MODE_DRV : XDP_MODE_SKB;\n\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) && cur_prog) {\n\t\t\tNL_SET_ERR_MSG(extack, \"XDP program already attached\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (!offload && dev_xdp_prog(dev, other_mode)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Native and generic XDP can't be active at the same time\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (!offload && bpf_prog_is_offloaded(new_prog->aux)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Using offloaded program without HW_MODE flag is not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bpf_prog_is_dev_bound(new_prog->aux) && !bpf_offload_dev_match(new_prog, dev)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Program bound to different device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_DEVMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_DEVMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_CPUMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_CPUMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* don't call drivers if the effective program didn't change */\n\tif (new_prog != cur_prog) {\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Underlying driver does not support XDP in native mode\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\terr = dev_xdp_install(dev, mode, bpf_op, extack, flags, new_prog);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (link)\n\t\tdev_xdp_set_link(dev, mode, link);\n\telse\n\t\tdev_xdp_set_prog(dev, mode, new_prog);\n\tif (cur_prog)\n\t\tbpf_prog_put(cur_prog);\n\n\treturn 0;\n}\n\nstatic int dev_xdp_attach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\treturn dev_xdp_attach(dev, extack, link, NULL, NULL, link->flags);\n}\n\nstatic int dev_xdp_detach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tmode = dev_xdp_mode(dev, link->flags);\n\tif (dev_xdp_link(dev, mode) != link)\n\t\treturn -EINVAL;\n\n\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\tdev_xdp_set_link(dev, mode, NULL);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\trtnl_lock();\n\n\t/* if racing with net_device's tear down, xdp_link->dev might be\n\t * already NULL, in which case link was already auto-detached\n\t */\n\tif (xdp_link->dev) {\n\t\tWARN_ON(dev_xdp_detach_link(xdp_link->dev, NULL, xdp_link));\n\t\txdp_link->dev = NULL;\n\t}\n\n\trtnl_unlock();\n}\n\nstatic int bpf_xdp_link_detach(struct bpf_link *link)\n{\n\tbpf_xdp_link_release(link);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\tkfree(xdp_link);\n}\n\nstatic void bpf_xdp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t     struct seq_file *seq)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tseq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);\n}\n\nstatic int bpf_xdp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t       struct bpf_link_info *info)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tinfo->xdp.ifindex = ifindex;\n\treturn 0;\n}\n\nstatic int bpf_xdp_link_update(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t       struct bpf_prog *old_prog)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err = 0;\n\n\trtnl_lock();\n\n\t/* link might have been auto-released already, so fail */\n\tif (!xdp_link->dev) {\n\t\terr = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog && link->prog != old_prog) {\n\t\terr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\told_prog = link->prog;\n\tif (old_prog->type != new_prog->type ||\n\t    old_prog->expected_attach_type != new_prog->expected_attach_type) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog == new_prog) {\n\t\t/* no-op, don't disturb drivers */\n\t\tbpf_prog_put(new_prog);\n\t\tgoto out_unlock;\n\t}\n\n\tmode = dev_xdp_mode(xdp_link->dev, xdp_link->flags);\n\tbpf_op = dev_xdp_bpf_op(xdp_link->dev, mode);\n\terr = dev_xdp_install(xdp_link->dev, mode, bpf_op, NULL,\n\t\t\t      xdp_link->flags, new_prog);\n\tif (err)\n\t\tgoto out_unlock;\n\n\told_prog = xchg(&link->prog, new_prog);\n\tbpf_prog_put(old_prog);\n\nout_unlock:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_xdp_link_lops = {\n\t.release = bpf_xdp_link_release,\n\t.dealloc = bpf_xdp_link_dealloc,\n\t.detach = bpf_xdp_link_detach,\n\t.show_fdinfo = bpf_xdp_link_show_fdinfo,\n\t.fill_link_info = bpf_xdp_link_fill_link_info,\n\t.update_prog = bpf_xdp_link_update,\n};\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_link_primer link_primer;\n\tstruct netlink_ext_ack extack = {};\n\tstruct bpf_xdp_link *link;\n\tstruct net_device *dev;\n\tint err, fd;\n\n\trtnl_lock();\n\tdev = dev_get_by_index(net, attr->link_create.target_ifindex);\n\tif (!dev) {\n\t\trtnl_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_XDP, &bpf_xdp_link_lops, prog);\n\tlink->dev = dev;\n\tlink->flags = attr->link_create.flags;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto unlock;\n\t}\n\n\terr = dev_xdp_attach_link(dev, &extack, link);\n\trtnl_unlock();\n\n\tif (err) {\n\t\tlink->dev = NULL;\n\t\tbpf_link_cleanup(&link_primer);\n\t\ttrace_bpf_xdp_link_attach_failed(extack._msg);\n\t\tgoto out_put_dev;\n\t}\n\n\tfd = bpf_link_settle(&link_primer);\n\t/* link itself doesn't hold dev's refcnt to not complicate shutdown */\n\tdev_put(dev);\n\treturn fd;\n\nunlock:\n\trtnl_unlock();\n\nout_put_dev:\n\tdev_put(dev);\n\treturn err;\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@expected_fd: old program fd that userspace expects to replace or clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags)\n{\n\tenum bpf_xdp_mode mode = dev_xdp_mode(dev, flags);\n\tstruct bpf_prog *new_prog = NULL, *old_prog = NULL;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (fd >= 0) {\n\t\tnew_prog = bpf_prog_get_type_dev(fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(new_prog))\n\t\t\treturn PTR_ERR(new_prog);\n\t}\n\n\tif (expected_fd >= 0) {\n\t\told_prog = bpf_prog_get_type_dev(expected_fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\terr = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = dev_xdp_attach(dev, extack, NULL, new_prog, old_prog, flags);\n\nerr_out:\n\tif (err && new_prog)\n\t\tbpf_prog_put(new_prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\treturn err;\n}\n\n/**\n * dev_index_reserve() - allocate an ifindex in a namespace\n * @net: the applicable net namespace\n * @ifindex: requested ifindex, pass %0 to get one allocated\n *\n * Allocate a ifindex for a new device. Caller must either use the ifindex\n * to store the device (via list_netdevice()) or call dev_index_release()\n * to give the index up.\n *\n * Return: a suitable unique value for a new device interface number or -errno.\n */\nstatic int dev_index_reserve(struct net *net, u32 ifindex)\n{\n\tint err;\n\n\tif (ifindex > INT_MAX) {\n\t\tDEBUG_NET_WARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ifindex)\n\t\terr = xa_alloc_cyclic(&net->dev_by_index, &ifindex, NULL,\n\t\t\t\t      xa_limit_31b, &net->ifindex, GFP_KERNEL);\n\telse\n\t\terr = xa_insert(&net->dev_by_index, ifindex, NULL, GFP_KERNEL);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn ifindex;\n}\n\nstatic void dev_index_release(struct net *net, int ifindex)\n{\n\t/* Expect only unused indexes, unlist_netdevice() removes the used */\n\tWARN_ON(xa_erase(&net->dev_by_index, ifindex));\n}\n\n/* Delayed registration/unregisteration */\nLIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tatomic_inc(&dev_net(dev)->dev_unreg_count);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\t__netdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t\telse\n\t\t\t\tnetdev_features_change(lower);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\tif (!(features & NETIF_F_RXCSUM)) {\n\t\t/* NETIF_F_GRO_HW implies doing RXCSUM since every packet\n\t\t * successfully merged by hardware must also have the\n\t\t * checksum verified by hardware.  If the user does not\n\t\t * want to enable RXCSUM, logically, we should disable GRO_HW.\n\t\t */\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GRO_HW since no RXCSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\t/* LRO/HW-GRO features cannot be combined with RX-FCS */\n\tif (features & NETIF_F_RXFCS) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_dbg(dev, \"Dropping LRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping HW-GRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_GRO_HW) && (features & NETIF_F_LRO)) {\n\t\tnetdev_dbg(dev, \"Dropping LRO feature since HW-GRO is requested.\\n\");\n\t\tfeatures &= ~NETIF_F_LRO;\n\t}\n\n\tif (features & NETIF_F_HW_TLS_TX) {\n\t\tbool ip_csum = (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) ==\n\t\t\t(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\tbool hw_csum = features & NETIF_F_HW_CSUM;\n\n\t\tif (!ip_csum && !hw_csum) {\n\t\t\tnetdev_dbg(dev, \"Dropping TLS TX HW offload feature since no CSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_RX) && !(features & NETIF_F_RXCSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS RX HW offload feature since no RXCSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off on an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_ctag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_ctag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_stag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_stag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (rootdev->operstate == IF_OPER_TESTING)\n\t\tnetif_testing_on(dev);\n\telse\n\t\tnetif_testing_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\tint err = 0;\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++) {\n\t\trx[i].dev = dev;\n\n\t\t/* XDP RX-queue setup */\n\t\terr = xdp_rxq_info_reg(&rx[i].xdp_rxq, dev, i, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_info;\n\t}\n\treturn 0;\n\nerr_rxq_info:\n\t/* Rollback successful reg's and free other resources */\n\twhile (i--)\n\t\txdp_rxq_info_unreg(&rx[i].xdp_rxq);\n\tkvfree(dev->_rx);\n\tdev->_rx = NULL;\n\treturn err;\n}\n\nstatic void netif_free_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\n\t/* netif_alloc_rx_queues alloc failed, resources have been unreg'ed */\n\tif (!dev->_rx)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++)\n\t\txdp_rxq_info_unreg(&dev->_rx[i].xdp_rxq);\n\n\tkvfree(dev->_rx);\n}\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\nstatic int netdev_do_alloc_pcpu_stats(struct net_device *dev)\n{\n\tvoid __percpu *v;\n\n\t/* Drivers implementing ndo_get_peer_dev must support tstat\n\t * accounting, so that skb_do_redirect() can bump the dev's\n\t * RX stats upon network namespace switch.\n\t */\n\tif (dev->netdev_ops->ndo_get_peer_dev &&\n\t    dev->pcpu_stat_type != NETDEV_PCPU_STAT_TSTATS)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn 0;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tv = dev->lstats = netdev_alloc_pcpu_stats(struct pcpu_lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tv = dev->tstats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tv = dev->dstats = netdev_alloc_pcpu_stats(struct pcpu_dstats);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn v ? 0 : -ENOMEM;\n}\n\nstatic void netdev_do_free_pcpu_stats(struct net_device *dev)\n{\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tfree_percpu(dev->lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tfree_percpu(dev->tstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tfree_percpu(dev->dstats);\n\t\tbreak;\n\t}\n}\n\n/**\n * register_netdevice() - register a network device\n * @dev: device to register\n *\n * Take a prepared network device structure and make it externally accessible.\n * A %NETDEV_REGISTER message is sent to the netdev notifier chain.\n * Callers must hold the rtnl lock - you may want register_netdev()\n * instead of this.\n */\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <\n\t\t     NETDEV_FEATURE_COUNT);\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tret = ethtool_check_ops(dev->ethtool_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tdev->name_node = netdev_name_node_head_alloc(dev);\n\tif (!dev->name_node)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto err_free_name;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = netdev_do_alloc_pcpu_stats(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = dev_index_reserve(net, dev->ifindex);\n\tif (ret < 0)\n\t\tgoto err_free_pcpu;\n\tdev->ifindex = ret;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= (NETIF_F_SOFT_FEATURES | NETIF_F_SOFT_FEATURES_OFF);\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->udp_tunnel_nic_info) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_ifindex_release;\n\n\tret = netdev_register_kobject(dev);\n\twrite_lock(&dev_base_lock);\n\tdev->reg_state = ret ? NETREG_UNREGISTERED : NETREG_REGISTERED;\n\twrite_unlock(&dev_base_lock);\n\tif (ret)\n\t\tgoto err_uninit_notify;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\n\tnetdev_hold(dev, &dev->dev_registered_tracker, GFP_KERNEL);\n\tlist_netdevice(dev);\n\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\t/* Expect explicit free_netdev() on failure */\n\t\tdev->needs_free_netdev = false;\n\t\tunregister_netdevice_queue(dev, NULL);\n\t\tgoto out;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);\n\nout:\n\treturn ret;\n\nerr_uninit_notify:\n\tcall_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);\nerr_ifindex_release:\n\tdev_index_release(net, dev->ifindex);\nerr_free_pcpu:\n\tnetdev_do_free_pcpu_stats(dev);\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\nerr_free_name:\n\tnetdev_name_node_free(dev->name_node);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* napi_busy_loop stats accounting wants this */\n\tdev_net_set(dev, &init_net);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\tif (rtnl_lock_killable())\n\t\treturn -EINTR;\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n#else\n\treturn refcount_read(&dev->dev_refcnt);\n#endif\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\nint netdev_unregister_timeout_secs __read_mostly = 10;\n\n#define WAIT_REFS_MIN_MSECS 1\n#define WAIT_REFS_MAX_MSECS 250\n/**\n * netdev_wait_allrefs_any - wait until all references are gone.\n * @list: list of net_devices to wait on\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic struct net_device *netdev_wait_allrefs_any(struct list_head *list)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tstruct net_device *dev;\n\tint wait = 0;\n\n\trebroadcast_time = warning_time = jiffies;\n\n\tlist_for_each_entry(dev, list, todo_list)\n\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\treturn dev;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t\t     &dev->state)) {\n\t\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t\t * pending on unregister. If this\n\t\t\t\t\t * happens, we simply run the queue\n\t\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t\t * for this device.\n\t\t\t\t\t */\n\t\t\t\t\tlinkwatch_run_queue();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\trcu_barrier();\n\n\t\tif (!wait) {\n\t\t\twait = WAIT_REFS_MIN_MSECS;\n\t\t} else {\n\t\t\tmsleep(wait);\n\t\t\twait = min(wait << 1, WAIT_REFS_MAX_MSECS);\n\t\t}\n\n\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\t\treturn dev;\n\n\t\tif (time_after(jiffies, warning_time +\n\t\t\t       READ_ONCE(netdev_unregister_timeout_secs) * HZ)) {\n\t\t\tlist_for_each_entry(dev, list, todo_list) {\n\t\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t\t dev->name, netdev_refcnt_read(dev));\n\t\t\t\tref_tracker_dir_print(&dev->refcnt_tracker, 10);\n\t\t\t}\n\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct net_device *dev, *tmp;\n\tstruct list_head list;\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head unlink_list;\n\n\tlist_replace_init(&net_unlink_list, &unlink_list);\n\n\twhile (!list_empty(&unlink_list)) {\n\t\tstruct net_device *dev = list_first_entry(&unlink_list,\n\t\t\t\t\t\t\t  struct net_device,\n\t\t\t\t\t\t\t  unlink_list);\n\t\tlist_del_init(&dev->unlink_list);\n\t\tdev->nested_level = dev->lower_level - 1;\n\t}\n#endif\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\tlist_for_each_entry_safe(dev, tmp, &list, todo_list) {\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tnetdev_WARN(dev, \"run_todo but not unregistering\\n\");\n\t\t\tlist_del(&dev->todo_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\twrite_lock(&dev_base_lock);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t\twrite_unlock(&dev_base_lock);\n\t\tlinkwatch_forget_dev(dev);\n\t}\n\n\twhile (!list_empty(&list)) {\n\t\tdev = netdev_wait_allrefs_any(&list);\n\t\tlist_del(&dev->todo_list);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev) != 1);\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\n\t\tnetdev_do_free_pcpu_stats(dev);\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\tif (atomic_dec_and_test(&dev_net(dev)->dev_unreg_count))\n\t\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(atomic_long_t);\n\tconst atomic_long_t *src = (atomic_long_t *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = (unsigned long)atomic_long_read(&src[i]);\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\nstruct net_device_core_stats __percpu *netdev_core_stats_alloc(struct net_device *dev)\n{\n\tstruct net_device_core_stats __percpu *p;\n\n\tp = alloc_percpu_gfp(struct net_device_core_stats,\n\t\t\t     GFP_ATOMIC | __GFP_NOWARN);\n\n\tif (p && cmpxchg(&dev->core_stats, NULL, p))\n\t\tfree_percpu(p);\n\n\t/* This READ_ONCE() pairs with the cmpxchg() above */\n\treturn READ_ONCE(dev->core_stats);\n}\nEXPORT_SYMBOL(netdev_core_stats_alloc);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tconst struct net_device_core_stats __percpu *p;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\n\t/* This READ_ONCE() pairs with the write in netdev_core_stats_alloc() */\n\tp = READ_ONCE(dev->core_stats);\n\tif (p) {\n\t\tconst struct net_device_core_stats *core_stats;\n\t\tint i;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tcore_stats = per_cpu_ptr(p, i);\n\t\t\tstorage->rx_dropped += READ_ONCE(core_stats->rx_dropped);\n\t\t\tstorage->tx_dropped += READ_ONCE(core_stats->tx_dropped);\n\t\t\tstorage->rx_nohandler += READ_ONCE(core_stats->rx_nohandler);\n\t\t\tstorage->rx_otherhost_dropped += READ_ONCE(core_stats->rx_otherhost_dropped);\n\t\t}\n\t}\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\n/**\n *\tdev_fetch_sw_netstats - get per-cpu network device statistics\n *\t@s: place to store stats\n *\t@netstats: per-cpu network stats to read from\n *\n *\tRead per-cpu network statistics and populate the related fields in @s.\n */\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tu64 rx_packets, rx_bytes, tx_packets, tx_bytes;\n\t\tconst struct pcpu_sw_netstats *stats;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(netstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\trx_packets = u64_stats_read(&stats->rx_packets);\n\t\t\trx_bytes   = u64_stats_read(&stats->rx_bytes);\n\t\t\ttx_packets = u64_stats_read(&stats->tx_packets);\n\t\t\ttx_bytes   = u64_stats_read(&stats->tx_bytes);\n\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\ts->rx_packets += rx_packets;\n\t\ts->rx_bytes   += rx_bytes;\n\t\ts->tx_packets += tx_packets;\n\t\ts->tx_bytes   += tx_bytes;\n\t}\n}\nEXPORT_SYMBOL_GPL(dev_fetch_sw_netstats);\n\n/**\n *\tdev_get_tstats64 - ndo_get_stats64 implementation\n *\t@dev: device to get statistics from\n *\t@s: place to store stats\n *\n *\tPopulate @s from dev->stats and dev->tstats. Can be used as\n *\tndo_get_stats64() callback.\n */\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_sw_netstats(s, dev->tstats);\n}\nEXPORT_SYMBOL_GPL(dev_get_tstats64);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tRCU_INIT_POINTER(queue->qdisc_sleeping, &noop_qdisc);\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\n/**\n * netdev_sw_irq_coalesce_default_on() - enable SW IRQ coalescing by default\n * @dev: netdev to enable the IRQ coalescing on\n *\n * Sets a conservative default for SW IRQ coalescing. Users can use\n * sysfs attributes to override the default values.\n */\nvoid netdev_sw_irq_coalesce_default_on(struct net_device *dev)\n{\n\tWARN_ON(dev->reg_state == NETREG_REGISTERED);\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\tdev->gro_flush_timeout = 20000;\n\t\tdev->napi_defer_hard_irqs = 1;\n\t}\n}\nEXPORT_SYMBOL_GPL(netdev_sw_irq_coalesce_default_on);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tunsigned int alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tref_tracker_dir_init(&dev->refcnt_tracker, 128, name);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\t__dev_hold(dev);\n#else\n\trefcount_set(&dev->dev_refcnt, 1);\n#endif\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->xdp_zc_max_segs = 1;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->gro_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->gso_ipv4_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->gro_ipv4_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->tso_max_size = TSO_LEGACY_MAX_SIZE;\n\tdev->tso_max_segs = TSO_MAX_SEGS;\n\tdev->upper_level = 1;\n\tdev->lower_level = 1;\n#ifdef CONFIG_LOCKDEP\n\tdev->nested_level = 0;\n\tINIT_LIST_HEAD(&dev->unlink_list);\n#endif\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tINIT_LIST_HEAD(&dev->net_notifier_list);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_netdev_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n#endif\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\n\t/* When called immediately after register_netdevice() failed the unwind\n\t * handling may still be dismantling the device. Handle that case by\n\t * deferring the free.\n\t */\n\tif (dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\t\tdev->needs_free_netdev = true;\n\t\treturn;\n\t}\n\n\tnetif_free_tx_queues(dev);\n\tnetif_free_rx_queues(dev);\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tref_tracker_dir_exit(&dev->refcnt_tracker);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n#endif\n\tfree_percpu(dev->core_stats);\n\tdev->core_stats = NULL;\n\tfree_percpu(dev->xdp_bulkq);\n\tdev->xdp_bulkq = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->unreg_list, &single);\n\t\tunregister_netdevice_many(&single);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\nvoid unregister_netdevice_many_notify(struct list_head *head,\n\t\t\t\t      u32 portid, const struct nlmsghdr *nlh)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\twrite_lock(&dev_base_lock);\n\t\tunlist_netdevice(dev, false);\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t\twrite_unlock(&dev_base_lock);\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\t\tdev_tcx_uninstall(dev);\n\t\tdev_xdp_uninstall(dev);\n\t\tbpf_dev_bound_netdev_unregister(dev);\n\n\t\tnetdev_offload_xstats_disable_all(dev);\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL, NULL, 0,\n\t\t\t\t\t\t     portid, nlh);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tnetdev_name_node_alt_flush(dev);\n\t\tnetdev_name_node_free(dev->name_node);\n\n\t\tcall_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL, portid, nlh);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tnetdev_put(dev, &dev->dev_registered_tracker);\n\t\tnet_set_todo(dev);\n\t}\n\n\tlist_del(head);\n}\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tunregister_netdevice_many_notify(head, 0, NULL);\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\t__dev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\t@new_ifindex: If not zero, specifies device index in the target\n *\t              namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint __dev_change_net_namespace(struct net_device *dev, struct net *net,\n\t\t\t       const char *pat, int new_ifindex)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net_old = dev_net(dev);\n\tchar new_name[IFNAMSIZ] = {};\n\tint err, new_nsid;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(net_old, net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (netdev_name_in_use(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\terr = dev_prep_valid_name(net, dev, pat, new_name);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\t/* Check that none of the altnames conflicts. */\n\terr = -EEXIST;\n\tnetdev_for_each_altname(dev, name_node)\n\t\tif (netdev_name_in_use(net, name_node->name))\n\t\t\tgoto out;\n\n\t/* Check that new_ifindex isn't used yet. */\n\tif (new_ifindex) {\n\t\terr = dev_index_reserve(net, new_ifindex);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t} else {\n\t\t/* If there is an ifindex conflict assign a new one */\n\t\terr = dev_index_reserve(net, dev->ifindex);\n\t\tif (err == -EBUSY)\n\t\t\terr = dev_index_reserve(net, 0);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tnew_ifindex = err;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\tunlist_netdevice(dev, true);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\n\tnew_nsid = peernet2id_alloc(dev_net(dev), net, GFP_KERNEL);\n\n\trtmsg_ifinfo_newnet(RTM_DELLINK, dev, ~0U, GFP_KERNEL, &new_nsid,\n\t\t\t    new_ifindex);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Move per-net netdevice notifiers that are following the netdevice */\n\tmove_netdevice_notifiers_dev_net(dev, net);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\tdev->ifindex = new_ifindex;\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\tif (new_name[0]) /* Rename the netdev to prepared name */\n\t\tstrscpy(dev->name, new_name, IFNAMSIZ);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Adapt owner in case owning user namespace of target network\n\t * namespace is different from the original one.\n\t */\n\terr = netdev_change_owner(dev, net_old, net);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(__dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t/* send out pending IPI's on offline CPU */\n\tnet_rps_send_ipi(remsd);\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc_array(NETDEV_HASHENTRIES, sizeof(*hash), GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tBUILD_BUG_ON(GRO_HASH_BUCKETS >\n\t\t     8 * sizeof_field(struct napi_struct, gro_bitmask));\n\n\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\txa_init_flags(&net->dev_by_index, XA_FLAGS_ALLOC1);\n\n\tRAW_INIT_NOTIFIER_HEAD(&net->netdev_chain);\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n\txa_destroy(&net->dev_by_index);\n\tif (net != &init_net)\n\t\tWARN_ON_ONCE(!list_empty(&net->dev_base_head));\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit_net(struct net *net)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\tASSERT_RTNL();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops && !dev->rtnl_link_ops->netns_refund)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\tif (netdev_name_in_use(&init_net, fb_name))\n\t\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%%d\");\n\n\t\tnetdev_for_each_altname_safe(dev, name_node, tmp)\n\t\t\tif (netdev_name_in_use(&init_net, name_node->name)) {\n\t\t\t\tnetdev_name_node_del(name_node);\n\t\t\t\tsynchronize_rcu();\n\t\t\t\t__netdev_name_node_alt_destroy(name_node);\n\t\t\t}\n\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tdefault_device_exit_net(net);\n\t\tcond_resched();\n\t}\n\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n#ifdef CONFIG_XFRM_OFFLOAD\n\t\tskb_queue_head_init(&sd->xfrm_backlog);\n#endif\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tINIT_CSD(&sd->csd, rps_trigger_softirq, sd);\n\t\tsd->cpu = i;\n#endif\n\t\tINIT_CSD(&sd->defer_csd, trigger_rx_softirq, sd);\n\t\tspin_lock_init(&sd->defer_lock);\n\n\t\tinit_gro_hash(&sd->backlog);\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"
                            }
                        },
                        {
                            "downstream_patch": "d3b7a9c7597b779039a51d7b34116fbe424bf2b7",
                            "downstream_commit": "05670a893565078ef75eaae6b60b84e540c73952",
                            "commit_date": "2025-01-09 13:33:41 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/core/dev.c\nHunk #1 succeeded at 3640 (offset -2 lines).",
                            "downstream_patch_content": "commit d3b7a9c7597b779039a51d7b34116fbe424bf2b7\nAuthor: Willem de Bruijn <willemb@google.com>\nDate:   Wed Jan 1 11:47:40 2025 -0500\n\n    net: reenable NETIF_F_IPV6_CSUM offload for BIG TCP packets\n    \n    [ Upstream commit 68e068cabd2c6c533ef934c2e5151609cf6ecc6d ]\n    \n    The blamed commit disabled hardware offoad of IPv6 packets with\n    extension headers on devices that advertise NETIF_F_IPV6_CSUM,\n    based on the definition of that feature in skbuff.h:\n    \n     *   * - %NETIF_F_IPV6_CSUM\n     *     - Driver (device) is only able to checksum plain\n     *       TCP or UDP packets over IPv6. These are specifically\n     *       unencapsulated packets of the form IPv6|TCP or\n     *       IPv6|UDP where the Next Header field in the IPv6\n     *       header is either TCP or UDP. IPv6 extension headers\n     *       are not supported with this feature. This feature\n     *       cannot be set in features for a device with\n     *       NETIF_F_HW_CSUM also set. This feature is being\n     *       DEPRECATED (see below).\n    \n    The change causes skb_warn_bad_offload to fire for BIG TCP\n    packets.\n    \n    [  496.310233] WARNING: CPU: 13 PID: 23472 at net/core/dev.c:3129 skb_warn_bad_offload+0xc4/0xe0\n    \n    [  496.310297]  ? skb_warn_bad_offload+0xc4/0xe0\n    [  496.310300]  skb_checksum_help+0x129/0x1f0\n    [  496.310303]  skb_csum_hwoffload_help+0x150/0x1b0\n    [  496.310306]  validate_xmit_skb+0x159/0x270\n    [  496.310309]  validate_xmit_skb_list+0x41/0x70\n    [  496.310312]  sch_direct_xmit+0x5c/0x250\n    [  496.310317]  __qdisc_run+0x388/0x620\n    \n    BIG TCP introduced an IPV6_TLV_JUMBO IPv6 extension header to\n    communicate packet length, as this is an IPv6 jumbogram. But, the\n    feature is only enabled on devices that support BIG TCP TSO. The\n    header is only present for PF_PACKET taps like tcpdump, and not\n    transmitted by physical devices.\n    \n    For this specific case of extension headers that are not\n    transmitted, return to the situation before the blamed commit\n    and support hardware offload.\n    \n    ipv6_has_hopopt_jumbo() tests not only whether this header is present,\n    but also that it is the only extension header before a terminal (L4)\n    header.\n    \n    Fixes: 04c20a9356f2 (\"net: skip offload for NETIF_F_IPV6_CSUM if ipv6 header contains extension\")\n    Reported-by: syzbot <syzkaller@googlegroups.com>\n    Reported-by: Eric Dumazet <edumazet@google.com>\n    Closes: https://lore.kernel.org/netdev/CANn89iK1hdC3Nt8KPhOtTF8vCPc1AHDCtse_BTNki1pWxAByTQ@mail.gmail.com/\n    Signed-off-by: Willem de Bruijn <willemb@google.com>\n    Reviewed-by: Eric Dumazet <edumazet@google.com>\n    Link: https://patch.msgid.link/20250101164909.1331680-1-willemdebruijn.kernel@gmail.com\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/net/core/dev.c b/net/core/dev.c\nindex 8453e14d301b..f3fa8353d262 100644\n--- a/net/core/dev.c\n+++ b/net/core/dev.c\n@@ -3640,8 +3640,10 @@ int skb_csum_hwoffload_help(struct sk_buff *skb,\n \n \tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n \t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n-\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n+\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr) &&\n+\t\t    !ipv6_has_hopopt_jumbo(skb))\n \t\t\tgoto sw_checksum;\n+\n \t\tswitch (skb->csum_offset) {\n \t\tcase offsetof(struct tcphdr, check):\n \t\tcase offsetof(struct udphdr, check):\n",
                            "downstream_file_content": {
                                "net/core/dev.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *      NET3    Protocol independent device support routines.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitmap.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/mm.h>\n#include <linux/smpboot.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/skbuff.h>\n#include <linux/kthread.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dsa.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/gro.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <net/tcx.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <trace/events/qdisc.h>\n#include <trace/events/xdp.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_netdev.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n#include <linux/net_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/devlink.h>\n#include <linux/pm_runtime.h>\n#include <linux/prandom.h>\n#include <linux/once_lite.h>\n#include <net/netdev_rx_queue.h>\n#include <net/page_pool/types.h>\n#include <net/page_pool/helpers.h>\n#include <net/rps.h>\n#include <linux/phy_link_topology.h>\n\n#include \"dev.h\"\n#include \"devmem.h\"\n#include \"net-sysfs.h\"\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\n\nstatic DEFINE_MUTEX(ifalias_mutex);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic DECLARE_RWSEM(devnet_rename_sem);\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\tunsigned int val = net->dev_base_seq + 1;\n\n\tWRITE_ONCE(net->dev_base_seq, val ?: 1);\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\n#ifndef CONFIG_PREEMPT_RT\n\nstatic DEFINE_STATIC_KEY_FALSE(use_backlog_threads_key);\n\nstatic int __init setup_backlog_napi_threads(char *arg)\n{\n\tstatic_branch_enable(&use_backlog_threads_key);\n\treturn 0;\n}\nearly_param(\"thread_backlog_napi\", setup_backlog_napi_threads);\n\nstatic bool use_backlog_threads(void)\n{\n\treturn static_branch_unlikely(&use_backlog_threads_key);\n}\n\n#else\n\nstatic bool use_backlog_threads(void)\n{\n\treturn true;\n}\n\n#endif\n\nstatic inline void backlog_lock_irq_save(struct softnet_data *sd,\n\t\t\t\t\t unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())\n\t\tspin_lock_irqsave(&sd->input_pkt_queue.lock, *flags);\n\telse\n\t\tlocal_irq_save(*flags);\n}\n\nstatic inline void backlog_lock_irq_disable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())\n\t\tspin_lock_irq(&sd->input_pkt_queue.lock);\n\telse\n\t\tlocal_irq_disable();\n}\n\nstatic inline void backlog_unlock_irq_restore(struct softnet_data *sd,\n\t\t\t\t\t      unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())\n\t\tspin_unlock_irqrestore(&sd->input_pkt_queue.lock, *flags);\n\telse\n\t\tlocal_irq_restore(*flags);\n}\n\nstatic inline void backlog_unlock_irq_enable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS) || use_backlog_threads())\n\t\tspin_unlock_irq(&sd->input_pkt_queue.lock);\n\telse\n\t\tlocal_irq_enable();\n}\n\nstatic struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,\n\t\t\t\t\t\t       const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = kmalloc(sizeof(*name_node), GFP_KERNEL);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_HLIST_NODE(&name_node->hlist);\n\tname_node->dev = dev;\n\tname_node->name = name;\n\treturn name_node;\n}\n\nstatic struct netdev_name_node *\nnetdev_name_node_head_alloc(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = netdev_name_node_alloc(dev, dev->name);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&name_node->list);\n\treturn name_node;\n}\n\nstatic void netdev_name_node_free(struct netdev_name_node *name_node)\n{\n\tkfree(name_node);\n}\n\nstatic void netdev_name_node_add(struct net *net,\n\t\t\t\t struct netdev_name_node *name_node)\n{\n\thlist_add_head_rcu(&name_node->hlist,\n\t\t\t   dev_name_hash(net, name_node->name));\n}\n\nstatic void netdev_name_node_del(struct netdev_name_node *name_node)\n{\n\thlist_del_rcu(&name_node->hlist);\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup(struct net *net,\n\t\t\t\t\t\t\tconst char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,\n\t\t\t\t\t\t\t    const char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry_rcu(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nbool netdev_name_in_use(struct net *net, const char *name)\n{\n\treturn netdev_name_node_lookup(net, name);\n}\nEXPORT_SYMBOL(netdev_name_in_use);\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (name_node)\n\t\treturn -EEXIST;\n\tname_node = netdev_name_node_alloc(dev, name);\n\tif (!name_node)\n\t\treturn -ENOMEM;\n\tnetdev_name_node_add(net, name_node);\n\t/* The node that holds dev->name acts as a head of per-device list. */\n\tlist_add_tail_rcu(&name_node->list, &dev->name_node->list);\n\n\treturn 0;\n}\n\nstatic void netdev_name_node_alt_free(struct rcu_head *head)\n{\n\tstruct netdev_name_node *name_node =\n\t\tcontainer_of(head, struct netdev_name_node, rcu);\n\n\tkfree(name_node->name);\n\tnetdev_name_node_free(name_node);\n}\n\nstatic void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)\n{\n\tnetdev_name_node_del(name_node);\n\tlist_del(&name_node->list);\n\tcall_rcu(&name_node->rcu, netdev_name_node_alt_free);\n}\n\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (!name_node)\n\t\treturn -ENOENT;\n\t/* lookup might have found our primary name or a name belonging\n\t * to another device.\n\t */\n\tif (name_node == dev->name_node || name_node->dev != dev)\n\t\treturn -EINVAL;\n\n\t__netdev_name_node_alt_destroy(name_node);\n\treturn 0;\n}\n\nstatic void netdev_name_node_alt_flush(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\n\tlist_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list) {\n\t\tlist_del(&name_node->list);\n\t\tnetdev_name_node_alt_free(&name_node->rcu);\n\t}\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\tnetdev_name_node_add(net, dev->name_node);\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_add(net, name_node);\n\n\t/* We reserved the ifindex, this can't fail */\n\tWARN_ON(xa_store(&net->dev_by_index, dev->ifindex, dev, GFP_KERNEL));\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\txa_erase(&net->dev_by_index, dev->ifindex);\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_del(name_node);\n\n\t/* Unlink dev from the device chain */\n\tlist_del_rcu(&dev->dev_list);\n\tnetdev_name_node_del(dev->name_node);\n\thlist_del_rcu(&dev->index_hlist);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data) = {\n\t.process_queue_bh_lock = INIT_LOCAL_LOCK(process_queue_bh_lock),\n};\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n/* Page_pool has a lockless array/stack to alloc/recycle pages.\n * PP consumers must pay attention to run APIs in the appropriate context\n * (e.g. NAPI context).\n */\nstatic DEFINE_PER_CPU(struct page_pool *, system_page_pool);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &net_hotdata.ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn READ_ONCE(dev->ifindex);\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\nstatic struct net_device_path *dev_fwd_path(struct net_device_path_stack *stack)\n{\n\tint k = stack->num_paths++;\n\n\tif (WARN_ON_ONCE(k >= NET_DEVICE_PATH_STACK_MAX))\n\t\treturn NULL;\n\n\treturn &stack->path[k];\n}\n\nint dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,\n\t\t\t  struct net_device_path_stack *stack)\n{\n\tconst struct net_device *last_dev;\n\tstruct net_device_path_ctx ctx = {\n\t\t.dev\t= dev,\n\t};\n\tstruct net_device_path *path;\n\tint ret = 0;\n\n\tmemcpy(ctx.daddr, daddr, sizeof(ctx.daddr));\n\tstack->num_paths = 0;\n\twhile (ctx.dev && ctx.dev->netdev_ops->ndo_fill_forward_path) {\n\t\tlast_dev = ctx.dev;\n\t\tpath = dev_fwd_path(stack);\n\t\tif (!path)\n\t\t\treturn -1;\n\n\t\tmemset(path, 0, sizeof(struct net_device_path));\n\t\tret = ctx.dev->netdev_ops->ndo_fill_forward_path(&ctx, path);\n\t\tif (ret < 0)\n\t\t\treturn -1;\n\n\t\tif (WARN_ON_ONCE(last_dev == ctx.dev))\n\t\t\treturn -1;\n\t}\n\n\tif (!ctx.dev)\n\t\treturn ret;\n\n\tpath = dev_fwd_path(stack);\n\tif (!path)\n\t\treturn -1;\n\tpath->type = DEV_PATH_ETHERNET;\n\tpath->dev = ctx.dev;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_fill_forward_path);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore.\n *\tIf the name is found a pointer to the device is returned.\n *\tIf the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup_rcu(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/* Deprecated for new users, call netdev_get_by_name() instead */\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\tnetdev_get_by_name() - find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\t@tracker: tracking object for the acquired reference\n *\t@gfp: allocation flags for the tracker\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use netdev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\nstruct net_device *netdev_get_by_name(struct net *net, const char *name,\n\t\t\t\t      netdevice_tracker *tracker, gfp_t gfp)\n{\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_name(net, name);\n\tif (dev)\n\t\tnetdev_tracker_alloc(dev, tracker, gfp);\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold the RTNL semaphore.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n/* Deprecated for new users, call netdev_get_by_index() instead */\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tnetdev_get_by_index() - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\t@tracker: tracking object for the acquired reference\n *\t@gfp: allocation flags for the tracker\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tnetdev_put() to indicate they have finished with it.\n */\nstruct net_device *netdev_get_by_index(struct net *net, int ifindex,\n\t\t\t\t       netdevice_tracker *tracker, gfp_t gfp)\n{\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_index(net, ifindex);\n\tif (dev)\n\t\tnetdev_tracker_alloc(dev, tracker, gfp);\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\nstatic DEFINE_SEQLOCK(netdev_rename_lock);\n\nvoid netdev_copy_name(struct net_device *dev, char *name)\n{\n\tunsigned int seq;\n\n\tdo {\n\t\tseq = read_seqbegin(&netdev_rename_lock);\n\t\tstrscpy(name, dev->name, IFNAMSIZ);\n\t} while (read_seqretry(&netdev_rename_lock, seq));\n}\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tint ret;\n\n\trcu_read_lock();\n\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tnetdev_copy_name(dev, name);\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tallow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strnlen(name, IFNAMSIZ) == IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@res: result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *res)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\tchar buf[IFNAMSIZ];\n\n\t/* Verify the string as this thing may have come from the user.\n\t * There must be one \"%d\" and no other \"%\" characters.\n\t */\n\tp = strchr(name, '%');\n\tif (!p || p[1] != 'd' || strchr(p + 2, '%'))\n\t\treturn -EINVAL;\n\n\t/* Use one page as a bit array of possible slots */\n\tinuse = bitmap_zalloc(max_netdevices, GFP_ATOMIC);\n\tif (!inuse)\n\t\treturn -ENOMEM;\n\n\tfor_each_netdev(net, d) {\n\t\tstruct netdev_name_node *name_node;\n\n\t\tnetdev_for_each_altname(d, name_node) {\n\t\t\tif (!sscanf(name_node->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/* avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, name_node->name, IFNAMSIZ))\n\t\t\t\t__set_bit(i, inuse);\n\t\t}\n\t\tif (!sscanf(d->name, name, &i))\n\t\t\tcontinue;\n\t\tif (i < 0 || i >= max_netdevices)\n\t\t\tcontinue;\n\n\t\t/* avoid cases where sscanf is not exact inverse of printf */\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t__set_bit(i, inuse);\n\t}\n\n\ti = find_first_zero_bit(inuse, max_netdevices);\n\tbitmap_free(inuse);\n\tif (i == max_netdevices)\n\t\treturn -ENFILE;\n\n\t/* 'res' and 'name' could overlap, use 'buf' as an intermediate buffer */\n\tstrscpy(buf, name, IFNAMSIZ);\n\tsnprintf(res, IFNAMSIZ, buf, i);\n\treturn i;\n}\n\n/* Returns negative errno or allocated unit id (see __dev_alloc_name()) */\nstatic int dev_prep_valid_name(struct net *net, struct net_device *dev,\n\t\t\t       const char *want_name, char *out_name,\n\t\t\t       int dup_errno)\n{\n\tif (!dev_valid_name(want_name))\n\t\treturn -EINVAL;\n\n\tif (strchr(want_name, '%'))\n\t\treturn __dev_alloc_name(net, want_name, out_name);\n\n\tif (netdev_name_in_use(net, want_name))\n\t\treturn -dup_errno;\n\tif (out_name != want_name)\n\t\tstrscpy(out_name, want_name, IFNAMSIZ);\n\treturn 0;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\treturn dev_prep_valid_name(dev_net(dev), dev, name, dev->name, ENFILE);\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tint ret;\n\n\tret = dev_prep_valid_name(net, dev, name, dev->name, EEXIST);\n\treturn ret < 0 ? ret : 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\n\tdown_write(&devnet_rename_sem);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\twrite_seqlock_bh(&netdev_rename_lock);\n\terr = dev_get_valid_name(net, dev, newname);\n\twrite_sequnlock_bh(&netdev_rename_lock);\n\n\tif (err < 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s%s\\n\", oldname,\n\t\t\t    dev->flags & IFF_UP ? \" (while UP)\" : \"\");\n\n\told_assign_type = dev->name_assign_type;\n\tWRITE_ONCE(dev->name_assign_type, NET_NAME_RENAMED);\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tWRITE_ONCE(dev->name_assign_type, old_assign_type);\n\t\tup_write(&devnet_rename_sem);\n\t\treturn ret;\n\t}\n\n\tup_write(&devnet_rename_sem);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\tnetdev_name_node_del(dev->name_node);\n\n\tsynchronize_net();\n\n\tnetdev_name_node_add(net, dev->name_node);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tdown_write(&devnet_rename_sem);\n\t\t\twrite_seqlock_bh(&netdev_rename_lock);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\twrite_sequnlock_bh(&netdev_rename_lock);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tWRITE_ONCE(dev->name_assign_type, old_assign_type);\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tnetdev_err(dev, \"name change rollback failed: %d\\n\",\n\t\t\t\t   ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tstruct dev_ifalias *new_alias = NULL;\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (len) {\n\t\tnew_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);\n\t\tif (!new_alias)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(new_alias->ifalias, alias, len);\n\t\tnew_alias->ifalias[len] = 0;\n\t}\n\n\tmutex_lock(&ifalias_mutex);\n\tnew_alias = rcu_replace_pointer(dev->ifalias, new_alias,\n\t\t\t\t\tmutex_is_locked(&ifalias_mutex));\n\tmutex_unlock(&ifalias_mutex);\n\n\tif (new_alias)\n\t\tkfree_rcu(new_alias, rcuhead);\n\n\treturn len;\n}\nEXPORT_SYMBOL(dev_set_alias);\n\n/**\n *\tdev_get_alias - get ifalias of a device\n *\t@dev: device\n *\t@name: buffer to store name of ifalias\n *\t@len: size of buffer\n *\n *\tget ifalias for a device.  Caller must make sure dev cannot go\n *\taway,  e.g. rcu read lock or own a reference count to device.\n */\nint dev_get_alias(const struct net_device *dev, char *name, size_t len)\n{\n\tconst struct dev_ifalias *alias;\n\tint ret = 0;\n\n\trcu_read_lock();\n\talias = rcu_dereference(dev->ifalias);\n\tif (alias)\n\t\tret = snprintf(name, len, \"%s\", alias->ifalias);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info.dev = dev,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL, 0, NULL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * __netdev_notify_peers - notify network peers about existence of @dev,\n * to be called when rtnl lock is already held.\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid __netdev_notify_peers(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n}\nEXPORT_SYMBOL(__netdev_notify_peers);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\t__netdev_notify_peers(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int napi_threaded_poll(void *data);\n\nstatic int napi_kthread_create(struct napi_struct *n)\n{\n\tint err = 0;\n\n\t/* Create and wake up the kthread once to put it in\n\t * TASK_INTERRUPTIBLE mode to avoid the blocked task\n\t * warning and work with loadavg.\n\t */\n\tn->thread = kthread_run(napi_threaded_poll, n, \"napi/%s-%d\",\n\t\t\t\tn->dev->name, n->napi_id);\n\tif (IS_ERR(n->thread)) {\n\t\terr = PTR_ERR(n->thread);\n\t\tpr_err(\"kthread_run failed with err %d\\n\", err);\n\t\tn->thread = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\tdev_addr_check(dev);\n\n\tif (!netif_device_present(dev)) {\n\t\t/* may be detached because parent is runtime-suspended */\n\t\tif (dev->dev.parent)\n\t\t\tpm_runtime_resume(dev->dev.parent);\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t}\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev: device to open\n *\t@extack: netlink extended ack\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP | IFF_RUNNING, GFP_KERNEL, 0, NULL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n/**\n *\tdev_disable_gro_hw - disable HW Generic Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable HW Generic Receive Offload (GRO_HW) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if Generic XDP is installed on\n *\tthe device.\n */\nstatic void dev_disable_gro_hw(struct net_device *dev)\n{\n\tdev->wanted_features &= ~NETIF_F_GRO_HW;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_GRO_HW))\n\t\tnetdev_WARN(dev, \"failed to disable GRO_HW!\\n\");\n}\n\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd)\n{\n#define N(val) \t\t\t\t\t\t\\\n\tcase NETDEV_##val:\t\t\t\t\\\n\t\treturn \"NETDEV_\" __stringify(val);\n\tswitch (cmd) {\n\tN(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)\n\tN(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)\n\tN(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)\n\tN(POST_INIT) N(PRE_UNINIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN)\n\tN(CHANGEUPPER) N(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA)\n\tN(BONDING_INFO) N(PRECHANGEUPPER) N(CHANGELOWERSTATE)\n\tN(UDP_TUNNEL_PUSH_INFO) N(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)\n\tN(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)\n\tN(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)\n\tN(PRE_CHANGEADDR) N(OFFLOAD_XSTATS_ENABLE) N(OFFLOAD_XSTATS_DISABLE)\n\tN(OFFLOAD_XSTATS_REPORT_USED) N(OFFLOAD_XSTATS_REPORT_DELTA)\n\tN(XDP_FEAT_CHANGE)\n\t}\n#undef N\n\treturn \"UNKNOWN_NETDEV_EVENT\";\n}\nEXPORT_SYMBOL_GPL(netdev_cmd_to_name);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t};\n\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int call_netdevice_register_notifiers(struct notifier_block *nb,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tint err;\n\n\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\treturn 0;\n}\n\nstatic void call_netdevice_unregister_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\tstruct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\tdev);\n\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t}\n\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n}\n\nstatic int call_netdevice_register_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t struct net *net)\n{\n\tstruct net_device *dev;\n\tint err;\n\n\tfor_each_netdev(net, dev) {\n\t\terr = call_netdevice_register_notifiers(nb, dev);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\treturn 0;\n\nrollback:\n\tfor_each_netdev_continue_reverse(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n\treturn err;\n}\n\nstatic void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t    struct net *net)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\terr = call_netdevice_register_net_notifiers(nb, net);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n\nrollback:\n\tfor_each_net_continue_reverse(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\nstatic int __register_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t     struct notifier_block *nb,\n\t\t\t\t\t     bool ignore_call_fail)\n{\n\tint err;\n\n\terr = raw_notifier_chain_register(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\tif (dev_boot_phase)\n\t\treturn 0;\n\n\terr = call_netdevice_register_net_notifiers(nb, net);\n\tif (err && !ignore_call_fail)\n\t\tgoto chain_unregister;\n\n\treturn 0;\n\nchain_unregister:\n\traw_notifier_chain_unregister(&net->netdev_chain, nb);\n\treturn err;\n}\n\nstatic int __unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t       struct notifier_block *nb)\n{\n\tint err;\n\n\terr = raw_notifier_chain_unregister(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\n\tcall_netdevice_unregister_net_notifiers(nb, net);\n\treturn 0;\n}\n\n/**\n * register_netdevice_notifier_net - register a per-netns network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(net, nb, false);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_net);\n\n/**\n * unregister_netdevice_notifier_net - unregister a per-netns\n *                                     network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier_net(). The notifier is unlinked from the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __unregister_netdevice_notifier_net(net, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_net);\n\nstatic void __move_netdevice_notifier_net(struct net *src_net,\n\t\t\t\t\t  struct net *dst_net,\n\t\t\t\t\t  struct notifier_block *nb)\n{\n\t__unregister_netdevice_notifier_net(src_net, nb);\n\t__register_netdevice_notifier_net(dst_net, nb, true);\n}\n\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(dev_net(dev), nb, false);\n\tif (!err) {\n\t\tnn->nb = nb;\n\t\tlist_add(&nn->list, &dev->net_notifier_list);\n\t}\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_dev_net);\n\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\tlist_del(&nn->list);\n\terr = __unregister_netdevice_notifier_net(dev_net(dev), nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);\n\nstatic void move_netdevice_notifiers_dev_net(struct net_device *dev,\n\t\t\t\t\t     struct net *net)\n{\n\tstruct netdev_net_notifier *nn;\n\n\tlist_for_each_entry(nn, &dev->net_notifier_list, list)\n\t\t__move_netdevice_notifier_net(dev_net(dev), net, nn->nb);\n}\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t  struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/* Run per-netns notifier block chain first, then run the global one.\n\t * Hopefully, one day, the global one is going to be removed after\n\t * all notifier block registrators get converted to be per-netns.\n\t */\n\tret = raw_notifier_call_chain(&net->netdev_chain, val, info);\n\tif (ret & NOTIFY_STOP_MASK)\n\t\treturn ret;\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers_info_robust - call per-netns notifier blocks\n *\t                                       for and rollback on error\n *\t@val_up: value passed unmodified to notifier function\n *\t@val_down: value passed unmodified to the notifier function when\n *\t           recovering from an error on @val_up\n *\t@info: notifier information data\n *\n *\tCall all per-netns network notifier blocks, but not notifier blocks on\n *\tthe global notifier chain. Parameters and return value are as for\n *\traw_notifier_call_chain_robust().\n */\n\nstatic int\ncall_netdevice_notifiers_info_robust(unsigned long val_up,\n\t\t\t\t     unsigned long val_down,\n\t\t\t\t     struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\n\tASSERT_RTNL();\n\n\treturn raw_notifier_call_chain_robust(&net->netdev_chain,\n\t\t\t\t\t      val_up, val_down, info);\n}\n\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t\t.extack = extack,\n\t};\n\n\treturn call_netdevice_notifiers_info(val, &info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\treturn call_netdevice_notifiers_extack(val, dev, NULL);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n/**\n *\tcall_netdevice_notifiers_mtu - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@arg: additional u32 argument passed to the notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\nstatic int call_netdevice_notifiers_mtu(unsigned long val,\n\t\t\t\t\tstruct net_device *dev, u32 arg)\n{\n\tstruct netdev_notifier_info_ext info = {\n\t\t.info.dev = dev,\n\t\t.ext.mtu = arg,\n\t};\n\n\tBUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);\n\n\treturn call_netdevice_notifiers_info(val, &info.info);\n}\n\n#ifdef CONFIG_NET_INGRESS\nstatic DEFINE_STATIC_KEY_FALSE(ingress_needed_key);\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_branch_inc(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_branch_dec(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic DEFINE_STATIC_KEY_FALSE(egress_needed_key);\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_branch_inc(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_branch_dec(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\n#ifdef CONFIG_NET_CLS_ACT\nDEFINE_STATIC_KEY_FALSE(tcf_bypass_check_needed_key);\nEXPORT_SYMBOL(tcf_bypass_check_needed_key);\n#endif\n\nDEFINE_STATIC_KEY_FALSE(netstamp_needed_key);\nEXPORT_SYMBOL(netstamp_needed_key);\n#ifdef CONFIG_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_branch_enable(&netstamp_needed_key);\n\telse\n\t\tstatic_branch_disable(&netstamp_needed_key);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted = atomic_read(&netstamp_wanted);\n\n\twhile (wanted > 0) {\n\t\tif (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted + 1))\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_inc(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted = atomic_read(&netstamp_wanted);\n\n\twhile (wanted > 1) {\n\t\tif (atomic_try_cmpxchg(&netstamp_wanted, &wanted, wanted - 1))\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_dec(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tskb->tstamp_type = SKB_CLOCK_REALTIME;\n\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\tskb->tstamp = ktime_get_real();\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\t\\\n\tif (static_branch_unlikely(&netstamp_needed_key)) {\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\t\t\\\n\t\t\t(SKB)->tstamp = ktime_get_real();\t\\\n\t}\t\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\treturn __is_skb_forwardable(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nstatic int __dev_forward_skb2(struct net_device *dev, struct sk_buff *skb,\n\t\t\t      bool check_mtu)\n{\n\tint ret = ____dev_forward_skb(dev, skb, check_mtu);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, false) ?: netif_rx_internal(skb);\n}\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * dev_nit_active - return true if any network interface taps are in use\n *\n * @dev: network device to check for the presence of taps\n */\nbool dev_nit_active(struct net_device *dev)\n{\n\treturn !list_empty(&net_hotdata.ptype_all) ||\n\t       !list_empty(&dev->ptype_all);\n}\nEXPORT_SYMBOL_GPL(dev_nit_active);\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct list_head *ptype_list = &net_hotdata.ptype_all;\n\tstruct packet_type *ptype, *pt_prev = NULL;\n\tstruct sk_buff *skb2 = NULL;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (READ_ONCE(ptype->ignore_outgoing))\n\t\t\tcontinue;\n\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &net_hotdata.ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tnetdev_warn(dev, \"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tnetdev_warn(dev, \"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\t    i, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\t/* walk through the TCs and see if it falls into any of them */\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\t/* didn't find it, just return -1 to indicate no match */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_txq_to_tc);\n\n#ifdef CONFIG_XPS\nstatic struct static_key xps_needed __read_mostly;\nstatic struct static_key xps_rxqs_needed __read_mostly;\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     struct xps_dev_maps *old_maps, int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tif (old_maps)\n\t\t\tRCU_INIT_POINTER(old_maps->attr_map[tci], NULL);\n\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev_maps->num_tc;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, NULL, tci, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void reset_xps_maps(struct net_device *dev,\n\t\t\t   struct xps_dev_maps *dev_maps,\n\t\t\t   enum xps_map_type type)\n{\n\tstatic_key_slow_dec_cpuslocked(&xps_needed);\n\tif (type == XPS_RXQS)\n\t\tstatic_key_slow_dec_cpuslocked(&xps_rxqs_needed);\n\n\tRCU_INIT_POINTER(dev->xps_maps[type], NULL);\n\n\tkfree_rcu(dev_maps, rcu);\n}\n\nstatic void clean_xps_maps(struct net_device *dev, enum xps_map_type type,\n\t\t\t   u16 offset, u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tbool active = false;\n\tint i, j;\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (!dev_maps)\n\t\treturn;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, j, offset, count);\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\n\tif (type == XPS_CPUS) {\n\t\tfor (i = offset + (count - 1); count--; i--)\n\t\t\tnetdev_queue_numa_node_write(\n\t\t\t\tnetdev_get_tx_queue(dev, i), NUMA_NO_NODE);\n\t}\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tif (!static_key_false(&xps_needed))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&xps_map_mutex);\n\n\tif (static_key_false(&xps_rxqs_needed))\n\t\tclean_xps_maps(dev, XPS_RXQS, offset, count);\n\n\tclean_xps_maps(dev, XPS_CPUS, offset, count);\n\n\tmutex_unlock(&xps_map_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,\n\t\t\t\t      u16 index, bool is_rxqs_map)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add tx-queue to this CPU's/rx-queue's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store tx-queue on this CPU's/rx-queue's\n\t *  map\n\t */\n\tif (is_rxqs_map)\n\t\tnew_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);\n\telse\n\t\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t\t       cpu_to_node(attr_index));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\n/* Copy xps maps at a given index */\nstatic void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,\n\t\t\t      struct xps_dev_maps *new_dev_maps, int index,\n\t\t\t      int tc, bool skip_tc)\n{\n\tint i, tci = index * dev_maps->num_tc;\n\tstruct xps_map *map;\n\n\t/* copy maps belonging to foreign traffic classes */\n\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\tif (i == tc && skip_tc)\n\t\t\tcontinue;\n\n\t\t/* fill in the new device map from the old device map */\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n}\n\n/* Must be called under cpus_read_lock */\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL, *old_dev_maps = NULL;\n\tconst unsigned long *online_mask = NULL;\n\tbool active = false, copy = false;\n\tint i, j, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tunsigned int nr_ids;\n\n\tWARN_ON_ONCE(index >= dev->num_tx_queues);\n\n\tif (dev->num_tc) {\n\t\t/* Do not allow XPS on subordinate device directly */\n\t\tnum_tc = dev->num_tc;\n\t\tif (num_tc < 0)\n\t\t\treturn -EINVAL;\n\n\t\t/* If queue belongs to subordinate dev use its map */\n\t\tdev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;\n\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (type == XPS_RXQS) {\n\t\tmaps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);\n\t\tnr_ids = dev->num_rx_queues;\n\t} else {\n\t\tmaps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);\n\t\tif (num_possible_cpus() > 1)\n\t\t\tonline_mask = cpumask_bits(cpu_online_mask);\n\t\tnr_ids = nr_cpu_ids;\n\t}\n\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\t/* The old dev_maps could be larger or smaller than the one we're\n\t * setting up now, as dev->num_tc or nr_ids could have been updated in\n\t * between. We could try to be smart, but let's be safe instead and only\n\t * copy foreign traffic classes if the two map sizes match.\n\t */\n\tif (dev_maps &&\n\t    dev_maps->num_tc == num_tc && dev_maps->nr_ids == nr_ids)\n\t\tcopy = true;\n\n\t/* allocate memory for queue storage */\n\tfor (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tif (!new_dev_maps) {\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\t\tif (!new_dev_maps) {\n\t\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tnew_dev_maps->nr_ids = nr_ids;\n\t\t\tnew_dev_maps->num_tc = num_tc;\n\t\t}\n\n\t\ttci = j * num_tc + tc;\n\t\tmap = copy ? xmap_dereference(dev_maps->attr_map[tci]) : NULL;\n\n\t\tmap = expand_xps_map(map, j, index, type == XPS_RXQS);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tif (!dev_maps) {\n\t\t/* Increment static keys at most once per type */\n\t\tstatic_key_slow_inc_cpuslocked(&xps_needed);\n\t\tif (type == XPS_RXQS)\n\t\t\tstatic_key_slow_inc_cpuslocked(&xps_rxqs_needed);\n\t}\n\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tbool skip_tc = false;\n\n\t\ttci = j * num_tc + tc;\n\t\tif (netif_attr_test_mask(j, mask, nr_ids) &&\n\t\t    netif_attr_test_online(j, online_mask, nr_ids)) {\n\t\t\t/* add tx-queue to CPU/rx-queue maps */\n\t\t\tint pos = 0;\n\n\t\t\tskip_tc = true;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (type == XPS_CPUS) {\n\t\t\t\tif (numa_node_id == -2)\n\t\t\t\t\tnuma_node_id = cpu_to_node(j);\n\t\t\t\telse if (numa_node_id != cpu_to_node(j))\n\t\t\t\t\tnuma_node_id = -1;\n\t\t\t}\n#endif\n\t\t}\n\n\t\tif (copy)\n\t\t\txps_copy_dev_maps(dev_maps, new_dev_maps, j, tc,\n\t\t\t\t\t  skip_tc);\n\t}\n\n\trcu_assign_pointer(dev->xps_maps[type], new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * dev_maps->num_tc; i--; tci++) {\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tif (!map)\n\t\t\t\tcontinue;\n\n\t\t\tif (copy) {\n\t\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\t\tif (map == new_map)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\told_dev_maps = dev_maps;\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\tif (type == XPS_CPUS)\n\t\t/* update Tx queue numa node */\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t\t     (numa_node_id >= 0) ?\n\t\t\t\t\t     numa_node_id : NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes tx-queue from unused CPUs/rx-queues */\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\ttci = j * dev_maps->num_tc;\n\n\t\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\t\tif (i == tc &&\n\t\t\t    netif_attr_test_mask(j, mask, dev_maps->nr_ids) &&\n\t\t\t    netif_attr_test_online(j, online_mask, dev_maps->nr_ids))\n\t\t\t\tcontinue;\n\n\t\t\tactive |= remove_xps_queue(dev_maps,\n\t\t\t\t\t\t   copy ? old_dev_maps : NULL,\n\t\t\t\t\t\t   tci, index);\n\t\t}\n\t}\n\n\tif (old_dev_maps)\n\t\tkfree_rcu(old_dev_maps, rcu);\n\n\t/* free map if not active */\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = copy ?\n\t\t\t      xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(__netif_set_xps_queue);\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nstatic void netdev_unbind_all_sb_channels(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n\t/* Unbind any subordinate channels */\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev)\n\t\t\tnetdev_unbind_sb_channel(dev, txq->sb_dev);\n\t}\n}\n\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\t/* Reset TC configuration of device */\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(sb_dev, 0);\n#endif\n\tmemset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));\n\tmemset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));\n\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev == sb_dev)\n\t\t\ttxq->sb_dev = NULL;\n\t}\n}\nEXPORT_SYMBOL(netdev_unbind_sb_channel);\n\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset)\n{\n\t/* Make certain the sb_dev and dev are already configured */\n\tif (sb_dev->num_tc >= 0 || tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\t/* We cannot hand out queues we don't have */\n\tif ((offset + count) > dev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\t/* Record the mapping */\n\tsb_dev->tc_to_txq[tc].count = count;\n\tsb_dev->tc_to_txq[tc].offset = offset;\n\n\t/* Provide a way for Tx queue to find the tc_to_txq map or\n\t * XPS map for itself.\n\t */\n\twhile (count--)\n\t\tnetdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_bind_sb_channel_queue);\n\nint netdev_set_sb_channel(struct net_device *dev, u16 channel)\n{\n\t/* Do not use a multiqueue device to represent a subordinate channel */\n\tif (netif_is_multiqueue(dev))\n\t\treturn -ENODEV;\n\n\t/* We allow channels 1 - 32767 to be used for subordinate channels.\n\t * Channel 0 is meant to be \"native\" mode and used only to represent\n\t * the main root device. We allow writing 0 to reset the device back\n\t * to normal mode after being used as a subordinate channel.\n\t */\n\tif (channel > S16_MAX)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = -channel;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_sb_channel);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater than real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tbool disabling;\n\tint rc;\n\n\tdisabling = txq < dev->real_num_tx_queues;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tdev_qdisc_change_real_num_tx(dev, txq);\n\n\t\tdev->real_num_tx_queues = txq;\n\n\t\tif (disabling) {\n\t\t\tsynchronize_net();\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t} else {\n\t\tdev->real_num_tx_queues = txq;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n *\tnetif_set_real_num_queues - set actual number of RX and TX queues used\n *\t@dev: Network device\n *\t@txq: Actual number of TX queues\n *\t@rxq: Actual number of RX queues\n *\n *\tSet the real number of both TX and RX queues.\n *\tDoes nothing if the number of queues is already correct.\n */\nint netif_set_real_num_queues(struct net_device *dev,\n\t\t\t      unsigned int txq, unsigned int rxq)\n{\n\tunsigned int old_rxq = dev->real_num_rx_queues;\n\tint err;\n\n\tif (txq < 1 || txq > dev->num_tx_queues ||\n\t    rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\t/* Start from increases, so the error path only does decreases -\n\t * decreases can't fail.\n\t */\n\tif (rxq > dev->real_num_rx_queues) {\n\t\terr = netif_set_real_num_rx_queues(dev, rxq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (txq > dev->real_num_tx_queues) {\n\t\terr = netif_set_real_num_tx_queues(dev, txq);\n\t\tif (err)\n\t\t\tgoto undo_rx;\n\t}\n\tif (rxq < dev->real_num_rx_queues)\n\t\tWARN_ON(netif_set_real_num_rx_queues(dev, rxq));\n\tif (txq < dev->real_num_tx_queues)\n\t\tWARN_ON(netif_set_real_num_tx_queues(dev, txq));\n\n\treturn 0;\nundo_rx:\n\tWARN_ON(netif_set_real_num_rx_queues(dev, old_rxq));\n\treturn err;\n}\nEXPORT_SYMBOL(netif_set_real_num_queues);\n\n/**\n * netif_set_tso_max_size() - set the max size of TSO frames supported\n * @dev:\tnetdev to update\n * @size:\tmax skb->len of a TSO frame\n *\n * Set the limit on the size of TSO super-frames the device can handle.\n * Unless explicitly set the stack will assume the value of\n * %GSO_LEGACY_MAX_SIZE.\n */\nvoid netif_set_tso_max_size(struct net_device *dev, unsigned int size)\n{\n\tdev->tso_max_size = min(GSO_MAX_SIZE, size);\n\tif (size < READ_ONCE(dev->gso_max_size))\n\t\tnetif_set_gso_max_size(dev, size);\n\tif (size < READ_ONCE(dev->gso_ipv4_max_size))\n\t\tnetif_set_gso_ipv4_max_size(dev, size);\n}\nEXPORT_SYMBOL(netif_set_tso_max_size);\n\n/**\n * netif_set_tso_max_segs() - set the max number of segs supported for TSO\n * @dev:\tnetdev to update\n * @segs:\tmax number of TCP segments\n *\n * Set the limit on the number of TCP segments the device can generate from\n * a single TSO super-frame.\n * Unless explicitly set the stack will assume the value of %GSO_MAX_SEGS.\n */\nvoid netif_set_tso_max_segs(struct net_device *dev, unsigned int segs)\n{\n\tdev->tso_max_segs = segs;\n\tif (segs < READ_ONCE(dev->gso_max_segs))\n\t\tnetif_set_gso_max_segs(dev, segs);\n}\nEXPORT_SYMBOL(netif_set_tso_max_segs);\n\n/**\n * netif_inherit_tso_max() - copy all TSO limits from a lower device to an upper\n * @to:\t\tnetdev to update\n * @from:\tnetdev from which to copy the limits\n */\nvoid netif_inherit_tso_max(struct net_device *to, const struct net_device *from)\n{\n\tnetif_set_tso_max_size(to, from->tso_max_size);\n\tnetif_set_tso_max_segs(to, from->tso_max_segs);\n}\nEXPORT_SYMBOL(netif_inherit_tso_max);\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * Default value is the number of physical cores if there are only 1 or 2, or\n * divided by 2 if there are more.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\tcpumask_var_t cpus;\n\tint cpu, count = 0;\n\n\tif (unlikely(is_kdump_kernel() || !zalloc_cpumask_var(&cpus, GFP_KERNEL)))\n\t\treturn 1;\n\n\tcpumask_copy(cpus, cpu_online_mask);\n\tfor_each_cpu(cpu, cpus) {\n\t\t++count;\n\t\tcpumask_andnot(cpus, cpus, topology_sibling_cpumask(cpu));\n\t}\n\tfree_cpumask_var(cpus);\n\n\treturn count > 2 ? DIV_ROUND_UP(count, 2) : count;\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_drop_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!netif_xmit_stopped(txq)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid dev_kfree_skb_irq_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(dev_kfree_skb_irq_reason);\n\nvoid dev_kfree_skb_any_reason(struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tif (in_hardirq() || irqs_disabled())\n\t\tdev_kfree_skb_irq_reason(skb, reason);\n\telse\n\t\tkfree_skb_reason(skb, reason);\n}\nEXPORT_SYMBOL(dev_kfree_skb_any_reason);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nstatic u16 skb_tx_hash(const struct net_device *dev,\n\t\t       const struct net_device *sb_dev,\n\t\t       struct sk_buff *skb)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = dev->real_num_tx_queues;\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = sb_dev->tc_to_txq[tc].offset;\n\t\tqcount = sb_dev->tc_to_txq[tc].count;\n\t\tif (unlikely(!qcount)) {\n\t\t\tnet_warn_ratelimited(\"%s: invalid qcount, qoffset %u for tc %u\\n\",\n\t\t\t\t\t     sb_dev->name, qoffset, tc);\n\t\t\tqoffset = 0;\n\t\t\tqcount = dev->real_num_tx_queues;\n\t\t}\n\t}\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tDEBUG_NET_WARN_ON_ONCE(qcount == 0);\n\t\thash = skb_get_rx_queue(skb);\n\t\tif (hash >= qoffset)\n\t\t\thash -= qoffset;\n\t\twhile (unlikely(hash >= qcount))\n\t\t\thash -= qcount;\n\t\treturn hash + qoffset;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\n\nvoid skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tskb_dump(KERN_WARNING, skb, false);\n\tWARN(1, \"%s: caps=(%pNF, %pNF)\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_is_gso(skb))) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!skb_frags_readable(skb)) {\n\t\treturn -EFAULT;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tret = -EINVAL;\n\tif (unlikely(offset >= skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset (%d) >= skb_headlen() (%u)\\n\",\n\t\t\t  offset, skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tif (unlikely(offset + sizeof(__sum16) > skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset+2 (%zu) > skb_headlen() (%u)\\n\",\n\t\t\t  offset + sizeof(__sum16), skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tret = skb_ensure_writable(skb, offset + sizeof(__sum16));\n\tif (ret)\n\t\tgoto out;\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__le32));\n\tif (ret)\n\t\tgoto out;\n\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb_reset_csum_not_inet(skb);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_crc32c_csum_help);\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb->data;\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn vlan_get_protocol_and_depth(skb, type, depth);\n}\n\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nstatic void do_netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tnetdev_err(dev, \"hw csum failure\\n\");\n\tskb_dump(KERN_ERR, skb, true);\n\tdump_stack();\n}\n\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tDO_ONCE_LITE(do_netdev_rx_csum_fault, dev, skb);\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* XXX: check that highmem exists at all on the given machine. */\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tstruct page *page = skb_frag_page(frag);\n\n\t\t\tif (page && PageHighMem(page))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, NULL);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > READ_ONCE(dev->gso_max_segs))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (unlikely(skb->len >= netif_get_gso_max_size(dev, skb)))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (!skb_shinfo(skb)->gso_type) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\t}\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (dev_nit_active(dev))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb_mark_not_on_list(skb);\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_tx_queue_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb_csum_is_sctp(skb)))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\tif (features & NETIF_F_HW_CSUM)\n\t\treturn 0;\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n\t\t\tgoto sw_checksum;\n\t\tswitch (skb->csum_offset) {\n\t\tcase offsetof(struct tcphdr, check):\n\t\tcase offsetof(struct udphdr, check):\n\t\t\treturn 0;\n\t\t}\n\t}\n\nsw_checksum:\n\treturn skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tskb = sk_validate_xmit_skb(skb, dev);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\tskb = validate_xmit_xfrm(skb, features, again);\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tdev_core_stats_tx_dropped_inc(dev);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\t/* in case skb won't be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev, again);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size && skb_transport_header_was_set(skb)) {\n\t\tu16 gso_segs = shinfo->gso_segs;\n\t\tunsigned int hdr_len;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_offset(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\t\tconst struct tcphdr *th;\n\t\t\tstruct tcphdr _tcphdr;\n\n\t\t\tth = skb_header_pointer(skb, hdr_len,\n\t\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\t\tif (likely(th))\n\t\t\t\thdr_len += __tcp_hdrlen(th);\n\t\t} else if (shinfo->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tstruct udphdr _udphdr;\n\n\t\t\tif (skb_header_pointer(skb, hdr_len,\n\t\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\t\thdr_len += sizeof(struct udphdr);\n\t\t}\n\n\t\tif (unlikely(shinfo->gso_type & SKB_GSO_DODGY)) {\n\t\t\tint payload = skb->len - hdr_len;\n\n\t\t\t/* Malicious packet. */\n\t\t\tif (payload <= 0)\n\t\t\t\treturn;\n\t\t\tgso_segs = DIV_ROUND_UP(payload, shinfo->gso_size);\n\t\t}\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic int dev_qdisc_enqueue(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t     struct sk_buff **to_free,\n\t\t\t     struct netdev_queue *txq)\n{\n\tint rc;\n\n\trc = q->enqueue(skb, q, to_free) & NET_XMIT_MASK;\n\tif (rc == NET_XMIT_SUCCESS)\n\t\ttrace_qdisc_enqueue(q, txq, skb);\n\treturn rc;\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\n\ttcf_set_drop_reason(skb, SKB_DROP_REASON_QDISC_DROP);\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tif (q->flags & TCQ_F_CAN_BYPASS && nolock_qdisc_is_empty(q) &&\n\t\t    qdisc_run_begin(q)) {\n\t\t\t/* Retest nolock_qdisc_is_empty() within the protection\n\t\t\t * of q->seqlock to protect from racing with requeuing.\n\t\t\t */\n\t\t\tif (unlikely(!nolock_qdisc_is_empty(q))) {\n\t\t\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\t\t\t__qdisc_run(q);\n\t\t\t\tqdisc_run_end(q);\n\n\t\t\t\tgoto no_lock_out;\n\t\t\t}\n\n\t\t\tqdisc_bstats_cpu_update(q, skb);\n\t\t\tif (sch_direct_xmit(skb, q, dev, txq, NULL, true) &&\n\t\t\t    !nolock_qdisc_is_empty(q))\n\t\t\t\t__qdisc_run(q);\n\n\t\t\tqdisc_run_end(q);\n\t\t\treturn NET_XMIT_SUCCESS;\n\t\t}\n\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tqdisc_run(q);\n\nno_lock_out:\n\t\tif (unlikely(to_free))\n\t\t\tkfree_skb_list_reason(to_free,\n\t\t\t\t\t      tcf_get_drop_reason(to_free));\n\t\treturn rc;\n\t}\n\n\tif (unlikely(READ_ONCE(q->owner) == smp_processor_id())) {\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_RECLASSIFY_LOOP);\n\t\treturn NET_XMIT_DROP;\n\t}\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t * On PREEMPT_RT it is possible to preempt the qdisc owner during xmit\n\t * and then other tasks will only enqueue packets. The packets will be\n\t * sent after the qdisc owner is scheduled again. To prevent this\n\t * scenario the task always serialize on the lock.\n\t */\n\tcontended = qdisc_is_running(q) || IS_ENABLED(CONFIG_PREEMPT_RT);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\n\t\tqdisc_run_end(q);\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\tWRITE_ONCE(q->owner, smp_processor_id());\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tWRITE_ONCE(q->owner, -1);\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t\tqdisc_run_end(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list_reason(to_free,\n\t\t\t\t      tcf_get_drop_reason(to_free));\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tconst struct netprio_map *map;\n\tconst struct sock *sk;\n\tunsigned int prioidx;\n\n\tif (skb->priority)\n\t\treturn;\n\tmap = rcu_dereference_bh(skb->dev->priomap);\n\tif (!map)\n\t\treturn;\n\tsk = skb_to_full_sk(skb);\n\tif (!sk)\n\t\treturn;\n\n\tprioidx = sock_cgroup_prioidx(&sk->sk_cgrp_data);\n\n\tif (prioidx < map->priomap_len)\n\t\tskb->priority = map->priomap[prioidx];\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tDEBUG_NET_WARN_ON_ONCE(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct netdev_queue *\nnetdev_tx_queue_mapping(struct net_device *dev, struct sk_buff *skb)\n{\n\tint qm = skb_get_queue_mapping(skb);\n\n\treturn netdev_get_tx_queue(dev, netdev_cap_txqueue(dev, qm));\n}\n\n#ifndef CONFIG_PREEMPT_RT\nstatic bool netdev_xmit_txqueue_skipped(void)\n{\n\treturn __this_cpu_read(softnet_data.xmit.skip_txqueue);\n}\n\nvoid netdev_xmit_skip_txqueue(bool skip)\n{\n\t__this_cpu_write(softnet_data.xmit.skip_txqueue, skip);\n}\nEXPORT_SYMBOL_GPL(netdev_xmit_skip_txqueue);\n\n#else\nstatic bool netdev_xmit_txqueue_skipped(void)\n{\n\treturn current->net_xmit.skip_txqueue;\n}\n\nvoid netdev_xmit_skip_txqueue(bool skip)\n{\n\tcurrent->net_xmit.skip_txqueue = skip;\n}\nEXPORT_SYMBOL_GPL(netdev_xmit_skip_txqueue);\n#endif\n#endif /* CONFIG_NET_EGRESS */\n\n#ifdef CONFIG_NET_XGRESS\nstatic int tc_run(struct tcx_entry *entry, struct sk_buff *skb,\n\t\t  enum skb_drop_reason *drop_reason)\n{\n\tint ret = TC_ACT_UNSPEC;\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(entry->miniq);\n\tstruct tcf_result res;\n\n\tif (!miniq)\n\t\treturn ret;\n\n\tif (static_branch_unlikely(&tcf_bypass_check_needed_key)) {\n\t\tif (tcf_block_bypass_sw(miniq->block))\n\t\t\treturn ret;\n\t}\n\n\ttc_skb_cb(skb)->mru = 0;\n\ttc_skb_cb(skb)->post_ct = false;\n\ttcf_set_drop_reason(skb, *drop_reason);\n\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\tret = tcf_classify(skb, miniq->block, miniq->filter_list, &res, false);\n\t/* Only tcf related quirks below. */\n\tswitch (ret) {\n\tcase TC_ACT_SHOT:\n\t\t*drop_reason = tcf_get_drop_reason(skb);\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\tbreak;\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(res.classid);\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn ret;\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(tcx_needed_key);\n\nvoid tcx_inc(void)\n{\n\tstatic_branch_inc(&tcx_needed_key);\n}\n\nvoid tcx_dec(void)\n{\n\tstatic_branch_dec(&tcx_needed_key);\n}\n\nstatic __always_inline enum tcx_action_base\ntcx_run(const struct bpf_mprog_entry *entry, struct sk_buff *skb,\n\tconst bool needs_mac)\n{\n\tconst struct bpf_mprog_fp *fp;\n\tconst struct bpf_prog *prog;\n\tint ret = TCX_NEXT;\n\n\tif (needs_mac)\n\t\t__skb_push(skb, skb->mac_len);\n\tbpf_mprog_foreach_prog(entry, fp, prog) {\n\t\tbpf_compute_data_pointers(skb);\n\t\tret = bpf_prog_run(prog, skb);\n\t\tif (ret != TCX_NEXT)\n\t\t\tbreak;\n\t}\n\tif (needs_mac)\n\t\t__skb_pull(skb, skb->mac_len);\n\treturn tcx_action_code(skb, ret);\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n\tstruct bpf_mprog_entry *entry = rcu_dereference_bh(skb->dev->tcx_ingress);\n\tenum skb_drop_reason drop_reason = SKB_DROP_REASON_TC_INGRESS;\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\tint sch_ret;\n\n\tif (!entry)\n\t\treturn skb;\n\n\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\ttcx_set_ingress(skb, true);\n\n\tif (static_branch_unlikely(&tcx_needed_key)) {\n\t\tsch_ret = tcx_run(entry, skb, true);\n\t\tif (sch_ret != TC_ACT_UNSPEC)\n\t\t\tgoto ingress_verdict;\n\t}\n\tsch_ret = tc_run(tcx_entry(entry), skb, &drop_reason);\ningress_verdict:\n\tswitch (sch_ret) {\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by BPF, so we can safely\n\t\t * push the L2 header back before redirecting to another\n\t\t * netdev.\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tif (skb_do_redirect(skb) == -EAGAIN) {\n\t\t\t__skb_pull(skb, skb->mac_len);\n\t\t\t*another = true;\n\t\t\tbreak;\n\t\t}\n\t\t*ret = NET_RX_SUCCESS;\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\treturn NULL;\n\tcase TC_ACT_SHOT:\n\t\tkfree_skb_reason(skb, drop_reason);\n\t\t*ret = NET_RX_DROP;\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\treturn NULL;\n\t/* used by tc_run */\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\tfallthrough;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_RX_SUCCESS;\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\treturn NULL;\n\t}\n\tbpf_net_ctx_clear(bpf_net_ctx);\n\n\treturn skb;\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct bpf_mprog_entry *entry = rcu_dereference_bh(dev->tcx_egress);\n\tenum skb_drop_reason drop_reason = SKB_DROP_REASON_TC_EGRESS;\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\tint sch_ret;\n\n\tif (!entry)\n\t\treturn skb;\n\n\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\n\n\t/* qdisc_skb_cb(skb)->pkt_len & tcx_set_ingress() was\n\t * already set by the caller.\n\t */\n\tif (static_branch_unlikely(&tcx_needed_key)) {\n\t\tsch_ret = tcx_run(entry, skb, false);\n\t\tif (sch_ret != TC_ACT_UNSPEC)\n\t\t\tgoto egress_verdict;\n\t}\n\tsch_ret = tc_run(tcx_entry(entry), skb, &drop_reason);\negress_verdict:\n\tswitch (sch_ret) {\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\treturn NULL;\n\tcase TC_ACT_SHOT:\n\t\tkfree_skb_reason(skb, drop_reason);\n\t\t*ret = NET_XMIT_DROP;\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\treturn NULL;\n\t/* used by tc_run */\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\tfallthrough;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\treturn NULL;\n\t}\n\tbpf_net_ctx_clear(bpf_net_ctx);\n\n\treturn skb;\n}\n#else\nstatic __always_inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n\treturn skb;\n}\n\nstatic __always_inline struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\treturn skb;\n}\n#endif /* CONFIG_NET_XGRESS */\n\n#ifdef CONFIG_XPS\nstatic int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct xps_dev_maps *dev_maps, unsigned int tci)\n{\n\tint tc = netdev_get_prio_tc_map(dev, skb->priority);\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\tif (tc >= dev_maps->num_tc || tci >= dev_maps->nr_ids)\n\t\treturn queue_index;\n\n\ttci *= dev_maps->num_tc;\n\ttci += tc;\n\n\tmap = rcu_dereference(dev_maps->attr_map[tci]);\n\tif (map) {\n\t\tif (map->len == 1)\n\t\t\tqueue_index = map->queues[0];\n\t\telse\n\t\t\tqueue_index = map->queues[reciprocal_scale(\n\t\t\t\t\t\tskb_get_hash(skb), map->len)];\n\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\tqueue_index = -1;\n\t}\n\treturn queue_index;\n}\n#endif\n\nstatic int get_xps_queue(struct net_device *dev, struct net_device *sb_dev,\n\t\t\t struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct sock *sk = skb->sk;\n\tint queue_index = -1;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn -1;\n\n\trcu_read_lock();\n\tif (!static_key_false(&xps_rxqs_needed))\n\t\tgoto get_cpus_map;\n\n\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_RXQS]);\n\tif (dev_maps) {\n\t\tint tci = sk_rx_queue_get(sk);\n\n\t\tif (tci >= 0)\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t}\n\nget_cpus_map:\n\tif (queue_index < 0) {\n\t\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_CPUS]);\n\t\tif (dev_maps) {\n\t\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_pick_tx_zero);\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tsb_dev = sb_dev ? : dev;\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, sb_dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, sb_dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\nEXPORT_SYMBOL(netdev_pick_tx);\n\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, sb_dev);\n\t\telse\n\t\t\tqueue_index = netdev_pick_tx(dev, skb, sb_dev);\n\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n * __dev_queue_xmit() - transmit a buffer\n * @skb:\tbuffer to transmit\n * @sb_dev:\tsuboordinate device used for L2 forwarding offload\n *\n * Queue a buffer for transmission to a network device. The caller must\n * have set the device and priority and built the buffer before calling\n * this function. The function can be called from an interrupt.\n *\n * When calling this method, interrupts MUST be enabled. This is because\n * the BH enable code must have IRQs enabled so that it will not deadlock.\n *\n * Regardless of the return value, the skb is consumed, so it is currently\n * difficult to retry a send to this method. (You can bump the ref count\n * before sending to hold a reference for retry if you are careful.)\n *\n * Return:\n * * 0\t\t\t\t- buffer successfully transmitted\n * * positive qdisc return code\t- NET_XMIT_DROP etc.\n * * negative errno\t\t- other errors\n */\nint __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq = NULL;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\tbool again = false;\n\n\tskb_reset_mac_header(skb);\n\tskb_assert_len(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n\ttcx_set_ingress(skb, false);\n#ifdef CONFIG_NET_EGRESS\n\tif (static_branch_unlikely(&egress_needed_key)) {\n\t\tif (nf_hook_egress_active()) {\n\t\t\tskb = nf_hook_egress(skb, &rc, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tnetdev_xmit_skip_txqueue(false);\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tnf_skip_egress(skb, false);\n\n\t\tif (netdev_xmit_txqueue_skipped())\n\t\t\ttxq = netdev_tx_queue_mapping(dev, skb);\n\t}\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\tif (!txq)\n\t\ttxq = netdev_core_pick_tx(dev, skb, sb_dev);\n\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\t/* Other cpus might concurrently change txq->xmit_lock_owner\n\t\t * to -1 or to their cpu id, but not to our id.\n\t\t */\n\t\tif (READ_ONCE(txq->xmit_lock_owner) != cpu) {\n\t\t\tif (dev_xmit_recursion())\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev, &again);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\tdev_xmit_recursion_inc();\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\tdev_xmit_recursion_dec();\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\nEXPORT_SYMBOL(__dev_queue_xmit);\n\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev, &again);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tskb_set_queue_mapping(skb, queue_id);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\tlocal_bh_disable();\n\n\tdev_xmit_recursion_inc();\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\tdev_xmit_recursion_dec();\n\n\tlocal_bh_enable();\n\treturn ret;\ndrop:\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\nEXPORT_SYMBOL(__dev_direct_xmit);\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\nstatic DEFINE_PER_CPU(struct task_struct *, backlog_napi);\n\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tstruct task_struct *thread;\n\n\tlockdep_assert_irqs_disabled();\n\n\tif (test_bit(NAPI_STATE_THREADED, &napi->state)) {\n\t\t/* Paired with smp_mb__before_atomic() in\n\t\t * napi_enable()/dev_set_threaded().\n\t\t * Use READ_ONCE() to guarantee a complete\n\t\t * read on napi->thread. Only call\n\t\t * wake_up_process() when it's not NULL.\n\t\t */\n\t\tthread = READ_ONCE(napi->thread);\n\t\tif (thread) {\n\t\t\tif (use_backlog_threads() && thread == raw_cpu_read(backlog_napi))\n\t\t\t\tgoto use_local_napi;\n\n\t\t\tset_bit(NAPI_STATE_SCHED_THREADED, &napi->state);\n\t\t\twake_up_process(thread);\n\t\t\treturn;\n\t\t}\n\t}\n\nuse_local_napi:\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\tWRITE_ONCE(napi->list_owner, smp_processor_id());\n\t/* If not called from net_rx_action()\n\t * we have to raise NET_RX_SOFTIRQ.\n\t */\n\tif (!sd->in_net_rx_action)\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\nstruct static_key_false rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key_false rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n\t\tu32 head;\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu16 rxq_index;\n\t\tu32 flow_id;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\tWRITE_ONCE(rflow->filter, rc);\n\t\tif (old_rflow->filter == rc)\n\t\t\tWRITE_ONCE(old_rflow->filter, RPS_NO_FILTER);\n\tout:\n#endif\n\t\thead = READ_ONCE(per_cpu(softnet_data, next_cpu).input_queue_head);\n\t\trps_input_queue_tail_save(&rflow->last_qtail, head);\n\t}\n\n\tWRITE_ONCE(rflow->cpu, next_cpu);\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(net_hotdata.rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match.\n\t\t * This READ_ONCE() pairs with WRITE_ONCE() from rps_record_sock_flow().\n\t\t */\n\t\tident = READ_ONCE(sock_flow_table->ents[hash & sock_flow_table->mask]);\n\t\tif ((ident ^ hash) & ~net_hotdata.rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & net_hotdata.rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(READ_ONCE(per_cpu(softnet_data, tcpu).input_queue_head) -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = READ_ONCE(rflow->cpu);\n\t\tif (READ_ONCE(rflow->filter) == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(READ_ONCE(per_cpu(softnet_data, cpu).input_queue_head) -\n\t\t\t   READ_ONCE(rflow->last_qtail)) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/* Called from hardirq (IPI) context */\nstatic void trigger_rx_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\tsmp_store_release(&sd->defer_ipi_scheduled, 0);\n}\n\n/*\n * After we queued a packet into sd->input_pkt_queue,\n * we need to make sure this queue is serviced soon.\n *\n * - If this is another cpu queue, link it to our rps_ipi_list,\n *   and make sure we will process rps_ipi_list from net_rx_action().\n *\n * - If this is our own queue, NAPI schedule our backlog.\n *   Note that this also raises NET_RX_SOFTIRQ.\n */\nstatic void napi_schedule_rps(struct softnet_data *sd)\n{\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n#ifdef CONFIG_RPS\n\tif (sd != mysd) {\n\t\tif (use_backlog_threads()) {\n\t\t\t__napi_schedule_irqoff(&sd->backlog);\n\t\t\treturn;\n\t\t}\n\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t/* If not called from net_rx_action() or napi_threaded_poll()\n\t\t * we have to raise NET_RX_SOFTIRQ.\n\t\t */\n\t\tif (!mysd->in_net_rx_action && !mysd->in_napi_threaded_poll)\n\t\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn;\n\t}\n#endif /* CONFIG_RPS */\n\t__napi_schedule_irqoff(&mysd->backlog);\n}\n\nvoid kick_defer_list_purge(struct softnet_data *sd, unsigned int cpu)\n{\n\tunsigned long flags;\n\n\tif (use_backlog_threads()) {\n\t\tbacklog_lock_irq_save(sd, &flags);\n\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state))\n\t\t\t__napi_schedule_irqoff(&sd->backlog);\n\n\t\tbacklog_unlock_irq_restore(sd, &flags);\n\n\t} else if (!cmpxchg(&sd->defer_ipi_scheduled, 0, 1)) {\n\t\tsmp_call_function_single_async(cpu, &sd->defer_csd);\n\t}\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (READ_ONCE(net_hotdata.max_backlog) >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tenum skb_drop_reason reason;\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\tint max_backlog;\n\tu32 tail;\n\n\treason = SKB_DROP_REASON_DEV_READY;\n\tif (!netif_running(skb->dev))\n\t\tgoto bad_dev;\n\n\treason = SKB_DROP_REASON_CPU_BACKLOG;\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tqlen = skb_queue_len_lockless(&sd->input_pkt_queue);\n\tmax_backlog = READ_ONCE(net_hotdata.max_backlog);\n\tif (unlikely(qlen > max_backlog))\n\t\tgoto cpu_backlog_drop;\n\tbacklog_lock_irq_save(sd, &flags);\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (!qlen) {\n\t\t\t/* Schedule NAPI for backlog device. We can use\n\t\t\t * non atomic operation as we own the queue lock.\n\t\t\t */\n\t\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED,\n\t\t\t\t\t\t&sd->backlog.state))\n\t\t\t\tnapi_schedule_rps(sd);\n\t\t}\n\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\ttail = rps_input_queue_tail_incr(sd);\n\t\tbacklog_unlock_irq_restore(sd, &flags);\n\n\t\t/* save the tail outside of the critical section */\n\t\trps_input_queue_tail_save(qtail, tail);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\n\tbacklog_unlock_irq_restore(sd, &flags);\n\ncpu_backlog_drop:\n\tatomic_inc(&sd->dropped);\nbad_dev:\n\tdev_core_stats_rx_dropped_inc(skb->dev);\n\tkfree_skb_reason(skb, reason);\n\treturn NET_RX_DROP;\n}\n\nstatic struct netdev_rx_queue *netif_get_rxqueue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_rx_queue *rxqueue;\n\n\trxqueue = dev->_rx;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\n\t\t\treturn rxqueue; /* Return first rxqueue */\n\t\t}\n\t\trxqueue += index;\n\t}\n\treturn rxqueue;\n}\n\nu32 bpf_prog_run_generic_xdp(struct sk_buff *skb, struct xdp_buff *xdp,\n\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tvoid *orig_data, *orig_data_end, *hard_start;\n\tstruct netdev_rx_queue *rxqueue;\n\tbool orig_bcast, orig_host;\n\tu32 mac_len, frame_sz;\n\t__be16 orig_eth_type;\n\tstruct ethhdr *eth;\n\tu32 metalen, act;\n\tint off;\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thard_start = skb->data - skb_headroom(skb);\n\n\t/* SKB \"head\" area always have tailroom for skb_shared_info */\n\tframe_sz = (void *)skb_end_pointer(skb) - hard_start;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\trxqueue = netif_get_rxqueue(skb);\n\txdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,\n\t\t\t skb_headlen(skb) + mac_len, true);\n\tif (skb_is_nonlinear(skb)) {\n\t\tskb_shinfo(skb)->xdp_frags_size = skb->data_len;\n\t\txdp_buff_set_frags_flag(xdp);\n\t} else {\n\t\txdp_buff_clear_frags_flag(xdp);\n\t}\n\n\torig_data_end = xdp->data_end;\n\torig_data = xdp->data;\n\teth = (struct ethhdr *)xdp->data;\n\torig_host = ether_addr_equal_64bits(eth->h_dest, skb->dev->dev_addr);\n\torig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);\n\torig_eth_type = eth->h_proto;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t/* check if bpf_xdp_adjust_head was used */\n\toff = xdp->data - orig_data;\n\tif (off) {\n\t\tif (off > 0)\n\t\t\t__skb_pull(skb, off);\n\t\telse if (off < 0)\n\t\t\t__skb_push(skb, -off);\n\n\t\tskb->mac_header += off;\n\t\tskb_reset_network_header(skb);\n\t}\n\n\t/* check if bpf_xdp_adjust_tail was used */\n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0) {\n\t\tskb_set_tail_pointer(skb, xdp->data_end - xdp->data);\n\t\tskb->len += off; /* positive on grow, negative on shrink */\n\t}\n\n\t/* XDP frag metadata (e.g. nr_frags) are updated in eBPF helpers\n\t * (e.g. bpf_xdp_adjust_tail), we need to update data_len here.\n\t */\n\tif (xdp_buff_has_frags(xdp))\n\t\tskb->data_len = skb_shinfo(skb)->xdp_frags_size;\n\telse\n\t\tskb->data_len = 0;\n\n\t/* check if XDP changed eth hdr such SKB needs update */\n\teth = (struct ethhdr *)xdp->data;\n\tif ((orig_eth_type != eth->h_proto) ||\n\t    (orig_host != ether_addr_equal_64bits(eth->h_dest,\n\t\t\t\t\t\t  skb->dev->dev_addr)) ||\n\t    (orig_bcast != is_multicast_ether_addr_64bits(eth->h_dest))) {\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->pkt_type = PACKET_HOST;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t}\n\n\t/* Redirect/Tx gives L2 packet, code that will reuse skb must __skb_pull\n\t * before calling us again on redirect path. We do not call do_redirect\n\t * as we leave that up to the caller.\n\t *\n\t * Caller is responsible for managing lifetime of skb (i.e. calling\n\t * kfree_skb in response to actions it cannot handle/XDP_DROP).\n\t */\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\tbreak;\n\tcase XDP_PASS:\n\t\tmetalen = xdp->data - xdp->data_meta;\n\t\tif (metalen)\n\t\t\tskb_metadata_set(skb, metalen);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\nstatic int\nnetif_skb_check_for_xdp(struct sk_buff **pskb, struct bpf_prog *prog)\n{\n\tstruct sk_buff *skb = *pskb;\n\tint err, hroom, troom;\n\n\tif (!skb_cow_data_for_xdp(this_cpu_read(system_page_pool), pskb, prog))\n\t\treturn 0;\n\n\t/* In case we have to go down the path and also linearize,\n\t * then lets do the pskb_expand_head() work just once here.\n\t */\n\throom = XDP_PACKET_HEADROOM - skb_headroom(skb);\n\ttroom = skb->tail + skb->data_len - skb->end;\n\terr = pskb_expand_head(skb,\n\t\t\t       hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,\n\t\t\t       troom > 0 ? troom + 128 : 0, GFP_ATOMIC);\n\tif (err)\n\t\treturn err;\n\n\treturn skb_linearize(skb);\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff **pskb,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tstruct sk_buff *skb = *pskb;\n\tu32 mac_len, act = XDP_DROP;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_is_redirected(skb))\n\t\treturn XDP_PASS;\n\n\t/* XDP packets must have sufficient headroom of XDP_PACKET_HEADROOM\n\t * bytes. This is the guarantee that also native XDP provides,\n\t * thus we need to do it here as well.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\t__skb_push(skb, mac_len);\n\n\tif (skb_cloned(skb) || skb_is_nonlinear(skb) ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tif (netif_skb_check_for_xdp(pskb, xdp_prog))\n\t\t\tgoto do_drop;\n\t}\n\n\t__skb_pull(*pskb, mac_len);\n\n\tact = bpf_prog_run_generic_xdp(*pskb, xdp, xdp_prog);\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\tcase XDP_PASS:\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action((*pskb)->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception((*pskb)->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(*pskb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior. This also means\n * that XDP packets are able to starve other packets going through a qdisc,\n * and DDOS attacks will be more effective. In-driver-XDP use dedicated TX\n * queues, so they do not have this starvation issue.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_core_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_frozen_or_drv_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tdev_core_stats_tx_dropped_inc(dev);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(generic_xdp_needed_key);\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff **pskb)\n{\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\n\tif (xdp_prog) {\n\t\tstruct xdp_buff xdp;\n\t\tu32 act;\n\t\tint err;\n\n\t\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\n\t\tact = netif_receive_generic_xdp(pskb, &xdp, xdp_prog);\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect((*pskb)->dev, *pskb,\n\t\t\t\t\t\t\t      &xdp, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t\tbreak;\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(*pskb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\t\treturn XDP_DROP;\n\t\t}\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tbpf_net_ctx_clear(bpf_net_ctx);\n\tkfree_skb_reason(*pskb, SKB_DROP_REASON_XDP);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(net_hotdata.tstamp_prequeue), skb);\n\n\ttrace_netif_rx(skb);\n\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, smp_processor_id(), &qtail);\n\t}\n\treturn ret;\n}\n\n/**\n *\t__netif_rx\t-\tSlightly optimized version of netif_rx\n *\t@skb: buffer to post\n *\n *\tThis behaves as netif_rx except that it does not disable bottom halves.\n *\tAs a result this function may only be invoked from the interrupt context\n *\t(either hard or soft interrupt).\n */\nint __netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\tlockdep_assert_once(hardirq_count() | softirq_count());\n\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netif_rx);\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process via the backlog NAPI device. It\n *\talways succeeds. The buffer may be dropped during processing for\n *\tcongestion control or by the protocol layers.\n *\tThe network buffer is passed via the backlog NAPI device. Modern NIC\n *\tdriver should use NAPI and GRO.\n *\tThis function can used from interrupt and from process context. The\n *\tcaller from process context must not disable interrupts before invoking\n *\tthis function.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\nint netif_rx(struct sk_buff *skb)\n{\n\tbool need_bh_off = !(hardirq_count() | softirq_count());\n\tint ret;\n\n\tif (need_bh_off)\n\t\tlocal_bh_disable();\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\tif (need_bh_off)\n\t\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nstatic __latent_entropy void net_tx_action(void)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb, net_tx_action);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action,\n\t\t\t\t\t\tget_kfree_skb_cb(skb)->reason, NULL);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__napi_kfree_skb(skb,\n\t\t\t\t\t\t get_kfree_skb_cb(skb)->reason);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\trcu_read_lock();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock = NULL;\n\n\t\t\thead = head->next_sched;\n\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\n\t\t\tif (!(q->flags & TCQ_F_NOLOCK)) {\n\t\t\t\troot_lock = qdisc_lock(q);\n\t\t\t\tspin_lock(root_lock);\n\t\t\t} else if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t\t     &q->state))) {\n\t\t\t\t/* There is a synchronize_net() between\n\t\t\t\t * STATE_DEACTIVATED flag being set and\n\t\t\t\t * qdisc_reset()/some_qdisc_is_busy() in\n\t\t\t\t * dev_deactivate(), so we can safely bail out\n\t\t\t\t * early here to avoid data race between\n\t\t\t\t * qdisc_deactivate() and some_qdisc_is_busy()\n\t\t\t\t * for lockless qdisc.\n\t\t\t\t */\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tif (root_lock)\n\t\t\t\tspin_unlock(root_lock);\n\t\t}\n\n\t\trcu_read_unlock();\n\t}\n\n\txfrm_dev_backlog(sd);\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\tif (dev->priv_flags & IFF_NO_RX_HANDLER)\n\t\treturn -EINVAL;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,\n\t\t\t\t    struct packet_type **ppt_prev)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!READ_ONCE(net_hotdata.tstamp_prequeue), skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (static_branch_unlikely(&generic_xdp_needed_key)) {\n\t\tint ret2;\n\n\t\tmigrate_disable();\n\t\tret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog),\n\t\t\t\t      &skb);\n\t\tmigrate_enable();\n\n\t\tif (ret2 != XDP_PASS) {\n\t\t\tret = NET_RX_DROP;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (eth_type_vlan(skb->protocol)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &net_hotdata.ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_branch_unlikely(&ingress_needed_key)) {\n\t\tbool another = false;\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev,\n\t\t\t\t\t &another);\n\t\tif (another)\n\t\t\tgoto another_round;\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tnf_skip_egress(skb, false);\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_redirect(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\t\tbreak;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {\ncheck_vlan_id:\n\t\tif (skb_vlan_tag_get_id(skb)) {\n\t\t\t/* Vlan id is non 0 and vlan_do_receive() above couldn't\n\t\t\t * find vlan device.\n\t\t\t */\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t} else if (eth_type_vlan(skb->protocol)) {\n\t\t\t/* Outer header is 802.1P with vlan 0, inner header is\n\t\t\t * 802.1Q or 802.1AD and vlan_do_receive() above could\n\t\t\t * not find vlan dev for vlan id 0.\n\t\t\t */\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb = skb_vlan_untag(skb);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\tif (vlan_do_receive(&skb))\n\t\t\t\t/* After stripping off 802.1P header with vlan 0\n\t\t\t\t * vlan dev is found for inner header.\n\t\t\t\t */\n\t\t\t\tgoto another_round;\n\t\t\telse if (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\telse\n\t\t\t\t/* We have stripped outer 802.1P vlan 0 header.\n\t\t\t\t * But could not find vlan dev.\n\t\t\t\t * check again for vlan id to set OTHERHOST.\n\t\t\t\t */\n\t\t\t\tgoto check_vlan_id;\n\t\t}\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\t*ppt_prev = pt_prev;\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tdev_core_stats_rx_dropped_inc(skb->dev);\n\t\telse\n\t\t\tdev_core_stats_rx_nohandler_inc(skb->dev);\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\t/* The invariant here is that if *ppt_prev is not NULL\n\t * then skb should also be non-NULL.\n\t *\n\t * Apparently *ppt_prev assignment above holds this invariant due to\n\t * skb dereferencing near it.\n\t */\n\t*pskb = skb;\n\treturn ret;\n}\n\nstatic int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct net_device *orig_dev = skb->dev;\n\tstruct packet_type *pt_prev = NULL;\n\tint ret;\n\n\tret = __netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\tif (pt_prev)\n\t\tret = INDIRECT_CALL_INET(pt_prev->func, ipv6_rcv, ip_rcv, skb,\n\t\t\t\t\t skb->dev, pt_prev, orig_dev);\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb_core - special purpose version of netif_receive_skb\n *\t@skb: buffer to process\n *\n *\tMore direct receive version of netif_receive_skb().  It should\n *\tonly be used by callers that have a need to skip RPS and Generic XDP.\n *\tCaller must also take care of handling if ``(page_is_)pfmemalloc``.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb_core(struct sk_buff *skb)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __netif_receive_skb_one_core(skb, false);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb_core);\n\nstatic inline void __netif_receive_skb_list_ptype(struct list_head *head,\n\t\t\t\t\t\t  struct packet_type *pt_prev,\n\t\t\t\t\t\t  struct net_device *orig_dev)\n{\n\tstruct sk_buff *skb, *next;\n\n\tif (!pt_prev)\n\t\treturn;\n\tif (list_empty(head))\n\t\treturn;\n\tif (pt_prev->list_func != NULL)\n\t\tINDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,\n\t\t\t\t   ip_list_rcv, head, pt_prev, orig_dev);\n\telse\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tskb_list_del_init(skb);\n\t\t\tpt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t\t}\n}\n\nstatic void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)\n{\n\t/* Fast-path assumptions:\n\t * - There is no RX handler.\n\t * - Only one packet_type matches.\n\t * If either of these fails, we will end up doing some per-packet\n\t * processing in-line, then handling the 'last ptype' for the whole\n\t * sublist.  This can't cause out-of-order delivery to any single ptype,\n\t * because the 'last ptype' must be constant across the sublist, and all\n\t * other ptypes are handled per-packet.\n\t */\n\t/* Current (common) ptype of sublist */\n\tstruct packet_type *pt_curr = NULL;\n\t/* Current (common) orig_dev of sublist */\n\tstruct net_device *od_curr = NULL;\n\tstruct sk_buff *skb, *next;\n\tLIST_HEAD(sublist);\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tstruct net_device *orig_dev = skb->dev;\n\t\tstruct packet_type *pt_prev = NULL;\n\n\t\tskb_list_del_init(skb);\n\t\t__netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\t\tif (!pt_prev)\n\t\t\tcontinue;\n\t\tif (pt_curr != pt_prev || od_curr != orig_dev) {\n\t\t\t/* dispatch old sublist */\n\t\t\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n\t\t\t/* start new sublist */\n\t\t\tINIT_LIST_HEAD(&sublist);\n\t\t\tpt_curr = pt_prev;\n\t\t\tod_curr = orig_dev;\n\t\t}\n\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\n\t/* dispatch final sublist */\n\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_one_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_one_core(skb, false);\n\n\treturn ret;\n}\n\nstatic void __netif_receive_skb_list(struct list_head *head)\n{\n\tunsigned long noreclaim_flag = 0;\n\tstruct sk_buff *skb, *next;\n\tbool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tif ((sk_memalloc_socks() && skb_pfmemalloc(skb)) != pfmemalloc) {\n\t\t\tstruct list_head sublist;\n\n\t\t\t/* Handle the previous sublist */\n\t\t\tlist_cut_before(&sublist, head, &skb->list);\n\t\t\tif (!list_empty(&sublist))\n\t\t\t\t__netif_receive_skb_list_core(&sublist, pfmemalloc);\n\t\t\tpfmemalloc = !pfmemalloc;\n\t\t\t/* See comments in __netif_receive_skb */\n\t\t\tif (pfmemalloc)\n\t\t\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\t\telse\n\t\t\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t\t}\n\t}\n\t/* Handle the remaining sublist */\n\tif (!list_empty(head))\n\t\t__netif_receive_skb_list_core(head, pfmemalloc);\n\t/* Restore pflags */\n\tif (pfmemalloc)\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_branch_dec(&generic_xdp_needed_key);\n\t\t} else if (new && !old) {\n\t\t\tstatic_branch_inc(&generic_xdp_needed_key);\n\t\t\tdev_disable_lro(dev);\n\t\t\tdev_disable_gro_hw(dev);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(net_hotdata.tstamp_prequeue), skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\nvoid netif_receive_skb_list_internal(struct list_head *head)\n{\n\tstruct sk_buff *skb, *next;\n\tLIST_HEAD(sublist);\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tnet_timestamp_check(READ_ONCE(net_hotdata.tstamp_prequeue),\n\t\t\t\t    skb);\n\t\tskb_list_del_init(skb);\n\t\tif (!skb_defer_rx_timestamp(skb))\n\t\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\tlist_splice_init(&sublist, head);\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\t\tif (cpu >= 0) {\n\t\t\t\t/* Will be handled, remove from list */\n\t\t\t\tskb_list_del_init(skb);\n\t\t\t\tenqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\t__netif_receive_skb_list(head);\n\trcu_read_unlock();\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_receive_skb_entry(skb);\n\n\tret = netif_receive_skb_internal(skb);\n\ttrace_netif_receive_skb_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/**\n *\tnetif_receive_skb_list - process many receive buffers from network\n *\t@head: list of skbs to process.\n *\n *\tSince return value of netif_receive_skb() is normally ignored, and\n *\twouldn't be meaningful for a list, this function returns void.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n */\nvoid netif_receive_skb_list(struct list_head *head)\n{\n\tstruct sk_buff *skb;\n\n\tif (list_empty(head))\n\t\treturn;\n\tif (trace_netif_receive_skb_list_entry_enabled()) {\n\t\tlist_for_each_entry(skb, head, list)\n\t\t\ttrace_netif_receive_skb_list_entry(skb);\n\t}\n\tnetif_receive_skb_list_internal(head);\n\ttrace_netif_receive_skb_list_exit(0);\n}\nEXPORT_SYMBOL(netif_receive_skb_list);\n\nstatic DEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\tbacklog_lock_irq_disable(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\trps_input_queue_head_incr(sd);\n\t\t}\n\t}\n\tbacklog_unlock_irq_enable(sd);\n\n\tlocal_lock_nested_bh(&softnet_data.process_queue_bh_lock);\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\trps_input_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_unlock_nested_bh(&softnet_data.process_queue_bh_lock);\n\tlocal_bh_enable();\n}\n\nstatic bool flush_required(int cpu)\n{\n#if IS_ENABLED(CONFIG_RPS)\n\tstruct softnet_data *sd = &per_cpu(softnet_data, cpu);\n\tbool do_flush;\n\n\tbacklog_lock_irq_disable(sd);\n\n\t/* as insertion into process_queue happens with the rps lock held,\n\t * process_queue access may race only with dequeue\n\t */\n\tdo_flush = !skb_queue_empty(&sd->input_pkt_queue) ||\n\t\t   !skb_queue_empty_lockless(&sd->process_queue);\n\tbacklog_unlock_irq_enable(sd);\n\n\treturn do_flush;\n#endif\n\t/* without RPS we can't safely check input_pkt_queue: during a\n\t * concurrent remote skb_queue_splice() we can detect as empty both\n\t * input_pkt_queue and process_queue even if the latter could end-up\n\t * containing a lot of packets.\n\t */\n\treturn true;\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tstatic cpumask_t flush_cpus;\n\tunsigned int cpu;\n\n\t/* since we are under rtnl lock protection we can use static data\n\t * for the cpumask and avoid allocating on stack the possibly\n\t * large mask\n\t */\n\tASSERT_RTNL();\n\n\tcpus_read_lock();\n\n\tcpumask_clear(&flush_cpus);\n\tfor_each_online_cpu(cpu) {\n\t\tif (flush_required(cpu)) {\n\t\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\t\t\tcpumask_set_cpu(cpu, &flush_cpus);\n\t\t}\n\t}\n\n\t/* we can have in flight packet[s] on the cpus we are not flushing,\n\t * synchronize_net() in unregister_netdevice_many() will take care of\n\t * them\n\t */\n\tfor_each_cpu(cpu, &flush_cpus)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tcpus_read_unlock();\n}\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (!use_backlog_threads() && remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn !use_backlog_threads() && sd->rps_ipi_list;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = READ_ONCE(net_hotdata.dev_rx_weight);\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\tlocal_lock_nested_bh(&softnet_data.process_queue_bh_lock);\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\tlocal_unlock_nested_bh(&softnet_data.process_queue_bh_lock);\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tif (++work >= quota) {\n\t\t\t\trps_input_queue_head_add(sd, work);\n\t\t\t\treturn work;\n\t\t\t}\n\n\t\t\tlocal_lock_nested_bh(&softnet_data.process_queue_bh_lock);\n\t\t}\n\t\tlocal_unlock_nested_bh(&softnet_data.process_queue_bh_lock);\n\n\t\tbacklog_lock_irq_disable(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state &= NAPIF_STATE_THREADED;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tlocal_lock_nested_bh(&softnet_data.process_queue_bh_lock);\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t\tlocal_unlock_nested_bh(&softnet_data.process_queue_bh_lock);\n\t\t}\n\t\tbacklog_unlock_irq_enable(sd);\n\t}\n\n\tif (work)\n\t\trps_input_queue_head_add(sd, work);\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable to\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long new, val = READ_ONCE(n->state);\n\n\tdo {\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked.\n *\n * On PREEMPT_RT enabled kernels this maps to __napi_schedule()\n * because the interrupt disabled assumption might not be true\n * due to force-threaded interrupts and spinlock substitution.\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\telse\n\t\t__napi_schedule(n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new, timeout = 0;\n\tbool ret = true;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (work_done) {\n\t\tif (n->gro_bitmask)\n\t\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tn->defer_hard_irqs_count = READ_ONCE(n->dev->napi_defer_hard_irqs);\n\t}\n\tif (n->defer_hard_irqs_count > 0) {\n\t\tn->defer_hard_irqs_count--;\n\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tif (timeout)\n\t\t\tret = false;\n\t}\n\tif (n->gro_bitmask) {\n\t\t/* When the NAPI instance uses a timeout and keeps postponing\n\t\t * it, we need to bound somehow the time packets are kept in\n\t\t * the GRO layer\n\t\t */\n\t\tnapi_gro_flush(n, !!timeout);\n\t}\n\n\tgro_normal_list(n);\n\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\tWRITE_ONCE(n->list_owner, -1);\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |\n\t\t\t      NAPIF_STATE_SCHED_THREADED |\n\t\t\t      NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\tif (timeout)\n\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstruct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\nstatic void skb_defer_free_flush(struct softnet_data *sd)\n{\n\tstruct sk_buff *skb, *next;\n\n\t/* Paired with WRITE_ONCE() in skb_attempt_defer_free() */\n\tif (!READ_ONCE(sd->defer_list))\n\t\treturn;\n\n\tspin_lock(&sd->defer_lock);\n\tskb = sd->defer_list;\n\tsd->defer_list = NULL;\n\tsd->defer_count = 0;\n\tspin_unlock(&sd->defer_lock);\n\n\twhile (skb != NULL) {\n\t\tnext = skb->next;\n\t\tnapi_consume_skb(skb, 1);\n\t\tskb = next;\n\t}\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\nstatic void __busy_poll_stop(struct napi_struct *napi, bool skip_schedule)\n{\n\tif (!skip_schedule) {\n\t\tgro_normal_list(napi);\n\t\t__napi_schedule(napi);\n\t\treturn;\n\t}\n\n\tif (napi->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(napi, HZ >= 1000);\n\t}\n\n\tgro_normal_list(napi);\n\tclear_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\nenum {\n\tNAPI_F_PREFER_BUSY_POLL\t= 1,\n\tNAPI_F_END_ON_RESCHED\t= 2,\n};\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock,\n\t\t\t   unsigned flags, u16 budget)\n{\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\tbool skip_schedule = false;\n\tunsigned long timeout;\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\n\n\tif (flags & NAPI_F_PREFER_BUSY_POLL) {\n\t\tnapi->defer_hard_irqs_count = READ_ONCE(napi->dev->napi_defer_hard_irqs);\n\t\ttimeout = READ_ONCE(napi->dev->gro_flush_timeout);\n\t\tif (napi->defer_hard_irqs_count && timeout) {\n\t\t\thrtimer_start(&napi->timer, ns_to_ktime(timeout), HRTIMER_MODE_REL_PINNED);\n\t\t\tskip_schedule = true;\n\t\t}\n\t}\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, budget);\n\t/* We can't gro_normal_list() here, because napi->poll() might have\n\t * rearmed the napi (napi_complete_done()) in which case it could\n\t * already be running on another CPU.\n\t */\n\ttrace_napi_poll(napi, rc, budget);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == budget)\n\t\t__busy_poll_stop(napi, skip_schedule);\n\tbpf_net_ctx_clear(bpf_net_ctx);\n\tlocal_bh_enable();\n}\n\nstatic void __napi_busy_loop(unsigned int napi_id,\n\t\t      bool (*loop_end)(void *, unsigned long),\n\t\t      void *loop_end_arg, unsigned flags, u16 budget)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\nrestart:\n\tnapi_poll = NULL;\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\treturn;\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL)) {\n\t\t\t\tif (flags & NAPI_F_PREFER_BUSY_POLL)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val) {\n\t\t\t\tif (flags & NAPI_F_PREFER_BUSY_POLL)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, budget);\n\t\ttrace_napi_poll(napi, work, budget);\n\t\tgro_normal_list(napi);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tskb_defer_free_flush(this_cpu_ptr(&softnet_data));\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (flags & NAPI_F_END_ON_RESCHED)\n\t\t\t\tbreak;\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock, flags, budget);\n\t\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\trcu_read_lock();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock, flags, budget);\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_enable();\n}\n\nvoid napi_busy_loop_rcu(unsigned int napi_id,\n\t\t\tbool (*loop_end)(void *, unsigned long),\n\t\t\tvoid *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned flags = NAPI_F_END_ON_RESCHED;\n\n\tif (prefer_busy_poll)\n\t\tflags |= NAPI_F_PREFER_BUSY_POLL;\n\n\t__napi_busy_loop(napi_id, loop_end, loop_end_arg, flags, budget);\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned flags = prefer_busy_poll ? NAPI_F_PREFER_BUSY_POLL : 0;\n\n\trcu_read_lock();\n\t__napi_busy_loop(napi_id, loop_end, loop_end_arg, flags, budget);\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nstatic void napi_hash_del(struct napi_struct *napi)\n{\n\tspin_lock(&napi_hash_lock);\n\n\thlist_del_init_rcu(&napi->napi_hash_node);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (!napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t__napi_schedule_irqoff(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void init_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tINIT_LIST_HEAD(&napi->gro_hash[i].list);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n\tnapi->gro_bitmask = 0;\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded)\n{\n\tstruct napi_struct *napi;\n\tint err = 0;\n\n\tif (dev->threaded == threaded)\n\t\treturn 0;\n\n\tif (threaded) {\n\t\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\t\tif (!napi->thread) {\n\t\t\t\terr = napi_kthread_create(napi);\n\t\t\t\tif (err) {\n\t\t\t\t\tthreaded = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tWRITE_ONCE(dev->threaded, threaded);\n\n\t/* Make sure kthread is created before THREADED bit\n\t * is set.\n\t */\n\tsmp_mb__before_atomic();\n\n\t/* Setting/unsetting threaded mode on a napi might not immediately\n\t * take effect, if the current napi instance is actively being\n\t * polled. In this case, the switch between threaded mode and\n\t * softirq mode will happen in the next round of napi_schedule().\n\t * This should not cause hiccups/stalls to the live traffic.\n\t */\n\tlist_for_each_entry(napi, &dev->napi_list, dev_list)\n\t\tassign_bit(NAPI_STATE_THREADED, &napi->state, threaded);\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_threaded);\n\n/**\n * netif_queue_set_napi - Associate queue with the napi\n * @dev: device to which NAPI and queue belong\n * @queue_index: Index of queue\n * @type: queue type as RX or TX\n * @napi: NAPI context, pass NULL to clear previously set NAPI\n *\n * Set queue with its corresponding napi context. This should be done after\n * registering the NAPI handler for the queue-vector and the queues have been\n * mapped to the corresponding interrupt vector.\n */\nvoid netif_queue_set_napi(struct net_device *dev, unsigned int queue_index,\n\t\t\t  enum netdev_queue_type type, struct napi_struct *napi)\n{\n\tstruct netdev_rx_queue *rxq;\n\tstruct netdev_queue *txq;\n\n\tif (WARN_ON_ONCE(napi && !napi->dev))\n\t\treturn;\n\tif (dev->reg_state >= NETREG_REGISTERED)\n\t\tASSERT_RTNL();\n\n\tswitch (type) {\n\tcase NETDEV_QUEUE_TYPE_RX:\n\t\trxq = __netif_get_rx_queue(dev, queue_index);\n\t\trxq->napi = napi;\n\t\treturn;\n\tcase NETDEV_QUEUE_TYPE_TX:\n\t\ttxq = netdev_get_tx_queue(dev, queue_index);\n\t\ttxq->napi = napi;\n\t\treturn;\n\tdefault:\n\t\treturn;\n\t}\n}\nEXPORT_SYMBOL(netif_queue_set_napi);\n\nvoid netif_napi_add_weight(struct net_device *dev, struct napi_struct *napi,\n\t\t\t   int (*poll)(struct napi_struct *, int), int weight)\n{\n\tif (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tINIT_HLIST_NODE(&napi->napi_hash_node);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tinit_gro_hash(napi);\n\tnapi->skb = NULL;\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tnetdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,\n\t\t\t\tweight);\n\tnapi->weight = weight;\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tnapi->list_owner = -1;\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tset_bit(NAPI_STATE_NPSVC, &napi->state);\n\tlist_add_rcu(&napi->dev_list, &dev->napi_list);\n\tnapi_hash_add(napi);\n\tnapi_get_frags_check(napi);\n\t/* Create kthread for this napi if dev->threaded is set.\n\t * Clear dev->threaded if kthread creation failed so that\n\t * threaded mode will not be enabled in napi_enable().\n\t */\n\tif (dev->threaded && napi_kthread_create(napi))\n\t\tdev->threaded = false;\n\tnetif_napi_set_irq(napi, -1);\n}\nEXPORT_SYMBOL(netif_napi_add_weight);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\twhile (val & (NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC)) {\n\t\t\tusleep_range(20, 200);\n\t\t\tval = READ_ONCE(n->state);\n\t\t}\n\n\t\tnew = val | NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC;\n\t\tnew &= ~(NAPIF_STATE_THREADED | NAPIF_STATE_PREFER_BUSY_POLL);\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: NAPI context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nvoid napi_enable(struct napi_struct *n)\n{\n\tunsigned long new, val = READ_ONCE(n->state);\n\n\tdo {\n\t\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &val));\n\n\t\tnew = val & ~(NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC);\n\t\tif (n->dev->threaded && n->thread)\n\t\t\tnew |= NAPIF_STATE_THREADED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n}\nEXPORT_SYMBOL(napi_enable);\n\nstatic void flush_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tstruct sk_buff *skb, *n;\n\n\t\tlist_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)\n\t\t\tkfree_skb(skb);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n}\n\n/* Must be called in process context */\nvoid __netif_napi_del(struct napi_struct *napi)\n{\n\tif (!test_and_clear_bit(NAPI_STATE_LISTED, &napi->state))\n\t\treturn;\n\n\tnapi_hash_del(napi);\n\tlist_del_rcu(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tflush_gro_hash(napi);\n\tnapi->gro_bitmask = 0;\n\n\tif (napi->thread) {\n\t\tkthread_stop(napi->thread);\n\t\tnapi->thread = NULL;\n\t}\n}\nEXPORT_SYMBOL(__netif_napi_del);\n\nstatic int __napi_poll(struct napi_struct *n, bool *repoll)\n{\n\tint work, weight;\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (napi_is_scheduled(n)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\n\t\txdp_do_check_flushed(n);\n\t}\n\n\tif (unlikely(work > weight))\n\t\tnetdev_err_once(n->dev, \"NAPI poll function %pS returned %d, exceeding its budget of %d.\\n\",\n\t\t\t\tn->poll, work, weight);\n\n\tif (likely(work < weight))\n\t\treturn work;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\treturn work;\n\t}\n\n\t/* The NAPI context has more processing work, but busy-polling\n\t * is preferred. Exit early.\n\t */\n\tif (napi_prefer_busy_poll(n)) {\n\t\tif (napi_complete_done(n, work)) {\n\t\t\t/* If timeout is not set, we need to make sure\n\t\t\t * that the NAPI is re-scheduled.\n\t\t\t */\n\t\t\tnapi_schedule(n);\n\t\t}\n\t\treturn work;\n\t}\n\n\tif (n->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\tgro_normal_list(n);\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\treturn work;\n\t}\n\n\t*repoll = true;\n\n\treturn work;\n}\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tbool do_repoll = false;\n\tvoid *have;\n\tint work;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\twork = __napi_poll(n, &do_repoll);\n\n\tif (do_repoll)\n\t\tlist_add_tail(&n->poll_list, repoll);\n\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic int napi_thread_wait(struct napi_struct *napi)\n{\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\twhile (!kthread_should_stop()) {\n\t\t/* Testing SCHED_THREADED bit here to make sure the current\n\t\t * kthread owns this napi and could poll on this napi.\n\t\t * Testing SCHED bit is not enough because SCHED bit might be\n\t\t * set by some other busy poll thread or by napi_disable().\n\t\t */\n\t\tif (test_bit(NAPI_STATE_SCHED_THREADED, &napi->state)) {\n\t\t\tWARN_ON(!list_empty(&napi->poll_list));\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn 0;\n\t\t}\n\n\t\tschedule();\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn -1;\n}\n\nstatic void napi_threaded_poll_loop(struct napi_struct *napi)\n{\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\tstruct softnet_data *sd;\n\tunsigned long last_qs = jiffies;\n\n\tfor (;;) {\n\t\tbool repoll = false;\n\t\tvoid *have;\n\n\t\tlocal_bh_disable();\n\t\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\n\n\t\tsd = this_cpu_ptr(&softnet_data);\n\t\tsd->in_napi_threaded_poll = true;\n\n\t\thave = netpoll_poll_lock(napi);\n\t\t__napi_poll(napi, &repoll);\n\t\tnetpoll_poll_unlock(have);\n\n\t\tsd->in_napi_threaded_poll = false;\n\t\tbarrier();\n\n\t\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\t\tlocal_irq_disable();\n\t\t\tnet_rps_action_and_irq_enable(sd);\n\t\t}\n\t\tskb_defer_free_flush(sd);\n\t\tbpf_net_ctx_clear(bpf_net_ctx);\n\t\tlocal_bh_enable();\n\n\t\tif (!repoll)\n\t\t\tbreak;\n\n\t\trcu_softirq_qs_periodic(last_qs);\n\t\tcond_resched();\n\t}\n}\n\nstatic int napi_threaded_poll(void *data)\n{\n\tstruct napi_struct *napi = data;\n\n\twhile (!napi_thread_wait(napi))\n\t\tnapi_threaded_poll_loop(napi);\n\n\treturn 0;\n}\n\nstatic __latent_entropy void net_rx_action(void)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(READ_ONCE(net_hotdata.netdev_budget_usecs));\n\tstruct bpf_net_context __bpf_net_ctx, *bpf_net_ctx;\n\tint budget = READ_ONCE(net_hotdata.netdev_budget);\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tbpf_net_ctx = bpf_net_ctx_set(&__bpf_net_ctx);\nstart:\n\tsd->in_net_rx_action = true;\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tskb_defer_free_flush(sd);\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (list_empty(&repoll)) {\n\t\t\t\tsd->in_net_rx_action = false;\n\t\t\t\tbarrier();\n\t\t\t\t/* We need to check if ____napi_schedule()\n\t\t\t\t * had refilled poll_list while\n\t\t\t\t * sd->in_net_rx_action was true.\n\t\t\t\t */\n\t\t\t\tif (!list_empty(&sd->poll_list))\n\t\t\t\t\tgoto start;\n\t\t\t\tif (!sd_has_rps_ipi_waiting(sd))\n\t\t\t\t\tgoto end;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\telse\n\t\tsd->in_net_rx_action = false;\n\n\tnet_rps_action_and_irq_enable(sd);\nend:\n\tbpf_net_ctx_clear(bpf_net_ctx);\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\tnetdevice_tracker dev_tracker;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* lookup ignore flag */\n\tbool ignore;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int ____netdev_has_upper_dev(struct net_device *upper_dev,\n\t\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct net_device *dev = (struct net_device *)priv->data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t     &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all_rcu - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t       &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nstatic struct net_device *__netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master) && !upper->ignore)\n\t\treturn upper->dev;\n\treturn NULL;\n}\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *__netdev_next_upper_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\t*ignore = upper->ignore;\n\n\treturn upper->dev;\n}\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nstatic int __netdev_walk_all_upper_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = __netdev_next_upper_dev(now, &iter, &ignore);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = netdev_next_upper_dev_rcu(now, &iter);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\nstatic bool __netdev_has_upper_dev(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn __netdev_walk_all_upper_dev(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t   &priv);\n}\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nstatic struct net_device *__netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\t*ignore = lower->ignore;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic int __netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = __netdev_next_lower_dev(now, &iter, &ignore);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_next_lower_dev_rcu);\n\nstatic u8 __netdev_upper_depth(struct net_device *dev)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore);\n\t     udev;\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < udev->upper_level)\n\t\t\tmax_depth = udev->upper_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic u8 __netdev_lower_depth(struct net_device *dev)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore);\n\t     ldev;\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < ldev->lower_level)\n\t\t\tmax_depth = ldev->lower_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic int __netdev_update_upper_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *__unused)\n{\n\tdev->upper_level = __netdev_upper_depth(dev) + 1;\n\treturn 0;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic LIST_HEAD(net_unlink_list);\n\nstatic void net_unlink_todo(struct net_device *dev)\n{\n\tif (list_empty(&dev->unlink_list))\n\t\tlist_add_tail(&dev->unlink_list, &net_unlink_list);\n}\n#endif\n\nstatic int __netdev_update_lower_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tdev->lower_level = __netdev_lower_depth(dev) + 1;\n\n#ifdef CONFIG_LOCKDEP\n\tif (!priv)\n\t\treturn 0;\n\n\tif (priv->flags & NESTED_SYNC_IMM)\n\t\tdev->nested_level = dev->lower_level - 1;\n\tif (priv->flags & NESTED_SYNC_TODO)\n\t\tnet_unlink_todo(dev);\n#endif\n\treturn 0;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev_rcu(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tadj->ignore = false;\n\tnetdev_hold(adj_dev, &adj->dev_tracker, GFP_KERNEL);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree(adj);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info,\n\t\t\t\t   struct netdev_nested_priv *priv,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t\t.extack = extack,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.master = master,\n\t\t.linking = true,\n\t\t.upper_info = upper_info,\n\t};\n\tstruct net_device *master_dev;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)\n\t\treturn -EMLINK;\n\n\tif (!master) {\n\t\tif (__netdev_has_upper_dev(dev, upper_dev))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tmaster_dev = __netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\treturn master_dev == upper_dev ? -EEXIST : -EBUSY;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, false,\n\t\t\t\t       NULL, NULL, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\nstatic void __netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev,\n\t\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.linking = false,\n\t};\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n}\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\t__netdev_upper_dev_unlink(dev, upper_dev, &priv);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\nstatic void __netdev_adjacent_dev_set(struct net_device *upper_dev,\n\t\t\t\t      struct net_device *lower_dev,\n\t\t\t\t      bool val)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(lower_dev, &upper_dev->adj_list.lower);\n\tif (adj)\n\t\tadj->ignore = val;\n\n\tadj = __netdev_find_adj(upper_dev, &lower_dev->adj_list.upper);\n\tif (adj)\n\t\tadj->ignore = val;\n}\n\nstatic void netdev_adjacent_dev_disable(struct net_device *upper_dev,\n\t\t\t\t\tstruct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, true);\n}\n\nstatic void netdev_adjacent_dev_enable(struct net_device *upper_dev,\n\t\t\t\t       struct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, false);\n}\n\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\tint err;\n\n\tif (!new_dev)\n\t\treturn 0;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_disable(dev, old_dev);\n\terr = __netdev_upper_dev_link(new_dev, dev, false, NULL, NULL, &priv,\n\t\t\t\t      extack);\n\tif (err) {\n\t\tif (old_dev && new_dev != old_dev)\n\t\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_adjacent_change_prepare);\n\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev || !old_dev)\n\t\treturn;\n\n\tif (new_dev == old_dev)\n\t\treturn;\n\n\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t__netdev_upper_dev_unlink(old_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_commit);\n\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev)\n\t\treturn;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\n\t__netdev_upper_dev_unlink(new_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_abort);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info info = {\n\t\t.info.dev = dev,\n\t};\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic int netdev_offload_xstats_enable_l3(struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\tint err;\n\tint rc;\n\n\tdev->offload_xstats_l3 = kzalloc(sizeof(*dev->offload_xstats_l3),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!dev->offload_xstats_l3)\n\t\treturn -ENOMEM;\n\n\trc = call_netdevice_notifiers_info_robust(NETDEV_OFFLOAD_XSTATS_ENABLE,\n\t\t\t\t\t\t  NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t\t\t  &info.info);\n\terr = notifier_to_errno(rc);\n\tif (err)\n\t\tgoto free_stats;\n\n\treturn 0;\n\nfree_stats:\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n\treturn err;\n}\n\nint netdev_offload_xstats_enable(struct net_device *dev,\n\t\t\t\t enum netdev_offload_xstats_type type,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn netdev_offload_xstats_enable_l3(dev, extack);\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enable);\n\nstatic void netdev_offload_xstats_disable_l3(struct net_device *dev)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\n\tcall_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t      &info.info);\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n}\n\nint netdev_offload_xstats_disable(struct net_device *dev,\n\t\t\t\t  enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\tif (!netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\tnetdev_offload_xstats_disable_l3(dev);\n\t\treturn 0;\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_disable);\n\nstatic void netdev_offload_xstats_disable_all(struct net_device *dev)\n{\n\tnetdev_offload_xstats_disable(dev, NETDEV_OFFLOAD_XSTATS_TYPE_L3);\n}\n\nstatic struct rtnl_hw_stats64 *\nnetdev_offload_xstats_get_ptr(const struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type)\n{\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn dev->offload_xstats_l3;\n\t}\n\n\tWARN_ON(1);\n\treturn NULL;\n}\n\nbool netdev_offload_xstats_enabled(const struct net_device *dev,\n\t\t\t\t   enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\treturn netdev_offload_xstats_get_ptr(dev, type);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enabled);\n\nstruct netdev_notifier_offload_xstats_ru {\n\tbool used;\n};\n\nstruct netdev_notifier_offload_xstats_rd {\n\tstruct rtnl_hw_stats64 stats;\n\tbool used;\n};\n\nstatic void netdev_hw_stats64_add(struct rtnl_hw_stats64 *dest,\n\t\t\t\t  const struct rtnl_hw_stats64 *src)\n{\n\tdest->rx_packets\t  += src->rx_packets;\n\tdest->tx_packets\t  += src->tx_packets;\n\tdest->rx_bytes\t\t  += src->rx_bytes;\n\tdest->tx_bytes\t\t  += src->tx_bytes;\n\tdest->rx_errors\t\t  += src->rx_errors;\n\tdest->tx_errors\t\t  += src->tx_errors;\n\tdest->rx_dropped\t  += src->rx_dropped;\n\tdest->tx_dropped\t  += src->tx_dropped;\n\tdest->multicast\t\t  += src->multicast;\n}\n\nstatic int netdev_offload_xstats_get_used(struct net_device *dev,\n\t\t\t\t\t  enum netdev_offload_xstats_type type,\n\t\t\t\t\t  bool *p_used,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_ru report_used = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_used = &report_used,\n\t};\n\tint rc;\n\n\tWARN_ON(!netdev_offload_xstats_enabled(dev, type));\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_USED,\n\t\t\t\t\t   &info.info);\n\t*p_used = report_used.used;\n\treturn notifier_to_errno(rc);\n}\n\nstatic int netdev_offload_xstats_get_stats(struct net_device *dev,\n\t\t\t\t\t   enum netdev_offload_xstats_type type,\n\t\t\t\t\t   struct rtnl_hw_stats64 *p_stats,\n\t\t\t\t\t   bool *p_used,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_rd report_delta = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_delta = &report_delta,\n\t};\n\tstruct rtnl_hw_stats64 *stats;\n\tint rc;\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn -EINVAL;\n\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_DELTA,\n\t\t\t\t\t   &info.info);\n\n\t/* Cache whatever we got, even if there was an error, otherwise the\n\t * successful stats retrievals would get lost.\n\t */\n\tnetdev_hw_stats64_add(stats, &report_delta.stats);\n\n\tif (p_stats)\n\t\t*p_stats = *stats;\n\t*p_used = report_delta.used;\n\n\treturn notifier_to_errno(rc);\n}\n\nint netdev_offload_xstats_get(struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t      struct rtnl_hw_stats64 *p_stats, bool *p_used,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (p_stats)\n\t\treturn netdev_offload_xstats_get_stats(dev, type, p_stats,\n\t\t\t\t\t\t       p_used, extack);\n\telse\n\t\treturn netdev_offload_xstats_get_used(dev, type, p_used,\n\t\t\t\t\t\t      extack);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_get);\n\nvoid\nnetdev_offload_xstats_report_delta(struct netdev_notifier_offload_xstats_rd *report_delta,\n\t\t\t\t   const struct rtnl_hw_stats64 *stats)\n{\n\treport_delta->used = true;\n\tnetdev_hw_stats64_add(&report_delta->stats, stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_delta);\n\nvoid\nnetdev_offload_xstats_report_used(struct netdev_notifier_offload_xstats_ru *report_used)\n{\n\treport_used->used = true;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_used);\n\nvoid netdev_offload_xstats_push_delta(struct net_device *dev,\n\t\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t\t      const struct rtnl_hw_stats64 *p_stats)\n{\n\tstruct rtnl_hw_stats64 *stats;\n\n\tASSERT_RTNL();\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn;\n\n\tnetdev_hw_stats64_add(stats, p_stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_push_delta);\n\n/**\n * netdev_get_xmit_slave - Get the xmit slave of master device\n * @dev: device\n * @skb: The packet\n * @all_slaves: assume all the slaves are active\n *\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n * %NULL is returned if no slave is found.\n */\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_xmit_slave)\n\t\treturn NULL;\n\treturn ops->ndo_get_xmit_slave(dev, skb, all_slaves);\n}\nEXPORT_SYMBOL(netdev_get_xmit_slave);\n\nstatic struct net_device *netdev_sk_get_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_sk_get_lower_dev)\n\t\treturn NULL;\n\treturn ops->ndo_sk_get_lower_dev(dev, sk);\n}\n\n/**\n * netdev_sk_get_lowest_dev - Get the lowest device in chain given device and socket\n * @dev: device\n * @sk: the socket\n *\n * %NULL is returned if no lower device is found.\n */\n\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct net_device *lower;\n\n\tlower = netdev_sk_get_lower_dev(dev, sk);\n\twhile (lower) {\n\t\tdev = lower;\n\t\tlower = netdev_sk_get_lower_dev(dev, sk);\n\t}\n\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_sk_get_lowest_dev);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\n/**\n * netdev_lower_state_changed - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info = {\n\t\t.info.dev = lower_dev,\n\t};\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tunsigned int promiscuity, flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tpromiscuity = dev->promiscuity + inc;\n\tif (promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (unlikely(inc > 0)) {\n\t\t\tnetdev_warn(dev, \"promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t\tflags = old_flags & ~IFF_PROMISC;\n\t} else {\n\t\tflags = old_flags | IFF_PROMISC;\n\t}\n\tWRITE_ONCE(dev->promiscuity, promiscuity);\n\tif (flags != old_flags) {\n\t\tWRITE_ONCE(dev->flags, flags);\n\t\tnetdev_info(dev, \"%s promiscuous mode\\n\",\n\t\t\t    dev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(audit_context(), GFP_ATOMIC,\n\t\t\t\t  AUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t  \"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\t  dev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t  (old_flags & IFF_PROMISC),\n\t\t\t\t  from_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\t  from_kuid(&init_user_ns, uid),\n\t\t\t\t  from_kgid(&init_user_ns, gid),\n\t\t\t\t  audit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC, 0, NULL);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\tunsigned int allmulti, flags;\n\n\tASSERT_RTNL();\n\n\tallmulti = dev->allmulti + inc;\n\tif (allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (unlikely(inc > 0)) {\n\t\t\tnetdev_warn(dev, \"allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t\tflags = old_flags & ~IFF_ALLMULTI;\n\t} else {\n\t\tflags = old_flags | IFF_ALLMULTI;\n\t}\n\tWRITE_ONCE(dev->allmulti, allmulti);\n\tif (flags != old_flags) {\n\t\tWRITE_ONCE(dev->flags, flags);\n\t\tnetdev_info(dev, \"%s allmulticast mode\\n\",\n\t\t\t    dev->flags & IFF_ALLMULTI ? \"entered\" : \"left\");\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags, 0, NULL);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (READ_ONCE(dev->flags) & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(READ_ONCE(dev->gflags) & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev, extack);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges, u32 portid,\n\t\t\tconst struct nlmsghdr *nlh)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC, portid, nlh);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info = {\n\t\t\t\t.dev = dev,\n\t\t\t},\n\t\t\t.flags_changed = changes,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\t@extack: netlink extended ack\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes, 0, NULL);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\t/* Pairs with all the lockless reads of dev->mtu in the stack */\n\tWRITE_ONCE(dev->mtu, new_mtu);\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\nint dev_validate_mtu(struct net_device *dev, int new_mtu,\n\t\t     struct netlink_ext_ack *extack)\n{\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu less than device minimum\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu greater than device maximum\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu_ext - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\t@extack: netlink extended ack\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu_ext(struct net_device *dev, int new_mtu,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\terr = dev_validate_mtu(dev, new_mtu, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t   orig_mtu);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t     new_mtu);\n\t\t}\n\t}\n\treturn err;\n}\n\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct netlink_ext_ack extack;\n\tint err;\n\n\tmemset(&extack, 0, sizeof(extack));\n\terr = dev_set_mtu_ext(dev, new_mtu, &extack);\n\tif (err && extack._msg)\n\t\tnet_err_ratelimited(\"%s: %s\\n\", dev->name, extack._msg);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_change_tx_queue_len - Change TX queue length of a netdevice\n *\t@dev: device\n *\t@new_len: new tx queue length\n */\nint dev_change_tx_queue_len(struct net_device *dev, unsigned long new_len)\n{\n\tunsigned int orig_len = dev->tx_queue_len;\n\tint res;\n\n\tif (new_len != (unsigned int)new_len)\n\t\treturn -ERANGE;\n\n\tif (new_len != orig_len) {\n\t\tWRITE_ONCE(dev->tx_queue_len, new_len);\n\t\tres = call_netdevice_notifiers(NETDEV_CHANGE_TX_QUEUE_LEN, dev);\n\t\tres = notifier_to_errno(res);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t\tres = dev_qdisc_change_tx_queue_len(dev);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t}\n\n\treturn 0;\n\nerr_rollback:\n\tnetdev_err(dev, \"refused to change device tx_queue_len\\n\");\n\tWRITE_ONCE(dev->tx_queue_len, orig_len);\n\treturn res;\n}\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\n\n/**\n *\tdev_pre_changeaddr_notify - Call NETDEV_PRE_CHANGEADDR.\n *\t@dev: device\n *\t@addr: new address\n *\t@extack: netlink extended ack\n */\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_pre_changeaddr_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.dev_addr = addr,\n\t};\n\tint rc;\n\n\trc = call_netdevice_notifiers_info(NETDEV_PRE_CHANGEADDR, &info.info);\n\treturn notifier_to_errno(rc);\n}\nEXPORT_SYMBOL(dev_pre_changeaddr_notify);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\t@extack: netlink extended ack\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = dev_pre_changeaddr_notify(dev, sa->sa_data, extack);\n\tif (err)\n\t\treturn err;\n\tif (memcmp(dev->dev_addr, sa->sa_data, dev->addr_len)) {\n\t\terr = ops->ndo_set_mac_address(dev, sa);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\nDECLARE_RWSEM(dev_addr_sem);\n\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tdown_write(&dev_addr_sem);\n\tret = dev_set_mac_address(dev, sa, extack);\n\tup_write(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_set_mac_address_user);\n\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name)\n{\n\tsize_t size = sizeof(sa->sa_data_min);\n\tstruct net_device *dev;\n\tint ret = 0;\n\n\tdown_read(&dev_addr_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_name_rcu(net, dev_name);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!dev->addr_len)\n\t\tmemset(sa->sa_data, 0, size);\n\telse\n\t\tmemcpy(sa->sa_data, dev->dev_addr,\n\t\t       min_t(size_t, size, dev->addr_len));\n\tsa->sa_family = dev->type;\n\nunlock:\n\trcu_read_unlock();\n\tup_read(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (ops->ndo_get_phys_port_name) {\n\t\terr = ops->ndo_get_phys_port_name(dev, name, len);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\treturn devlink_compat_phys_port_name_get(dev, name, len);\n}\n\n/**\n *\tdev_get_port_parent_id - Get the device's port parent identifier\n *\t@dev: network device\n *\t@ppid: pointer to a storage for the port's parent identifier\n *\t@recurse: allow/disallow recursion to lower devices\n *\n *\tGet the devices's port parent identifier\n */\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid,\n\t\t\t   bool recurse)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct netdev_phys_item_id first = { };\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\tint err;\n\n\tif (ops->ndo_get_port_parent_id) {\n\t\terr = ops->ndo_get_port_parent_id(dev, ppid);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\n\terr = devlink_compat_switch_id_get(dev, ppid);\n\tif (!recurse || err != -EOPNOTSUPP)\n\t\treturn err;\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter) {\n\t\terr = dev_get_port_parent_id(lower_dev, ppid, true);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!first.id_len)\n\t\t\tfirst = *ppid;\n\t\telse if (memcmp(&first, ppid, sizeof(*ppid)))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_get_port_parent_id);\n\n/**\n *\tnetdev_port_same_parent_id - Indicate if two network devices have\n *\tthe same port parent identifier\n *\t@a: first network device\n *\t@b: second network device\n */\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)\n{\n\tstruct netdev_phys_item_id a_id = { };\n\tstruct netdev_phys_item_id b_id = { };\n\n\tif (dev_get_port_parent_id(a, &a_id, true) ||\n\t    dev_get_port_parent_id(b, &b_id, true))\n\t\treturn false;\n\n\treturn netdev_phys_item_id_same(&a_id, &b_id);\n}\nEXPORT_SYMBOL(netdev_port_same_parent_id);\n\n/**\n *\tdev_change_proto_down - set carrier according to proto_down.\n *\n *\t@dev: device\n *\t@proto_down: new value\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tif (!dev->change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\tif (proto_down)\n\t\tnetif_carrier_off(dev);\n\telse\n\t\tnetif_carrier_on(dev);\n\tWRITE_ONCE(dev->proto_down, proto_down);\n\treturn 0;\n}\n\n/**\n *\tdev_change_proto_down_reason - proto down reason\n *\n *\t@dev: device\n *\t@mask: proto down mask\n *\t@value: proto down value\n */\nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value)\n{\n\tu32 proto_down_reason;\n\tint b;\n\n\tif (!mask) {\n\t\tproto_down_reason = value;\n\t} else {\n\t\tproto_down_reason = dev->proto_down_reason;\n\t\tfor_each_set_bit(b, &mask, 32) {\n\t\t\tif (value & (1 << b))\n\t\t\t\tproto_down_reason |= BIT(b);\n\t\t\telse\n\t\t\t\tproto_down_reason &= ~BIT(b);\n\t\t}\n\t}\n\tWRITE_ONCE(dev->proto_down_reason, proto_down_reason);\n}\n\nstruct bpf_xdp_link {\n\tstruct bpf_link link;\n\tstruct net_device *dev; /* protected by rtnl_lock, no refcnt held */\n\tint flags;\n};\n\nstatic enum bpf_xdp_mode dev_xdp_mode(struct net_device *dev, u32 flags)\n{\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\treturn XDP_MODE_HW;\n\tif (flags & XDP_FLAGS_DRV_MODE)\n\t\treturn XDP_MODE_DRV;\n\tif (flags & XDP_FLAGS_SKB_MODE)\n\t\treturn XDP_MODE_SKB;\n\treturn dev->netdev_ops->ndo_bpf ? XDP_MODE_DRV : XDP_MODE_SKB;\n}\n\nstatic bpf_op_t dev_xdp_bpf_op(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tswitch (mode) {\n\tcase XDP_MODE_SKB:\n\t\treturn generic_xdp_install;\n\tcase XDP_MODE_DRV:\n\tcase XDP_MODE_HW:\n\t\treturn dev->netdev_ops->ndo_bpf;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_xdp_link *dev_xdp_link(struct net_device *dev,\n\t\t\t\t\t enum bpf_xdp_mode mode)\n{\n\treturn dev->xdp_state[mode].link;\n}\n\nstatic struct bpf_prog *dev_xdp_prog(struct net_device *dev,\n\t\t\t\t     enum bpf_xdp_mode mode)\n{\n\tstruct bpf_xdp_link *link = dev_xdp_link(dev, mode);\n\n\tif (link)\n\t\treturn link->link.prog;\n\treturn dev->xdp_state[mode].prog;\n}\n\nu8 dev_xdp_prog_count(struct net_device *dev)\n{\n\tu8 count = 0;\n\tint i;\n\n\tfor (i = 0; i < __MAX_XDP_MODE; i++)\n\t\tif (dev->xdp_state[i].prog || dev->xdp_state[i].link)\n\t\t\tcount++;\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(dev_xdp_prog_count);\n\nint dev_xdp_propagate(struct net_device *dev, struct netdev_bpf *bpf)\n{\n\tif (!dev->netdev_ops->ndo_bpf)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev_get_min_mp_channel_count(dev)) {\n\t\tNL_SET_ERR_MSG(bpf->extack, \"unable to propagate XDP to device using memory provider\");\n\t\treturn -EBUSY;\n\t}\n\n\treturn dev->netdev_ops->ndo_bpf(dev, bpf);\n}\nEXPORT_SYMBOL_GPL(dev_xdp_propagate);\n\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tstruct bpf_prog *prog = dev_xdp_prog(dev, mode);\n\n\treturn prog ? prog->aux->id : 0;\n}\n\nstatic void dev_xdp_set_link(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_xdp_link *link)\n{\n\tdev->xdp_state[mode].link = link;\n\tdev->xdp_state[mode].prog = NULL;\n}\n\nstatic void dev_xdp_set_prog(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_prog *prog)\n{\n\tdev->xdp_state[mode].link = NULL;\n\tdev->xdp_state[mode].prog = prog;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t   bpf_op_t bpf_op, struct netlink_ext_ack *extack,\n\t\t\t   u32 flags, struct bpf_prog *prog)\n{\n\tstruct netdev_bpf xdp;\n\tint err;\n\n\tif (dev_get_min_mp_channel_count(dev)) {\n\t\tNL_SET_ERR_MSG(extack, \"unable to install XDP to device using memory provider\");\n\t\treturn -EBUSY;\n\t}\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = mode == XDP_MODE_HW ? XDP_SETUP_PROG_HW : XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\t/* Drivers assume refcnt is already incremented (i.e, prog pointer is\n\t * \"moved\" into driver), so they don't increment it on their own, but\n\t * they do decrement refcnt when program is detached or replaced.\n\t * Given net_device also owns link/prog, we need to bump refcnt here\n\t * to prevent drivers from underflowing it.\n\t */\n\tif (prog)\n\t\tbpf_prog_inc(prog);\n\terr = bpf_op(dev, &xdp);\n\tif (err) {\n\t\tif (prog)\n\t\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\tif (mode != XDP_MODE_HW)\n\t\tbpf_prog_change_xdp(dev_xdp_prog(dev, mode), prog);\n\n\treturn 0;\n}\n\nstatic void dev_xdp_uninstall(struct net_device *dev)\n{\n\tstruct bpf_xdp_link *link;\n\tstruct bpf_prog *prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tfor (mode = XDP_MODE_SKB; mode < __MAX_XDP_MODE; mode++) {\n\t\tprog = dev_xdp_prog(dev, mode);\n\t\tif (!prog)\n\t\t\tcontinue;\n\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op)\n\t\t\tcontinue;\n\n\t\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\n\t\t/* auto-detach link from net device */\n\t\tlink = dev_xdp_link(dev, mode);\n\t\tif (link)\n\t\t\tlink->dev = NULL;\n\t\telse\n\t\t\tbpf_prog_put(prog);\n\n\t\tdev_xdp_set_link(dev, mode, NULL);\n\t}\n}\n\nstatic int dev_xdp_attach(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t\t  struct bpf_xdp_link *link, struct bpf_prog *new_prog,\n\t\t\t  struct bpf_prog *old_prog, u32 flags)\n{\n\tunsigned int num_modes = hweight32(flags & XDP_FLAGS_MODES);\n\tstruct bpf_prog *cur_prog;\n\tstruct net_device *upper;\n\tstruct list_head *iter;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* either link or prog attachment, never both */\n\tif (link && (new_prog || old_prog))\n\t\treturn -EINVAL;\n\t/* link supports only XDP mode flags */\n\tif (link && (flags & ~XDP_FLAGS_MODES)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid XDP flags for BPF link attachment\");\n\t\treturn -EINVAL;\n\t}\n\t/* just one XDP mode bit should be set, zero defaults to drv/skb mode */\n\tif (num_modes > 1) {\n\t\tNL_SET_ERR_MSG(extack, \"Only one XDP mode flag can be set\");\n\t\treturn -EINVAL;\n\t}\n\t/* avoid ambiguity if offload + drv/skb mode progs are both loaded */\n\tif (!num_modes && dev_xdp_prog_count(dev) > 1) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"More than one program loaded, unset mode is ambiguous\");\n\t\treturn -EINVAL;\n\t}\n\t/* old_prog != NULL implies XDP_FLAGS_REPLACE is set */\n\tif (old_prog && !(flags & XDP_FLAGS_REPLACE)) {\n\t\tNL_SET_ERR_MSG(extack, \"XDP_FLAGS_REPLACE is not specified\");\n\t\treturn -EINVAL;\n\t}\n\n\tmode = dev_xdp_mode(dev, flags);\n\t/* can't replace attached link */\n\tif (dev_xdp_link(dev, mode)) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active BPF XDP link\");\n\t\treturn -EBUSY;\n\t}\n\n\t/* don't allow if an upper device already has a program */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter) {\n\t\tif (dev_xdp_prog_count(upper) > 0) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot attach when an upper device already has a program\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\tcur_prog = dev_xdp_prog(dev, mode);\n\t/* can't replace attached prog with link */\n\tif (link && cur_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active XDP program with BPF link\");\n\t\treturn -EBUSY;\n\t}\n\tif ((flags & XDP_FLAGS_REPLACE) && cur_prog != old_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Active program does not match expected\");\n\t\treturn -EEXIST;\n\t}\n\n\t/* put effective new program into new_prog */\n\tif (link)\n\t\tnew_prog = link->link.prog;\n\n\tif (new_prog) {\n\t\tbool offload = mode == XDP_MODE_HW;\n\t\tenum bpf_xdp_mode other_mode = mode == XDP_MODE_SKB\n\t\t\t\t\t       ? XDP_MODE_DRV : XDP_MODE_SKB;\n\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) && cur_prog) {\n\t\t\tNL_SET_ERR_MSG(extack, \"XDP program already attached\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (!offload && dev_xdp_prog(dev, other_mode)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Native and generic XDP can't be active at the same time\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (!offload && bpf_prog_is_offloaded(new_prog->aux)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Using offloaded program without HW_MODE flag is not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (bpf_prog_is_dev_bound(new_prog->aux) && !bpf_offload_dev_match(new_prog, dev)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Program bound to different device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_DEVMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_DEVMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_CPUMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_CPUMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* don't call drivers if the effective program didn't change */\n\tif (new_prog != cur_prog) {\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Underlying driver does not support XDP in native mode\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\terr = dev_xdp_install(dev, mode, bpf_op, extack, flags, new_prog);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (link)\n\t\tdev_xdp_set_link(dev, mode, link);\n\telse\n\t\tdev_xdp_set_prog(dev, mode, new_prog);\n\tif (cur_prog)\n\t\tbpf_prog_put(cur_prog);\n\n\treturn 0;\n}\n\nstatic int dev_xdp_attach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\treturn dev_xdp_attach(dev, extack, link, NULL, NULL, link->flags);\n}\n\nstatic int dev_xdp_detach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tmode = dev_xdp_mode(dev, link->flags);\n\tif (dev_xdp_link(dev, mode) != link)\n\t\treturn -EINVAL;\n\n\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\tdev_xdp_set_link(dev, mode, NULL);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\trtnl_lock();\n\n\t/* if racing with net_device's tear down, xdp_link->dev might be\n\t * already NULL, in which case link was already auto-detached\n\t */\n\tif (xdp_link->dev) {\n\t\tWARN_ON(dev_xdp_detach_link(xdp_link->dev, NULL, xdp_link));\n\t\txdp_link->dev = NULL;\n\t}\n\n\trtnl_unlock();\n}\n\nstatic int bpf_xdp_link_detach(struct bpf_link *link)\n{\n\tbpf_xdp_link_release(link);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\tkfree(xdp_link);\n}\n\nstatic void bpf_xdp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t     struct seq_file *seq)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tseq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);\n}\n\nstatic int bpf_xdp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t       struct bpf_link_info *info)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tinfo->xdp.ifindex = ifindex;\n\treturn 0;\n}\n\nstatic int bpf_xdp_link_update(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t       struct bpf_prog *old_prog)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err = 0;\n\n\trtnl_lock();\n\n\t/* link might have been auto-released already, so fail */\n\tif (!xdp_link->dev) {\n\t\terr = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog && link->prog != old_prog) {\n\t\terr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\told_prog = link->prog;\n\tif (old_prog->type != new_prog->type ||\n\t    old_prog->expected_attach_type != new_prog->expected_attach_type) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog == new_prog) {\n\t\t/* no-op, don't disturb drivers */\n\t\tbpf_prog_put(new_prog);\n\t\tgoto out_unlock;\n\t}\n\n\tmode = dev_xdp_mode(xdp_link->dev, xdp_link->flags);\n\tbpf_op = dev_xdp_bpf_op(xdp_link->dev, mode);\n\terr = dev_xdp_install(xdp_link->dev, mode, bpf_op, NULL,\n\t\t\t      xdp_link->flags, new_prog);\n\tif (err)\n\t\tgoto out_unlock;\n\n\told_prog = xchg(&link->prog, new_prog);\n\tbpf_prog_put(old_prog);\n\nout_unlock:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_xdp_link_lops = {\n\t.release = bpf_xdp_link_release,\n\t.dealloc = bpf_xdp_link_dealloc,\n\t.detach = bpf_xdp_link_detach,\n\t.show_fdinfo = bpf_xdp_link_show_fdinfo,\n\t.fill_link_info = bpf_xdp_link_fill_link_info,\n\t.update_prog = bpf_xdp_link_update,\n};\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_link_primer link_primer;\n\tstruct netlink_ext_ack extack = {};\n\tstruct bpf_xdp_link *link;\n\tstruct net_device *dev;\n\tint err, fd;\n\n\trtnl_lock();\n\tdev = dev_get_by_index(net, attr->link_create.target_ifindex);\n\tif (!dev) {\n\t\trtnl_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_XDP, &bpf_xdp_link_lops, prog);\n\tlink->dev = dev;\n\tlink->flags = attr->link_create.flags;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto unlock;\n\t}\n\n\terr = dev_xdp_attach_link(dev, &extack, link);\n\trtnl_unlock();\n\n\tif (err) {\n\t\tlink->dev = NULL;\n\t\tbpf_link_cleanup(&link_primer);\n\t\ttrace_bpf_xdp_link_attach_failed(extack._msg);\n\t\tgoto out_put_dev;\n\t}\n\n\tfd = bpf_link_settle(&link_primer);\n\t/* link itself doesn't hold dev's refcnt to not complicate shutdown */\n\tdev_put(dev);\n\treturn fd;\n\nunlock:\n\trtnl_unlock();\n\nout_put_dev:\n\tdev_put(dev);\n\treturn err;\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@expected_fd: old program fd that userspace expects to replace or clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags)\n{\n\tenum bpf_xdp_mode mode = dev_xdp_mode(dev, flags);\n\tstruct bpf_prog *new_prog = NULL, *old_prog = NULL;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (fd >= 0) {\n\t\tnew_prog = bpf_prog_get_type_dev(fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(new_prog))\n\t\t\treturn PTR_ERR(new_prog);\n\t}\n\n\tif (expected_fd >= 0) {\n\t\told_prog = bpf_prog_get_type_dev(expected_fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\terr = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = dev_xdp_attach(dev, extack, NULL, new_prog, old_prog, flags);\n\nerr_out:\n\tif (err && new_prog)\n\t\tbpf_prog_put(new_prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\treturn err;\n}\n\nu32 dev_get_min_mp_channel_count(const struct net_device *dev)\n{\n\tint i;\n\n\tASSERT_RTNL();\n\n\tfor (i = dev->real_num_rx_queues - 1; i >= 0; i--)\n\t\tif (dev->_rx[i].mp_params.mp_priv)\n\t\t\t/* The channel count is the idx plus 1. */\n\t\t\treturn i + 1;\n\n\treturn 0;\n}\n\n/**\n * dev_index_reserve() - allocate an ifindex in a namespace\n * @net: the applicable net namespace\n * @ifindex: requested ifindex, pass %0 to get one allocated\n *\n * Allocate a ifindex for a new device. Caller must either use the ifindex\n * to store the device (via list_netdevice()) or call dev_index_release()\n * to give the index up.\n *\n * Return: a suitable unique value for a new device interface number or -errno.\n */\nstatic int dev_index_reserve(struct net *net, u32 ifindex)\n{\n\tint err;\n\n\tif (ifindex > INT_MAX) {\n\t\tDEBUG_NET_WARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ifindex)\n\t\terr = xa_alloc_cyclic(&net->dev_by_index, &ifindex, NULL,\n\t\t\t\t      xa_limit_31b, &net->ifindex, GFP_KERNEL);\n\telse\n\t\terr = xa_insert(&net->dev_by_index, ifindex, NULL, GFP_KERNEL);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn ifindex;\n}\n\nstatic void dev_index_release(struct net *net, int ifindex)\n{\n\t/* Expect only unused indexes, unlist_netdevice() removes the used */\n\tWARN_ON(xa_erase(&net->dev_by_index, ifindex));\n}\n\n/* Delayed registration/unregisteration */\nLIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\natomic_t dev_unreg_count = ATOMIC_INIT(0);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\t__netdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t\telse\n\t\t\t\tnetdev_features_change(lower);\n\t\t}\n\t}\n}\n\nstatic bool netdev_has_ip_or_hw_csum(netdev_features_t features)\n{\n\tnetdev_features_t ip_csum_mask = NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\n\tbool ip_csum = (features & ip_csum_mask) == ip_csum_mask;\n\tbool hw_csum = features & NETIF_F_HW_CSUM;\n\n\treturn ip_csum || hw_csum;\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\tif (!(features & NETIF_F_RXCSUM)) {\n\t\t/* NETIF_F_GRO_HW implies doing RXCSUM since every packet\n\t\t * successfully merged by hardware must also have the\n\t\t * checksum verified by hardware.  If the user does not\n\t\t * want to enable RXCSUM, logically, we should disable GRO_HW.\n\t\t */\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GRO_HW since no RXCSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\t/* LRO/HW-GRO features cannot be combined with RX-FCS */\n\tif (features & NETIF_F_RXFCS) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_dbg(dev, \"Dropping LRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping HW-GRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_GRO_HW) && (features & NETIF_F_LRO)) {\n\t\tnetdev_dbg(dev, \"Dropping LRO feature since HW-GRO is requested.\\n\");\n\t\tfeatures &= ~NETIF_F_LRO;\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_TX) && !netdev_has_ip_or_hw_csum(features)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS TX HW offload feature since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_RX) && !(features & NETIF_F_RXCSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS RX HW offload feature since no RXCSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\t}\n\n\tif ((features & NETIF_F_GSO_UDP_L4) && !netdev_has_ip_or_hw_csum(features)) {\n\t\tnetdev_dbg(dev, \"Dropping USO feature since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO_UDP_L4;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off on an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_ctag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_ctag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_stag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_stag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (rootdev->operstate == IF_OPER_TESTING)\n\t\tnetif_testing_on(dev);\n\telse\n\t\tnetif_testing_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\tint err = 0;\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++) {\n\t\trx[i].dev = dev;\n\n\t\t/* XDP RX-queue setup */\n\t\terr = xdp_rxq_info_reg(&rx[i].xdp_rxq, dev, i, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_info;\n\t}\n\treturn 0;\n\nerr_rxq_info:\n\t/* Rollback successful reg's and free other resources */\n\twhile (i--)\n\t\txdp_rxq_info_unreg(&rx[i].xdp_rxq);\n\tkvfree(dev->_rx);\n\tdev->_rx = NULL;\n\treturn err;\n}\n\nstatic void netif_free_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\n\t/* netif_alloc_rx_queues alloc failed, resources have been unreg'ed */\n\tif (!dev->_rx)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++)\n\t\txdp_rxq_info_unreg(&dev->_rx[i].xdp_rxq);\n\n\tkvfree(dev->_rx);\n}\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\nstatic int netdev_do_alloc_pcpu_stats(struct net_device *dev)\n{\n\tvoid __percpu *v;\n\n\t/* Drivers implementing ndo_get_peer_dev must support tstat\n\t * accounting, so that skb_do_redirect() can bump the dev's\n\t * RX stats upon network namespace switch.\n\t */\n\tif (dev->netdev_ops->ndo_get_peer_dev &&\n\t    dev->pcpu_stat_type != NETDEV_PCPU_STAT_TSTATS)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn 0;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tv = dev->lstats = netdev_alloc_pcpu_stats(struct pcpu_lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tv = dev->tstats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tv = dev->dstats = netdev_alloc_pcpu_stats(struct pcpu_dstats);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn v ? 0 : -ENOMEM;\n}\n\nstatic void netdev_do_free_pcpu_stats(struct net_device *dev)\n{\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tfree_percpu(dev->lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tfree_percpu(dev->tstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tfree_percpu(dev->dstats);\n\t\tbreak;\n\t}\n}\n\nstatic void netdev_free_phy_link_topology(struct net_device *dev)\n{\n\tstruct phy_link_topology *topo = dev->link_topo;\n\n\tif (IS_ENABLED(CONFIG_PHYLIB) && topo) {\n\t\txa_destroy(&topo->phys);\n\t\tkfree(topo);\n\t\tdev->link_topo = NULL;\n\t}\n}\n\n/**\n * register_netdevice() - register a network device\n * @dev: device to register\n *\n * Take a prepared network device structure and make it externally accessible.\n * A %NETDEV_REGISTER message is sent to the netdev notifier chain.\n * Callers must hold the rtnl lock - you may want register_netdev()\n * instead of this.\n */\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <\n\t\t     NETDEV_FEATURE_COUNT);\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tret = ethtool_check_ops(dev->ethtool_ops);\n\tif (ret)\n\t\treturn ret;\n\n\t/* rss ctx ID 0 is reserved for the default context, start from 1 */\n\txa_init_flags(&dev->ethtool->rss_ctx, XA_FLAGS_ALLOC1);\n\tmutex_init(&dev->ethtool->rss_lock);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tdev->name_node = netdev_name_node_head_alloc(dev);\n\tif (!dev->name_node)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto err_free_name;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = netdev_do_alloc_pcpu_stats(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = dev_index_reserve(net, dev->ifindex);\n\tif (ret < 0)\n\t\tgoto err_free_pcpu;\n\tdev->ifindex = ret;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= (NETIF_F_SOFT_FEATURES | NETIF_F_SOFT_FEATURES_OFF);\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->udp_tunnel_nic_info) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_ifindex_release;\n\n\tret = netdev_register_kobject(dev);\n\n\tWRITE_ONCE(dev->reg_state, ret ? NETREG_UNREGISTERED : NETREG_REGISTERED);\n\n\tif (ret)\n\t\tgoto err_uninit_notify;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\n\tnetdev_hold(dev, &dev->dev_registered_tracker, GFP_KERNEL);\n\tlist_netdevice(dev);\n\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\t/* Expect explicit free_netdev() on failure */\n\t\tdev->needs_free_netdev = false;\n\t\tunregister_netdevice_queue(dev, NULL);\n\t\tgoto out;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);\n\nout:\n\treturn ret;\n\nerr_uninit_notify:\n\tcall_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);\nerr_ifindex_release:\n\tdev_index_release(net, dev->ifindex);\nerr_free_pcpu:\n\tnetdev_do_free_pcpu_stats(dev);\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\nerr_free_name:\n\tnetdev_name_node_free(dev->name_node);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/* Initialize the core of a dummy net device.\n * This is useful if you are calling this function after alloc_netdev(),\n * since it does not memset the net_device fields.\n */\nstatic void init_dummy_netdev_core(struct net_device *dev)\n{\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* napi_busy_loop stats accounting wants this */\n\tdev_net_set(dev, &init_net);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n}\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initializes the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nvoid init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * as they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\tinit_dummy_netdev_core(dev);\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\tif (rtnl_lock_killable())\n\t\treturn -EINTR;\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n#else\n\treturn refcount_read(&dev->dev_refcnt);\n#endif\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\nint netdev_unregister_timeout_secs __read_mostly = 10;\n\n#define WAIT_REFS_MIN_MSECS 1\n#define WAIT_REFS_MAX_MSECS 250\n/**\n * netdev_wait_allrefs_any - wait until all references are gone.\n * @list: list of net_devices to wait on\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic struct net_device *netdev_wait_allrefs_any(struct list_head *list)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tstruct net_device *dev;\n\tint wait = 0;\n\n\trebroadcast_time = warning_time = jiffies;\n\n\tlist_for_each_entry(dev, list, todo_list)\n\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\treturn dev;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t\t     &dev->state)) {\n\t\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t\t * pending on unregister. If this\n\t\t\t\t\t * happens, we simply run the queue\n\t\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t\t * for this device.\n\t\t\t\t\t */\n\t\t\t\t\tlinkwatch_run_queue();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\trcu_barrier();\n\n\t\tif (!wait) {\n\t\t\twait = WAIT_REFS_MIN_MSECS;\n\t\t} else {\n\t\t\tmsleep(wait);\n\t\t\twait = min(wait << 1, WAIT_REFS_MAX_MSECS);\n\t\t}\n\n\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\t\treturn dev;\n\n\t\tif (time_after(jiffies, warning_time +\n\t\t\t       READ_ONCE(netdev_unregister_timeout_secs) * HZ)) {\n\t\t\tlist_for_each_entry(dev, list, todo_list) {\n\t\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t\t dev->name, netdev_refcnt_read(dev));\n\t\t\t\tref_tracker_dir_print(&dev->refcnt_tracker, 10);\n\t\t\t}\n\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct net_device *dev, *tmp;\n\tstruct list_head list;\n\tint cnt;\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head unlink_list;\n\n\tlist_replace_init(&net_unlink_list, &unlink_list);\n\n\twhile (!list_empty(&unlink_list)) {\n\t\tstruct net_device *dev = list_first_entry(&unlink_list,\n\t\t\t\t\t\t\t  struct net_device,\n\t\t\t\t\t\t\t  unlink_list);\n\t\tlist_del_init(&dev->unlink_list);\n\t\tdev->nested_level = dev->lower_level - 1;\n\t}\n#endif\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\tlist_for_each_entry_safe(dev, tmp, &list, todo_list) {\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tnetdev_WARN(dev, \"run_todo but not unregistering\\n\");\n\t\t\tlist_del(&dev->todo_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\tWRITE_ONCE(dev->reg_state, NETREG_UNREGISTERED);\n\t\tlinkwatch_sync_dev(dev);\n\t}\n\n\tcnt = 0;\n\twhile (!list_empty(&list)) {\n\t\tdev = netdev_wait_allrefs_any(&list);\n\t\tlist_del(&dev->todo_list);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev) != 1);\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\n\t\tnetdev_do_free_pcpu_stats(dev);\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\tcnt++;\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n\tif (cnt && atomic_sub_and_test(cnt, &dev_unreg_count))\n\t\twake_up(&netdev_unregistering_wq);\n}\n\n/* Collate per-cpu network dstats statistics\n *\n * Read per-cpu network statistics from dev->dstats and populate the related\n * fields in @s.\n */\nstatic void dev_fetch_dstats(struct rtnl_link_stats64 *s,\n\t\t\t     const struct pcpu_dstats __percpu *dstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tu64 rx_packets, rx_bytes, rx_drops;\n\t\tu64 tx_packets, tx_bytes, tx_drops;\n\t\tconst struct pcpu_dstats *stats;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(dstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\trx_packets = u64_stats_read(&stats->rx_packets);\n\t\t\trx_bytes   = u64_stats_read(&stats->rx_bytes);\n\t\t\trx_drops   = u64_stats_read(&stats->rx_drops);\n\t\t\ttx_packets = u64_stats_read(&stats->tx_packets);\n\t\t\ttx_bytes   = u64_stats_read(&stats->tx_bytes);\n\t\t\ttx_drops   = u64_stats_read(&stats->tx_drops);\n\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\ts->rx_packets += rx_packets;\n\t\ts->rx_bytes   += rx_bytes;\n\t\ts->rx_dropped += rx_drops;\n\t\ts->tx_packets += tx_packets;\n\t\ts->tx_bytes   += tx_bytes;\n\t\ts->tx_dropped += tx_drops;\n\t}\n}\n\n/* ndo_get_stats64 implementation for dtstats-based accounting.\n *\n * Populate @s from dev->stats and dev->dstats. This is used internally by the\n * core for NETDEV_PCPU_STAT_DSTAT-type stats collection.\n */\nstatic void dev_get_dstats64(const struct net_device *dev,\n\t\t\t     struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_dstats(s, dev->dstats);\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(atomic_long_t);\n\tconst atomic_long_t *src = (atomic_long_t *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = (unsigned long)atomic_long_read(&src[i]);\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\nstatic __cold struct net_device_core_stats __percpu *netdev_core_stats_alloc(\n\t\tstruct net_device *dev)\n{\n\tstruct net_device_core_stats __percpu *p;\n\n\tp = alloc_percpu_gfp(struct net_device_core_stats,\n\t\t\t     GFP_ATOMIC | __GFP_NOWARN);\n\n\tif (p && cmpxchg(&dev->core_stats, NULL, p))\n\t\tfree_percpu(p);\n\n\t/* This READ_ONCE() pairs with the cmpxchg() above */\n\treturn READ_ONCE(dev->core_stats);\n}\n\nnoinline void netdev_core_stats_inc(struct net_device *dev, u32 offset)\n{\n\t/* This READ_ONCE() pairs with the write in netdev_core_stats_alloc() */\n\tstruct net_device_core_stats __percpu *p = READ_ONCE(dev->core_stats);\n\tunsigned long __percpu *field;\n\n\tif (unlikely(!p)) {\n\t\tp = netdev_core_stats_alloc(dev);\n\t\tif (!p)\n\t\t\treturn;\n\t}\n\n\tfield = (unsigned long __percpu *)((void __percpu *)p + offset);\n\tthis_cpu_inc(*field);\n}\nEXPORT_SYMBOL_GPL(netdev_core_stats_inc);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tconst struct net_device_core_stats __percpu *p;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else if (dev->pcpu_stat_type == NETDEV_PCPU_STAT_TSTATS) {\n\t\tdev_get_tstats64(dev, storage);\n\t} else if (dev->pcpu_stat_type == NETDEV_PCPU_STAT_DSTATS) {\n\t\tdev_get_dstats64(dev, storage);\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\n\t/* This READ_ONCE() pairs with the write in netdev_core_stats_alloc() */\n\tp = READ_ONCE(dev->core_stats);\n\tif (p) {\n\t\tconst struct net_device_core_stats *core_stats;\n\t\tint i;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tcore_stats = per_cpu_ptr(p, i);\n\t\t\tstorage->rx_dropped += READ_ONCE(core_stats->rx_dropped);\n\t\t\tstorage->tx_dropped += READ_ONCE(core_stats->tx_dropped);\n\t\t\tstorage->rx_nohandler += READ_ONCE(core_stats->rx_nohandler);\n\t\t\tstorage->rx_otherhost_dropped += READ_ONCE(core_stats->rx_otherhost_dropped);\n\t\t}\n\t}\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\n/**\n *\tdev_fetch_sw_netstats - get per-cpu network device statistics\n *\t@s: place to store stats\n *\t@netstats: per-cpu network stats to read from\n *\n *\tRead per-cpu network statistics and populate the related fields in @s.\n */\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tu64 rx_packets, rx_bytes, tx_packets, tx_bytes;\n\t\tconst struct pcpu_sw_netstats *stats;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(netstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\trx_packets = u64_stats_read(&stats->rx_packets);\n\t\t\trx_bytes   = u64_stats_read(&stats->rx_bytes);\n\t\t\ttx_packets = u64_stats_read(&stats->tx_packets);\n\t\t\ttx_bytes   = u64_stats_read(&stats->tx_bytes);\n\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\ts->rx_packets += rx_packets;\n\t\ts->rx_bytes   += rx_bytes;\n\t\ts->tx_packets += tx_packets;\n\t\ts->tx_bytes   += tx_bytes;\n\t}\n}\nEXPORT_SYMBOL_GPL(dev_fetch_sw_netstats);\n\n/**\n *\tdev_get_tstats64 - ndo_get_stats64 implementation\n *\t@dev: device to get statistics from\n *\t@s: place to store stats\n *\n *\tPopulate @s from dev->stats and dev->tstats. Can be used as\n *\tndo_get_stats64() callback.\n */\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_sw_netstats(s, dev->tstats);\n}\nEXPORT_SYMBOL_GPL(dev_get_tstats64);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tRCU_INIT_POINTER(queue->qdisc_sleeping, &noop_qdisc);\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\n/**\n * netdev_sw_irq_coalesce_default_on() - enable SW IRQ coalescing by default\n * @dev: netdev to enable the IRQ coalescing on\n *\n * Sets a conservative default for SW IRQ coalescing. Users can use\n * sysfs attributes to override the default values.\n */\nvoid netdev_sw_irq_coalesce_default_on(struct net_device *dev)\n{\n\tWARN_ON(dev->reg_state == NETREG_REGISTERED);\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\tdev->gro_flush_timeout = 20000;\n\t\tdev->napi_defer_hard_irqs = 1;\n\t}\n}\nEXPORT_SYMBOL_GPL(netdev_sw_irq_coalesce_default_on);\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tdev = kvzalloc(struct_size(dev, priv, sizeof_priv),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!dev)\n\t\treturn NULL;\n\n\tdev->priv_len = sizeof_priv;\n\n\tref_tracker_dir_init(&dev->refcnt_tracker, 128, name);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\t__dev_hold(dev);\n#else\n\trefcount_set(&dev->dev_refcnt, 1);\n#endif\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->xdp_zc_max_segs = 1;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->gro_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->gso_ipv4_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->gro_ipv4_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->tso_max_size = TSO_LEGACY_MAX_SIZE;\n\tdev->tso_max_segs = TSO_MAX_SEGS;\n\tdev->upper_level = 1;\n\tdev->lower_level = 1;\n#ifdef CONFIG_LOCKDEP\n\tdev->nested_level = 0;\n\tINIT_LIST_HEAD(&dev->unlink_list);\n#endif\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tINIT_LIST_HEAD(&dev->net_notifier_list);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n\tdev->ethtool = kzalloc(sizeof(*dev->ethtool), GFP_KERNEL_ACCOUNT);\n\tif (!dev->ethtool)\n\t\tgoto free_all;\n\n\tstrscpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_netdev_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n#endif\n\tkvfree(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\n\t/* When called immediately after register_netdevice() failed the unwind\n\t * handling may still be dismantling the device. Handle that case by\n\t * deferring the free.\n\t */\n\tif (dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\t\tdev->needs_free_netdev = true;\n\t\treturn;\n\t}\n\n\tkfree(dev->ethtool);\n\tnetif_free_tx_queues(dev);\n\tnetif_free_rx_queues(dev);\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tref_tracker_dir_exit(&dev->refcnt_tracker);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n#endif\n\tfree_percpu(dev->core_stats);\n\tdev->core_stats = NULL;\n\tfree_percpu(dev->xdp_bulkq);\n\tdev->xdp_bulkq = NULL;\n\n\tnetdev_free_phy_link_topology(dev);\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED ||\n\t    dev->reg_state == NETREG_DUMMY) {\n\t\tkvfree(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tWRITE_ONCE(dev->reg_state, NETREG_RELEASED);\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n * alloc_netdev_dummy - Allocate and initialize a dummy net device.\n * @sizeof_priv: size of private data to allocate space for\n *\n * Return: the allocated net_device on success, NULL otherwise\n */\nstruct net_device *alloc_netdev_dummy(int sizeof_priv)\n{\n\treturn alloc_netdev(sizeof_priv, \"dummy#\", NET_NAME_UNKNOWN,\n\t\t\t    init_dummy_netdev_core);\n}\nEXPORT_SYMBOL_GPL(alloc_netdev_dummy);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\nstatic void netdev_rss_contexts_free(struct net_device *dev)\n{\n\tstruct ethtool_rxfh_context *ctx;\n\tunsigned long context;\n\n\tmutex_lock(&dev->ethtool->rss_lock);\n\txa_for_each(&dev->ethtool->rss_ctx, context, ctx) {\n\t\tstruct ethtool_rxfh_param rxfh;\n\n\t\trxfh.indir = ethtool_rxfh_context_indir(ctx);\n\t\trxfh.key = ethtool_rxfh_context_key(ctx);\n\t\trxfh.hfunc = ctx->hfunc;\n\t\trxfh.input_xfrm = ctx->input_xfrm;\n\t\trxfh.rss_context = context;\n\t\trxfh.rss_delete = true;\n\n\t\txa_erase(&dev->ethtool->rss_ctx, context);\n\t\tif (dev->ethtool_ops->create_rxfh_context)\n\t\t\tdev->ethtool_ops->remove_rxfh_context(dev, ctx,\n\t\t\t\t\t\t\t      context, NULL);\n\t\telse\n\t\t\tdev->ethtool_ops->set_rxfh(dev, &rxfh, NULL);\n\t\tkfree(ctx);\n\t}\n\txa_destroy(&dev->ethtool->rss_ctx);\n\tmutex_unlock(&dev->ethtool->rss_lock);\n}\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->unreg_list, &single);\n\t\tunregister_netdevice_many(&single);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\nvoid unregister_netdevice_many_notify(struct list_head *head,\n\t\t\t\t      u32 portid, const struct nlmsghdr *nlh)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\tint cnt = 0;\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\t\tWRITE_ONCE(dev->reg_state, NETREG_UNREGISTERING);\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\t\tdev_tcx_uninstall(dev);\n\t\tdev_xdp_uninstall(dev);\n\t\tbpf_dev_bound_netdev_unregister(dev);\n\t\tdev_dmabuf_uninstall(dev);\n\n\t\tnetdev_offload_xstats_disable_all(dev);\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL, NULL, 0,\n\t\t\t\t\t\t     portid, nlh);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tnetdev_name_node_alt_flush(dev);\n\t\tnetdev_name_node_free(dev->name_node);\n\n\t\tnetdev_rss_contexts_free(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_PRE_UNINIT, dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tmutex_destroy(&dev->ethtool->rss_lock);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL, portid, nlh);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tnetdev_put(dev, &dev->dev_registered_tracker);\n\t\tnet_set_todo(dev);\n\t\tcnt++;\n\t}\n\tatomic_add(cnt, &dev_unreg_count);\n\n\tlist_del(head);\n}\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack won't be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tunregister_netdevice_many_notify(head, 0, NULL);\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\t__dev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\t@new_ifindex: If not zero, specifies device index in the target\n *\t              namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint __dev_change_net_namespace(struct net_device *dev, struct net *net,\n\t\t\t       const char *pat, int new_ifindex)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net_old = dev_net(dev);\n\tchar new_name[IFNAMSIZ] = {};\n\tint err, new_nsid;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->netns_local)\n\t\tgoto out;\n\n\t/* Ensure the device has been registered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(net_old, net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (netdev_name_in_use(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\terr = dev_prep_valid_name(net, dev, pat, new_name, EEXIST);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\t/* Check that none of the altnames conflicts. */\n\terr = -EEXIST;\n\tnetdev_for_each_altname(dev, name_node)\n\t\tif (netdev_name_in_use(net, name_node->name))\n\t\t\tgoto out;\n\n\t/* Check that new_ifindex isn't used yet. */\n\tif (new_ifindex) {\n\t\terr = dev_index_reserve(net, new_ifindex);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t} else {\n\t\t/* If there is an ifindex conflict assign a new one */\n\t\terr = dev_index_reserve(net, dev->ifindex);\n\t\tif (err == -EBUSY)\n\t\t\terr = dev_index_reserve(net, 0);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tnew_ifindex = err;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\n\tnew_nsid = peernet2id_alloc(dev_net(dev), net, GFP_KERNEL);\n\n\trtmsg_ifinfo_newnet(RTM_DELLINK, dev, ~0U, GFP_KERNEL, &new_nsid,\n\t\t\t    new_ifindex);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Move per-net netdevice notifiers that are following the netdevice */\n\tmove_netdevice_notifiers_dev_net(dev, net);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\tdev->ifindex = new_ifindex;\n\n\tif (new_name[0]) {\n\t\t/* Rename the netdev to prepared name */\n\t\twrite_seqlock_bh(&netdev_rename_lock);\n\t\tstrscpy(dev->name, new_name, IFNAMSIZ);\n\t\twrite_sequnlock_bh(&netdev_rename_lock);\n\t}\n\n\t/* Fixup kobjects */\n\tdev_set_uevent_suppress(&dev->dev, 1);\n\terr = device_rename(&dev->dev, dev->name);\n\tdev_set_uevent_suppress(&dev->dev, 0);\n\tWARN_ON(err);\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Adapt owner in case owning user namespace of target network\n\t * namespace is different from the original one.\n\t */\n\terr = netdev_change_owner(dev, net_old, net);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL, 0, NULL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(__dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state &= NAPIF_STATE_THREADED;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n\tif (!use_backlog_threads()) {\n#ifdef CONFIG_RPS\n\t\tremsd = oldsd->rps_ipi_list;\n\t\toldsd->rps_ipi_list = NULL;\n#endif\n\t\t/* send out pending IPI's on offline CPU */\n\t\tnet_rps_send_ipi(remsd);\n\t}\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx(skb);\n\t\trps_input_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx(skb);\n\t\trps_input_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc_array(NETDEV_HASHENTRIES, sizeof(*hash), GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tBUILD_BUG_ON(GRO_HASH_BUCKETS >\n\t\t     8 * sizeof_field(struct napi_struct, gro_bitmask));\n\n\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\txa_init_flags(&net->dev_by_index, XA_FLAGS_ALLOC1);\n\n\tRAW_INIT_NOTIFIER_HEAD(&net->netdev_chain);\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n\txa_destroy(&net->dev_by_index);\n\tif (net != &init_net)\n\t\tWARN_ON_ONCE(!list_empty(&net->dev_base_head));\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit_net(struct net *net)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\tASSERT_RTNL();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->netns_local)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops && !dev->rtnl_link_ops->netns_refund)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\tif (netdev_name_in_use(&init_net, fb_name))\n\t\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%%d\");\n\n\t\tnetdev_for_each_altname_safe(dev, name_node, tmp)\n\t\t\tif (netdev_name_in_use(&init_net, name_node->name))\n\t\t\t\t__netdev_name_node_alt_destroy(name_node);\n\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tdefault_device_exit_net(net);\n\t\tcond_resched();\n\t}\n\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit_batch = default_device_exit_batch,\n};\n\nstatic void __init net_dev_struct_check(void)\n{\n\t/* TX read-mostly hotpath */\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, priv_flags_fast);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, netdev_ops);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, header_ops);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, _tx);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, real_num_tx_queues);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, gso_max_size);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, gso_ipv4_max_size);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, gso_max_segs);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, gso_partial_features);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, num_tc);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, mtu);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, needed_headroom);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, tc_to_txq);\n#ifdef CONFIG_XPS\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, xps_maps);\n#endif\n#ifdef CONFIG_NETFILTER_EGRESS\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, nf_hooks_egress);\n#endif\n#ifdef CONFIG_NET_XGRESS\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_tx, tcx_egress);\n#endif\n\tCACHELINE_ASSERT_GROUP_SIZE(struct net_device, net_device_read_tx, 160);\n\n\t/* TXRX read-mostly hotpath */\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_txrx, lstats);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_txrx, state);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_txrx, flags);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_txrx, hard_header_len);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_txrx, features);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_txrx, ip6_ptr);\n\tCACHELINE_ASSERT_GROUP_SIZE(struct net_device, net_device_read_txrx, 46);\n\n\t/* RX read-mostly hotpath */\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, ptype_specific);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, ifindex);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, real_num_rx_queues);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, _rx);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, gro_flush_timeout);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, napi_defer_hard_irqs);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, gro_max_size);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, gro_ipv4_max_size);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, rx_handler);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, rx_handler_data);\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, nd_net);\n#ifdef CONFIG_NETPOLL\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, npinfo);\n#endif\n#ifdef CONFIG_NET_XGRESS\n\tCACHELINE_ASSERT_GROUP_MEMBER(struct net_device, net_device_read_rx, tcx_ingress);\n#endif\n\tCACHELINE_ASSERT_GROUP_SIZE(struct net_device, net_device_read_rx, 104);\n}\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/* We allocate 256 pages for each CPU if PAGE_SHIFT is 12 */\n#define SYSTEM_PERCPU_PAGE_POOL_SIZE\t((1 << 20) / PAGE_SIZE)\n\nstatic int net_page_pool_create(int cpuid)\n{\n#if IS_ENABLED(CONFIG_PAGE_POOL)\n\tstruct page_pool_params page_pool_params = {\n\t\t.pool_size = SYSTEM_PERCPU_PAGE_POOL_SIZE,\n\t\t.flags = PP_FLAG_SYSTEM_POOL,\n\t\t.nid = cpu_to_mem(cpuid),\n\t};\n\tstruct page_pool *pp_ptr;\n\n\tpp_ptr = page_pool_create_percpu(&page_pool_params, cpuid);\n\tif (IS_ERR(pp_ptr))\n\t\treturn -ENOMEM;\n\n\tper_cpu(system_page_pool, cpuid) = pp_ptr;\n#endif\n\treturn 0;\n}\n\nstatic int backlog_napi_should_run(unsigned int cpu)\n{\n\tstruct softnet_data *sd = per_cpu_ptr(&softnet_data, cpu);\n\tstruct napi_struct *napi = &sd->backlog;\n\n\treturn test_bit(NAPI_STATE_SCHED_THREADED, &napi->state);\n}\n\nstatic void run_backlog_napi(unsigned int cpu)\n{\n\tstruct softnet_data *sd = per_cpu_ptr(&softnet_data, cpu);\n\n\tnapi_threaded_poll_loop(&sd->backlog);\n}\n\nstatic void backlog_napi_setup(unsigned int cpu)\n{\n\tstruct softnet_data *sd = per_cpu_ptr(&softnet_data, cpu);\n\tstruct napi_struct *napi = &sd->backlog;\n\n\tnapi->thread = this_cpu_read(backlog_napi);\n\tset_bit(NAPI_STATE_THREADED, &napi->state);\n}\n\nstatic struct smp_hotplug_thread backlog_threads = {\n\t.store\t\t\t= &backlog_napi,\n\t.thread_should_run\t= backlog_napi_should_run,\n\t.thread_fn\t\t= run_backlog_napi,\n\t.thread_comm\t\t= \"backlog_napi/%u\",\n\t.setup\t\t\t= backlog_napi_setup,\n};\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tnet_dev_struct_check();\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n#ifdef CONFIG_XFRM_OFFLOAD\n\t\tskb_queue_head_init(&sd->xfrm_backlog);\n#endif\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tINIT_CSD(&sd->csd, rps_trigger_softirq, sd);\n\t\tsd->cpu = i;\n#endif\n\t\tINIT_CSD(&sd->defer_csd, trigger_rx_softirq, sd);\n\t\tspin_lock_init(&sd->defer_lock);\n\n\t\tinit_gro_hash(&sd->backlog);\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t\tINIT_LIST_HEAD(&sd->backlog.poll_list);\n\n\t\tif (net_page_pool_create(i))\n\t\t\tgoto out;\n\t}\n\tif (use_backlog_threads())\n\t\tsmpboot_register_percpu_thread(&backlog_threads);\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\n\n\t/* avoid static key IPIs to isolated CPUs */\n\tif (housekeeping_enabled(HK_TYPE_MISC))\n\t\tnet_enable_timestamp();\nout:\n\tif (rc < 0) {\n\t\tfor_each_possible_cpu(i) {\n\t\t\tstruct page_pool *pp_ptr;\n\n\t\t\tpp_ptr = per_cpu(system_page_pool, i);\n\t\t\tif (!pp_ptr)\n\t\t\t\tcontinue;\n\n\t\t\tpage_pool_destroy(pp_ptr);\n\t\t\tper_cpu(system_page_pool, i) = NULL;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From 68e068cabd2c6c533ef934c2e5151609cf6ecc6d Mon Sep 17 00:00:00 2001\nFrom: Willem de Bruijn <willemb@google.com>\nDate: Wed, 1 Jan 2025 11:47:40 -0500\nSubject: [PATCH] net: reenable NETIF_F_IPV6_CSUM offload for BIG TCP packets\n\nThe blamed commit disabled hardware offoad of IPv6 packets with\nextension headers on devices that advertise NETIF_F_IPV6_CSUM,\nbased on the definition of that feature in skbuff.h:\n\n *   * - %NETIF_F_IPV6_CSUM\n *     - Driver (device) is only able to checksum plain\n *       TCP or UDP packets over IPv6. These are specifically\n *       unencapsulated packets of the form IPv6|TCP or\n *       IPv6|UDP where the Next Header field in the IPv6\n *       header is either TCP or UDP. IPv6 extension headers\n *       are not supported with this feature. This feature\n *       cannot be set in features for a device with\n *       NETIF_F_HW_CSUM also set. This feature is being\n *       DEPRECATED (see below).\n\nThe change causes skb_warn_bad_offload to fire for BIG TCP\npackets.\n\n[  496.310233] WARNING: CPU: 13 PID: 23472 at net/core/dev.c:3129 skb_warn_bad_offload+0xc4/0xe0\n\n[  496.310297]  ? skb_warn_bad_offload+0xc4/0xe0\n[  496.310300]  skb_checksum_help+0x129/0x1f0\n[  496.310303]  skb_csum_hwoffload_help+0x150/0x1b0\n[  496.310306]  validate_xmit_skb+0x159/0x270\n[  496.310309]  validate_xmit_skb_list+0x41/0x70\n[  496.310312]  sch_direct_xmit+0x5c/0x250\n[  496.310317]  __qdisc_run+0x388/0x620\n\nBIG TCP introduced an IPV6_TLV_JUMBO IPv6 extension header to\ncommunicate packet length, as this is an IPv6 jumbogram. But, the\nfeature is only enabled on devices that support BIG TCP TSO. The\nheader is only present for PF_PACKET taps like tcpdump, and not\ntransmitted by physical devices.\n\nFor this specific case of extension headers that are not\ntransmitted, return to the situation before the blamed commit\nand support hardware offload.\n\nipv6_has_hopopt_jumbo() tests not only whether this header is present,\nbut also that it is the only extension header before a terminal (L4)\nheader.\n\nFixes: 04c20a9356f2 (\"net: skip offload for NETIF_F_IPV6_CSUM if ipv6 header contains extension\")\nReported-by: syzbot <syzkaller@googlegroups.com>\nReported-by: Eric Dumazet <edumazet@google.com>\nCloses: https://lore.kernel.org/netdev/CANn89iK1hdC3Nt8KPhOtTF8vCPc1AHDCtse_BTNki1pWxAByTQ@mail.gmail.com/\nSigned-off-by: Willem de Bruijn <willemb@google.com>\nReviewed-by: Eric Dumazet <edumazet@google.com>\nLink: https://patch.msgid.link/20250101164909.1331680-1-willemdebruijn.kernel@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/core/dev.c | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a/net/core/dev.c b/net/core/dev.c\nindex 45a8c3dd4a64..faa23042df38 100644\n--- a/net/core/dev.c\n+++ b/net/core/dev.c\n@@ -3642,8 +3642,10 @@ int skb_csum_hwoffload_help(struct sk_buff *skb,\n \n \tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n \t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n-\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n+\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr) &&\n+\t\t    !ipv6_has_hopopt_jumbo(skb))\n \t\t\tgoto sw_checksum;\n+\n \t\tswitch (skb->csum_offset) {\n \t\tcase offsetof(struct tcphdr, check):\n \t\tcase offsetof(struct udphdr, check):\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/core/dev.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *      NET3    Protocol independent device support routines.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/skbuff.h>\n#include <linux/kthread.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dsa.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/gro.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <trace/events/qdisc.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_netdev.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n#include <linux/net_namespace.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/devlink.h>\n#include <linux/pm_runtime.h>\n#include <linux/prandom.h>\n#include <linux/once_lite.h>\n\n#include \"dev.h\"\n#include \"net-sysfs.h\"\n\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct netdev_notifier_info *info);\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\nstatic DEFINE_MUTEX(ifalias_mutex);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic DECLARE_RWSEM(devnet_rename_sem);\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock_irqsave(struct softnet_data *sd,\n\t\t\t\t    unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_lock_irqsave(&sd->input_pkt_queue.lock, *flags);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_save(*flags);\n}\n\nstatic inline void rps_lock_irq_disable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_lock_irq(&sd->input_pkt_queue.lock);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_disable();\n}\n\nstatic inline void rps_unlock_irq_restore(struct softnet_data *sd,\n\t\t\t\t\t  unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_unlock_irqrestore(&sd->input_pkt_queue.lock, *flags);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_restore(*flags);\n}\n\nstatic inline void rps_unlock_irq_enable(struct softnet_data *sd)\n{\n\tif (IS_ENABLED(CONFIG_RPS))\n\t\tspin_unlock_irq(&sd->input_pkt_queue.lock);\n\telse if (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_enable();\n}\n\nstatic struct netdev_name_node *netdev_name_node_alloc(struct net_device *dev,\n\t\t\t\t\t\t       const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = kmalloc(sizeof(*name_node), GFP_KERNEL);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_HLIST_NODE(&name_node->hlist);\n\tname_node->dev = dev;\n\tname_node->name = name;\n\treturn name_node;\n}\n\nstatic struct netdev_name_node *\nnetdev_name_node_head_alloc(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\n\tname_node = netdev_name_node_alloc(dev, dev->name);\n\tif (!name_node)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&name_node->list);\n\treturn name_node;\n}\n\nstatic void netdev_name_node_free(struct netdev_name_node *name_node)\n{\n\tkfree(name_node);\n}\n\nstatic void netdev_name_node_add(struct net *net,\n\t\t\t\t struct netdev_name_node *name_node)\n{\n\thlist_add_head_rcu(&name_node->hlist,\n\t\t\t   dev_name_hash(net, name_node->name));\n}\n\nstatic void netdev_name_node_del(struct netdev_name_node *name_node)\n{\n\thlist_del_rcu(&name_node->hlist);\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup(struct net *net,\n\t\t\t\t\t\t\tconst char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nstatic struct netdev_name_node *netdev_name_node_lookup_rcu(struct net *net,\n\t\t\t\t\t\t\t    const char *name)\n{\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\tstruct netdev_name_node *name_node;\n\n\thlist_for_each_entry_rcu(name_node, head, hlist)\n\t\tif (!strcmp(name_node->name, name))\n\t\t\treturn name_node;\n\treturn NULL;\n}\n\nbool netdev_name_in_use(struct net *net, const char *name)\n{\n\treturn netdev_name_node_lookup(net, name);\n}\nEXPORT_SYMBOL(netdev_name_in_use);\n\nint netdev_name_node_alt_create(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (name_node)\n\t\treturn -EEXIST;\n\tname_node = netdev_name_node_alloc(dev, name);\n\tif (!name_node)\n\t\treturn -ENOMEM;\n\tnetdev_name_node_add(net, name_node);\n\t/* The node that holds dev->name acts as a head of per-device list. */\n\tlist_add_tail(&name_node->list, &dev->name_node->list);\n\n\treturn 0;\n}\n\nstatic void __netdev_name_node_alt_destroy(struct netdev_name_node *name_node)\n{\n\tlist_del(&name_node->list);\n\tkfree(name_node->name);\n\tnetdev_name_node_free(name_node);\n}\n\nint netdev_name_node_alt_destroy(struct net_device *dev, const char *name)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tname_node = netdev_name_node_lookup(net, name);\n\tif (!name_node)\n\t\treturn -ENOENT;\n\t/* lookup might have found our primary name or a name belonging\n\t * to another device.\n\t */\n\tif (name_node == dev->name_node || name_node->dev != dev)\n\t\treturn -EINVAL;\n\n\tnetdev_name_node_del(name_node);\n\tsynchronize_rcu();\n\t__netdev_name_node_alt_destroy(name_node);\n\n\treturn 0;\n}\n\nstatic void netdev_name_node_alt_flush(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\n\tlist_for_each_entry_safe(name_node, tmp, &dev->name_node->list, list)\n\t\t__netdev_name_node_alt_destroy(name_node);\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\tnetdev_name_node_add(net, dev->name_node);\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock(&dev_base_lock);\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_add(net, name_node);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev, bool lock)\n{\n\tstruct netdev_name_node *name_node;\n\n\tASSERT_RTNL();\n\n\tnetdev_for_each_altname(dev, name_node)\n\t\tnetdev_name_node_del(name_node);\n\n\t/* Unlink dev from the device chain */\n\tif (lock)\n\t\twrite_lock(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\tnetdev_name_node_del(dev->name_node);\n\thlist_del_rcu(&dev->index_hlist);\n\tif (lock)\n\t\twrite_unlock(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\nstatic struct net_device_path *dev_fwd_path(struct net_device_path_stack *stack)\n{\n\tint k = stack->num_paths++;\n\n\tif (WARN_ON_ONCE(k >= NET_DEVICE_PATH_STACK_MAX))\n\t\treturn NULL;\n\n\treturn &stack->path[k];\n}\n\nint dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,\n\t\t\t  struct net_device_path_stack *stack)\n{\n\tconst struct net_device *last_dev;\n\tstruct net_device_path_ctx ctx = {\n\t\t.dev\t= dev,\n\t};\n\tstruct net_device_path *path;\n\tint ret = 0;\n\n\tmemcpy(ctx.daddr, daddr, sizeof(ctx.daddr));\n\tstack->num_paths = 0;\n\twhile (ctx.dev && ctx.dev->netdev_ops->ndo_fill_forward_path) {\n\t\tlast_dev = ctx.dev;\n\t\tpath = dev_fwd_path(stack);\n\t\tif (!path)\n\t\t\treturn -1;\n\n\t\tmemset(path, 0, sizeof(struct net_device_path));\n\t\tret = ctx.dev->netdev_ops->ndo_fill_forward_path(&ctx, path);\n\t\tif (ret < 0)\n\t\t\treturn -1;\n\n\t\tif (WARN_ON_ONCE(last_dev == ctx.dev))\n\t\t\treturn -1;\n\t}\n\n\tif (!ctx.dev)\n\t\treturn ret;\n\n\tpath = dev_fwd_path(stack);\n\tif (!path)\n\t\treturn -1;\n\tpath->type = DEV_PATH_ETHERNET;\n\tpath->dev = ctx.dev;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dev_fill_forward_path);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct netdev_name_node *node_name;\n\n\tnode_name = netdev_name_node_lookup_rcu(net, name);\n\treturn node_name ? node_name->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tint ret;\n\n\tdown_read(&devnet_rename_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tstrcpy(name, dev->name);\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\tup_read(&devnet_rename_sem);\n\treturn ret;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tallow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strnlen(name, IFNAMSIZ) == IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tp = strchr(name, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tstruct netdev_name_node *name_node;\n\n\t\t\tnetdev_for_each_altname(d, name_node) {\n\t\t\t\tif (!sscanf(name_node->name, name, &i))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\t\tif (!strncmp(buf, name_node->name, IFNAMSIZ))\n\t\t\t\t\t__set_bit(i, inuse);\n\t\t\t}\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\t__set_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!netdev_name_in_use(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\nstatic int dev_prep_valid_name(struct net *net, struct net_device *dev,\n\t\t\t       const char *want_name, char *out_name)\n{\n\tint ret;\n\n\tif (!dev_valid_name(want_name))\n\t\treturn -EINVAL;\n\n\tif (strchr(want_name, '%')) {\n\t\tret = __dev_alloc_name(net, want_name, out_name);\n\t\treturn ret < 0 ? ret : 0;\n\t} else if (netdev_name_in_use(net, want_name)) {\n\t\treturn -EEXIST;\n\t} else if (out_name != want_name) {\n\t\tstrscpy(out_name, want_name, IFNAMSIZ);\n\t}\n\n\treturn 0;\n}\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tBUG_ON(!net);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrscpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\treturn dev_alloc_name_ns(dev_net(dev), dev, name);\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = dev_prep_valid_name(net, dev, name, buf);\n\tif (ret >= 0)\n\t\tstrscpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\n\t/* Some auto-enslaved devices e.g. failover slaves are\n\t * special, as userspace might rename the device after\n\t * the interface had been brought up and running since\n\t * the point kernel initiated auto-enslavement. Allow\n\t * live name change even when these slave devices are\n\t * up and running.\n\t *\n\t * Typically, users of these auto-enslaving devices\n\t * don't actually care about slave name change, as\n\t * they are supposed to operate on master interface\n\t * directly.\n\t */\n\tif (dev->flags & IFF_UP &&\n\t    likely(!(dev->priv_flags & IFF_LIVE_RENAME_OK)))\n\t\treturn -EBUSY;\n\n\tdown_write(&devnet_rename_sem);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\tup_write(&devnet_rename_sem);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\tup_write(&devnet_rename_sem);\n\t\treturn ret;\n\t}\n\n\tup_write(&devnet_rename_sem);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock(&dev_base_lock);\n\tnetdev_name_node_del(dev->name_node);\n\twrite_unlock(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock(&dev_base_lock);\n\tnetdev_name_node_add(net, dev->name_node);\n\twrite_unlock(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\tdown_write(&devnet_rename_sem);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tnetdev_err(dev, \"name change rollback failed: %d\\n\",\n\t\t\t\t   ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tstruct dev_ifalias *new_alias = NULL;\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (len) {\n\t\tnew_alias = kmalloc(sizeof(*new_alias) + len + 1, GFP_KERNEL);\n\t\tif (!new_alias)\n\t\t\treturn -ENOMEM;\n\n\t\tmemcpy(new_alias->ifalias, alias, len);\n\t\tnew_alias->ifalias[len] = 0;\n\t}\n\n\tmutex_lock(&ifalias_mutex);\n\tnew_alias = rcu_replace_pointer(dev->ifalias, new_alias,\n\t\t\t\t\tmutex_is_locked(&ifalias_mutex));\n\tmutex_unlock(&ifalias_mutex);\n\n\tif (new_alias)\n\t\tkfree_rcu(new_alias, rcuhead);\n\n\treturn len;\n}\nEXPORT_SYMBOL(dev_set_alias);\n\n/**\n *\tdev_get_alias - get ifalias of a device\n *\t@dev: device\n *\t@name: buffer to store name of ifalias\n *\t@len: size of buffer\n *\n *\tget ifalias for a device.  Caller must make sure dev cannot go\n *\taway,  e.g. rcu read lock or own a reference count to device.\n */\nint dev_get_alias(const struct net_device *dev, char *name, size_t len)\n{\n\tconst struct dev_ifalias *alias;\n\tint ret = 0;\n\n\trcu_read_lock();\n\talias = rcu_dereference(dev->ifalias);\n\tif (alias)\n\t\tret = snprintf(name, len, \"%s\", alias->ifalias);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info.dev = dev,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * __netdev_notify_peers - notify network peers about existence of @dev,\n * to be called when rtnl lock is already held.\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid __netdev_notify_peers(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n}\nEXPORT_SYMBOL(__netdev_notify_peers);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\t__netdev_notify_peers(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int napi_threaded_poll(void *data);\n\nstatic int napi_kthread_create(struct napi_struct *n)\n{\n\tint err = 0;\n\n\t/* Create and wake up the kthread once to put it in\n\t * TASK_INTERRUPTIBLE mode to avoid the blocked task\n\t * warning and work with loadavg.\n\t */\n\tn->thread = kthread_run(napi_threaded_poll, n, \"napi/%s-%d\",\n\t\t\t\tn->dev->name, n->napi_id);\n\tif (IS_ERR(n->thread)) {\n\t\terr = PTR_ERR(n->thread);\n\t\tpr_err(\"kthread_run failed with err %d\\n\", err);\n\t\tn->thread = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int __dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\tdev_addr_check(dev);\n\n\tif (!netif_device_present(dev)) {\n\t\t/* may be detached because parent is runtime-suspended */\n\t\tif (dev->dev.parent)\n\t\t\tpm_runtime_resume(dev->dev.parent);\n\t\tif (!netif_device_present(dev))\n\t\t\treturn -ENODEV;\n\t}\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers_extack(NETDEV_PRE_UP, dev, extack);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev: device to open\n *\t@extack: netlink extended ack\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\n/**\n *\tdev_disable_gro_hw - disable HW Generic Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable HW Generic Receive Offload (GRO_HW) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if Generic XDP is installed on\n *\tthe device.\n */\nstatic void dev_disable_gro_hw(struct net_device *dev)\n{\n\tdev->wanted_features &= ~NETIF_F_GRO_HW;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_GRO_HW))\n\t\tnetdev_WARN(dev, \"failed to disable GRO_HW!\\n\");\n}\n\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd)\n{\n#define N(val) \t\t\t\t\t\t\\\n\tcase NETDEV_##val:\t\t\t\t\\\n\t\treturn \"NETDEV_\" __stringify(val);\n\tswitch (cmd) {\n\tN(UP) N(DOWN) N(REBOOT) N(CHANGE) N(REGISTER) N(UNREGISTER)\n\tN(CHANGEMTU) N(CHANGEADDR) N(GOING_DOWN) N(CHANGENAME) N(FEAT_CHANGE)\n\tN(BONDING_FAILOVER) N(PRE_UP) N(PRE_TYPE_CHANGE) N(POST_TYPE_CHANGE)\n\tN(POST_INIT) N(RELEASE) N(NOTIFY_PEERS) N(JOIN) N(CHANGEUPPER)\n\tN(RESEND_IGMP) N(PRECHANGEMTU) N(CHANGEINFODATA) N(BONDING_INFO)\n\tN(PRECHANGEUPPER) N(CHANGELOWERSTATE) N(UDP_TUNNEL_PUSH_INFO)\n\tN(UDP_TUNNEL_DROP_INFO) N(CHANGE_TX_QUEUE_LEN)\n\tN(CVLAN_FILTER_PUSH_INFO) N(CVLAN_FILTER_DROP_INFO)\n\tN(SVLAN_FILTER_PUSH_INFO) N(SVLAN_FILTER_DROP_INFO)\n\tN(PRE_CHANGEADDR) N(OFFLOAD_XSTATS_ENABLE) N(OFFLOAD_XSTATS_DISABLE)\n\tN(OFFLOAD_XSTATS_REPORT_USED) N(OFFLOAD_XSTATS_REPORT_DELTA)\n\t}\n#undef N\n\treturn \"UNKNOWN_NETDEV_EVENT\";\n}\nEXPORT_SYMBOL_GPL(netdev_cmd_to_name);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t};\n\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int call_netdevice_register_notifiers(struct notifier_block *nb,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tint err;\n\n\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn 0;\n\n\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\treturn 0;\n}\n\nstatic void call_netdevice_unregister_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\tstruct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\tdev);\n\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t}\n\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n}\n\nstatic int call_netdevice_register_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t struct net *net)\n{\n\tstruct net_device *dev;\n\tint err;\n\n\tfor_each_netdev(net, dev) {\n\t\terr = call_netdevice_register_notifiers(nb, dev);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\treturn 0;\n\nrollback:\n\tfor_each_netdev_continue_reverse(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n\treturn err;\n}\n\nstatic void call_netdevice_unregister_net_notifiers(struct notifier_block *nb,\n\t\t\t\t\t\t    struct net *net)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev(net, dev)\n\t\tcall_netdevice_unregister_notifiers(nb, dev);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\terr = call_netdevice_register_net_notifiers(nb, net);\n\t\tif (err)\n\t\t\tgoto rollback;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n\nrollback:\n\tfor_each_net_continue_reverse(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net *net;\n\tint err;\n\n\t/* Close race with setup_net() and cleanup_net() */\n\tdown_write(&pernet_ops_rwsem);\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net)\n\t\tcall_netdevice_unregister_net_notifiers(nb, net);\n\nunlock:\n\trtnl_unlock();\n\tup_write(&pernet_ops_rwsem);\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\nstatic int __register_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t     struct notifier_block *nb,\n\t\t\t\t\t     bool ignore_call_fail)\n{\n\tint err;\n\n\terr = raw_notifier_chain_register(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\tif (dev_boot_phase)\n\t\treturn 0;\n\n\terr = call_netdevice_register_net_notifiers(nb, net);\n\tif (err && !ignore_call_fail)\n\t\tgoto chain_unregister;\n\n\treturn 0;\n\nchain_unregister:\n\traw_notifier_chain_unregister(&net->netdev_chain, nb);\n\treturn err;\n}\n\nstatic int __unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t\t       struct notifier_block *nb)\n{\n\tint err;\n\n\terr = raw_notifier_chain_unregister(&net->netdev_chain, nb);\n\tif (err)\n\t\treturn err;\n\n\tcall_netdevice_unregister_net_notifiers(nb, net);\n\treturn 0;\n}\n\n/**\n * register_netdevice_notifier_net - register a per-netns network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(net, nb, false);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_net);\n\n/**\n * unregister_netdevice_notifier_net - unregister a per-netns\n *                                     network notifier block\n * @net: network namespace\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __unregister_netdevice_notifier_net(net, nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_net);\n\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = __register_netdevice_notifier_net(dev_net(dev), nb, false);\n\tif (!err) {\n\t\tnn->nb = nb;\n\t\tlist_add(&nn->list, &dev->net_notifier_list);\n\t}\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdevice_notifier_dev_net);\n\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn)\n{\n\tint err;\n\n\trtnl_lock();\n\tlist_del(&nn->list);\n\terr = __unregister_netdevice_notifier_net(dev_net(dev), nb);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier_dev_net);\n\nstatic void move_netdevice_notifiers_dev_net(struct net_device *dev,\n\t\t\t\t\t     struct net *net)\n{\n\tstruct netdev_net_notifier *nn;\n\n\tlist_for_each_entry(nn, &dev->net_notifier_list, list) {\n\t\t__unregister_netdevice_notifier_net(dev_net(dev), nn->nb);\n\t\t__register_netdevice_notifier_net(net, nn->nb, true);\n\t}\n}\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/* Run per-netns notifier block chain first, then run the global one.\n\t * Hopefully, one day, the global one is going to be removed after\n\t * all notifier block registrators get converted to be per-netns.\n\t */\n\tret = raw_notifier_call_chain(&net->netdev_chain, val, info);\n\tif (ret & NOTIFY_STOP_MASK)\n\t\treturn ret;\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers_info_robust - call per-netns notifier blocks\n *\t                                       for and rollback on error\n *\t@val_up: value passed unmodified to notifier function\n *\t@val_down: value passed unmodified to the notifier function when\n *\t           recovering from an error on @val_up\n *\t@info: notifier information data\n *\n *\tCall all per-netns network notifier blocks, but not notifier blocks on\n *\tthe global notifier chain. Parameters and return value are as for\n *\traw_notifier_call_chain_robust().\n */\n\nstatic int\ncall_netdevice_notifiers_info_robust(unsigned long val_up,\n\t\t\t\t     unsigned long val_down,\n\t\t\t\t     struct netdev_notifier_info *info)\n{\n\tstruct net *net = dev_net(info->dev);\n\n\tASSERT_RTNL();\n\n\treturn raw_notifier_call_chain_robust(&net->netdev_chain,\n\t\t\t\t\t      val_up, val_down, info);\n}\n\nstatic int call_netdevice_notifiers_extack(unsigned long val,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_info info = {\n\t\t.dev = dev,\n\t\t.extack = extack,\n\t};\n\n\treturn call_netdevice_notifiers_info(val, &info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\treturn call_netdevice_notifiers_extack(val, dev, NULL);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n/**\n *\tcall_netdevice_notifiers_mtu - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@arg: additional u32 argument passed to the notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\nstatic int call_netdevice_notifiers_mtu(unsigned long val,\n\t\t\t\t\tstruct net_device *dev, u32 arg)\n{\n\tstruct netdev_notifier_info_ext info = {\n\t\t.info.dev = dev,\n\t\t.ext.mtu = arg,\n\t};\n\n\tBUILD_BUG_ON(offsetof(struct netdev_notifier_info_ext, info) != 0);\n\n\treturn call_netdevice_notifiers_info(val, &info.info);\n}\n\n#ifdef CONFIG_NET_INGRESS\nstatic DEFINE_STATIC_KEY_FALSE(ingress_needed_key);\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_branch_inc(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_branch_dec(&ingress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic DEFINE_STATIC_KEY_FALSE(egress_needed_key);\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_branch_inc(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_branch_dec(&egress_needed_key);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nDEFINE_STATIC_KEY_FALSE(netstamp_needed_key);\nEXPORT_SYMBOL(netstamp_needed_key);\n#ifdef CONFIG_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_branch_enable(&netstamp_needed_key);\n\telse\n\t\tstatic_branch_disable(&netstamp_needed_key);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 0)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted + 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_inc(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef CONFIG_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 1)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted - 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_branch_dec(&netstamp_needed_key);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tskb->mono_delivery_time = 0;\n\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\tskb->tstamp = ktime_get_real();\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\t\\\n\tif (static_branch_unlikely(&netstamp_needed_key)) {\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\t\t\\\n\t\t\t(SKB)->tstamp = ktime_get_real();\t\\\n\t}\t\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\treturn __is_skb_forwardable(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nstatic int __dev_forward_skb2(struct net_device *dev, struct sk_buff *skb,\n\t\t\t      bool check_mtu)\n{\n\tint ret = ____dev_forward_skb(dev, skb, check_mtu);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, true);\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb2(dev, skb, false) ?: netif_rx_internal(skb);\n}\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * dev_nit_active - return true if any network interface taps are in use\n *\n * @dev: network device to check for the presence of taps\n */\nbool dev_nit_active(struct net_device *dev)\n{\n\treturn !list_empty(&ptype_all) || !list_empty(&dev->ptype_all);\n}\nEXPORT_SYMBOL_GPL(dev_nit_active);\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (READ_ONCE(ptype->ignore_outgoing))\n\t\t\tcontinue;\n\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tnetdev_warn(dev, \"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tnetdev_warn(dev, \"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\t    i, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\t/* walk through the TCs and see if it falls into any of them */\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\t/* didn't find it, just return -1 to indicate no match */\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_txq_to_tc);\n\n#ifdef CONFIG_XPS\nstatic struct static_key xps_needed __read_mostly;\nstatic struct static_key xps_rxqs_needed __read_mostly;\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     struct xps_dev_maps *old_maps, int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tif (old_maps)\n\t\t\tRCU_INIT_POINTER(old_maps->attr_map[tci], NULL);\n\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev_maps->num_tc;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, NULL, tci, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void reset_xps_maps(struct net_device *dev,\n\t\t\t   struct xps_dev_maps *dev_maps,\n\t\t\t   enum xps_map_type type)\n{\n\tstatic_key_slow_dec_cpuslocked(&xps_needed);\n\tif (type == XPS_RXQS)\n\t\tstatic_key_slow_dec_cpuslocked(&xps_rxqs_needed);\n\n\tRCU_INIT_POINTER(dev->xps_maps[type], NULL);\n\n\tkfree_rcu(dev_maps, rcu);\n}\n\nstatic void clean_xps_maps(struct net_device *dev, enum xps_map_type type,\n\t\t\t   u16 offset, u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tbool active = false;\n\tint i, j;\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (!dev_maps)\n\t\treturn;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, j, offset, count);\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\n\tif (type == XPS_CPUS) {\n\t\tfor (i = offset + (count - 1); count--; i--)\n\t\t\tnetdev_queue_numa_node_write(\n\t\t\t\tnetdev_get_tx_queue(dev, i), NUMA_NO_NODE);\n\t}\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tif (!static_key_false(&xps_needed))\n\t\treturn;\n\n\tcpus_read_lock();\n\tmutex_lock(&xps_map_mutex);\n\n\tif (static_key_false(&xps_rxqs_needed))\n\t\tclean_xps_maps(dev, XPS_RXQS, offset, count);\n\n\tclean_xps_maps(dev, XPS_CPUS, offset, count);\n\n\tmutex_unlock(&xps_map_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map, int attr_index,\n\t\t\t\t      u16 index, bool is_rxqs_map)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add tx-queue to this CPU's/rx-queue's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store tx-queue on this CPU's/rx-queue's\n\t *  map\n\t */\n\tif (is_rxqs_map)\n\t\tnew_map = kzalloc(XPS_MAP_SIZE(alloc_len), GFP_KERNEL);\n\telse\n\t\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t\t       cpu_to_node(attr_index));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\n/* Copy xps maps at a given index */\nstatic void xps_copy_dev_maps(struct xps_dev_maps *dev_maps,\n\t\t\t      struct xps_dev_maps *new_dev_maps, int index,\n\t\t\t      int tc, bool skip_tc)\n{\n\tint i, tci = index * dev_maps->num_tc;\n\tstruct xps_map *map;\n\n\t/* copy maps belonging to foreign traffic classes */\n\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\tif (i == tc && skip_tc)\n\t\t\tcontinue;\n\n\t\t/* fill in the new device map from the old device map */\n\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n}\n\n/* Must be called under cpus_read_lock */\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL, *old_dev_maps = NULL;\n\tconst unsigned long *online_mask = NULL;\n\tbool active = false, copy = false;\n\tint i, j, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tunsigned int nr_ids;\n\n\tWARN_ON_ONCE(index >= dev->num_tx_queues);\n\n\tif (dev->num_tc) {\n\t\t/* Do not allow XPS on subordinate device directly */\n\t\tnum_tc = dev->num_tc;\n\t\tif (num_tc < 0)\n\t\t\treturn -EINVAL;\n\n\t\t/* If queue belongs to subordinate dev use its map */\n\t\tdev = netdev_get_tx_queue(dev, index)->sb_dev ? : dev;\n\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps[type]);\n\tif (type == XPS_RXQS) {\n\t\tmaps_sz = XPS_RXQ_DEV_MAPS_SIZE(num_tc, dev->num_rx_queues);\n\t\tnr_ids = dev->num_rx_queues;\n\t} else {\n\t\tmaps_sz = XPS_CPU_DEV_MAPS_SIZE(num_tc);\n\t\tif (num_possible_cpus() > 1)\n\t\t\tonline_mask = cpumask_bits(cpu_online_mask);\n\t\tnr_ids = nr_cpu_ids;\n\t}\n\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\t/* The old dev_maps could be larger or smaller than the one we're\n\t * setting up now, as dev->num_tc or nr_ids could have been updated in\n\t * between. We could try to be smart, but let's be safe instead and only\n\t * copy foreign traffic classes if the two map sizes match.\n\t */\n\tif (dev_maps &&\n\t    dev_maps->num_tc == num_tc && dev_maps->nr_ids == nr_ids)\n\t\tcopy = true;\n\n\t/* allocate memory for queue storage */\n\tfor (j = -1; j = netif_attrmask_next_and(j, online_mask, mask, nr_ids),\n\t     j < nr_ids;) {\n\t\tif (!new_dev_maps) {\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\t\tif (!new_dev_maps) {\n\t\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tnew_dev_maps->nr_ids = nr_ids;\n\t\t\tnew_dev_maps->num_tc = num_tc;\n\t\t}\n\n\t\ttci = j * num_tc + tc;\n\t\tmap = copy ? xmap_dereference(dev_maps->attr_map[tci]) : NULL;\n\n\t\tmap = expand_xps_map(map, j, index, type == XPS_RXQS);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->attr_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tif (!dev_maps) {\n\t\t/* Increment static keys at most once per type */\n\t\tstatic_key_slow_inc_cpuslocked(&xps_needed);\n\t\tif (type == XPS_RXQS)\n\t\t\tstatic_key_slow_inc_cpuslocked(&xps_rxqs_needed);\n\t}\n\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tbool skip_tc = false;\n\n\t\ttci = j * num_tc + tc;\n\t\tif (netif_attr_test_mask(j, mask, nr_ids) &&\n\t\t    netif_attr_test_online(j, online_mask, nr_ids)) {\n\t\t\t/* add tx-queue to CPU/rx-queue maps */\n\t\t\tint pos = 0;\n\n\t\t\tskip_tc = true;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (type == XPS_CPUS) {\n\t\t\t\tif (numa_node_id == -2)\n\t\t\t\t\tnuma_node_id = cpu_to_node(j);\n\t\t\t\telse if (numa_node_id != cpu_to_node(j))\n\t\t\t\t\tnuma_node_id = -1;\n\t\t\t}\n#endif\n\t\t}\n\n\t\tif (copy)\n\t\t\txps_copy_dev_maps(dev_maps, new_dev_maps, j, tc,\n\t\t\t\t\t  skip_tc);\n\t}\n\n\trcu_assign_pointer(dev->xps_maps[type], new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * dev_maps->num_tc; i--; tci++) {\n\t\t\tmap = xmap_dereference(dev_maps->attr_map[tci]);\n\t\t\tif (!map)\n\t\t\t\tcontinue;\n\n\t\t\tif (copy) {\n\t\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\t\tif (map == new_map)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tRCU_INIT_POINTER(dev_maps->attr_map[tci], NULL);\n\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\told_dev_maps = dev_maps;\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\tif (type == XPS_CPUS)\n\t\t/* update Tx queue numa node */\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t\t     (numa_node_id >= 0) ?\n\t\t\t\t\t     numa_node_id : NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes tx-queue from unused CPUs/rx-queues */\n\tfor (j = 0; j < dev_maps->nr_ids; j++) {\n\t\ttci = j * dev_maps->num_tc;\n\n\t\tfor (i = 0; i < dev_maps->num_tc; i++, tci++) {\n\t\t\tif (i == tc &&\n\t\t\t    netif_attr_test_mask(j, mask, dev_maps->nr_ids) &&\n\t\t\t    netif_attr_test_online(j, online_mask, dev_maps->nr_ids))\n\t\t\t\tcontinue;\n\n\t\t\tactive |= remove_xps_queue(dev_maps,\n\t\t\t\t\t\t   copy ? old_dev_maps : NULL,\n\t\t\t\t\t\t   tci, index);\n\t\t}\n\t}\n\n\tif (old_dev_maps)\n\t\tkfree_rcu(old_dev_maps, rcu);\n\n\t/* free map if not active */\n\tif (!active)\n\t\treset_xps_maps(dev, dev_maps, type);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor (j = 0; j < nr_ids; j++) {\n\t\tfor (i = num_tc, tci = j * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->attr_map[tci]);\n\t\t\tmap = copy ?\n\t\t\t      xmap_dereference(dev_maps->attr_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(__netif_set_xps_queue);\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tint ret;\n\n\tcpus_read_lock();\n\tret =  __netif_set_xps_queue(dev, cpumask_bits(mask), index, XPS_CPUS);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nstatic void netdev_unbind_all_sb_channels(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n\t/* Unbind any subordinate channels */\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev)\n\t\t\tnetdev_unbind_sb_channel(dev, txq->sb_dev);\n\t}\n}\n\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\t/* Reset TC configuration of device */\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tnetdev_unbind_all_sb_channels(dev);\n\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct netdev_queue *txq = &dev->_tx[dev->num_tx_queues];\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(sb_dev, 0);\n#endif\n\tmemset(sb_dev->tc_to_txq, 0, sizeof(sb_dev->tc_to_txq));\n\tmemset(sb_dev->prio_tc_map, 0, sizeof(sb_dev->prio_tc_map));\n\n\twhile (txq-- != &dev->_tx[0]) {\n\t\tif (txq->sb_dev == sb_dev)\n\t\t\ttxq->sb_dev = NULL;\n\t}\n}\nEXPORT_SYMBOL(netdev_unbind_sb_channel);\n\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset)\n{\n\t/* Make certain the sb_dev and dev are already configured */\n\tif (sb_dev->num_tc >= 0 || tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\t/* We cannot hand out queues we don't have */\n\tif ((offset + count) > dev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\t/* Record the mapping */\n\tsb_dev->tc_to_txq[tc].count = count;\n\tsb_dev->tc_to_txq[tc].offset = offset;\n\n\t/* Provide a way for Tx queue to find the tc_to_txq map or\n\t * XPS map for itself.\n\t */\n\twhile (count--)\n\t\tnetdev_get_tx_queue(dev, count + offset)->sb_dev = sb_dev;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_bind_sb_channel_queue);\n\nint netdev_set_sb_channel(struct net_device *dev, u16 channel)\n{\n\t/* Do not use a multiqueue device to represent a subordinate channel */\n\tif (netif_is_multiqueue(dev))\n\t\treturn -ENODEV;\n\n\t/* We allow channels 1 - 32767 to be used for subordinate channels.\n\t * Channel 0 is meant to be \"native\" mode and used only to represent\n\t * the main root device. We allow writing 0 to reset the device back\n\t * to normal mode after being used as a subordinate channel.\n\t */\n\tif (channel > S16_MAX)\n\t\treturn -EINVAL;\n\n\tdev->num_tc = -channel;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_sb_channel);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater than real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tbool disabling;\n\tint rc;\n\n\tdisabling = txq < dev->real_num_tx_queues;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tdev_qdisc_change_real_num_tx(dev, txq);\n\n\t\tdev->real_num_tx_queues = txq;\n\n\t\tif (disabling) {\n\t\t\tsynchronize_net();\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t} else {\n\t\tdev->real_num_tx_queues = txq;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n *\tnetif_set_real_num_queues - set actual number of RX and TX queues used\n *\t@dev: Network device\n *\t@txq: Actual number of TX queues\n *\t@rxq: Actual number of RX queues\n *\n *\tSet the real number of both TX and RX queues.\n *\tDoes nothing if the number of queues is already correct.\n */\nint netif_set_real_num_queues(struct net_device *dev,\n\t\t\t      unsigned int txq, unsigned int rxq)\n{\n\tunsigned int old_rxq = dev->real_num_rx_queues;\n\tint err;\n\n\tif (txq < 1 || txq > dev->num_tx_queues ||\n\t    rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\t/* Start from increases, so the error path only does decreases -\n\t * decreases can't fail.\n\t */\n\tif (rxq > dev->real_num_rx_queues) {\n\t\terr = netif_set_real_num_rx_queues(dev, rxq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (txq > dev->real_num_tx_queues) {\n\t\terr = netif_set_real_num_tx_queues(dev, txq);\n\t\tif (err)\n\t\t\tgoto undo_rx;\n\t}\n\tif (rxq < dev->real_num_rx_queues)\n\t\tWARN_ON(netif_set_real_num_rx_queues(dev, rxq));\n\tif (txq < dev->real_num_tx_queues)\n\t\tWARN_ON(netif_set_real_num_tx_queues(dev, txq));\n\n\treturn 0;\nundo_rx:\n\tWARN_ON(netif_set_real_num_rx_queues(dev, old_rxq));\n\treturn err;\n}\nEXPORT_SYMBOL(netif_set_real_num_queues);\n\n/**\n * netif_set_tso_max_size() - set the max size of TSO frames supported\n * @dev:\tnetdev to update\n * @size:\tmax skb->len of a TSO frame\n *\n * Set the limit on the size of TSO super-frames the device can handle.\n * Unless explicitly set the stack will assume the value of\n * %GSO_LEGACY_MAX_SIZE.\n */\nvoid netif_set_tso_max_size(struct net_device *dev, unsigned int size)\n{\n\tdev->tso_max_size = min(GSO_MAX_SIZE, size);\n\tif (size < READ_ONCE(dev->gso_max_size))\n\t\tnetif_set_gso_max_size(dev, size);\n}\nEXPORT_SYMBOL(netif_set_tso_max_size);\n\n/**\n * netif_set_tso_max_segs() - set the max number of segs supported for TSO\n * @dev:\tnetdev to update\n * @segs:\tmax number of TCP segments\n *\n * Set the limit on the number of TCP segments the device can generate from\n * a single TSO super-frame.\n * Unless explicitly set the stack will assume the value of %GSO_MAX_SEGS.\n */\nvoid netif_set_tso_max_segs(struct net_device *dev, unsigned int segs)\n{\n\tdev->tso_max_segs = segs;\n\tif (segs < READ_ONCE(dev->gso_max_segs))\n\t\tnetif_set_gso_max_segs(dev, segs);\n}\nEXPORT_SYMBOL(netif_set_tso_max_segs);\n\n/**\n * netif_inherit_tso_max() - copy all TSO limits from a lower device to an upper\n * @to:\t\tnetdev to update\n * @from:\tnetdev from which to copy the limits\n */\nvoid netif_inherit_tso_max(struct net_device *to, const struct net_device *from)\n{\n\tnetif_set_tso_max_size(to, from->tso_max_size);\n\tnetif_set_tso_max_segs(to, from->tso_max_segs);\n}\nEXPORT_SYMBOL(netif_inherit_tso_max);\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * Default value is the number of physical cores if there are only 1 or 2, or\n * divided by 2 if there are more.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\tcpumask_var_t cpus;\n\tint cpu, count = 0;\n\n\tif (unlikely(is_kdump_kernel() || !zalloc_cpumask_var(&cpus, GFP_KERNEL)))\n\t\treturn 1;\n\n\tcpumask_copy(cpus, cpu_online_mask);\n\tfor_each_cpu(cpu, cpus) {\n\t\t++count;\n\t\tcpumask_andnot(cpus, cpus, topology_sibling_cpumask(cpu));\n\t}\n\tfree_cpumask_var(cpus);\n\n\treturn count > 2 ? DIV_ROUND_UP(count, 2) : count;\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!netif_xmit_stopped(txq)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_hardirq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse if (unlikely(reason == SKB_REASON_DROPPED))\n\t\tkfree_skb(skb);\n\telse\n\t\tconsume_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nstatic u16 skb_tx_hash(const struct net_device *dev,\n\t\t       const struct net_device *sb_dev,\n\t\t       struct sk_buff *skb)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = dev->real_num_tx_queues;\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = sb_dev->tc_to_txq[tc].offset;\n\t\tqcount = sb_dev->tc_to_txq[tc].count;\n\t\tif (unlikely(!qcount)) {\n\t\t\tnet_warn_ratelimited(\"%s: invalid qcount, qoffset %u for tc %u\\n\",\n\t\t\t\t\t     sb_dev->name, qoffset, tc);\n\t\t\tqoffset = 0;\n\t\t\tqcount = dev->real_num_tx_queues;\n\t\t}\n\t}\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tDEBUG_NET_WARN_ON_ONCE(qcount == 0);\n\t\thash = skb_get_rx_queue(skb);\n\t\tif (hash >= qoffset)\n\t\t\thash -= qoffset;\n\t\twhile (unlikely(hash >= qcount))\n\t\t\thash -= qcount;\n\t\treturn hash + qoffset;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tskb_dump(KERN_WARNING, skb, false);\n\tWARN(1, \"%s: caps=(%pNF, %pNF)\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_is_gso(skb))) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tret = -EINVAL;\n\tif (unlikely(offset >= skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset (%d) >= skb_headlen() (%u)\\n\",\n\t\t\t  offset, skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tif (unlikely(offset + sizeof(__sum16) > skb_headlen(skb))) {\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n\t\tWARN_ONCE(true, \"offset+2 (%zu) > skb_headlen() (%u)\\n\",\n\t\t\t  offset + sizeof(__sum16), skb_headlen(skb));\n\t\tgoto out;\n\t}\n\tret = skb_ensure_writable(skb, offset + sizeof(__sum16));\n\tif (ret)\n\t\tgoto out;\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = skb_ensure_writable(skb, offset + sizeof(__le32));\n\tif (ret)\n\t\tgoto out;\n\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_not_inet = 0;\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb->data;\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn vlan_get_protocol_and_depth(skb, type, depth);\n}\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL &&\n\t\t       skb->ip_summed != CHECKSUM_UNNECESSARY;\n\n\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_GSO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tstruct sk_buff *segs;\n\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\t/* We're going to init ->check field in TCP or UDP header */\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\t/* Only report GSO partial support if it will enable us to\n\t * support segmentation on this frame without needing additional\n\t * work.\n\t */\n\tif (features & NETIF_F_GSO_PARTIAL) {\n\t\tnetdev_features_t partial_features = NETIF_F_GSO_ROBUST;\n\t\tstruct net_device *dev = skb->dev;\n\n\t\tpartial_features |= dev->features & dev->gso_partial_features;\n\t\tif (!skb_gso_ok(skb, features | partial_features))\n\t\t\tfeatures &= ~NETIF_F_GSO_PARTIAL;\n\t}\n\n\tBUILD_BUG_ON(SKB_GSO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tsegs = skb_mac_gso_segment(skb, features);\n\n\tif (segs != skb && unlikely(skb_needs_check(skb, tx_path) && !IS_ERR(segs)))\n\t\tskb_warn_bad_offload(skb);\n\n\treturn segs;\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nstatic void do_netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tnetdev_err(dev, \"hw csum failure\\n\");\n\tskb_dump(KERN_ERR, skb, true);\n\tdump_stack();\n}\n\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb)\n{\n\tDO_ONCE_LITE(do_netdev_rx_csum_fault, dev, skb);\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* XXX: check that highmem exists at all on the given machine. */\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, NULL);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > READ_ONCE(dev->gso_max_segs))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (unlikely(skb->len >= READ_ONCE(dev->gso_max_size)))\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\tif (!skb_shinfo(skb)->gso_type) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\t}\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (dev_nit_active(dev))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb_mark_not_on_list(skb);\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_tx_queue_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb_csum_is_sctp(skb)))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\tif (features & NETIF_F_HW_CSUM)\n\t\treturn 0;\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6) &&\n\t\t    skb_network_header_len(skb) != sizeof(struct ipv6hdr))\n\t\t\tgoto sw_checksum;\n\t\tswitch (skb->csum_offset) {\n\t\tcase offsetof(struct tcphdr, check):\n\t\tcase offsetof(struct udphdr, check):\n\t\t\treturn 0;\n\t\t}\n\t}\n\nsw_checksum:\n\treturn skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tskb = sk_validate_xmit_skb(skb, dev);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\tskb = validate_xmit_xfrm(skb, features, again);\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tdev_core_stats_tx_dropped_inc(dev);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb_mark_not_on_list(skb);\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev, again);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size && skb_transport_header_was_set(skb)) {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))) {\n\t\t\tconst struct tcphdr *th;\n\t\t\tstruct tcphdr _tcphdr;\n\n\t\t\tth = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t\tsizeof(_tcphdr), &_tcphdr);\n\t\t\tif (likely(th))\n\t\t\t\thdr_len += __tcp_hdrlen(th);\n\t\t} else if (shinfo->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tstruct udphdr _udphdr;\n\n\t\t\tif (skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t       sizeof(_udphdr), &_udphdr))\n\t\t\t\thdr_len += sizeof(struct udphdr);\n\t\t}\n\n\t\tif (unlikely(shinfo->gso_type & SKB_GSO_DODGY)) {\n\t\t\tint payload = skb->len - hdr_len;\n\n\t\t\t/* Malicious packet. */\n\t\t\tif (payload <= 0)\n\t\t\t\treturn;\n\t\t\tgso_segs = DIV_ROUND_UP(payload, shinfo->gso_size);\n\t\t}\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic int dev_qdisc_enqueue(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t     struct sk_buff **to_free,\n\t\t\t     struct netdev_queue *txq)\n{\n\tint rc;\n\n\trc = q->enqueue(skb, q, to_free) & NET_XMIT_MASK;\n\tif (rc == NET_XMIT_SUCCESS)\n\t\ttrace_qdisc_enqueue(q, txq, skb);\n\treturn rc;\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\n\tif (q->flags & TCQ_F_NOLOCK) {\n\t\tif (q->flags & TCQ_F_CAN_BYPASS && nolock_qdisc_is_empty(q) &&\n\t\t    qdisc_run_begin(q)) {\n\t\t\t/* Retest nolock_qdisc_is_empty() within the protection\n\t\t\t * of q->seqlock to protect from racing with requeuing.\n\t\t\t */\n\t\t\tif (unlikely(!nolock_qdisc_is_empty(q))) {\n\t\t\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\t\t\t__qdisc_run(q);\n\t\t\t\tqdisc_run_end(q);\n\n\t\t\t\tgoto no_lock_out;\n\t\t\t}\n\n\t\t\tqdisc_bstats_cpu_update(q, skb);\n\t\t\tif (sch_direct_xmit(skb, q, dev, txq, NULL, true) &&\n\t\t\t    !nolock_qdisc_is_empty(q))\n\t\t\t\t__qdisc_run(q);\n\n\t\t\tqdisc_run_end(q);\n\t\t\treturn NET_XMIT_SUCCESS;\n\t\t}\n\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tqdisc_run(q);\n\nno_lock_out:\n\t\tif (unlikely(to_free))\n\t\t\tkfree_skb_list_reason(to_free,\n\t\t\t\t\t      SKB_DROP_REASON_QDISC_DROP);\n\t\treturn rc;\n\t}\n\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t * On PREEMPT_RT it is possible to preempt the qdisc owner during xmit\n\t * and then other tasks will only enqueue packets. The packets will be\n\t * sent after the qdisc owner is scheduled again. To prevent this\n\t * scenario the task always serialize on the lock.\n\t */\n\tcontended = qdisc_is_running(q) || IS_ENABLED(CONFIG_PREEMPT_RT);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\n\t\tqdisc_run_end(q);\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = dev_qdisc_enqueue(skb, q, &to_free, txq);\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t\tqdisc_run_end(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list_reason(to_free, SKB_DROP_REASON_QDISC_DROP);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tconst struct netprio_map *map;\n\tconst struct sock *sk;\n\tunsigned int prioidx;\n\n\tif (skb->priority)\n\t\treturn;\n\tmap = rcu_dereference_bh(skb->dev->priomap);\n\tif (!map)\n\t\treturn;\n\tsk = skb_to_full_sk(skb);\n\tif (!sk)\n\t\treturn;\n\n\tprioidx = sock_cgroup_prioidx(&sk->sk_cgrp_data);\n\n\tif (prioidx < map->priomap_len)\n\t\tskb->priority = map->priomap[prioidx];\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tDEBUG_NET_WARN_ON_ONCE(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(dev->miniq_egress);\n\tstruct tcf_result cl_res;\n\n\tif (!miniq)\n\t\treturn skb;\n\n\t/* qdisc_skb_cb(skb)->pkt_len was already set by the caller. */\n\ttc_skb_cb(skb)->mru = 0;\n\ttc_skb_cb(skb)->post_ct = false;\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\n\tswitch (tcf_classify(skb, miniq->block, miniq->filter_list, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\t*ret = NET_XMIT_DROP;\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_EGRESS);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\n\treturn skb;\n}\n\nstatic struct netdev_queue *\nnetdev_tx_queue_mapping(struct net_device *dev, struct sk_buff *skb)\n{\n\tint qm = skb_get_queue_mapping(skb);\n\n\treturn netdev_get_tx_queue(dev, netdev_cap_txqueue(dev, qm));\n}\n\nstatic bool netdev_xmit_txqueue_skipped(void)\n{\n\treturn __this_cpu_read(softnet_data.xmit.skip_txqueue);\n}\n\nvoid netdev_xmit_skip_txqueue(bool skip)\n{\n\t__this_cpu_write(softnet_data.xmit.skip_txqueue, skip);\n}\nEXPORT_SYMBOL_GPL(netdev_xmit_skip_txqueue);\n#endif /* CONFIG_NET_EGRESS */\n\n#ifdef CONFIG_XPS\nstatic int __get_xps_queue_idx(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct xps_dev_maps *dev_maps, unsigned int tci)\n{\n\tint tc = netdev_get_prio_tc_map(dev, skb->priority);\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\tif (tc >= dev_maps->num_tc || tci >= dev_maps->nr_ids)\n\t\treturn queue_index;\n\n\ttci *= dev_maps->num_tc;\n\ttci += tc;\n\n\tmap = rcu_dereference(dev_maps->attr_map[tci]);\n\tif (map) {\n\t\tif (map->len == 1)\n\t\t\tqueue_index = map->queues[0];\n\t\telse\n\t\t\tqueue_index = map->queues[reciprocal_scale(\n\t\t\t\t\t\tskb_get_hash(skb), map->len)];\n\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\tqueue_index = -1;\n\t}\n\treturn queue_index;\n}\n#endif\n\nstatic int get_xps_queue(struct net_device *dev, struct net_device *sb_dev,\n\t\t\t struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct sock *sk = skb->sk;\n\tint queue_index = -1;\n\n\tif (!static_key_false(&xps_needed))\n\t\treturn -1;\n\n\trcu_read_lock();\n\tif (!static_key_false(&xps_rxqs_needed))\n\t\tgoto get_cpus_map;\n\n\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_RXQS]);\n\tif (dev_maps) {\n\t\tint tci = sk_rx_queue_get(sk);\n\n\t\tif (tci >= 0)\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t}\n\nget_cpus_map:\n\tif (queue_index < 0) {\n\t\tdev_maps = rcu_dereference(sb_dev->xps_maps[XPS_CPUS]);\n\t\tif (dev_maps) {\n\t\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\t\tqueue_index = __get_xps_queue_idx(dev, skb, dev_maps,\n\t\t\t\t\t\t\t  tci);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_pick_tx_zero);\n\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev)\n{\n\treturn (u16)raw_smp_processor_id() % dev->real_num_tx_queues;\n}\nEXPORT_SYMBOL(dev_pick_tx_cpu_id);\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tsb_dev = sb_dev ? : dev;\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, sb_dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, sb_dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\nEXPORT_SYMBOL(netdev_pick_tx);\n\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, sb_dev);\n\t\telse\n\t\t\tqueue_index = netdev_pick_tx(dev, skb, sb_dev);\n\n\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n * __dev_queue_xmit() - transmit a buffer\n * @skb:\tbuffer to transmit\n * @sb_dev:\tsuboordinate device used for L2 forwarding offload\n *\n * Queue a buffer for transmission to a network device. The caller must\n * have set the device and priority and built the buffer before calling\n * this function. The function can be called from an interrupt.\n *\n * When calling this method, interrupts MUST be enabled. This is because\n * the BH enable code must have IRQs enabled so that it will not deadlock.\n *\n * Regardless of the return value, the skb is consumed, so it is currently\n * difficult to retry a send to this method. (You can bump the ref count\n * before sending to hold a reference for retry if you are careful.)\n *\n * Return:\n * * 0\t\t\t\t- buffer successfully transmitted\n * * positive qdisc return code\t- NET_XMIT_DROP etc.\n * * negative errno\t\t- other errors\n */\nint __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq = NULL;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\tbool again = false;\n\n\tskb_reset_mac_header(skb);\n\tskb_assert_len(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_at_ingress = 0;\n#endif\n#ifdef CONFIG_NET_EGRESS\n\tif (static_branch_unlikely(&egress_needed_key)) {\n\t\tif (nf_hook_egress_active()) {\n\t\t\tskb = nf_hook_egress(skb, &rc, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tnetdev_xmit_skip_txqueue(false);\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\tnf_skip_egress(skb, false);\n\n\t\tif (netdev_xmit_txqueue_skipped())\n\t\t\ttxq = netdev_tx_queue_mapping(dev, skb);\n\t}\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\tif (!txq)\n\t\ttxq = netdev_core_pick_tx(dev, skb, sb_dev);\n\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\t/* Other cpus might concurrently change txq->xmit_lock_owner\n\t\t * to -1 or to their cpu id, but not to our id.\n\t\t */\n\t\tif (READ_ONCE(txq->xmit_lock_owner) != cpu) {\n\t\t\tif (dev_xmit_recursion())\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev, &again);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\tdev_xmit_recursion_inc();\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\tdev_xmit_recursion_dec();\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\nEXPORT_SYMBOL(__dev_queue_xmit);\n\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct sk_buff *orig_skb = skb;\n\tstruct netdev_queue *txq;\n\tint ret = NETDEV_TX_BUSY;\n\tbool again = false;\n\n\tif (unlikely(!netif_running(dev) ||\n\t\t     !netif_carrier_ok(dev)))\n\t\tgoto drop;\n\n\tskb = validate_xmit_skb_list(skb, dev, &again);\n\tif (skb != orig_skb)\n\t\tgoto drop;\n\n\tskb_set_queue_mapping(skb, queue_id);\n\ttxq = skb_get_tx_queue(dev, skb);\n\n\tlocal_bh_disable();\n\n\tdev_xmit_recursion_inc();\n\tHARD_TX_LOCK(dev, txq, smp_processor_id());\n\tif (!netif_xmit_frozen_or_drv_stopped(txq))\n\t\tret = netdev_start_xmit(skb, dev, txq, false);\n\tHARD_TX_UNLOCK(dev, txq);\n\tdev_xmit_recursion_dec();\n\n\tlocal_bh_enable();\n\treturn ret;\ndrop:\n\tdev_core_stats_tx_dropped_inc(dev);\n\tkfree_skb_list(skb);\n\treturn NET_XMIT_DROP;\n}\nEXPORT_SYMBOL(__dev_direct_xmit);\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nunsigned int sysctl_skb_defer_max __read_mostly = 64;\nint netdev_budget __read_mostly = 300;\n/* Must be at least 2 jiffes to guarantee 1 jiffy timeout */\nunsigned int __read_mostly netdev_budget_usecs = 2 * USEC_PER_SEC / HZ;\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tstruct task_struct *thread;\n\n\tlockdep_assert_irqs_disabled();\n\n\tif (test_bit(NAPI_STATE_THREADED, &napi->state)) {\n\t\t/* Paired with smp_mb__before_atomic() in\n\t\t * napi_enable()/dev_set_threaded().\n\t\t * Use READ_ONCE() to guarantee a complete\n\t\t * read on napi->thread. Only call\n\t\t * wake_up_process() when it's not NULL.\n\t\t */\n\t\tthread = READ_ONCE(napi->thread);\n\t\tif (thread) {\n\t\t\t/* Avoid doing set_bit() if the thread is in\n\t\t\t * INTERRUPTIBLE state, cause napi_thread_wait()\n\t\t\t * makes sure to proceed with napi polling\n\t\t\t * if the thread is explicitly woken from here.\n\t\t\t */\n\t\t\tif (READ_ONCE(thread->__state) != TASK_INTERRUPTIBLE)\n\t\t\t\tset_bit(NAPI_STATE_SCHED_THREADED, &napi->state);\n\t\t\twake_up_process(thread);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key_false rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key_false rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match.\n\t\t * This READ_ONCE() pairs with WRITE_ONCE() from rps_record_sock_flow().\n\t\t */\n\t\tident = READ_ONCE(sock_flow_table->ents[hash & sock_flow_table->mask]);\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = READ_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/* Called from hardirq (IPI) context */\nstatic void trigger_rx_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\tsmp_store_release(&sd->defer_ipi_scheduled, 0);\n}\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int napi_schedule_rps(struct softnet_data *sd)\n{\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n#ifdef CONFIG_RPS\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\t__napi_schedule_irqoff(&mysd->backlog);\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (READ_ONCE(netdev_max_backlog) >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tenum skb_drop_reason reason;\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\treason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tsd = &per_cpu(softnet_data, cpu);\n\n\trps_lock_irqsave(sd, &flags);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= READ_ONCE(netdev_max_backlog) && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock_irq_restore(sd, &flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state))\n\t\t\tnapi_schedule_rps(sd);\n\t\tgoto enqueue;\n\t}\n\treason = SKB_DROP_REASON_CPU_BACKLOG;\n\ndrop:\n\tsd->dropped++;\n\trps_unlock_irq_restore(sd, &flags);\n\n\tdev_core_stats_rx_dropped_inc(skb->dev);\n\tkfree_skb_reason(skb, reason);\n\treturn NET_RX_DROP;\n}\n\nstatic struct netdev_rx_queue *netif_get_rxqueue(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_rx_queue *rxqueue;\n\n\trxqueue = dev->_rx;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\n\t\t\treturn rxqueue; /* Return first rxqueue */\n\t\t}\n\t\trxqueue += index;\n\t}\n\treturn rxqueue;\n}\n\nu32 bpf_prog_run_generic_xdp(struct sk_buff *skb, struct xdp_buff *xdp,\n\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tvoid *orig_data, *orig_data_end, *hard_start;\n\tstruct netdev_rx_queue *rxqueue;\n\tbool orig_bcast, orig_host;\n\tu32 mac_len, frame_sz;\n\t__be16 orig_eth_type;\n\tstruct ethhdr *eth;\n\tu32 metalen, act;\n\tint off;\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thard_start = skb->data - skb_headroom(skb);\n\n\t/* SKB \"head\" area always have tailroom for skb_shared_info */\n\tframe_sz = (void *)skb_end_pointer(skb) - hard_start;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\trxqueue = netif_get_rxqueue(skb);\n\txdp_init_buff(xdp, frame_sz, &rxqueue->xdp_rxq);\n\txdp_prepare_buff(xdp, hard_start, skb_headroom(skb) - mac_len,\n\t\t\t skb_headlen(skb) + mac_len, true);\n\n\torig_data_end = xdp->data_end;\n\torig_data = xdp->data;\n\teth = (struct ethhdr *)xdp->data;\n\torig_host = ether_addr_equal_64bits(eth->h_dest, skb->dev->dev_addr);\n\torig_bcast = is_multicast_ether_addr_64bits(eth->h_dest);\n\torig_eth_type = eth->h_proto;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t/* check if bpf_xdp_adjust_head was used */\n\toff = xdp->data - orig_data;\n\tif (off) {\n\t\tif (off > 0)\n\t\t\t__skb_pull(skb, off);\n\t\telse if (off < 0)\n\t\t\t__skb_push(skb, -off);\n\n\t\tskb->mac_header += off;\n\t\tskb_reset_network_header(skb);\n\t}\n\n\t/* check if bpf_xdp_adjust_tail was used */\n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0) {\n\t\tskb_set_tail_pointer(skb, xdp->data_end - xdp->data);\n\t\tskb->len += off; /* positive on grow, negative on shrink */\n\t}\n\n\t/* check if XDP changed eth hdr such SKB needs update */\n\teth = (struct ethhdr *)xdp->data;\n\tif ((orig_eth_type != eth->h_proto) ||\n\t    (orig_host != ether_addr_equal_64bits(eth->h_dest,\n\t\t\t\t\t\t  skb->dev->dev_addr)) ||\n\t    (orig_bcast != is_multicast_ether_addr_64bits(eth->h_dest))) {\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->pkt_type = PACKET_HOST;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t}\n\n\t/* Redirect/Tx gives L2 packet, code that will reuse skb must __skb_pull\n\t * before calling us again on redirect path. We do not call do_redirect\n\t * as we leave that up to the caller.\n\t *\n\t * Caller is responsible for managing lifetime of skb (i.e. calling\n\t * kfree_skb in response to actions it cannot handle/XDP_DROP).\n\t */\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\tbreak;\n\tcase XDP_PASS:\n\t\tmetalen = xdp->data - xdp->data_meta;\n\t\tif (metalen)\n\t\t\tskb_metadata_set(skb, metalen);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tu32 act = XDP_DROP;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_is_redirected(skb))\n\t\treturn XDP_PASS;\n\n\t/* XDP packets must be linear and must have sufficient headroom\n\t * of XDP_PACKET_HEADROOM bytes. This is the guarantee that also\n\t * native XDP provides, thus we need to do it here as well.\n\t */\n\tif (skb_cloned(skb) || skb_is_nonlinear(skb) ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tint hroom = XDP_PACKET_HEADROOM - skb_headroom(skb);\n\t\tint troom = skb->tail + skb->data_len - skb->end;\n\n\t\t/* In case we have to go down the path and also linearize,\n\t\t * then lets do the pskb_expand_head() work just once here.\n\t\t */\n\t\tif (pskb_expand_head(skb,\n\t\t\t\t     hroom > 0 ? ALIGN(hroom, NET_SKB_PAD) : 0,\n\t\t\t\t     troom > 0 ? troom + 128 : 0, GFP_ATOMIC))\n\t\t\tgoto do_drop;\n\t\tif (skb_linearize(skb))\n\t\t\tgoto do_drop;\n\t}\n\n\tact = bpf_prog_run_generic_xdp(skb, xdp, xdp_prog);\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\tcase XDP_PASS:\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior. This also means\n * that XDP packets are able to starve other packets going through a qdisc,\n * and DDOS attacks will be more effective. In-driver-XDP use dedicated TX\n * queues, so they do not have this starvation issue.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_core_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_frozen_or_drv_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tdev_core_stats_tx_dropped_inc(dev);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(generic_xdp_needed_key);\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tstruct xdp_buff xdp;\n\t\tu32 act;\n\t\tint err;\n\n\t\tact = netif_receive_generic_xdp(skb, &xdp, xdp_prog);\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      &xdp, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t\tbreak;\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_XDP);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\ttrace_netif_rx(skb);\n\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, smp_processor_id(), &qtail);\n\t}\n\treturn ret;\n}\n\n/**\n *\t__netif_rx\t-\tSlightly optimized version of netif_rx\n *\t@skb: buffer to post\n *\n *\tThis behaves as netif_rx except that it does not disable bottom halves.\n *\tAs a result this function may only be invoked from the interrupt context\n *\t(either hard or soft interrupt).\n */\nint __netif_rx(struct sk_buff *skb)\n{\n\tint ret;\n\n\tlockdep_assert_once(hardirq_count() | softirq_count());\n\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netif_rx);\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process via the backlog NAPI device. It\n *\talways succeeds. The buffer may be dropped during processing for\n *\tcongestion control or by the protocol layers.\n *\tThe network buffer is passed via the backlog NAPI device. Modern NIC\n *\tdriver should use NAPI and GRO.\n *\tThis function can used from interrupt and from process context. The\n *\tcaller from process context must not disable interrupts before invoking\n *\tthis function.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\nint netif_rx(struct sk_buff *skb)\n{\n\tbool need_bh_off = !(hardirq_count() | softirq_count());\n\tint ret;\n\n\tif (need_bh_off)\n\t\tlocal_bh_disable();\n\ttrace_netif_rx_entry(skb);\n\tret = netif_rx_internal(skb);\n\ttrace_netif_rx_exit(ret);\n\tif (need_bh_off)\n\t\tlocal_bh_enable();\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_rx);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action,\n\t\t\t\t\t\tSKB_DROP_REASON_NOT_SPECIFIED);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\trcu_read_lock();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock = NULL;\n\n\t\t\thead = head->next_sched;\n\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\n\t\t\tif (!(q->flags & TCQ_F_NOLOCK)) {\n\t\t\t\troot_lock = qdisc_lock(q);\n\t\t\t\tspin_lock(root_lock);\n\t\t\t} else if (unlikely(test_bit(__QDISC_STATE_DEACTIVATED,\n\t\t\t\t\t\t     &q->state))) {\n\t\t\t\t/* There is a synchronize_net() between\n\t\t\t\t * STATE_DEACTIVATED flag being set and\n\t\t\t\t * qdisc_reset()/some_qdisc_is_busy() in\n\t\t\t\t * dev_deactivate(), so we can safely bail out\n\t\t\t\t * early here to avoid data race between\n\t\t\t\t * qdisc_deactivate() and some_qdisc_is_busy()\n\t\t\t\t * for lockless qdisc.\n\t\t\t\t */\n\t\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tif (root_lock)\n\t\t\t\tspin_unlock(root_lock);\n\t\t}\n\n\t\trcu_read_unlock();\n\t}\n\n\txfrm_dev_backlog(sd);\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev, bool *another)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct mini_Qdisc *miniq = rcu_dereference_bh(skb->dev->miniq_ingress);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!miniq)\n\t\treturn skb;\n\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\ttc_skb_cb(skb)->mru = 0;\n\ttc_skb_cb(skb)->post_ct = false;\n\tskb->tc_at_ingress = 1;\n\tmini_qdisc_bstats_cpu_update(miniq, skb);\n\n\tswitch (tcf_classify(skb, miniq->block, miniq->filter_list, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tmini_qdisc_qstats_cpu_drop(miniq);\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_TC_INGRESS);\n\t\t*ret = NET_RX_DROP;\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tif (skb_do_redirect(skb) == -EAGAIN) {\n\t\t\t__skb_pull(skb, skb->mac_len);\n\t\t\t*another = true;\n\t\t\tbreak;\n\t\t}\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\tcase TC_ACT_CONSUMED:\n\t\t*ret = NET_RX_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\tif (dev->priv_flags & IFF_NO_RX_HANDLER)\n\t\treturn -EINVAL;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,\n\t\t\t\t    struct packet_type **ppt_prev)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (static_branch_unlikely(&generic_xdp_needed_key)) {\n\t\tint ret2;\n\n\t\tmigrate_disable();\n\t\tret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\tmigrate_enable();\n\n\t\tif (ret2 != XDP_PASS) {\n\t\t\tret = NET_RX_DROP;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (eth_type_vlan(skb->protocol)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_branch_unlikely(&ingress_needed_key)) {\n\t\tbool another = false;\n\n\t\tnf_skip_egress(skb, true);\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev,\n\t\t\t\t\t &another);\n\t\tif (another)\n\t\t\tgoto another_round;\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tnf_skip_egress(skb, false);\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_redirect(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\t\tbreak;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {\ncheck_vlan_id:\n\t\tif (skb_vlan_tag_get_id(skb)) {\n\t\t\t/* Vlan id is non 0 and vlan_do_receive() above couldn't\n\t\t\t * find vlan device.\n\t\t\t */\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t} else if (eth_type_vlan(skb->protocol)) {\n\t\t\t/* Outer header is 802.1P with vlan 0, inner header is\n\t\t\t * 802.1Q or 802.1AD and vlan_do_receive() above could\n\t\t\t * not find vlan dev for vlan id 0.\n\t\t\t */\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb = skb_vlan_untag(skb);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\tif (vlan_do_receive(&skb))\n\t\t\t\t/* After stripping off 802.1P header with vlan 0\n\t\t\t\t * vlan dev is found for inner header.\n\t\t\t\t */\n\t\t\t\tgoto another_round;\n\t\t\telse if (unlikely(!skb))\n\t\t\t\tgoto out;\n\t\t\telse\n\t\t\t\t/* We have stripped outer 802.1P vlan 0 header.\n\t\t\t\t * But could not find vlan dev.\n\t\t\t\t * check again for vlan id to set OTHERHOST.\n\t\t\t\t */\n\t\t\t\tgoto check_vlan_id;\n\t\t}\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\t__vlan_hwaccel_clear_tag(skb);\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\t*ppt_prev = pt_prev;\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tdev_core_stats_rx_dropped_inc(skb->dev);\n\t\telse\n\t\t\tdev_core_stats_rx_nohandler_inc(skb->dev);\n\t\tkfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\t/* The invariant here is that if *ppt_prev is not NULL\n\t * then skb should also be non-NULL.\n\t *\n\t * Apparently *ppt_prev assignment above holds this invariant due to\n\t * skb dereferencing near it.\n\t */\n\t*pskb = skb;\n\treturn ret;\n}\n\nstatic int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct net_device *orig_dev = skb->dev;\n\tstruct packet_type *pt_prev = NULL;\n\tint ret;\n\n\tret = __netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\tif (pt_prev)\n\t\tret = INDIRECT_CALL_INET(pt_prev->func, ipv6_rcv, ip_rcv, skb,\n\t\t\t\t\t skb->dev, pt_prev, orig_dev);\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb_core - special purpose version of netif_receive_skb\n *\t@skb: buffer to process\n *\n *\tMore direct receive version of netif_receive_skb().  It should\n *\tonly be used by callers that have a need to skip RPS and Generic XDP.\n *\tCaller must also take care of handling if ``(page_is_)pfmemalloc``.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb_core(struct sk_buff *skb)\n{\n\tint ret;\n\n\trcu_read_lock();\n\tret = __netif_receive_skb_one_core(skb, false);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb_core);\n\nstatic inline void __netif_receive_skb_list_ptype(struct list_head *head,\n\t\t\t\t\t\t  struct packet_type *pt_prev,\n\t\t\t\t\t\t  struct net_device *orig_dev)\n{\n\tstruct sk_buff *skb, *next;\n\n\tif (!pt_prev)\n\t\treturn;\n\tif (list_empty(head))\n\t\treturn;\n\tif (pt_prev->list_func != NULL)\n\t\tINDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,\n\t\t\t\t   ip_list_rcv, head, pt_prev, orig_dev);\n\telse\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tskb_list_del_init(skb);\n\t\t\tpt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t\t}\n}\n\nstatic void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)\n{\n\t/* Fast-path assumptions:\n\t * - There is no RX handler.\n\t * - Only one packet_type matches.\n\t * If either of these fails, we will end up doing some per-packet\n\t * processing in-line, then handling the 'last ptype' for the whole\n\t * sublist.  This can't cause out-of-order delivery to any single ptype,\n\t * because the 'last ptype' must be constant across the sublist, and all\n\t * other ptypes are handled per-packet.\n\t */\n\t/* Current (common) ptype of sublist */\n\tstruct packet_type *pt_curr = NULL;\n\t/* Current (common) orig_dev of sublist */\n\tstruct net_device *od_curr = NULL;\n\tstruct list_head sublist;\n\tstruct sk_buff *skb, *next;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tstruct net_device *orig_dev = skb->dev;\n\t\tstruct packet_type *pt_prev = NULL;\n\n\t\tskb_list_del_init(skb);\n\t\t__netif_receive_skb_core(&skb, pfmemalloc, &pt_prev);\n\t\tif (!pt_prev)\n\t\t\tcontinue;\n\t\tif (pt_curr != pt_prev || od_curr != orig_dev) {\n\t\t\t/* dispatch old sublist */\n\t\t\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n\t\t\t/* start new sublist */\n\t\t\tINIT_LIST_HEAD(&sublist);\n\t\t\tpt_curr = pt_prev;\n\t\t\tod_curr = orig_dev;\n\t\t}\n\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\n\t/* dispatch final sublist */\n\t__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_one_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_one_core(skb, false);\n\n\treturn ret;\n}\n\nstatic void __netif_receive_skb_list(struct list_head *head)\n{\n\tunsigned long noreclaim_flag = 0;\n\tstruct sk_buff *skb, *next;\n\tbool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */\n\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tif ((sk_memalloc_socks() && skb_pfmemalloc(skb)) != pfmemalloc) {\n\t\t\tstruct list_head sublist;\n\n\t\t\t/* Handle the previous sublist */\n\t\t\tlist_cut_before(&sublist, head, &skb->list);\n\t\t\tif (!list_empty(&sublist))\n\t\t\t\t__netif_receive_skb_list_core(&sublist, pfmemalloc);\n\t\t\tpfmemalloc = !pfmemalloc;\n\t\t\t/* See comments in __netif_receive_skb */\n\t\t\tif (pfmemalloc)\n\t\t\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\t\telse\n\t\t\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t\t}\n\t}\n\t/* Handle the remaining sublist */\n\tif (!list_empty(head))\n\t\t__netif_receive_skb_list_core(head, pfmemalloc);\n\t/* Restore pflags */\n\tif (pfmemalloc)\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_branch_dec(&generic_xdp_needed_key);\n\t\t} else if (new && !old) {\n\t\t\tstatic_branch_inc(&generic_xdp_needed_key);\n\t\t\tdev_disable_lro(dev);\n\t\t\tdev_disable_gro_hw(dev);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\nvoid netif_receive_skb_list_internal(struct list_head *head)\n{\n\tstruct sk_buff *skb, *next;\n\tstruct list_head sublist;\n\n\tINIT_LIST_HEAD(&sublist);\n\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\tnet_timestamp_check(READ_ONCE(netdev_tstamp_prequeue), skb);\n\t\tskb_list_del_init(skb);\n\t\tif (!skb_defer_rx_timestamp(skb))\n\t\t\tlist_add_tail(&skb->list, &sublist);\n\t}\n\tlist_splice_init(&sublist, head);\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rps_needed)) {\n\t\tlist_for_each_entry_safe(skb, next, head, list) {\n\t\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\t\tif (cpu >= 0) {\n\t\t\t\t/* Will be handled, remove from list */\n\t\t\t\tskb_list_del_init(skb);\n\t\t\t\tenqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\t}\n\t\t}\n\t}\n#endif\n\t__netif_receive_skb_list(head);\n\trcu_read_unlock();\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\ttrace_netif_receive_skb_entry(skb);\n\n\tret = netif_receive_skb_internal(skb);\n\ttrace_netif_receive_skb_exit(ret);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\n/**\n *\tnetif_receive_skb_list - process many receive buffers from network\n *\t@head: list of skbs to process.\n *\n *\tSince return value of netif_receive_skb() is normally ignored, and\n *\twouldn't be meaningful for a list, this function returns void.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n */\nvoid netif_receive_skb_list(struct list_head *head)\n{\n\tstruct sk_buff *skb;\n\n\tif (list_empty(head))\n\t\treturn;\n\tif (trace_netif_receive_skb_list_entry_enabled()) {\n\t\tlist_for_each_entry(skb, head, list)\n\t\t\ttrace_netif_receive_skb_list_entry(skb);\n\t}\n\tnetif_receive_skb_list_internal(head);\n\ttrace_netif_receive_skb_list_exit(0);\n}\nEXPORT_SYMBOL(netif_receive_skb_list);\n\nstatic DEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trps_lock_irq_disable(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock_irq_enable(sd);\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic bool flush_required(int cpu)\n{\n#if IS_ENABLED(CONFIG_RPS)\n\tstruct softnet_data *sd = &per_cpu(softnet_data, cpu);\n\tbool do_flush;\n\n\trps_lock_irq_disable(sd);\n\n\t/* as insertion into process_queue happens with the rps lock held,\n\t * process_queue access may race only with dequeue\n\t */\n\tdo_flush = !skb_queue_empty(&sd->input_pkt_queue) ||\n\t\t   !skb_queue_empty_lockless(&sd->process_queue);\n\trps_unlock_irq_enable(sd);\n\n\treturn do_flush;\n#endif\n\t/* without RPS we can't safely check input_pkt_queue: during a\n\t * concurrent remote skb_queue_splice() we can detect as empty both\n\t * input_pkt_queue and process_queue even if the latter could end-up\n\t * containing a lot of packets.\n\t */\n\treturn true;\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tstatic cpumask_t flush_cpus;\n\tunsigned int cpu;\n\n\t/* since we are under rtnl lock protection we can use static data\n\t * for the cpumask and avoid allocating on stack the possibly\n\t * large mask\n\t */\n\tASSERT_RTNL();\n\n\tcpus_read_lock();\n\n\tcpumask_clear(&flush_cpus);\n\tfor_each_online_cpu(cpu) {\n\t\tif (flush_required(cpu)) {\n\t\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\t\t\tcpumask_set_cpu(cpu, &flush_cpus);\n\t\t}\n\t}\n\n\t/* we can have in flight packet[s] on the cpus we are not flushing,\n\t * synchronize_net() in unregister_netdevice_many() will take care of\n\t * them\n\t */\n\tfor_each_cpu(cpu, &flush_cpus)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tcpus_read_unlock();\n}\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = READ_ONCE(dev_rx_weight);\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\trps_lock_irq_disable(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock_irq_enable(sd);\n\t}\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable to\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked.\n *\n * On PREEMPT_RT enabled kernels this maps to __napi_schedule()\n * because the interrupt disabled assumption might not be true\n * due to force-threaded interrupts and spinlock substitution.\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\telse\n\t\t__napi_schedule(n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new, timeout = 0;\n\tbool ret = true;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (work_done) {\n\t\tif (n->gro_bitmask)\n\t\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tn->defer_hard_irqs_count = READ_ONCE(n->dev->napi_defer_hard_irqs);\n\t}\n\tif (n->defer_hard_irqs_count > 0) {\n\t\tn->defer_hard_irqs_count--;\n\t\ttimeout = READ_ONCE(n->dev->gro_flush_timeout);\n\t\tif (timeout)\n\t\t\tret = false;\n\t}\n\tif (n->gro_bitmask) {\n\t\t/* When the NAPI instance uses a timeout and keeps postponing\n\t\t * it, we need to bound somehow the time packets are kept in\n\t\t * the GRO layer\n\t\t */\n\t\tnapi_gro_flush(n, !!timeout);\n\t}\n\n\tgro_normal_list(n);\n\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |\n\t\t\t      NAPIF_STATE_SCHED_THREADED |\n\t\t\t      NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\tif (timeout)\n\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\treturn ret;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\nstatic void __busy_poll_stop(struct napi_struct *napi, bool skip_schedule)\n{\n\tif (!skip_schedule) {\n\t\tgro_normal_list(napi);\n\t\t__napi_schedule(napi);\n\t\treturn;\n\t}\n\n\tif (napi->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(napi, HZ >= 1000);\n\t}\n\n\tgro_normal_list(napi);\n\tclear_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock, bool prefer_busy_poll,\n\t\t\t   u16 budget)\n{\n\tbool skip_schedule = false;\n\tunsigned long timeout;\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\tif (prefer_busy_poll) {\n\t\tnapi->defer_hard_irqs_count = READ_ONCE(napi->dev->napi_defer_hard_irqs);\n\t\ttimeout = READ_ONCE(napi->dev->gro_flush_timeout);\n\t\tif (napi->defer_hard_irqs_count && timeout) {\n\t\t\thrtimer_start(&napi->timer, ns_to_ktime(timeout), HRTIMER_MODE_REL_PINNED);\n\t\t\tskip_schedule = true;\n\t\t}\n\t}\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, budget);\n\t/* We can't gro_normal_list() here, because napi->poll() might have\n\t * rearmed the napi (napi_complete_done()) in which case it could\n\t * already be running on another CPU.\n\t */\n\ttrace_napi_poll(napi, rc, budget);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == budget)\n\t\t__busy_poll_stop(napi, skip_schedule);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg, bool prefer_busy_poll, u16 budget)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL)) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val) {\n\t\t\t\tif (prefer_busy_poll)\n\t\t\t\t\tset_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t\t\tgoto count;\n\t\t\t}\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, budget);\n\t\ttrace_napi_poll(napi, work, budget);\n\t\tgro_normal_list(napi);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock, prefer_busy_poll, budget);\n\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nstatic void napi_hash_del(struct napi_struct *napi)\n{\n\tspin_lock(&napi_hash_lock);\n\n\thlist_del_init_rcu(&napi->napi_hash_node);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (!napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state)) {\n\t\tclear_bit(NAPI_STATE_PREFER_BUSY_POLL, &napi->state);\n\t\t__napi_schedule_irqoff(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void init_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tINIT_LIST_HEAD(&napi->gro_hash[i].list);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n\tnapi->gro_bitmask = 0;\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded)\n{\n\tstruct napi_struct *napi;\n\tint err = 0;\n\n\tif (dev->threaded == threaded)\n\t\treturn 0;\n\n\tif (threaded) {\n\t\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\t\tif (!napi->thread) {\n\t\t\t\terr = napi_kthread_create(napi);\n\t\t\t\tif (err) {\n\t\t\t\t\tthreaded = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdev->threaded = threaded;\n\n\t/* Make sure kthread is created before THREADED bit\n\t * is set.\n\t */\n\tsmp_mb__before_atomic();\n\n\t/* Setting/unsetting threaded mode on a napi might not immediately\n\t * take effect, if the current napi instance is actively being\n\t * polled. In this case, the switch between threaded mode and\n\t * softirq mode will happen in the next round of napi_schedule().\n\t * This should not cause hiccups/stalls to the live traffic.\n\t */\n\tlist_for_each_entry(napi, &dev->napi_list, dev_list) {\n\t\tif (threaded)\n\t\t\tset_bit(NAPI_STATE_THREADED, &napi->state);\n\t\telse\n\t\t\tclear_bit(NAPI_STATE_THREADED, &napi->state);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_threaded);\n\nvoid netif_napi_add_weight(struct net_device *dev, struct napi_struct *napi,\n\t\t\t   int (*poll)(struct napi_struct *, int), int weight)\n{\n\tif (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &napi->state)))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&napi->poll_list);\n\tINIT_HLIST_NODE(&napi->napi_hash_node);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tinit_gro_hash(napi);\n\tnapi->skb = NULL;\n\tINIT_LIST_HEAD(&napi->rx_list);\n\tnapi->rx_count = 0;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tnetdev_err_once(dev, \"%s() called with weight %d\\n\", __func__,\n\t\t\t\tweight);\n\tnapi->weight = weight;\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tset_bit(NAPI_STATE_NPSVC, &napi->state);\n\tlist_add_rcu(&napi->dev_list, &dev->napi_list);\n\tnapi_hash_add(napi);\n\tnapi_get_frags_check(napi);\n\t/* Create kthread for this napi if dev->threaded is set.\n\t * Clear dev->threaded if kthread creation failed so that\n\t * threaded mode will not be enabled in napi_enable().\n\t */\n\tif (dev->threaded && napi_kthread_create(napi))\n\t\tdev->threaded = 0;\n}\nEXPORT_SYMBOL(netif_napi_add_weight);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\tfor ( ; ; ) {\n\t\tval = READ_ONCE(n->state);\n\t\tif (val & (NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC)) {\n\t\t\tusleep_range(20, 200);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnew = val | NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC;\n\t\tnew &= ~(NAPIF_STATE_THREADED | NAPIF_STATE_PREFER_BUSY_POLL);\n\n\t\tif (cmpxchg(&n->state, val, new) == val)\n\t\t\tbreak;\n\t}\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: NAPI context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nvoid napi_enable(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &val));\n\n\t\tnew = val & ~(NAPIF_STATE_SCHED | NAPIF_STATE_NPSVC);\n\t\tif (n->dev->threaded && n->thread)\n\t\t\tnew |= NAPIF_STATE_THREADED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n}\nEXPORT_SYMBOL(napi_enable);\n\nstatic void flush_gro_hash(struct napi_struct *napi)\n{\n\tint i;\n\n\tfor (i = 0; i < GRO_HASH_BUCKETS; i++) {\n\t\tstruct sk_buff *skb, *n;\n\n\t\tlist_for_each_entry_safe(skb, n, &napi->gro_hash[i].list, list)\n\t\t\tkfree_skb(skb);\n\t\tnapi->gro_hash[i].count = 0;\n\t}\n}\n\n/* Must be called in process context */\nvoid __netif_napi_del(struct napi_struct *napi)\n{\n\tif (!test_and_clear_bit(NAPI_STATE_LISTED, &napi->state))\n\t\treturn;\n\n\tnapi_hash_del(napi);\n\tlist_del_rcu(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tflush_gro_hash(napi);\n\tnapi->gro_bitmask = 0;\n\n\tif (napi->thread) {\n\t\tkthread_stop(napi->thread);\n\t\tnapi->thread = NULL;\n\t}\n}\nEXPORT_SYMBOL(__netif_napi_del);\n\nstatic int __napi_poll(struct napi_struct *n, bool *repoll)\n{\n\tint work, weight;\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tif (unlikely(work > weight))\n\t\tnetdev_err_once(n->dev, \"NAPI poll function %pS returned %d, exceeding its budget of %d.\\n\",\n\t\t\t\tn->poll, work, weight);\n\n\tif (likely(work < weight))\n\t\treturn work;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\treturn work;\n\t}\n\n\t/* The NAPI context has more processing work, but busy-polling\n\t * is preferred. Exit early.\n\t */\n\tif (napi_prefer_busy_poll(n)) {\n\t\tif (napi_complete_done(n, work)) {\n\t\t\t/* If timeout is not set, we need to make sure\n\t\t\t * that the NAPI is re-scheduled.\n\t\t\t */\n\t\t\tnapi_schedule(n);\n\t\t}\n\t\treturn work;\n\t}\n\n\tif (n->gro_bitmask) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\tgro_normal_list(n);\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\treturn work;\n\t}\n\n\t*repoll = true;\n\n\treturn work;\n}\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tbool do_repoll = false;\n\tvoid *have;\n\tint work;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\twork = __napi_poll(n, &do_repoll);\n\n\tif (do_repoll)\n\t\tlist_add_tail(&n->poll_list, repoll);\n\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic int napi_thread_wait(struct napi_struct *napi)\n{\n\tbool woken = false;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\twhile (!kthread_should_stop()) {\n\t\t/* Testing SCHED_THREADED bit here to make sure the current\n\t\t * kthread owns this napi and could poll on this napi.\n\t\t * Testing SCHED bit is not enough because SCHED bit might be\n\t\t * set by some other busy poll thread or by napi_disable().\n\t\t */\n\t\tif (test_bit(NAPI_STATE_SCHED_THREADED, &napi->state) || woken) {\n\t\t\tWARN_ON(!list_empty(&napi->poll_list));\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn 0;\n\t\t}\n\n\t\tschedule();\n\t\t/* woken being true indicates this thread owns this napi. */\n\t\twoken = true;\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn -1;\n}\n\nstatic int napi_threaded_poll(void *data)\n{\n\tstruct napi_struct *napi = data;\n\tvoid *have;\n\n\twhile (!napi_thread_wait(napi)) {\n\t\tunsigned long last_qs = jiffies;\n\n\t\tfor (;;) {\n\t\t\tbool repoll = false;\n\n\t\t\tlocal_bh_disable();\n\n\t\t\thave = netpoll_poll_lock(napi);\n\t\t\t__napi_poll(napi, &repoll);\n\t\t\tnetpoll_poll_unlock(have);\n\n\t\t\tlocal_bh_enable();\n\n\t\t\tif (!repoll)\n\t\t\t\tbreak;\n\n\t\t\trcu_softirq_qs_periodic(last_qs);\n\t\t\tcond_resched();\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void skb_defer_free_flush(struct softnet_data *sd)\n{\n\tstruct sk_buff *skb, *next;\n\tunsigned long flags;\n\n\t/* Paired with WRITE_ONCE() in skb_attempt_defer_free() */\n\tif (!READ_ONCE(sd->defer_list))\n\t\treturn;\n\n\tspin_lock_irqsave(&sd->defer_lock, flags);\n\tskb = sd->defer_list;\n\tsd->defer_list = NULL;\n\tsd->defer_count = 0;\n\tspin_unlock_irqrestore(&sd->defer_lock, flags);\n\n\twhile (skb != NULL) {\n\t\tnext = skb->next;\n\t\tnapi_consume_skb(skb, 1);\n\t\tskb = next;\n\t}\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(READ_ONCE(netdev_budget_usecs));\n\tint budget = READ_ONCE(netdev_budget);\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tskb_defer_free_flush(sd);\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\tgoto end;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\nend:;\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\tnetdevice_tracker dev_tracker;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* lookup ignore flag */\n\tbool ignore;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int ____netdev_has_upper_dev(struct net_device *upper_dev,\n\t\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct net_device *dev = (struct net_device *)priv->data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t     &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all_rcu - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)upper_dev,\n\t};\n\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t       &priv);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\nstatic struct net_device *__netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master) && !upper->ignore)\n\t\treturn upper->dev;\n\treturn NULL;\n}\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *__netdev_next_upper_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\t*ignore = upper->ignore;\n\n\treturn upper->dev;\n}\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nstatic int __netdev_walk_all_upper_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = __netdev_next_upper_dev(now, &iter, &ignore);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *udev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.upper;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tudev = netdev_next_upper_dev_rcu(now, &iter);\n\t\t\tif (!udev)\n\t\t\t\tbreak;\n\n\t\t\tnext = udev;\n\t\t\tniter = &udev->adj_list.upper;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\nstatic bool __netdev_has_upper_dev(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = (void *)upper_dev,\n\t};\n\n\tASSERT_RTNL();\n\n\treturn __netdev_walk_all_upper_dev(dev, ____netdev_has_upper_dev,\n\t\t\t\t\t   &priv);\n}\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nstatic struct net_device *__netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct list_head **iter,\n\t\t\t\t\t\t  bool *ignore)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\t*ignore = lower->ignore;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic int __netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t\t       int (*fn)(struct net_device *dev,\n\t\t\t\t\t struct netdev_nested_priv *priv),\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\tbool ignore;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = __netdev_next_lower_dev(now, &iter, &ignore);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\t\t\tif (ignore)\n\t\t\t\tcontinue;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_next_lower_dev_rcu);\n\nstatic u8 __netdev_upper_depth(struct net_device *dev)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore);\n\t     udev;\n\t     udev = __netdev_next_upper_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < udev->upper_level)\n\t\t\tmax_depth = udev->upper_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic u8 __netdev_lower_depth(struct net_device *dev)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tu8 max_depth = 0;\n\tbool ignore;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore);\n\t     ldev;\n\t     ldev = __netdev_next_lower_dev(dev, &iter, &ignore)) {\n\t\tif (ignore)\n\t\t\tcontinue;\n\t\tif (max_depth < ldev->lower_level)\n\t\t\tmax_depth = ldev->lower_level;\n\t}\n\n\treturn max_depth;\n}\n\nstatic int __netdev_update_upper_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *__unused)\n{\n\tdev->upper_level = __netdev_upper_depth(dev) + 1;\n\treturn 0;\n}\n\n#ifdef CONFIG_LOCKDEP\nstatic LIST_HEAD(net_unlink_list);\n\nstatic void net_unlink_todo(struct net_device *dev)\n{\n\tif (list_empty(&dev->unlink_list))\n\t\tlist_add_tail(&dev->unlink_list, &net_unlink_list);\n}\n#endif\n\nstatic int __netdev_update_lower_level(struct net_device *dev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tdev->lower_level = __netdev_lower_depth(dev) + 1;\n\n#ifdef CONFIG_LOCKDEP\n\tif (!priv)\n\t\treturn 0;\n\n\tif (priv->flags & NESTED_SYNC_IMM)\n\t\tdev->nested_level = dev->lower_level - 1;\n\tif (priv->flags & NESTED_SYNC_TODO)\n\t\tnet_unlink_todo(dev);\n#endif\n\treturn 0;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv)\n{\n\tstruct net_device *ldev, *next, *now, *dev_stack[MAX_NEST_DEV + 1];\n\tstruct list_head *niter, *iter, *iter_stack[MAX_NEST_DEV + 1];\n\tint ret, cur = 0;\n\n\tnow = dev;\n\titer = &dev->adj_list.lower;\n\n\twhile (1) {\n\t\tif (now != dev) {\n\t\t\tret = fn(now, priv);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tnext = NULL;\n\t\twhile (1) {\n\t\t\tldev = netdev_next_lower_dev_rcu(now, &iter);\n\t\t\tif (!ldev)\n\t\t\t\tbreak;\n\n\t\t\tnext = ldev;\n\t\t\tniter = &ldev->adj_list.lower;\n\t\t\tdev_stack[cur] = now;\n\t\t\titer_stack[cur++] = iter;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!next) {\n\t\t\tif (!cur)\n\t\t\t\treturn 0;\n\t\t\tnext = dev_stack[--cur];\n\t\t\tniter = iter_stack[cur];\n\t\t}\n\n\t\tnow = next;\n\t\titer = niter;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tadj->ignore = false;\n\tnetdev_hold(adj_dev, &adj->dev_tracker, GFP_KERNEL);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree(adj);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tnetdev_put(adj_dev, &adj->dev_tracker);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info,\n\t\t\t\t   struct netdev_nested_priv *priv,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t\t.extack = extack,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.master = master,\n\t\t.linking = true,\n\t\t.upper_info = upper_info,\n\t};\n\tstruct net_device *master_dev;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (__netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif ((dev->lower_level + upper_dev->upper_level) > MAX_NEST_DEV)\n\t\treturn -EMLINK;\n\n\tif (!master) {\n\t\tif (__netdev_has_upper_dev(dev, upper_dev))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tmaster_dev = __netdev_master_upper_dev_get(dev);\n\t\tif (master_dev)\n\t\t\treturn master_dev == upper_dev ? -EEXIST : -EBUSY;\n\t}\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, false,\n\t\t\t\t       NULL, NULL, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n * @extack: netlink extended ack\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info, &priv, extack);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\nstatic void __netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t\t      struct net_device *upper_dev,\n\t\t\t\t      struct netdev_nested_priv *priv)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info = {\n\t\t.info = {\n\t\t\t.dev = dev,\n\t\t},\n\t\t.upper_dev = upper_dev,\n\t\t.linking = false,\n\t};\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_update_upper_level(dev, NULL);\n\t__netdev_walk_all_lower_dev(dev, __netdev_update_upper_level, NULL);\n\n\t__netdev_update_lower_level(upper_dev, priv);\n\t__netdev_walk_all_upper_dev(upper_dev, __netdev_update_lower_level,\n\t\t\t\t    priv);\n}\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\t__netdev_upper_dev_unlink(dev, upper_dev, &priv);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\nstatic void __netdev_adjacent_dev_set(struct net_device *upper_dev,\n\t\t\t\t      struct net_device *lower_dev,\n\t\t\t\t      bool val)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = __netdev_find_adj(lower_dev, &upper_dev->adj_list.lower);\n\tif (adj)\n\t\tadj->ignore = val;\n\n\tadj = __netdev_find_adj(upper_dev, &lower_dev->adj_list.upper);\n\tif (adj)\n\t\tadj->ignore = val;\n}\n\nstatic void netdev_adjacent_dev_disable(struct net_device *upper_dev,\n\t\t\t\t\tstruct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, true);\n}\n\nstatic void netdev_adjacent_dev_enable(struct net_device *upper_dev,\n\t\t\t\t       struct net_device *lower_dev)\n{\n\t__netdev_adjacent_dev_set(upper_dev, lower_dev, false);\n}\n\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\tint err;\n\n\tif (!new_dev)\n\t\treturn 0;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_disable(dev, old_dev);\n\terr = __netdev_upper_dev_link(new_dev, dev, false, NULL, NULL, &priv,\n\t\t\t\t      extack);\n\tif (err) {\n\t\tif (old_dev && new_dev != old_dev)\n\t\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_adjacent_change_prepare);\n\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = NESTED_SYNC_IMM | NESTED_SYNC_TODO,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev || !old_dev)\n\t\treturn;\n\n\tif (new_dev == old_dev)\n\t\treturn;\n\n\tnetdev_adjacent_dev_enable(dev, old_dev);\n\t__netdev_upper_dev_unlink(old_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_commit);\n\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.flags = 0,\n\t\t.data = NULL,\n\t};\n\n\tif (!new_dev)\n\t\treturn;\n\n\tif (old_dev && new_dev != old_dev)\n\t\tnetdev_adjacent_dev_enable(dev, old_dev);\n\n\t__netdev_upper_dev_unlink(new_dev, dev, &priv);\n}\nEXPORT_SYMBOL(netdev_adjacent_change_abort);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info info = {\n\t\t.info.dev = dev,\n\t};\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic int netdev_offload_xstats_enable_l3(struct net_device *dev,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\tint err;\n\tint rc;\n\n\tdev->offload_xstats_l3 = kzalloc(sizeof(*dev->offload_xstats_l3),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!dev->offload_xstats_l3)\n\t\treturn -ENOMEM;\n\n\trc = call_netdevice_notifiers_info_robust(NETDEV_OFFLOAD_XSTATS_ENABLE,\n\t\t\t\t\t\t  NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t\t\t  &info.info);\n\terr = notifier_to_errno(rc);\n\tif (err)\n\t\tgoto free_stats;\n\n\treturn 0;\n\nfree_stats:\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n\treturn err;\n}\n\nint netdev_offload_xstats_enable(struct net_device *dev,\n\t\t\t\t enum netdev_offload_xstats_type type,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn netdev_offload_xstats_enable_l3(dev, extack);\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enable);\n\nstatic void netdev_offload_xstats_disable_l3(struct net_device *dev)\n{\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.type = NETDEV_OFFLOAD_XSTATS_TYPE_L3,\n\t};\n\n\tcall_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_DISABLE,\n\t\t\t\t      &info.info);\n\tkfree(dev->offload_xstats_l3);\n\tdev->offload_xstats_l3 = NULL;\n}\n\nint netdev_offload_xstats_disable(struct net_device *dev,\n\t\t\t\t  enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\tif (!netdev_offload_xstats_enabled(dev, type))\n\t\treturn -EALREADY;\n\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\tnetdev_offload_xstats_disable_l3(dev);\n\t\treturn 0;\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_disable);\n\nstatic void netdev_offload_xstats_disable_all(struct net_device *dev)\n{\n\tnetdev_offload_xstats_disable(dev, NETDEV_OFFLOAD_XSTATS_TYPE_L3);\n}\n\nstatic struct rtnl_hw_stats64 *\nnetdev_offload_xstats_get_ptr(const struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type)\n{\n\tswitch (type) {\n\tcase NETDEV_OFFLOAD_XSTATS_TYPE_L3:\n\t\treturn dev->offload_xstats_l3;\n\t}\n\n\tWARN_ON(1);\n\treturn NULL;\n}\n\nbool netdev_offload_xstats_enabled(const struct net_device *dev,\n\t\t\t\t   enum netdev_offload_xstats_type type)\n{\n\tASSERT_RTNL();\n\n\treturn netdev_offload_xstats_get_ptr(dev, type);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_enabled);\n\nstruct netdev_notifier_offload_xstats_ru {\n\tbool used;\n};\n\nstruct netdev_notifier_offload_xstats_rd {\n\tstruct rtnl_hw_stats64 stats;\n\tbool used;\n};\n\nstatic void netdev_hw_stats64_add(struct rtnl_hw_stats64 *dest,\n\t\t\t\t  const struct rtnl_hw_stats64 *src)\n{\n\tdest->rx_packets\t  += src->rx_packets;\n\tdest->tx_packets\t  += src->tx_packets;\n\tdest->rx_bytes\t\t  += src->rx_bytes;\n\tdest->tx_bytes\t\t  += src->tx_bytes;\n\tdest->rx_errors\t\t  += src->rx_errors;\n\tdest->tx_errors\t\t  += src->tx_errors;\n\tdest->rx_dropped\t  += src->rx_dropped;\n\tdest->tx_dropped\t  += src->tx_dropped;\n\tdest->multicast\t\t  += src->multicast;\n}\n\nstatic int netdev_offload_xstats_get_used(struct net_device *dev,\n\t\t\t\t\t  enum netdev_offload_xstats_type type,\n\t\t\t\t\t  bool *p_used,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_ru report_used = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_used = &report_used,\n\t};\n\tint rc;\n\n\tWARN_ON(!netdev_offload_xstats_enabled(dev, type));\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_USED,\n\t\t\t\t\t   &info.info);\n\t*p_used = report_used.used;\n\treturn notifier_to_errno(rc);\n}\n\nstatic int netdev_offload_xstats_get_stats(struct net_device *dev,\n\t\t\t\t\t   enum netdev_offload_xstats_type type,\n\t\t\t\t\t   struct rtnl_hw_stats64 *p_stats,\n\t\t\t\t\t   bool *p_used,\n\t\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_offload_xstats_rd report_delta = {};\n\tstruct netdev_notifier_offload_xstats_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.type = type,\n\t\t.report_delta = &report_delta,\n\t};\n\tstruct rtnl_hw_stats64 *stats;\n\tint rc;\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn -EINVAL;\n\n\trc = call_netdevice_notifiers_info(NETDEV_OFFLOAD_XSTATS_REPORT_DELTA,\n\t\t\t\t\t   &info.info);\n\n\t/* Cache whatever we got, even if there was an error, otherwise the\n\t * successful stats retrievals would get lost.\n\t */\n\tnetdev_hw_stats64_add(stats, &report_delta.stats);\n\n\tif (p_stats)\n\t\t*p_stats = *stats;\n\t*p_used = report_delta.used;\n\n\treturn notifier_to_errno(rc);\n}\n\nint netdev_offload_xstats_get(struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t      struct rtnl_hw_stats64 *p_stats, bool *p_used,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tASSERT_RTNL();\n\n\tif (p_stats)\n\t\treturn netdev_offload_xstats_get_stats(dev, type, p_stats,\n\t\t\t\t\t\t       p_used, extack);\n\telse\n\t\treturn netdev_offload_xstats_get_used(dev, type, p_used,\n\t\t\t\t\t\t      extack);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_get);\n\nvoid\nnetdev_offload_xstats_report_delta(struct netdev_notifier_offload_xstats_rd *report_delta,\n\t\t\t\t   const struct rtnl_hw_stats64 *stats)\n{\n\treport_delta->used = true;\n\tnetdev_hw_stats64_add(&report_delta->stats, stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_delta);\n\nvoid\nnetdev_offload_xstats_report_used(struct netdev_notifier_offload_xstats_ru *report_used)\n{\n\treport_used->used = true;\n}\nEXPORT_SYMBOL(netdev_offload_xstats_report_used);\n\nvoid netdev_offload_xstats_push_delta(struct net_device *dev,\n\t\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t\t      const struct rtnl_hw_stats64 *p_stats)\n{\n\tstruct rtnl_hw_stats64 *stats;\n\n\tASSERT_RTNL();\n\n\tstats = netdev_offload_xstats_get_ptr(dev, type);\n\tif (WARN_ON(!stats))\n\t\treturn;\n\n\tnetdev_hw_stats64_add(stats, p_stats);\n}\nEXPORT_SYMBOL(netdev_offload_xstats_push_delta);\n\n/**\n * netdev_get_xmit_slave - Get the xmit slave of master device\n * @dev: device\n * @skb: The packet\n * @all_slaves: assume all the slaves are active\n *\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n * %NULL is returned if no slave is found.\n */\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_xmit_slave)\n\t\treturn NULL;\n\treturn ops->ndo_get_xmit_slave(dev, skb, all_slaves);\n}\nEXPORT_SYMBOL(netdev_get_xmit_slave);\n\nstatic struct net_device *netdev_sk_get_lower_dev(struct net_device *dev,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_sk_get_lower_dev)\n\t\treturn NULL;\n\treturn ops->ndo_sk_get_lower_dev(dev, sk);\n}\n\n/**\n * netdev_sk_get_lowest_dev - Get the lowest device in chain given device and socket\n * @dev: device\n * @sk: the socket\n *\n * %NULL is returned if no lower device is found.\n */\n\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct net_device *lower;\n\n\tlower = netdev_sk_get_lower_dev(dev, sk);\n\twhile (lower) {\n\t\tdev = lower;\n\t\tlower = netdev_sk_get_lower_dev(dev, sk);\n\t}\n\n\treturn dev;\n}\nEXPORT_SYMBOL(netdev_sk_get_lowest_dev);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\n/**\n * netdev_lower_state_changed - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info = {\n\t\t.info.dev = lower_dev,\n\t};\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tnetdev_warn(dev, \"promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(audit_context(), GFP_ATOMIC,\n\t\t\t\t  AUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t  \"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\t  dev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t  (old_flags & IFF_PROMISC),\n\t\t\t\t  from_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\t  from_kuid(&init_user_ns, uid),\n\t\t\t\t  from_kgid(&init_user_ns, gid),\n\t\t\t\t  audit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tnetdev_warn(dev, \"allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\");\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev, extack);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info = {\n\t\t\t.info = {\n\t\t\t\t.dev = dev,\n\t\t\t},\n\t\t\t.flags_changed = changes,\n\t\t};\n\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\t@extack: netlink extended ack\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags, extack);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\t/* Pairs with all the lockless reads of dev->mtu in the stack */\n\tWRITE_ONCE(dev->mtu, new_mtu);\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\nint dev_validate_mtu(struct net_device *dev, int new_mtu,\n\t\t     struct netlink_ext_ack *extack)\n{\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu less than device minimum\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tNL_SET_ERR_MSG(extack, \"mtu greater than device maximum\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_mtu_ext - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\t@extack: netlink extended ack\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu_ext(struct net_device *dev, int new_mtu,\n\t\t    struct netlink_ext_ack *extack)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\terr = dev_validate_mtu(dev, new_mtu, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t   orig_mtu);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers_mtu(NETDEV_CHANGEMTU, dev,\n\t\t\t\t\t\t     new_mtu);\n\t\t}\n\t}\n\treturn err;\n}\n\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct netlink_ext_ack extack;\n\tint err;\n\n\tmemset(&extack, 0, sizeof(extack));\n\terr = dev_set_mtu_ext(dev, new_mtu, &extack);\n\tif (err && extack._msg)\n\t\tnet_err_ratelimited(\"%s: %s\\n\", dev->name, extack._msg);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_change_tx_queue_len - Change TX queue length of a netdevice\n *\t@dev: device\n *\t@new_len: new tx queue length\n */\nint dev_change_tx_queue_len(struct net_device *dev, unsigned long new_len)\n{\n\tunsigned int orig_len = dev->tx_queue_len;\n\tint res;\n\n\tif (new_len != (unsigned int)new_len)\n\t\treturn -ERANGE;\n\n\tif (new_len != orig_len) {\n\t\tdev->tx_queue_len = new_len;\n\t\tres = call_netdevice_notifiers(NETDEV_CHANGE_TX_QUEUE_LEN, dev);\n\t\tres = notifier_to_errno(res);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t\tres = dev_qdisc_change_tx_queue_len(dev);\n\t\tif (res)\n\t\t\tgoto err_rollback;\n\t}\n\n\treturn 0;\n\nerr_rollback:\n\tnetdev_err(dev, \"refused to change device tx_queue_len\\n\");\n\tdev->tx_queue_len = orig_len;\n\treturn res;\n}\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\n\n/**\n *\tdev_pre_changeaddr_notify - Call NETDEV_PRE_CHANGEADDR.\n *\t@dev: device\n *\t@addr: new address\n *\t@extack: netlink extended ack\n */\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct netdev_notifier_pre_changeaddr_info info = {\n\t\t.info.dev = dev,\n\t\t.info.extack = extack,\n\t\t.dev_addr = addr,\n\t};\n\tint rc;\n\n\trc = call_netdevice_notifiers_info(NETDEV_PRE_CHANGEADDR, &info.info);\n\treturn notifier_to_errno(rc);\n}\nEXPORT_SYMBOL(dev_pre_changeaddr_notify);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\t@extack: netlink extended ack\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = dev_pre_changeaddr_notify(dev, sa->sa_data, extack);\n\tif (err)\n\t\treturn err;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\nstatic DECLARE_RWSEM(dev_addr_sem);\n\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint ret;\n\n\tdown_write(&dev_addr_sem);\n\tret = dev_set_mac_address(dev, sa, extack);\n\tup_write(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_set_mac_address_user);\n\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name)\n{\n\tsize_t size = sizeof(sa->sa_data_min);\n\tstruct net_device *dev;\n\tint ret = 0;\n\n\tdown_read(&dev_addr_sem);\n\trcu_read_lock();\n\n\tdev = dev_get_by_name_rcu(net, dev_name);\n\tif (!dev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tif (!dev->addr_len)\n\t\tmemset(sa->sa_data, 0, size);\n\telse\n\t\tmemcpy(sa->sa_data, dev->dev_addr,\n\t\t       min_t(size_t, size, dev->addr_len));\n\tsa->sa_family = dev->type;\n\nunlock:\n\trcu_read_unlock();\n\tup_read(&dev_addr_sem);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_get_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (ops->ndo_get_phys_port_name) {\n\t\terr = ops->ndo_get_phys_port_name(dev, name, len);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\treturn devlink_compat_phys_port_name_get(dev, name, len);\n}\n\n/**\n *\tdev_get_port_parent_id - Get the device's port parent identifier\n *\t@dev: network device\n *\t@ppid: pointer to a storage for the port's parent identifier\n *\t@recurse: allow/disallow recursion to lower devices\n *\n *\tGet the devices's port parent identifier\n */\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid,\n\t\t\t   bool recurse)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct netdev_phys_item_id first = { };\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\tint err;\n\n\tif (ops->ndo_get_port_parent_id) {\n\t\terr = ops->ndo_get_port_parent_id(dev, ppid);\n\t\tif (err != -EOPNOTSUPP)\n\t\t\treturn err;\n\t}\n\n\terr = devlink_compat_switch_id_get(dev, ppid);\n\tif (!recurse || err != -EOPNOTSUPP)\n\t\treturn err;\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter) {\n\t\terr = dev_get_port_parent_id(lower_dev, ppid, true);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!first.id_len)\n\t\t\tfirst = *ppid;\n\t\telse if (memcmp(&first, ppid, sizeof(*ppid)))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(dev_get_port_parent_id);\n\n/**\n *\tnetdev_port_same_parent_id - Indicate if two network devices have\n *\tthe same port parent identifier\n *\t@a: first network device\n *\t@b: second network device\n */\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b)\n{\n\tstruct netdev_phys_item_id a_id = { };\n\tstruct netdev_phys_item_id b_id = { };\n\n\tif (dev_get_port_parent_id(a, &a_id, true) ||\n\t    dev_get_port_parent_id(b, &b_id, true))\n\t\treturn false;\n\n\treturn netdev_phys_item_id_same(&a_id, &b_id);\n}\nEXPORT_SYMBOL(netdev_port_same_parent_id);\n\n/**\n *\tdev_change_proto_down - set carrier according to proto_down.\n *\n *\t@dev: device\n *\t@proto_down: new value\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tif (!(dev->priv_flags & IFF_CHANGE_PROTO_DOWN))\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\tif (proto_down)\n\t\tnetif_carrier_off(dev);\n\telse\n\t\tnetif_carrier_on(dev);\n\tdev->proto_down = proto_down;\n\treturn 0;\n}\n\n/**\n *\tdev_change_proto_down_reason - proto down reason\n *\n *\t@dev: device\n *\t@mask: proto down mask\n *\t@value: proto down value\n */\nvoid dev_change_proto_down_reason(struct net_device *dev, unsigned long mask,\n\t\t\t\t  u32 value)\n{\n\tint b;\n\n\tif (!mask) {\n\t\tdev->proto_down_reason = value;\n\t} else {\n\t\tfor_each_set_bit(b, &mask, 32) {\n\t\t\tif (value & (1 << b))\n\t\t\t\tdev->proto_down_reason |= BIT(b);\n\t\t\telse\n\t\t\t\tdev->proto_down_reason &= ~BIT(b);\n\t\t}\n\t}\n}\n\nstruct bpf_xdp_link {\n\tstruct bpf_link link;\n\tstruct net_device *dev; /* protected by rtnl_lock, no refcnt held */\n\tint flags;\n};\n\nstatic enum bpf_xdp_mode dev_xdp_mode(struct net_device *dev, u32 flags)\n{\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\treturn XDP_MODE_HW;\n\tif (flags & XDP_FLAGS_DRV_MODE)\n\t\treturn XDP_MODE_DRV;\n\tif (flags & XDP_FLAGS_SKB_MODE)\n\t\treturn XDP_MODE_SKB;\n\treturn dev->netdev_ops->ndo_bpf ? XDP_MODE_DRV : XDP_MODE_SKB;\n}\n\nstatic bpf_op_t dev_xdp_bpf_op(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tswitch (mode) {\n\tcase XDP_MODE_SKB:\n\t\treturn generic_xdp_install;\n\tcase XDP_MODE_DRV:\n\tcase XDP_MODE_HW:\n\t\treturn dev->netdev_ops->ndo_bpf;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bpf_xdp_link *dev_xdp_link(struct net_device *dev,\n\t\t\t\t\t enum bpf_xdp_mode mode)\n{\n\treturn dev->xdp_state[mode].link;\n}\n\nstatic struct bpf_prog *dev_xdp_prog(struct net_device *dev,\n\t\t\t\t     enum bpf_xdp_mode mode)\n{\n\tstruct bpf_xdp_link *link = dev_xdp_link(dev, mode);\n\n\tif (link)\n\t\treturn link->link.prog;\n\treturn dev->xdp_state[mode].prog;\n}\n\nu8 dev_xdp_prog_count(struct net_device *dev)\n{\n\tu8 count = 0;\n\tint i;\n\n\tfor (i = 0; i < __MAX_XDP_MODE; i++)\n\t\tif (dev->xdp_state[i].prog || dev->xdp_state[i].link)\n\t\t\tcount++;\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(dev_xdp_prog_count);\n\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode)\n{\n\tstruct bpf_prog *prog = dev_xdp_prog(dev, mode);\n\n\treturn prog ? prog->aux->id : 0;\n}\n\nstatic void dev_xdp_set_link(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_xdp_link *link)\n{\n\tdev->xdp_state[mode].link = link;\n\tdev->xdp_state[mode].prog = NULL;\n}\n\nstatic void dev_xdp_set_prog(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t     struct bpf_prog *prog)\n{\n\tdev->xdp_state[mode].link = NULL;\n\tdev->xdp_state[mode].prog = prog;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, enum bpf_xdp_mode mode,\n\t\t\t   bpf_op_t bpf_op, struct netlink_ext_ack *extack,\n\t\t\t   u32 flags, struct bpf_prog *prog)\n{\n\tstruct netdev_bpf xdp;\n\tint err;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = mode == XDP_MODE_HW ? XDP_SETUP_PROG_HW : XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\t/* Drivers assume refcnt is already incremented (i.e, prog pointer is\n\t * \"moved\" into driver), so they don't increment it on their own, but\n\t * they do decrement refcnt when program is detached or replaced.\n\t * Given net_device also owns link/prog, we need to bump refcnt here\n\t * to prevent drivers from underflowing it.\n\t */\n\tif (prog)\n\t\tbpf_prog_inc(prog);\n\terr = bpf_op(dev, &xdp);\n\tif (err) {\n\t\tif (prog)\n\t\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\tif (mode != XDP_MODE_HW)\n\t\tbpf_prog_change_xdp(dev_xdp_prog(dev, mode), prog);\n\n\treturn 0;\n}\n\nstatic void dev_xdp_uninstall(struct net_device *dev)\n{\n\tstruct bpf_xdp_link *link;\n\tstruct bpf_prog *prog;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tfor (mode = XDP_MODE_SKB; mode < __MAX_XDP_MODE; mode++) {\n\t\tprog = dev_xdp_prog(dev, mode);\n\t\tif (!prog)\n\t\t\tcontinue;\n\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op)\n\t\t\tcontinue;\n\n\t\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\n\t\t/* auto-detach link from net device */\n\t\tlink = dev_xdp_link(dev, mode);\n\t\tif (link)\n\t\t\tlink->dev = NULL;\n\t\telse\n\t\t\tbpf_prog_put(prog);\n\n\t\tdev_xdp_set_link(dev, mode, NULL);\n\t}\n}\n\nstatic int dev_xdp_attach(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t\t  struct bpf_xdp_link *link, struct bpf_prog *new_prog,\n\t\t\t  struct bpf_prog *old_prog, u32 flags)\n{\n\tunsigned int num_modes = hweight32(flags & XDP_FLAGS_MODES);\n\tstruct bpf_prog *cur_prog;\n\tstruct net_device *upper;\n\tstruct list_head *iter;\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* either link or prog attachment, never both */\n\tif (link && (new_prog || old_prog))\n\t\treturn -EINVAL;\n\t/* link supports only XDP mode flags */\n\tif (link && (flags & ~XDP_FLAGS_MODES)) {\n\t\tNL_SET_ERR_MSG(extack, \"Invalid XDP flags for BPF link attachment\");\n\t\treturn -EINVAL;\n\t}\n\t/* just one XDP mode bit should be set, zero defaults to drv/skb mode */\n\tif (num_modes > 1) {\n\t\tNL_SET_ERR_MSG(extack, \"Only one XDP mode flag can be set\");\n\t\treturn -EINVAL;\n\t}\n\t/* avoid ambiguity if offload + drv/skb mode progs are both loaded */\n\tif (!num_modes && dev_xdp_prog_count(dev) > 1) {\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"More than one program loaded, unset mode is ambiguous\");\n\t\treturn -EINVAL;\n\t}\n\t/* old_prog != NULL implies XDP_FLAGS_REPLACE is set */\n\tif (old_prog && !(flags & XDP_FLAGS_REPLACE)) {\n\t\tNL_SET_ERR_MSG(extack, \"XDP_FLAGS_REPLACE is not specified\");\n\t\treturn -EINVAL;\n\t}\n\n\tmode = dev_xdp_mode(dev, flags);\n\t/* can't replace attached link */\n\tif (dev_xdp_link(dev, mode)) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active BPF XDP link\");\n\t\treturn -EBUSY;\n\t}\n\n\t/* don't allow if an upper device already has a program */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter) {\n\t\tif (dev_xdp_prog_count(upper) > 0) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Cannot attach when an upper device already has a program\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\tcur_prog = dev_xdp_prog(dev, mode);\n\t/* can't replace attached prog with link */\n\tif (link && cur_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Can't replace active XDP program with BPF link\");\n\t\treturn -EBUSY;\n\t}\n\tif ((flags & XDP_FLAGS_REPLACE) && cur_prog != old_prog) {\n\t\tNL_SET_ERR_MSG(extack, \"Active program does not match expected\");\n\t\treturn -EEXIST;\n\t}\n\n\t/* put effective new program into new_prog */\n\tif (link)\n\t\tnew_prog = link->link.prog;\n\n\tif (new_prog) {\n\t\tbool offload = mode == XDP_MODE_HW;\n\t\tenum bpf_xdp_mode other_mode = mode == XDP_MODE_SKB\n\t\t\t\t\t       ? XDP_MODE_DRV : XDP_MODE_SKB;\n\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) && cur_prog) {\n\t\t\tNL_SET_ERR_MSG(extack, \"XDP program already attached\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tif (!offload && dev_xdp_prog(dev, other_mode)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Native and generic XDP can't be active at the same time\");\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tif (!offload && bpf_prog_is_dev_bound(new_prog->aux)) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Using device-bound program without HW_MODE flag is not supported\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_DEVMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_DEVMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (new_prog->expected_attach_type == BPF_XDP_CPUMAP) {\n\t\t\tNL_SET_ERR_MSG(extack, \"BPF_XDP_CPUMAP programs can not be attached to a device\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* don't call drivers if the effective program didn't change */\n\tif (new_prog != cur_prog) {\n\t\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\t\tif (!bpf_op) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Underlying driver does not support XDP in native mode\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\terr = dev_xdp_install(dev, mode, bpf_op, extack, flags, new_prog);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (link)\n\t\tdev_xdp_set_link(dev, mode, link);\n\telse\n\t\tdev_xdp_set_prog(dev, mode, new_prog);\n\tif (cur_prog)\n\t\tbpf_prog_put(cur_prog);\n\n\treturn 0;\n}\n\nstatic int dev_xdp_attach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\treturn dev_xdp_attach(dev, extack, link, NULL, NULL, link->flags);\n}\n\nstatic int dev_xdp_detach_link(struct net_device *dev,\n\t\t\t       struct netlink_ext_ack *extack,\n\t\t\t       struct bpf_xdp_link *link)\n{\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\n\tASSERT_RTNL();\n\n\tmode = dev_xdp_mode(dev, link->flags);\n\tif (dev_xdp_link(dev, mode) != link)\n\t\treturn -EINVAL;\n\n\tbpf_op = dev_xdp_bpf_op(dev, mode);\n\tWARN_ON(dev_xdp_install(dev, mode, bpf_op, NULL, 0, NULL));\n\tdev_xdp_set_link(dev, mode, NULL);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\trtnl_lock();\n\n\t/* if racing with net_device's tear down, xdp_link->dev might be\n\t * already NULL, in which case link was already auto-detached\n\t */\n\tif (xdp_link->dev) {\n\t\tWARN_ON(dev_xdp_detach_link(xdp_link->dev, NULL, xdp_link));\n\t\txdp_link->dev = NULL;\n\t}\n\n\trtnl_unlock();\n}\n\nstatic int bpf_xdp_link_detach(struct bpf_link *link)\n{\n\tbpf_xdp_link_release(link);\n\treturn 0;\n}\n\nstatic void bpf_xdp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\n\tkfree(xdp_link);\n}\n\nstatic void bpf_xdp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t     struct seq_file *seq)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tseq_printf(seq, \"ifindex:\\t%u\\n\", ifindex);\n}\n\nstatic int bpf_xdp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t       struct bpf_link_info *info)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tu32 ifindex = 0;\n\n\trtnl_lock();\n\tif (xdp_link->dev)\n\t\tifindex = xdp_link->dev->ifindex;\n\trtnl_unlock();\n\n\tinfo->xdp.ifindex = ifindex;\n\treturn 0;\n}\n\nstatic int bpf_xdp_link_update(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t       struct bpf_prog *old_prog)\n{\n\tstruct bpf_xdp_link *xdp_link = container_of(link, struct bpf_xdp_link, link);\n\tenum bpf_xdp_mode mode;\n\tbpf_op_t bpf_op;\n\tint err = 0;\n\n\trtnl_lock();\n\n\t/* link might have been auto-released already, so fail */\n\tif (!xdp_link->dev) {\n\t\terr = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog && link->prog != old_prog) {\n\t\terr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\told_prog = link->prog;\n\tif (old_prog->type != new_prog->type ||\n\t    old_prog->expected_attach_type != new_prog->expected_attach_type) {\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (old_prog == new_prog) {\n\t\t/* no-op, don't disturb drivers */\n\t\tbpf_prog_put(new_prog);\n\t\tgoto out_unlock;\n\t}\n\n\tmode = dev_xdp_mode(xdp_link->dev, xdp_link->flags);\n\tbpf_op = dev_xdp_bpf_op(xdp_link->dev, mode);\n\terr = dev_xdp_install(xdp_link->dev, mode, bpf_op, NULL,\n\t\t\t      xdp_link->flags, new_prog);\n\tif (err)\n\t\tgoto out_unlock;\n\n\told_prog = xchg(&link->prog, new_prog);\n\tbpf_prog_put(old_prog);\n\nout_unlock:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_xdp_link_lops = {\n\t.release = bpf_xdp_link_release,\n\t.dealloc = bpf_xdp_link_dealloc,\n\t.detach = bpf_xdp_link_detach,\n\t.show_fdinfo = bpf_xdp_link_show_fdinfo,\n\t.fill_link_info = bpf_xdp_link_fill_link_info,\n\t.update_prog = bpf_xdp_link_update,\n};\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_xdp_link *link;\n\tstruct net_device *dev;\n\tint err, fd;\n\n\trtnl_lock();\n\tdev = dev_get_by_index(net, attr->link_create.target_ifindex);\n\tif (!dev) {\n\t\trtnl_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_XDP, &bpf_xdp_link_lops, prog);\n\tlink->dev = dev;\n\tlink->flags = attr->link_create.flags;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto unlock;\n\t}\n\n\terr = dev_xdp_attach_link(dev, NULL, link);\n\trtnl_unlock();\n\n\tif (err) {\n\t\tlink->dev = NULL;\n\t\tbpf_link_cleanup(&link_primer);\n\t\tgoto out_put_dev;\n\t}\n\n\tfd = bpf_link_settle(&link_primer);\n\t/* link itself doesn't hold dev's refcnt to not complicate shutdown */\n\tdev_put(dev);\n\treturn fd;\n\nunlock:\n\trtnl_unlock();\n\nout_put_dev:\n\tdev_put(dev);\n\treturn err;\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@expected_fd: old program fd that userspace expects to replace or clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, int expected_fd, u32 flags)\n{\n\tenum bpf_xdp_mode mode = dev_xdp_mode(dev, flags);\n\tstruct bpf_prog *new_prog = NULL, *old_prog = NULL;\n\tint err;\n\n\tASSERT_RTNL();\n\n\tif (fd >= 0) {\n\t\tnew_prog = bpf_prog_get_type_dev(fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(new_prog))\n\t\t\treturn PTR_ERR(new_prog);\n\t}\n\n\tif (expected_fd >= 0) {\n\t\told_prog = bpf_prog_get_type_dev(expected_fd, BPF_PROG_TYPE_XDP,\n\t\t\t\t\t\t mode != XDP_MODE_SKB);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\terr = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\terr = dev_xdp_attach(dev, extack, NULL, new_prog, old_prog, flags);\n\nerr_out:\n\tif (err && new_prog)\n\t\tbpf_prog_put(new_prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\treturn err;\n}\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nLIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tatomic_inc(&dev_net(dev)->dev_unreg_count);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\t__netdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t\telse\n\t\t\t\tnetdev_features_change(lower);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\tif (!(features & NETIF_F_RXCSUM)) {\n\t\t/* NETIF_F_GRO_HW implies doing RXCSUM since every packet\n\t\t * successfully merged by hardware must also have the\n\t\t * checksum verified by hardware.  If the user does not\n\t\t * want to enable RXCSUM, logically, we should disable GRO_HW.\n\t\t */\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GRO_HW since no RXCSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\t/* LRO/HW-GRO features cannot be combined with RX-FCS */\n\tif (features & NETIF_F_RXFCS) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_dbg(dev, \"Dropping LRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_dbg(dev, \"Dropping HW-GRO feature since RX-FCS is requested.\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_GRO_HW) && (features & NETIF_F_LRO)) {\n\t\tnetdev_dbg(dev, \"Dropping LRO feature since HW-GRO is requested.\\n\");\n\t\tfeatures &= ~NETIF_F_LRO;\n\t}\n\n\tif (features & NETIF_F_HW_TLS_TX) {\n\t\tbool ip_csum = (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) ==\n\t\t\t(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\tbool hw_csum = features & NETIF_F_HW_CSUM;\n\n\t\tif (!ip_csum && !hw_csum) {\n\t\t\tnetdev_dbg(dev, \"Dropping TLS TX HW offload feature since no CSUM feature.\\n\");\n\t\t\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\t\t}\n\t}\n\n\tif ((features & NETIF_F_HW_TLS_RX) && !(features & NETIF_F_RXCSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TLS RX HW offload feature since no RXCSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off on an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_ctag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_ctag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tif (diff & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\tif (features & NETIF_F_HW_VLAN_STAG_FILTER) {\n\t\t\t\tdev->features = features;\n\t\t\t\terr |= vlan_get_rx_stag_filter_info(dev);\n\t\t\t} else {\n\t\t\t\tvlan_drop_rx_stag_filter_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (rootdev->operstate == IF_OPER_TESTING)\n\t\tnetif_testing_on(dev);\n\telse\n\t\tnetif_testing_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\tint err = 0;\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++) {\n\t\trx[i].dev = dev;\n\n\t\t/* XDP RX-queue setup */\n\t\terr = xdp_rxq_info_reg(&rx[i].xdp_rxq, dev, i, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_info;\n\t}\n\treturn 0;\n\nerr_rxq_info:\n\t/* Rollback successful reg's and free other resources */\n\twhile (i--)\n\t\txdp_rxq_info_unreg(&rx[i].xdp_rxq);\n\tkvfree(dev->_rx);\n\tdev->_rx = NULL;\n\treturn err;\n}\n\nstatic void netif_free_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\n\t/* netif_alloc_rx_queues alloc failed, resources have been unreg'ed */\n\tif (!dev->_rx)\n\t\treturn;\n\n\tfor (i = 0; i < count; i++)\n\t\txdp_rxq_info_unreg(&dev->_rx[i].xdp_rxq);\n\n\tkvfree(dev->_rx);\n}\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\nstatic int netdev_do_alloc_pcpu_stats(struct net_device *dev)\n{\n\tvoid __percpu *v;\n\n\t/* Drivers implementing ndo_get_peer_dev must support tstat\n\t * accounting, so that skb_do_redirect() can bump the dev's\n\t * RX stats upon network namespace switch.\n\t */\n\tif (dev->netdev_ops->ndo_get_peer_dev &&\n\t    dev->pcpu_stat_type != NETDEV_PCPU_STAT_TSTATS)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn 0;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tv = dev->lstats = netdev_alloc_pcpu_stats(struct pcpu_lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tv = dev->tstats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tv = dev->dstats = netdev_alloc_pcpu_stats(struct pcpu_dstats);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn v ? 0 : -ENOMEM;\n}\n\nstatic void netdev_do_free_pcpu_stats(struct net_device *dev)\n{\n\tswitch (dev->pcpu_stat_type) {\n\tcase NETDEV_PCPU_STAT_NONE:\n\t\treturn;\n\tcase NETDEV_PCPU_STAT_LSTATS:\n\t\tfree_percpu(dev->lstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_TSTATS:\n\t\tfree_percpu(dev->tstats);\n\t\tbreak;\n\tcase NETDEV_PCPU_STAT_DSTATS:\n\t\tfree_percpu(dev->dstats);\n\t\tbreak;\n\t}\n}\n\n/**\n * register_netdevice() - register a network device\n * @dev: device to register\n *\n * Take a prepared network device structure and make it externally accessible.\n * A %NETDEV_REGISTER message is sent to the netdev notifier chain.\n * Callers must hold the rtnl lock - you may want register_netdev()\n * instead of this.\n */\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUILD_BUG_ON(sizeof(netdev_features_t) * BITS_PER_BYTE <\n\t\t     NETDEV_FEATURE_COUNT);\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tret = ethtool_check_ops(dev->ethtool_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = -ENOMEM;\n\tdev->name_node = netdev_name_node_head_alloc(dev);\n\tif (!dev->name_node)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto err_free_name;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = netdev_do_alloc_pcpu_stats(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_free_pcpu;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= (NETIF_F_SOFT_FEATURES | NETIF_F_SOFT_FEATURES_OFF);\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->udp_tunnel_nic_info) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_free_pcpu;\n\n\tret = netdev_register_kobject(dev);\n\twrite_lock(&dev_base_lock);\n\tdev->reg_state = ret ? NETREG_UNREGISTERED : NETREG_REGISTERED;\n\twrite_unlock(&dev_base_lock);\n\tif (ret)\n\t\tgoto err_free_pcpu;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\n\tnetdev_hold(dev, &dev->dev_registered_tracker, GFP_KERNEL);\n\tlist_netdevice(dev);\n\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\t/* Expect explicit free_netdev() on failure */\n\t\tdev->needs_free_netdev = false;\n\t\tunregister_netdevice_queue(dev, NULL);\n\t\tgoto out;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_free_pcpu:\n\tnetdev_do_free_pcpu_stats(dev);\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\nerr_free_name:\n\tnetdev_name_node_free(dev->name_node);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* napi_busy_loop stats accounting wants this */\n\tdev_net_set(dev, &init_net);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\tif (rtnl_lock_killable())\n\t\treturn -EINTR;\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n#else\n\treturn refcount_read(&dev->dev_refcnt);\n#endif\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\nint netdev_unregister_timeout_secs __read_mostly = 10;\n\n#define WAIT_REFS_MIN_MSECS 1\n#define WAIT_REFS_MAX_MSECS 250\n/**\n * netdev_wait_allrefs_any - wait until all references are gone.\n * @list: list of net_devices to wait on\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic struct net_device *netdev_wait_allrefs_any(struct list_head *list)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tstruct net_device *dev;\n\tint wait = 0;\n\n\trebroadcast_time = warning_time = jiffies;\n\n\tlist_for_each_entry(dev, list, todo_list)\n\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\treturn dev;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t\t     &dev->state)) {\n\t\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t\t * pending on unregister. If this\n\t\t\t\t\t * happens, we simply run the queue\n\t\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t\t * for this device.\n\t\t\t\t\t */\n\t\t\t\t\tlinkwatch_run_queue();\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\trcu_barrier();\n\n\t\tif (!wait) {\n\t\t\twait = WAIT_REFS_MIN_MSECS;\n\t\t} else {\n\t\t\tmsleep(wait);\n\t\t\twait = min(wait << 1, WAIT_REFS_MAX_MSECS);\n\t\t}\n\n\t\tlist_for_each_entry(dev, list, todo_list)\n\t\t\tif (netdev_refcnt_read(dev) == 1)\n\t\t\t\treturn dev;\n\n\t\tif (time_after(jiffies, warning_time +\n\t\t\t       READ_ONCE(netdev_unregister_timeout_secs) * HZ)) {\n\t\t\tlist_for_each_entry(dev, list, todo_list) {\n\t\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t\t dev->name, netdev_refcnt_read(dev));\n\t\t\t\tref_tracker_dir_print(&dev->refcnt_tracker, 10);\n\t\t\t}\n\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct net_device *dev, *tmp;\n\tstruct list_head list;\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head unlink_list;\n\n\tlist_replace_init(&net_unlink_list, &unlink_list);\n\n\twhile (!list_empty(&unlink_list)) {\n\t\tstruct net_device *dev = list_first_entry(&unlink_list,\n\t\t\t\t\t\t\t  struct net_device,\n\t\t\t\t\t\t\t  unlink_list);\n\t\tlist_del_init(&dev->unlink_list);\n\t\tdev->nested_level = dev->lower_level - 1;\n\t}\n#endif\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\tlist_for_each_entry_safe(dev, tmp, &list, todo_list) {\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tnetdev_WARN(dev, \"run_todo but not unregistering\\n\");\n\t\t\tlist_del(&dev->todo_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\twrite_lock(&dev_base_lock);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t\twrite_unlock(&dev_base_lock);\n\t\tlinkwatch_forget_dev(dev);\n\t}\n\n\twhile (!list_empty(&list)) {\n\t\tdev = netdev_wait_allrefs_any(&list);\n\t\tlist_del(&dev->todo_list);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev) != 1);\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\n\t\tnetdev_do_free_pcpu_stats(dev);\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\tif (atomic_dec_and_test(&dev_net(dev)->dev_unreg_count))\n\t\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(atomic_long_t);\n\tconst atomic_long_t *src = (atomic_long_t *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = (unsigned long)atomic_long_read(&src[i]);\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\nstruct net_device_core_stats __percpu *netdev_core_stats_alloc(struct net_device *dev)\n{\n\tstruct net_device_core_stats __percpu *p;\n\n\tp = alloc_percpu_gfp(struct net_device_core_stats,\n\t\t\t     GFP_ATOMIC | __GFP_NOWARN);\n\n\tif (p && cmpxchg(&dev->core_stats, NULL, p))\n\t\tfree_percpu(p);\n\n\t/* This READ_ONCE() pairs with the cmpxchg() above */\n\treturn READ_ONCE(dev->core_stats);\n}\nEXPORT_SYMBOL(netdev_core_stats_alloc);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tconst struct net_device_core_stats __percpu *p;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\n\t/* This READ_ONCE() pairs with the write in netdev_core_stats_alloc() */\n\tp = READ_ONCE(dev->core_stats);\n\tif (p) {\n\t\tconst struct net_device_core_stats *core_stats;\n\t\tint i;\n\n\t\tfor_each_possible_cpu(i) {\n\t\t\tcore_stats = per_cpu_ptr(p, i);\n\t\t\tstorage->rx_dropped += READ_ONCE(core_stats->rx_dropped);\n\t\t\tstorage->tx_dropped += READ_ONCE(core_stats->tx_dropped);\n\t\t\tstorage->rx_nohandler += READ_ONCE(core_stats->rx_nohandler);\n\t\t\tstorage->rx_otherhost_dropped += READ_ONCE(core_stats->rx_otherhost_dropped);\n\t\t}\n\t}\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\n/**\n *\tdev_fetch_sw_netstats - get per-cpu network device statistics\n *\t@s: place to store stats\n *\t@netstats: per-cpu network stats to read from\n *\n *\tRead per-cpu network statistics and populate the related fields in @s.\n */\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tu64 rx_packets, rx_bytes, tx_packets, tx_bytes;\n\t\tconst struct pcpu_sw_netstats *stats;\n\t\tunsigned int start;\n\n\t\tstats = per_cpu_ptr(netstats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin_irq(&stats->syncp);\n\t\t\trx_packets = u64_stats_read(&stats->rx_packets);\n\t\t\trx_bytes   = u64_stats_read(&stats->rx_bytes);\n\t\t\ttx_packets = u64_stats_read(&stats->tx_packets);\n\t\t\ttx_bytes   = u64_stats_read(&stats->tx_bytes);\n\t\t} while (u64_stats_fetch_retry_irq(&stats->syncp, start));\n\n\t\ts->rx_packets += rx_packets;\n\t\ts->rx_bytes   += rx_bytes;\n\t\ts->tx_packets += tx_packets;\n\t\ts->tx_bytes   += tx_bytes;\n\t}\n}\nEXPORT_SYMBOL_GPL(dev_fetch_sw_netstats);\n\n/**\n *\tdev_get_tstats64 - ndo_get_stats64 implementation\n *\t@dev: device to get statistics from\n *\t@s: place to store stats\n *\n *\tPopulate @s from dev->stats and dev->tstats. Can be used as\n *\tndo_get_stats64() callback.\n */\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tnetdev_stats_to_stats64(s, &dev->stats);\n\tdev_fetch_sw_netstats(s, dev->tstats);\n}\nEXPORT_SYMBOL_GPL(dev_get_tstats64);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tRCU_INIT_POINTER(queue->qdisc_sleeping, &noop_qdisc);\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tunsigned int alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tref_tracker_dir_init(&dev->refcnt_tracker, 128);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\t__dev_hold(dev);\n#else\n\trefcount_set(&dev->dev_refcnt, 1);\n#endif\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_LEGACY_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\tdev->gro_max_size = GRO_LEGACY_MAX_SIZE;\n\tdev->tso_max_size = TSO_LEGACY_MAX_SIZE;\n\tdev->tso_max_segs = TSO_MAX_SEGS;\n\tdev->upper_level = 1;\n\tdev->lower_level = 1;\n#ifdef CONFIG_LOCKDEP\n\tdev->nested_level = 0;\n\tINIT_LIST_HEAD(&dev->unlink_list);\n#endif\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n\tINIT_LIST_HEAD(&dev->net_notifier_list);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_netdev_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n#endif\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\n\tmight_sleep();\n\n\t/* When called immediately after register_netdevice() failed the unwind\n\t * handling may still be dismantling the device. Handle that case by\n\t * deferring the free.\n\t */\n\tif (dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\t\tdev->needs_free_netdev = true;\n\t\treturn;\n\t}\n\n\tnetif_free_tx_queues(dev);\n\tnetif_free_rx_queues(dev);\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tref_tracker_dir_exit(&dev->refcnt_tracker);\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n#endif\n\tfree_percpu(dev->core_stats);\n\tdev->core_stats = NULL;\n\tfree_percpu(dev->xdp_bulkq);\n\tdev->xdp_bulkq = NULL;\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->unreg_list, &single);\n\t\tunregister_netdevice_many(&single);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tif (list_empty(head))\n\t\treturn;\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\twrite_lock(&dev_base_lock);\n\t\tunlist_netdevice(dev, false);\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t\twrite_unlock(&dev_base_lock);\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\t\tdev_xdp_uninstall(dev);\n\n\t\tnetdev_offload_xstats_disable_all(dev);\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL, NULL, 0);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tnetdev_name_node_alt_flush(dev);\n\t\tnetdev_name_node_free(dev->name_node);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tnetdev_put(dev, &dev->dev_registered_tracker);\n\t\tnet_set_todo(dev);\n\t}\n\n\tlist_del(head);\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\t__dev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\t@new_ifindex: If not zero, specifies device index in the target\n *\t              namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint __dev_change_net_namespace(struct net_device *dev, struct net *net,\n\t\t\t       const char *pat, int new_ifindex)\n{\n\tstruct netdev_name_node *name_node;\n\tstruct net *net_old = dev_net(dev);\n\tchar new_name[IFNAMSIZ] = {};\n\tint err, new_nsid;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(net_old, net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (netdev_name_in_use(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\terr = dev_prep_valid_name(net, dev, pat, new_name);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\t/* Check that none of the altnames conflicts. */\n\terr = -EEXIST;\n\tnetdev_for_each_altname(dev, name_node)\n\t\tif (netdev_name_in_use(net, name_node->name))\n\t\t\tgoto out;\n\n\t/* Check that new_ifindex isn't used yet. */\n\terr = -EBUSY;\n\tif (new_ifindex && __dev_get_by_index(net, new_ifindex))\n\t\tgoto out;\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\tunlist_netdevice(dev, true);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\n\tnew_nsid = peernet2id_alloc(dev_net(dev), net, GFP_KERNEL);\n\t/* If there is an ifindex conflict assign a new one */\n\tif (!new_ifindex) {\n\t\tif (__dev_get_by_index(net, dev->ifindex))\n\t\t\tnew_ifindex = dev_new_index(net);\n\t\telse\n\t\t\tnew_ifindex = dev->ifindex;\n\t}\n\n\trtmsg_ifinfo_newnet(RTM_DELLINK, dev, ~0U, GFP_KERNEL, &new_nsid,\n\t\t\t    new_ifindex);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Move per-net netdevice notifiers that are following the netdevice */\n\tmove_netdevice_notifiers_dev_net(dev, net);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\tdev->ifindex = new_ifindex;\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\tif (new_name[0]) /* Rename the netdev to prepared name */\n\t\tstrscpy(dev->name, new_name, IFNAMSIZ);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Adapt owner in case owning user namespace of target network\n\t * namespace is different from the original one.\n\t */\n\terr = netdev_change_owner(dev, net_old, net);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(__dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t/* send out pending IPI's on offline CPU */\n\tnet_rps_send_ipi(remsd);\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc_array(NETDEV_HASHENTRIES, sizeof(*hash), GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tBUILD_BUG_ON(GRO_HASH_BUCKETS >\n\t\t     8 * sizeof_field(struct napi_struct, gro_bitmask));\n\n\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\tRAW_INIT_NOTIFIER_HEAD(&net->netdev_chain);\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n\tif (net != &init_net)\n\t\tWARN_ON_ONCE(!list_empty(&net->dev_base_head));\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit_net(struct net *net)\n{\n\tstruct netdev_name_node *name_node, *tmp;\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\tASSERT_RTNL();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops && !dev->rtnl_link_ops->netns_refund)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\tif (netdev_name_in_use(&init_net, fb_name))\n\t\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%%d\");\n\n\t\tnetdev_for_each_altname_safe(dev, name_node, tmp)\n\t\t\tif (netdev_name_in_use(&init_net, name_node->name)) {\n\t\t\t\tnetdev_name_node_del(name_node);\n\t\t\t\tsynchronize_rcu();\n\t\t\t\t__netdev_name_node_alt_destroy(name_node);\n\t\t\t}\n\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tdefault_device_exit_net(net);\n\t\tcond_resched();\n\t}\n\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n#ifdef CONFIG_XFRM_OFFLOAD\n\t\tskb_queue_head_init(&sd->xfrm_backlog);\n#endif\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tINIT_CSD(&sd->csd, rps_trigger_softirq, sd);\n\t\tsd->cpu = i;\n#endif\n\t\tINIT_CSD(&sd->defer_csd, trigger_rx_softirq, sd);\n\t\tspin_lock_init(&sd->defer_lock);\n\n\t\tinit_gro_hash(&sd->backlog);\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21631",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21631.json",
            "patch_attempts": [
                {
                    "upstream_commit": "cc0331e29fce4c3c2eaedeb7029360be6ed1185c",
                    "upstream_commit_date": "2025-01-09 06:52:46 -0700",
                    "upstream_patch": "fcede1f0a043ccefe9bc6ad57f12718e42f63f1d",
                    "total_versions_tested": 4,
                    "successful_patches": 4,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "2550149fcdf2934155ff625d76ad4e3d4b25bbc6",
                            "downstream_commit": "64b0aebed9455477b1900fb34751e9ec4428e675",
                            "commit_date": "2025-01-17 13:34:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file block/bfq-iosched.c\nHunk #1 succeeded at 6733 (offset -111 lines).",
                            "downstream_patch_content": "commit 2550149fcdf2934155ff625d76ad4e3d4b25bbc6\nAuthor: Yu Kuai <yukuai3@huawei.com>\nDate:   Wed Jan 8 16:41:48 2025 +0800\n\n    block, bfq: fix waker_bfqq UAF after bfq_split_bfqq()\n    \n    [ Upstream commit fcede1f0a043ccefe9bc6ad57f12718e42f63f1d ]\n    \n    Our syzkaller report a following UAF for v6.6:\n    \n    BUG: KASAN: slab-use-after-free in bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n    Read of size 8 at addr ffff8881b57147d8 by task fsstress/232726\n    \n    CPU: 2 PID: 232726 Comm: fsstress Not tainted 6.6.0-g3629d1885222 #39\n    Call Trace:\n     <TASK>\n     __dump_stack lib/dump_stack.c:88 [inline]\n     dump_stack_lvl+0x91/0xf0 lib/dump_stack.c:106\n     print_address_description.constprop.0+0x66/0x300 mm/kasan/report.c:364\n     print_report+0x3e/0x70 mm/kasan/report.c:475\n     kasan_report+0xb8/0xf0 mm/kasan/report.c:588\n     hlist_add_head include/linux/list.h:1023 [inline]\n     bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Allocated by task 232719:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     __kasan_slab_alloc+0x87/0x90 mm/kasan/common.c:328\n     kasan_slab_alloc include/linux/kasan.h:188 [inline]\n     slab_post_alloc_hook mm/slab.h:768 [inline]\n     slab_alloc_node mm/slub.c:3492 [inline]\n     kmem_cache_alloc_node+0x1b8/0x6f0 mm/slub.c:3537\n     bfq_get_queue+0x215/0x1f00 block/bfq-iosched.c:5869\n     bfq_get_bfqq_handle_split+0x167/0x5f0 block/bfq-iosched.c:6776\n     bfq_init_rq+0x13a4/0x17a0 block/bfq-iosched.c:6938\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh_nowait+0x15a/0x240 fs/ext4/super.c:217\n     ext4_read_bh_lock+0xac/0xd0 fs/ext4/super.c:242\n     ext4_bread_batch+0x268/0x500 fs/ext4/inode.c:958\n     __ext4_find_entry+0x448/0x10f0 fs/ext4/namei.c:1671\n     ext4_lookup_entry fs/ext4/namei.c:1774 [inline]\n     ext4_lookup.part.0+0x359/0x6f0 fs/ext4/namei.c:1842\n     ext4_lookup+0x72/0x90 fs/ext4/namei.c:1839\n     __lookup_slow+0x257/0x480 fs/namei.c:1696\n     lookup_slow fs/namei.c:1713 [inline]\n     walk_component+0x454/0x5c0 fs/namei.c:2004\n     link_path_walk.part.0+0x773/0xda0 fs/namei.c:2331\n     link_path_walk fs/namei.c:3826 [inline]\n     path_openat+0x1b9/0x520 fs/namei.c:3826\n     do_filp_open+0x1b7/0x400 fs/namei.c:3857\n     do_sys_openat2+0x5dc/0x6e0 fs/open.c:1428\n     do_sys_open fs/open.c:1443 [inline]\n     __do_sys_openat fs/open.c:1459 [inline]\n     __se_sys_openat fs/open.c:1454 [inline]\n     __x64_sys_openat+0x148/0x200 fs/open.c:1454\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Freed by task 232726:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     kasan_save_free_info+0x2b/0x50 mm/kasan/generic.c:522\n     ____kasan_slab_free mm/kasan/common.c:236 [inline]\n     __kasan_slab_free+0x12a/0x1b0 mm/kasan/common.c:244\n     kasan_slab_free include/linux/kasan.h:164 [inline]\n     slab_free_hook mm/slub.c:1827 [inline]\n     slab_free_freelist_hook mm/slub.c:1853 [inline]\n     slab_free mm/slub.c:3820 [inline]\n     kmem_cache_free+0x110/0x760 mm/slub.c:3842\n     bfq_put_queue+0x6a7/0xfb0 block/bfq-iosched.c:5428\n     bfq_forget_entity block/bfq-wf2q.c:634 [inline]\n     bfq_put_idle_entity+0x142/0x240 block/bfq-wf2q.c:645\n     bfq_forget_idle+0x189/0x1e0 block/bfq-wf2q.c:671\n     bfq_update_vtime block/bfq-wf2q.c:1280 [inline]\n     __bfq_lookup_next_entity block/bfq-wf2q.c:1374 [inline]\n     bfq_lookup_next_entity+0x350/0x480 block/bfq-wf2q.c:1433\n     bfq_update_next_in_service+0x1c0/0x4f0 block/bfq-wf2q.c:128\n     bfq_deactivate_entity+0x10a/0x240 block/bfq-wf2q.c:1188\n     bfq_deactivate_bfqq block/bfq-wf2q.c:1592 [inline]\n     bfq_del_bfqq_busy+0x2e8/0xad0 block/bfq-wf2q.c:1659\n     bfq_release_process_ref+0x1cc/0x220 block/bfq-iosched.c:3139\n     bfq_split_bfqq+0x481/0xdf0 block/bfq-iosched.c:6754\n     bfq_init_rq+0xf29/0x17a0 block/bfq-iosched.c:6934\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    commit 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after\n    splitting\") fix the problem that if waker_bfqq is in the merge chain,\n    and current is the only procress, waker_bfqq can be freed from\n    bfq_split_bfqq(). However, the case that waker_bfqq is not in the merge\n    chain is missed, and if the procress reference of waker_bfqq is 0,\n    waker_bfqq can be freed as well.\n    \n    Fix the problem by checking procress reference if waker_bfqq is not in\n    the merge_chain.\n    \n    Fixes: 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after splitting\")\n    Signed-off-by: Hou Tao <houtao1@huawei.com>\n    Signed-off-by: Yu Kuai <yukuai3@huawei.com>\n    Reviewed-by: Jan Kara <jack@suse.cz>\n    Link: https://lore.kernel.org/r/20250108084148.1549973-1-yukuai1@huaweicloud.com\n    Signed-off-by: Jens Axboe <axboe@kernel.dk>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/block/bfq-iosched.c b/block/bfq-iosched.c\nindex 8e797782cfe3..f75945764653 100644\n--- a/block/bfq-iosched.c\n+++ b/block/bfq-iosched.c\n@@ -6733,16 +6733,24 @@ static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n \t\tif (new_bfqq == waker_bfqq) {\n \t\t\t/*\n \t\t\t * If waker_bfqq is in the merge chain, and current\n-\t\t\t * is the only procress.\n+\t\t\t * is the only process, waker_bfqq can be freed.\n \t\t\t */\n \t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n \t\t\t\treturn NULL;\n-\t\t\tbreak;\n+\n+\t\t\treturn waker_bfqq;\n \t\t}\n \n \t\tnew_bfqq = new_bfqq->new_bfqq;\n \t}\n \n+\t/*\n+\t * If waker_bfqq is not in the merge chain, and it's procress reference\n+\t * is 0, waker_bfqq can be freed.\n+\t */\n+\tif (bfqq_process_refs(waker_bfqq) == 0)\n+\t\treturn NULL;\n+\n \treturn waker_bfqq;\n }\n \n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "be3eed59ac01f429ac10aaa46e26f653bcf581ab",
                            "downstream_commit": "53e25b10a28edaf8c2a1d3916fd8929501a50dfc",
                            "commit_date": "2025-01-17 13:36:25 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file block/bfq-iosched.c\nHunk #1 succeeded at 6843 (offset -1 lines).",
                            "downstream_patch_content": "commit be3eed59ac01f429ac10aaa46e26f653bcf581ab\nAuthor: Yu Kuai <yukuai3@huawei.com>\nDate:   Wed Jan 8 16:41:48 2025 +0800\n\n    block, bfq: fix waker_bfqq UAF after bfq_split_bfqq()\n    \n    [ Upstream commit fcede1f0a043ccefe9bc6ad57f12718e42f63f1d ]\n    \n    Our syzkaller report a following UAF for v6.6:\n    \n    BUG: KASAN: slab-use-after-free in bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n    Read of size 8 at addr ffff8881b57147d8 by task fsstress/232726\n    \n    CPU: 2 PID: 232726 Comm: fsstress Not tainted 6.6.0-g3629d1885222 #39\n    Call Trace:\n     <TASK>\n     __dump_stack lib/dump_stack.c:88 [inline]\n     dump_stack_lvl+0x91/0xf0 lib/dump_stack.c:106\n     print_address_description.constprop.0+0x66/0x300 mm/kasan/report.c:364\n     print_report+0x3e/0x70 mm/kasan/report.c:475\n     kasan_report+0xb8/0xf0 mm/kasan/report.c:588\n     hlist_add_head include/linux/list.h:1023 [inline]\n     bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Allocated by task 232719:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     __kasan_slab_alloc+0x87/0x90 mm/kasan/common.c:328\n     kasan_slab_alloc include/linux/kasan.h:188 [inline]\n     slab_post_alloc_hook mm/slab.h:768 [inline]\n     slab_alloc_node mm/slub.c:3492 [inline]\n     kmem_cache_alloc_node+0x1b8/0x6f0 mm/slub.c:3537\n     bfq_get_queue+0x215/0x1f00 block/bfq-iosched.c:5869\n     bfq_get_bfqq_handle_split+0x167/0x5f0 block/bfq-iosched.c:6776\n     bfq_init_rq+0x13a4/0x17a0 block/bfq-iosched.c:6938\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh_nowait+0x15a/0x240 fs/ext4/super.c:217\n     ext4_read_bh_lock+0xac/0xd0 fs/ext4/super.c:242\n     ext4_bread_batch+0x268/0x500 fs/ext4/inode.c:958\n     __ext4_find_entry+0x448/0x10f0 fs/ext4/namei.c:1671\n     ext4_lookup_entry fs/ext4/namei.c:1774 [inline]\n     ext4_lookup.part.0+0x359/0x6f0 fs/ext4/namei.c:1842\n     ext4_lookup+0x72/0x90 fs/ext4/namei.c:1839\n     __lookup_slow+0x257/0x480 fs/namei.c:1696\n     lookup_slow fs/namei.c:1713 [inline]\n     walk_component+0x454/0x5c0 fs/namei.c:2004\n     link_path_walk.part.0+0x773/0xda0 fs/namei.c:2331\n     link_path_walk fs/namei.c:3826 [inline]\n     path_openat+0x1b9/0x520 fs/namei.c:3826\n     do_filp_open+0x1b7/0x400 fs/namei.c:3857\n     do_sys_openat2+0x5dc/0x6e0 fs/open.c:1428\n     do_sys_open fs/open.c:1443 [inline]\n     __do_sys_openat fs/open.c:1459 [inline]\n     __se_sys_openat fs/open.c:1454 [inline]\n     __x64_sys_openat+0x148/0x200 fs/open.c:1454\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Freed by task 232726:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     kasan_save_free_info+0x2b/0x50 mm/kasan/generic.c:522\n     ____kasan_slab_free mm/kasan/common.c:236 [inline]\n     __kasan_slab_free+0x12a/0x1b0 mm/kasan/common.c:244\n     kasan_slab_free include/linux/kasan.h:164 [inline]\n     slab_free_hook mm/slub.c:1827 [inline]\n     slab_free_freelist_hook mm/slub.c:1853 [inline]\n     slab_free mm/slub.c:3820 [inline]\n     kmem_cache_free+0x110/0x760 mm/slub.c:3842\n     bfq_put_queue+0x6a7/0xfb0 block/bfq-iosched.c:5428\n     bfq_forget_entity block/bfq-wf2q.c:634 [inline]\n     bfq_put_idle_entity+0x142/0x240 block/bfq-wf2q.c:645\n     bfq_forget_idle+0x189/0x1e0 block/bfq-wf2q.c:671\n     bfq_update_vtime block/bfq-wf2q.c:1280 [inline]\n     __bfq_lookup_next_entity block/bfq-wf2q.c:1374 [inline]\n     bfq_lookup_next_entity+0x350/0x480 block/bfq-wf2q.c:1433\n     bfq_update_next_in_service+0x1c0/0x4f0 block/bfq-wf2q.c:128\n     bfq_deactivate_entity+0x10a/0x240 block/bfq-wf2q.c:1188\n     bfq_deactivate_bfqq block/bfq-wf2q.c:1592 [inline]\n     bfq_del_bfqq_busy+0x2e8/0xad0 block/bfq-wf2q.c:1659\n     bfq_release_process_ref+0x1cc/0x220 block/bfq-iosched.c:3139\n     bfq_split_bfqq+0x481/0xdf0 block/bfq-iosched.c:6754\n     bfq_init_rq+0xf29/0x17a0 block/bfq-iosched.c:6934\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    commit 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after\n    splitting\") fix the problem that if waker_bfqq is in the merge chain,\n    and current is the only procress, waker_bfqq can be freed from\n    bfq_split_bfqq(). However, the case that waker_bfqq is not in the merge\n    chain is missed, and if the procress reference of waker_bfqq is 0,\n    waker_bfqq can be freed as well.\n    \n    Fix the problem by checking procress reference if waker_bfqq is not in\n    the merge_chain.\n    \n    Fixes: 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after splitting\")\n    Signed-off-by: Hou Tao <houtao1@huawei.com>\n    Signed-off-by: Yu Kuai <yukuai3@huawei.com>\n    Reviewed-by: Jan Kara <jack@suse.cz>\n    Link: https://lore.kernel.org/r/20250108084148.1549973-1-yukuai1@huaweicloud.com\n    Signed-off-by: Jens Axboe <axboe@kernel.dk>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/block/bfq-iosched.c b/block/bfq-iosched.c\nindex dd8ca3f7ba60..617d6802b8a0 100644\n--- a/block/bfq-iosched.c\n+++ b/block/bfq-iosched.c\n@@ -6843,16 +6843,24 @@ static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n \t\tif (new_bfqq == waker_bfqq) {\n \t\t\t/*\n \t\t\t * If waker_bfqq is in the merge chain, and current\n-\t\t\t * is the only procress.\n+\t\t\t * is the only process, waker_bfqq can be freed.\n \t\t\t */\n \t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n \t\t\t\treturn NULL;\n-\t\t\tbreak;\n+\n+\t\t\treturn waker_bfqq;\n \t\t}\n \n \t\tnew_bfqq = new_bfqq->new_bfqq;\n \t}\n \n+\t/*\n+\t * If waker_bfqq is not in the merge chain, and it's procress reference\n+\t * is 0, waker_bfqq can be freed.\n+\t */\n+\tif (bfqq_process_refs(waker_bfqq) == 0)\n+\t\treturn NULL;\n+\n \treturn waker_bfqq;\n }\n \n",
                            "downstream_file_content": {
                                "block/bfq-iosched.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Budget Fair Queueing (BFQ) I/O scheduler.\n *\n * Based on ideas and code from CFQ:\n * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>\n *\n * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it>\n *\t\t      Paolo Valente <paolo.valente@unimore.it>\n *\n * Copyright (C) 2010 Paolo Valente <paolo.valente@unimore.it>\n *                    Arianna Avanzini <avanzini@google.com>\n *\n * Copyright (C) 2017 Paolo Valente <paolo.valente@linaro.org>\n *\n * BFQ is a proportional-share I/O scheduler, with some extra\n * low-latency capabilities. BFQ also supports full hierarchical\n * scheduling through cgroups. Next paragraphs provide an introduction\n * on BFQ inner workings. Details on BFQ benefits, usage and\n * limitations can be found in Documentation/block/bfq-iosched.rst.\n *\n * BFQ is a proportional-share storage-I/O scheduling algorithm based\n * on the slice-by-slice service scheme of CFQ. But BFQ assigns\n * budgets, measured in number of sectors, to processes instead of\n * time slices. The device is not granted to the in-service process\n * for a given time slice, but until it has exhausted its assigned\n * budget. This change from the time to the service domain enables BFQ\n * to distribute the device throughput among processes as desired,\n * without any distortion due to throughput fluctuations, or to device\n * internal queueing. BFQ uses an ad hoc internal scheduler, called\n * B-WF2Q+, to schedule processes according to their budgets. More\n * precisely, BFQ schedules queues associated with processes. Each\n * process/queue is assigned a user-configurable weight, and B-WF2Q+\n * guarantees that each queue receives a fraction of the throughput\n * proportional to its weight. Thanks to the accurate policy of\n * B-WF2Q+, BFQ can afford to assign high budgets to I/O-bound\n * processes issuing sequential requests (to boost the throughput),\n * and yet guarantee a low latency to interactive and soft real-time\n * applications.\n *\n * In particular, to provide these low-latency guarantees, BFQ\n * explicitly privileges the I/O of two classes of time-sensitive\n * applications: interactive and soft real-time. In more detail, BFQ\n * behaves this way if the low_latency parameter is set (default\n * configuration). This feature enables BFQ to provide applications in\n * these classes with a very low latency.\n *\n * To implement this feature, BFQ constantly tries to detect whether\n * the I/O requests in a bfq_queue come from an interactive or a soft\n * real-time application. For brevity, in these cases, the queue is\n * said to be interactive or soft real-time. In both cases, BFQ\n * privileges the service of the queue, over that of non-interactive\n * and non-soft-real-time queues. This privileging is performed,\n * mainly, by raising the weight of the queue. So, for brevity, we\n * call just weight-raising periods the time periods during which a\n * queue is privileged, because deemed interactive or soft real-time.\n *\n * The detection of soft real-time queues/applications is described in\n * detail in the comments on the function\n * bfq_bfqq_softrt_next_start. On the other hand, the detection of an\n * interactive queue works as follows: a queue is deemed interactive\n * if it is constantly non empty only for a limited time interval,\n * after which it does become empty. The queue may be deemed\n * interactive again (for a limited time), if it restarts being\n * constantly non empty, provided that this happens only after the\n * queue has remained empty for a given minimum idle time.\n *\n * By default, BFQ computes automatically the above maximum time\n * interval, i.e., the time interval after which a constantly\n * non-empty queue stops being deemed interactive. Since a queue is\n * weight-raised while it is deemed interactive, this maximum time\n * interval happens to coincide with the (maximum) duration of the\n * weight-raising for interactive queues.\n *\n * Finally, BFQ also features additional heuristics for\n * preserving both a low latency and a high throughput on NCQ-capable,\n * rotational or flash-based devices, and to get the job done quickly\n * for applications consisting in many I/O-bound processes.\n *\n * NOTE: if the main or only goal, with a given device, is to achieve\n * the maximum-possible throughput at all times, then do switch off\n * all low-latency heuristics for that device, by setting low_latency\n * to 0.\n *\n * BFQ is described in [1], where also a reference to the initial,\n * more theoretical paper on BFQ can be found. The interested reader\n * can find in the latter paper full details on the main algorithm, as\n * well as formulas of the guarantees and formal proofs of all the\n * properties.  With respect to the version of BFQ presented in these\n * papers, this implementation adds a few more heuristics, such as the\n * ones that guarantee a low latency to interactive and soft real-time\n * applications, and a hierarchical extension based on H-WF2Q+.\n *\n * B-WF2Q+ is based on WF2Q+, which is described in [2], together with\n * H-WF2Q+, while the augmented tree used here to implement B-WF2Q+\n * with O(log N) complexity derives from the one introduced with EEVDF\n * in [3].\n *\n * [1] P. Valente, A. Avanzini, \"Evolution of the BFQ Storage I/O\n *     Scheduler\", Proceedings of the First Workshop on Mobile System\n *     Technologies (MST-2015), May 2015.\n *     http://algogroup.unimore.it/people/paolo/disk_sched/mst-2015.pdf\n *\n * [2] Jon C.R. Bennett and H. Zhang, \"Hierarchical Packet Fair Queueing\n *     Algorithms\", IEEE/ACM Transactions on Networking, 5(5):675-689,\n *     Oct 1997.\n *\n * http://www.cs.cmu.edu/~hzhang/papers/TON-97-Oct.ps.gz\n *\n * [3] I. Stoica and H. Abdel-Wahab, \"Earliest Eligible Virtual Deadline\n *     First: A Flexible and Accurate Mechanism for Proportional Share\n *     Resource Allocation\", technical report.\n *\n * http://www.cs.berkeley.edu/~istoica/papers/eevdf-tr-95.pdf\n */\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/cgroup.h>\n#include <linux/ktime.h>\n#include <linux/rbtree.h>\n#include <linux/ioprio.h>\n#include <linux/sbitmap.h>\n#include <linux/delay.h>\n#include <linux/backing-dev.h>\n\n#include <trace/events/block.h>\n\n#include \"elevator.h\"\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n#include \"bfq-iosched.h\"\n#include \"blk-wbt.h\"\n\n#define BFQ_BFQQ_FNS(name)\t\t\t\t\t\t\\\nvoid bfq_mark_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__set_bit(BFQQF_##name, &(bfqq)->flags);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nvoid bfq_clear_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__clear_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nint bfq_bfqq_##name(const struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\n\nBFQ_BFQQ_FNS(just_created);\nBFQ_BFQQ_FNS(busy);\nBFQ_BFQQ_FNS(wait_request);\nBFQ_BFQQ_FNS(non_blocking_wait_rq);\nBFQ_BFQQ_FNS(fifo_expire);\nBFQ_BFQQ_FNS(has_short_ttime);\nBFQ_BFQQ_FNS(sync);\nBFQ_BFQQ_FNS(IO_bound);\nBFQ_BFQQ_FNS(in_large_burst);\nBFQ_BFQQ_FNS(coop);\nBFQ_BFQQ_FNS(split_coop);\nBFQ_BFQQ_FNS(softrt_update);\n#undef BFQ_BFQQ_FNS\t\t\t\t\t\t\\\n\n/* Expiration time of async (0) and sync (1) requests, in ns. */\nstatic const u64 bfq_fifo_expire[2] = { NSEC_PER_SEC / 4, NSEC_PER_SEC / 8 };\n\n/* Maximum backwards seek (magic number lifted from CFQ), in KiB. */\nstatic const int bfq_back_max = 16 * 1024;\n\n/* Penalty of a backwards seek, in number of sectors. */\nstatic const int bfq_back_penalty = 2;\n\n/* Idling period duration, in ns. */\nstatic u64 bfq_slice_idle = NSEC_PER_SEC / 125;\n\n/* Minimum number of assigned budgets for which stats are safe to compute. */\nstatic const int bfq_stats_min_budgets = 194;\n\n/* Default maximum budget values, in sectors and number of requests. */\nstatic const int bfq_default_max_budget = 16 * 1024;\n\n/*\n * When a sync request is dispatched, the queue that contains that\n * request, and all the ancestor entities of that queue, are charged\n * with the number of sectors of the request. In contrast, if the\n * request is async, then the queue and its ancestor entities are\n * charged with the number of sectors of the request, multiplied by\n * the factor below. This throttles the bandwidth for async I/O,\n * w.r.t. to sync I/O, and it is done to counter the tendency of async\n * writes to steal I/O throughput to reads.\n *\n * The current value of this parameter is the result of a tuning with\n * several hardware and software configurations. We tried to find the\n * lowest value for which writes do not cause noticeable problems to\n * reads. In fact, the lower this parameter, the stabler I/O control,\n * in the following respect.  The lower this parameter is, the less\n * the bandwidth enjoyed by a group decreases\n * - when the group does writes, w.r.t. to when it does reads;\n * - when other groups do reads, w.r.t. to when they do writes.\n */\nstatic const int bfq_async_charge_factor = 3;\n\n/* Default timeout values, in jiffies, approximating CFQ defaults. */\nconst int bfq_timeout = HZ / 8;\n\n/*\n * Time limit for merging (see comments in bfq_setup_cooperator). Set\n * to the slowest value that, in our tests, proved to be effective in\n * removing false positives, while not causing true positives to miss\n * queue merging.\n *\n * As can be deduced from the low time limit below, queue merging, if\n * successful, happens at the very beginning of the I/O of the involved\n * cooperating processes, as a consequence of the arrival of the very\n * first requests from each cooperator.  After that, there is very\n * little chance to find cooperators.\n */\nstatic const unsigned long bfq_merge_time_limit = HZ/10;\n\nstatic struct kmem_cache *bfq_pool;\n\n/* Below this threshold (in ns), we consider thinktime immediate. */\n#define BFQ_MIN_TT\t\t(2 * NSEC_PER_MSEC)\n\n/* hw_tag detection: parallel requests threshold and min samples needed. */\n#define BFQ_HW_QUEUE_THRESHOLD\t3\n#define BFQ_HW_QUEUE_SAMPLES\t32\n\n#define BFQQ_SEEK_THR\t\t(sector_t)(8 * 100)\n#define BFQQ_SECT_THR_NONROT\t(sector_t)(2 * 32)\n#define BFQ_RQ_SEEKY(bfqd, last_pos, rq) \\\n\t(get_sdist(last_pos, rq) >\t\t\t\\\n\t BFQQ_SEEK_THR &&\t\t\t\t\\\n\t (!blk_queue_nonrot(bfqd->queue) ||\t\t\\\n\t  blk_rq_sectors(rq) < BFQQ_SECT_THR_NONROT))\n#define BFQQ_CLOSE_THR\t\t(sector_t)(8 * 1024)\n#define BFQQ_SEEKY(bfqq)\t(hweight32(bfqq->seek_history) > 19)\n/*\n * Sync random I/O is likely to be confused with soft real-time I/O,\n * because it is characterized by limited throughput and apparently\n * isochronous arrival pattern. To avoid false positives, queues\n * containing only random (seeky) I/O are prevented from being tagged\n * as soft real-time.\n */\n#define BFQQ_TOTALLY_SEEKY(bfqq)\t(bfqq->seek_history == -1)\n\n/* Min number of samples required to perform peak-rate update */\n#define BFQ_RATE_MIN_SAMPLES\t32\n/* Min observation time interval required to perform a peak-rate update (ns) */\n#define BFQ_RATE_MIN_INTERVAL\t(300*NSEC_PER_MSEC)\n/* Target observation time interval for a peak-rate update (ns) */\n#define BFQ_RATE_REF_INTERVAL\tNSEC_PER_SEC\n\n/*\n * Shift used for peak-rate fixed precision calculations.\n * With\n * - the current shift: 16 positions\n * - the current type used to store rate: u32\n * - the current unit of measure for rate: [sectors/usec], or, more precisely,\n *   [(sectors/usec) / 2^BFQ_RATE_SHIFT] to take into account the shift,\n * the range of rates that can be stored is\n * [1 / 2^BFQ_RATE_SHIFT, 2^(32 - BFQ_RATE_SHIFT)] sectors/usec =\n * [1 / 2^16, 2^16] sectors/usec = [15e-6, 65536] sectors/usec =\n * [15, 65G] sectors/sec\n * Which, assuming a sector size of 512B, corresponds to a range of\n * [7.5K, 33T] B/sec\n */\n#define BFQ_RATE_SHIFT\t\t16\n\n/*\n * When configured for computing the duration of the weight-raising\n * for interactive queues automatically (see the comments at the\n * beginning of this file), BFQ does it using the following formula:\n * duration = (ref_rate / r) * ref_wr_duration,\n * where r is the peak rate of the device, and ref_rate and\n * ref_wr_duration are two reference parameters.  In particular,\n * ref_rate is the peak rate of the reference storage device (see\n * below), and ref_wr_duration is about the maximum time needed, with\n * BFQ and while reading two files in parallel, to load typical large\n * applications on the reference device (see the comments on\n * max_service_from_wr below, for more details on how ref_wr_duration\n * is obtained).  In practice, the slower/faster the device at hand\n * is, the more/less it takes to load applications with respect to the\n * reference device.  Accordingly, the longer/shorter BFQ grants\n * weight raising to interactive applications.\n *\n * BFQ uses two different reference pairs (ref_rate, ref_wr_duration),\n * depending on whether the device is rotational or non-rotational.\n *\n * In the following definitions, ref_rate[0] and ref_wr_duration[0]\n * are the reference values for a rotational device, whereas\n * ref_rate[1] and ref_wr_duration[1] are the reference values for a\n * non-rotational device. The reference rates are not the actual peak\n * rates of the devices used as a reference, but slightly lower\n * values. The reason for using slightly lower values is that the\n * peak-rate estimator tends to yield slightly lower values than the\n * actual peak rate (it can yield the actual peak rate only if there\n * is only one process doing I/O, and the process does sequential\n * I/O).\n *\n * The reference peak rates are measured in sectors/usec, left-shifted\n * by BFQ_RATE_SHIFT.\n */\nstatic int ref_rate[2] = {14000, 33000};\n/*\n * To improve readability, a conversion function is used to initialize\n * the following array, which entails that the array can be\n * initialized only in a function.\n */\nstatic int ref_wr_duration[2];\n\n/*\n * BFQ uses the above-detailed, time-based weight-raising mechanism to\n * privilege interactive tasks. This mechanism is vulnerable to the\n * following false positives: I/O-bound applications that will go on\n * doing I/O for much longer than the duration of weight\n * raising. These applications have basically no benefit from being\n * weight-raised at the beginning of their I/O. On the opposite end,\n * while being weight-raised, these applications\n * a) unjustly steal throughput to applications that may actually need\n * low latency;\n * b) make BFQ uselessly perform device idling; device idling results\n * in loss of device throughput with most flash-based storage, and may\n * increase latencies when used purposelessly.\n *\n * BFQ tries to reduce these problems, by adopting the following\n * countermeasure. To introduce this countermeasure, we need first to\n * finish explaining how the duration of weight-raising for\n * interactive tasks is computed.\n *\n * For a bfq_queue deemed as interactive, the duration of weight\n * raising is dynamically adjusted, as a function of the estimated\n * peak rate of the device, so as to be equal to the time needed to\n * execute the 'largest' interactive task we benchmarked so far. By\n * largest task, we mean the task for which each involved process has\n * to do more I/O than for any of the other tasks we benchmarked. This\n * reference interactive task is the start-up of LibreOffice Writer,\n * and in this task each process/bfq_queue needs to have at most ~110K\n * sectors transferred.\n *\n * This last piece of information enables BFQ to reduce the actual\n * duration of weight-raising for at least one class of I/O-bound\n * applications: those doing sequential or quasi-sequential I/O. An\n * example is file copy. In fact, once started, the main I/O-bound\n * processes of these applications usually consume the above 110K\n * sectors in much less time than the processes of an application that\n * is starting, because these I/O-bound processes will greedily devote\n * almost all their CPU cycles only to their target,\n * throughput-friendly I/O operations. This is even more true if BFQ\n * happens to be underestimating the device peak rate, and thus\n * overestimating the duration of weight raising. But, according to\n * our measurements, once transferred 110K sectors, these processes\n * have no right to be weight-raised any longer.\n *\n * Basing on the last consideration, BFQ ends weight-raising for a\n * bfq_queue if the latter happens to have received an amount of\n * service at least equal to the following constant. The constant is\n * set to slightly more than 110K, to have a minimum safety margin.\n *\n * This early ending of weight-raising reduces the amount of time\n * during which interactive false positives cause the two problems\n * described at the beginning of these comments.\n */\nstatic const unsigned long max_service_from_wr = 120000;\n\n/*\n * Maximum time between the creation of two queues, for stable merge\n * to be activated (in ms)\n */\nstatic const unsigned long bfq_activation_stable_merging = 600;\n/*\n * Minimum time to be waited before evaluating delayed stable merge (in ms)\n */\nstatic const unsigned long bfq_late_stable_merging = 600;\n\n#define RQ_BIC(rq)\t\t((struct bfq_io_cq *)((rq)->elv.priv[0]))\n#define RQ_BFQQ(rq)\t\t((rq)->elv.priv[1])\n\nstruct bfq_queue *bic_to_bfqq(struct bfq_io_cq *bic, bool is_sync,\n\t\t\t      unsigned int actuator_idx)\n{\n\tif (is_sync)\n\t\treturn bic->bfqq[1][actuator_idx];\n\n\treturn bic->bfqq[0][actuator_idx];\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq);\n\nvoid bic_set_bfqq(struct bfq_io_cq *bic,\n\t\t  struct bfq_queue *bfqq,\n\t\t  bool is_sync,\n\t\t  unsigned int actuator_idx)\n{\n\tstruct bfq_queue *old_bfqq = bic->bfqq[is_sync][actuator_idx];\n\n\t/*\n\t * If bfqq != NULL, then a non-stable queue merge between\n\t * bic->bfqq and bfqq is happening here. This causes troubles\n\t * in the following case: bic->bfqq has also been scheduled\n\t * for a possible stable merge with bic->stable_merge_bfqq,\n\t * and bic->stable_merge_bfqq == bfqq happens to\n\t * hold. Troubles occur because bfqq may then undergo a split,\n\t * thereby becoming eligible for a stable merge. Yet, if\n\t * bic->stable_merge_bfqq points exactly to bfqq, then bfqq\n\t * would be stably merged with itself. To avoid this anomaly,\n\t * we cancel the stable merge if\n\t * bic->stable_merge_bfqq == bfqq.\n\t */\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[actuator_idx];\n\n\t/* Clear bic pointer if bfqq is detached from this bic */\n\tif (old_bfqq && old_bfqq->bic == bic)\n\t\told_bfqq->bic = NULL;\n\n\tif (is_sync)\n\t\tbic->bfqq[1][actuator_idx] = bfqq;\n\telse\n\t\tbic->bfqq[0][actuator_idx] = bfqq;\n\n\tif (bfqq && bfqq_data->stable_merge_bfqq == bfqq) {\n\t\t/*\n\t\t * Actually, these same instructions are executed also\n\t\t * in bfq_setup_cooperator, in case of abort or actual\n\t\t * execution of a stable merge. We could avoid\n\t\t * repeating these instructions there too, but if we\n\t\t * did so, we would nest even more complexity in this\n\t\t * function.\n\t\t */\n\t\tbfq_put_stable_ref(bfqq_data->stable_merge_bfqq);\n\n\t\tbfqq_data->stable_merge_bfqq = NULL;\n\t}\n}\n\nstruct bfq_data *bic_to_bfqd(struct bfq_io_cq *bic)\n{\n\treturn bic->icq.q->elevator->elevator_data;\n}\n\n/**\n * icq_to_bic - convert iocontext queue structure to bfq_io_cq.\n * @icq: the iocontext queue.\n */\nstatic struct bfq_io_cq *icq_to_bic(struct io_cq *icq)\n{\n\t/* bic->icq is the first member, %NULL will convert to %NULL */\n\treturn container_of(icq, struct bfq_io_cq, icq);\n}\n\n/**\n * bfq_bic_lookup - search into @ioc a bic associated to @bfqd.\n * @q: the request queue.\n */\nstatic struct bfq_io_cq *bfq_bic_lookup(struct request_queue *q)\n{\n\tstruct bfq_io_cq *icq;\n\tunsigned long flags;\n\n\tif (!current->io_context)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\ticq = icq_to_bic(ioc_lookup_icq(q));\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\n\treturn icq;\n}\n\n/*\n * Scheduler run of queue, if there are requests pending and no one in the\n * driver that will restart queueing.\n */\nvoid bfq_schedule_dispatch(struct bfq_data *bfqd)\n{\n\tlockdep_assert_held(&bfqd->lock);\n\n\tif (bfqd->queued != 0) {\n\t\tbfq_log(bfqd, \"schedule dispatch\");\n\t\tblk_mq_run_hw_queues(bfqd->queue, true);\n\t}\n}\n\n#define bfq_class_idle(bfqq)\t((bfqq)->ioprio_class == IOPRIO_CLASS_IDLE)\n\n#define bfq_sample_valid(samples)\t((samples) > 80)\n\n/*\n * Lifted from AS - choose which of rq1 and rq2 that is best served now.\n * We choose the request that is closer to the head right now.  Distance\n * behind the head is penalized and only allowed to a certain extent.\n */\nstatic struct request *bfq_choose_req(struct bfq_data *bfqd,\n\t\t\t\t      struct request *rq1,\n\t\t\t\t      struct request *rq2,\n\t\t\t\t      sector_t last)\n{\n\tsector_t s1, s2, d1 = 0, d2 = 0;\n\tunsigned long back_max;\n#define BFQ_RQ1_WRAP\t0x01 /* request 1 wraps */\n#define BFQ_RQ2_WRAP\t0x02 /* request 2 wraps */\n\tunsigned int wrap = 0; /* bit mask: requests behind the disk head? */\n\n\tif (!rq1 || rq1 == rq2)\n\t\treturn rq2;\n\tif (!rq2)\n\t\treturn rq1;\n\n\tif (rq_is_sync(rq1) && !rq_is_sync(rq2))\n\t\treturn rq1;\n\telse if (rq_is_sync(rq2) && !rq_is_sync(rq1))\n\t\treturn rq2;\n\tif ((rq1->cmd_flags & REQ_META) && !(rq2->cmd_flags & REQ_META))\n\t\treturn rq1;\n\telse if ((rq2->cmd_flags & REQ_META) && !(rq1->cmd_flags & REQ_META))\n\t\treturn rq2;\n\n\ts1 = blk_rq_pos(rq1);\n\ts2 = blk_rq_pos(rq2);\n\n\t/*\n\t * By definition, 1KiB is 2 sectors.\n\t */\n\tback_max = bfqd->bfq_back_max * 2;\n\n\t/*\n\t * Strict one way elevator _except_ in the case where we allow\n\t * short backward seeks which are biased as twice the cost of a\n\t * similar forward seek.\n\t */\n\tif (s1 >= last)\n\t\td1 = s1 - last;\n\telse if (s1 + back_max >= last)\n\t\td1 = (last - s1) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ1_WRAP;\n\n\tif (s2 >= last)\n\t\td2 = s2 - last;\n\telse if (s2 + back_max >= last)\n\t\td2 = (last - s2) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ2_WRAP;\n\n\t/* Found required data */\n\n\t/*\n\t * By doing switch() on the bit mask \"wrap\" we avoid having to\n\t * check two variables for all permutations: --> faster!\n\t */\n\tswitch (wrap) {\n\tcase 0: /* common case for CFQ: rq1 and rq2 not wrapped */\n\t\tif (d1 < d2)\n\t\t\treturn rq1;\n\t\telse if (d2 < d1)\n\t\t\treturn rq2;\n\n\t\tif (s1 >= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\n\tcase BFQ_RQ2_WRAP:\n\t\treturn rq1;\n\tcase BFQ_RQ1_WRAP:\n\t\treturn rq2;\n\tcase BFQ_RQ1_WRAP|BFQ_RQ2_WRAP: /* both rqs wrapped */\n\tdefault:\n\t\t/*\n\t\t * Since both rqs are wrapped,\n\t\t * start with the one that's further behind head\n\t\t * (--> only *one* back seek required),\n\t\t * since back seek takes more time than forward.\n\t\t */\n\t\tif (s1 <= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\t}\n}\n\n#define BFQ_LIMIT_INLINE_DEPTH 16\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\nstatic bool bfqq_request_over_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_io_cq *bic, blk_opf_t opf,\n\t\t\t\t    unsigned int act_idx, int limit)\n{\n\tstruct bfq_entity *inline_entities[BFQ_LIMIT_INLINE_DEPTH];\n\tstruct bfq_entity **entities = inline_entities;\n\tint alloc_depth = BFQ_LIMIT_INLINE_DEPTH;\n\tstruct bfq_sched_data *sched_data;\n\tstruct bfq_entity *entity;\n\tstruct bfq_queue *bfqq;\n\tunsigned long wsum;\n\tbool ret = false;\n\tint depth;\n\tint level;\n\nretry:\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bic_to_bfqq(bic, op_is_sync(opf), act_idx);\n\tif (!bfqq)\n\t\tgoto out;\n\n\tentity = &bfqq->entity;\n\tif (!entity->on_st_or_in_serv)\n\t\tgoto out;\n\n\t/* +1 for bfqq entity, root cgroup not included */\n\tdepth = bfqg_to_blkg(bfqq_group(bfqq))->blkcg->css.cgroup->level + 1;\n\tif (depth > alloc_depth) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tif (entities != inline_entities)\n\t\t\tkfree(entities);\n\t\tentities = kmalloc_array(depth, sizeof(*entities), GFP_NOIO);\n\t\tif (!entities)\n\t\t\treturn false;\n\t\talloc_depth = depth;\n\t\tgoto retry;\n\t}\n\n\tsched_data = entity->sched_data;\n\t/* Gather our ancestors as we need to traverse them in reverse order */\n\tlevel = 0;\n\tfor_each_entity(entity) {\n\t\t/*\n\t\t * If at some level entity is not even active, allow request\n\t\t * queueing so that BFQ knows there's work to do and activate\n\t\t * entities.\n\t\t */\n\t\tif (!entity->on_st_or_in_serv)\n\t\t\tgoto out;\n\t\t/* Uh, more parents than cgroup subsystem thinks? */\n\t\tif (WARN_ON_ONCE(level >= depth))\n\t\t\tbreak;\n\t\tentities[level++] = entity;\n\t}\n\tWARN_ON_ONCE(level != depth);\n\tfor (level--; level >= 0; level--) {\n\t\tentity = entities[level];\n\t\tif (level > 0) {\n\t\t\twsum = bfq_entity_service_tree(entity)->wsum;\n\t\t} else {\n\t\t\tint i;\n\t\t\t/*\n\t\t\t * For bfqq itself we take into account service trees\n\t\t\t * of all higher priority classes and multiply their\n\t\t\t * weights so that low prio queue from higher class\n\t\t\t * gets more requests than high prio queue from lower\n\t\t\t * class.\n\t\t\t */\n\t\t\twsum = 0;\n\t\t\tfor (i = 0; i <= bfqq->ioprio_class - 1; i++) {\n\t\t\t\twsum = wsum * IOPRIO_BE_NR +\n\t\t\t\t\tsched_data->service_tree[i].wsum;\n\t\t\t}\n\t\t}\n\t\tif (!wsum)\n\t\t\tcontinue;\n\t\tlimit = DIV_ROUND_CLOSEST(limit * entity->weight, wsum);\n\t\tif (entity->allocated >= limit) {\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t\"too many requests: allocated %d limit %d level %d\",\n\t\t\t\tentity->allocated, limit, level);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tspin_unlock_irq(&bfqd->lock);\n\tif (entities != inline_entities)\n\t\tkfree(entities);\n\treturn ret;\n}\n#else\nstatic bool bfqq_request_over_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_io_cq *bic, blk_opf_t opf,\n\t\t\t\t    unsigned int act_idx, int limit)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Async I/O can easily starve sync I/O (both sync reads and sync\n * writes), by consuming all tags. Similarly, storms of sync writes,\n * such as those that sync(2) may trigger, can starve sync reads.\n * Limit depths of async I/O and sync writes so as to counter both\n * problems.\n *\n * Also if a bfq queue or its parent cgroup consume more tags than would be\n * appropriate for their weight, we trim the available tag depth to 1. This\n * avoids a situation where one cgroup can starve another cgroup from tags and\n * thus block service differentiation among cgroups. Note that because the\n * queue / cgroup already has many requests allocated and queued, this does not\n * significantly affect service guarantees coming from the BFQ scheduling\n * algorithm.\n */\nstatic void bfq_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data)\n{\n\tstruct bfq_data *bfqd = data->q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(data->q);\n\tint depth;\n\tunsigned limit = data->q->nr_requests;\n\tunsigned int act_idx;\n\n\t/* Sync reads have full depth available */\n\tif (op_is_sync(opf) && !op_is_write(opf)) {\n\t\tdepth = 0;\n\t} else {\n\t\tdepth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(opf)];\n\t\tlimit = (limit * depth) >> bfqd->full_depth_shift;\n\t}\n\n\tfor (act_idx = 0; bic && act_idx < bfqd->num_actuators; act_idx++) {\n\t\t/* Fast path to check if bfqq is already allocated. */\n\t\tif (!bic_to_bfqq(bic, op_is_sync(opf), act_idx))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Does queue (or any parent entity) exceed number of\n\t\t * requests that should be available to it? Heavily\n\t\t * limit depth so that it cannot consume more\n\t\t * available requests and thus starve other entities.\n\t\t */\n\t\tif (bfqq_request_over_limit(bfqd, bic, opf, act_idx, limit)) {\n\t\t\tdepth = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbfq_log(bfqd, \"[%s] wr_busy %d sync %d depth %u\",\n\t\t__func__, bfqd->wr_busy_queues, op_is_sync(opf), depth);\n\tif (depth)\n\t\tdata->shallow_depth = depth;\n}\n\nstatic struct bfq_queue *\nbfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,\n\t\t     sector_t sector, struct rb_node **ret_parent,\n\t\t     struct rb_node ***rb_link)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tparent = NULL;\n\tp = &root->rb_node;\n\twhile (*p) {\n\t\tstruct rb_node **n;\n\n\t\tparent = *p;\n\t\tbfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\n\t\t/*\n\t\t * Sort strictly based on sector. Smallest to the left,\n\t\t * largest to the right.\n\t\t */\n\t\tif (sector > blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_right;\n\t\telse if (sector < blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_left;\n\t\telse\n\t\t\tbreak;\n\t\tp = n;\n\t\tbfqq = NULL;\n\t}\n\n\t*ret_parent = parent;\n\tif (rb_link)\n\t\t*rb_link = p;\n\n\tbfq_log(bfqd, \"rq_pos_tree_lookup %llu: returning %d\",\n\t\t(unsigned long long)sector,\n\t\tbfqq ? bfqq->pid : 0);\n\n\treturn bfqq;\n}\n\nstatic bool bfq_too_late_for_merging(struct bfq_queue *bfqq)\n{\n\treturn bfqq->service_from_backlogged > 0 &&\n\t\ttime_is_before_jiffies(bfqq->first_IO_time +\n\t\t\t\t       bfq_merge_time_limit);\n}\n\n/*\n * The following function is not marked as __cold because it is\n * actually cold, but for the same performance goal described in the\n * comments on the likely() at the beginning of\n * bfq_setup_cooperator(). Unexpectedly, to reach an even lower\n * execution time for the case where this function is not invoked, we\n * had to add an unlikely() in each involved if().\n */\nvoid __cold\nbfq_pos_tree_add_move(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *__bfqq;\n\n\tif (bfqq->pos_root) {\n\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\tbfqq->pos_root = NULL;\n\t}\n\n\t/* oom_bfqq does not participate in queue merging */\n\tif (bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq cannot be merged any longer (see comments in\n\t * bfq_setup_cooperator): no point in adding bfqq into the\n\t * position tree.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn;\n\n\tif (bfq_class_idle(bfqq))\n\t\treturn;\n\tif (!bfqq->next_rq)\n\t\treturn;\n\n\tbfqq->pos_root = &bfqq_group(bfqq)->rq_pos_tree;\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, bfqq->pos_root,\n\t\t\tblk_rq_pos(bfqq->next_rq), &parent, &p);\n\tif (!__bfqq) {\n\t\trb_link_node(&bfqq->pos_node, parent, p);\n\t\trb_insert_color(&bfqq->pos_node, bfqq->pos_root);\n\t} else\n\t\tbfqq->pos_root = NULL;\n}\n\n/*\n * The following function returns false either if every active queue\n * must receive the same share of the throughput (symmetric scenario),\n * or, as a special case, if bfqq must receive a share of the\n * throughput lower than or equal to the share that every other active\n * queue must receive.  If bfqq does sync I/O, then these are the only\n * two cases where bfqq happens to be guaranteed its share of the\n * throughput even if I/O dispatching is not plugged when bfqq remains\n * temporarily empty (for more details, see the comments in the\n * function bfq_better_to_idle()). For this reason, the return value\n * of this function is used to check whether I/O-dispatch plugging can\n * be avoided.\n *\n * The above first case (symmetric scenario) occurs when:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) all active groups at the same level in the groups tree have the same\n *    weight,\n * 4) all active groups at the same level in the groups tree have the same\n *    number of children.\n *\n * Unfortunately, keeping the necessary state for evaluating exactly\n * the last two symmetry sub-conditions above would be quite complex\n * and time consuming. Therefore this function evaluates, instead,\n * only the following stronger three sub-conditions, for which it is\n * much easier to maintain the needed state:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) there is at most one active group.\n * In particular, the last condition is always true if hierarchical\n * support or the cgroups interface are not enabled, thus no state\n * needs to be maintained in this case.\n */\nstatic bool bfq_asymmetric_scenario(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tbool smallest_weight = bfqq &&\n\t\tbfqq->weight_counter &&\n\t\tbfqq->weight_counter ==\n\t\tcontainer_of(\n\t\t\trb_first_cached(&bfqd->queue_weights_tree),\n\t\t\tstruct bfq_weight_counter,\n\t\t\tweights_node);\n\n\t/*\n\t * For queue weights to differ, queue_weights_tree must contain\n\t * at least two nodes.\n\t */\n\tbool varied_queue_weights = !smallest_weight &&\n\t\t!RB_EMPTY_ROOT(&bfqd->queue_weights_tree.rb_root) &&\n\t\t(bfqd->queue_weights_tree.rb_root.rb_node->rb_left ||\n\t\t bfqd->queue_weights_tree.rb_root.rb_node->rb_right);\n\n\tbool multiple_classes_busy =\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[1]) ||\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[2]) ||\n\t\t(bfqd->busy_queues[1] && bfqd->busy_queues[2]);\n\n\treturn varied_queue_weights || multiple_classes_busy\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\t       || bfqd->num_groups_with_pending_reqs > 1\n#endif\n\t\t;\n}\n\n/*\n * If the weight-counter tree passed as input contains no counter for\n * the weight of the input queue, then add that counter; otherwise just\n * increment the existing counter.\n *\n * Note that weight-counter trees contain few nodes in mostly symmetric\n * scenarios. For example, if all queues have the same weight, then the\n * weight-counter tree for the queues may contain at most one node.\n * This holds even if low_latency is on, because weight-raised queues\n * are not inserted in the tree.\n * In most scenarios, the rate at which nodes are created/destroyed\n * should be low too.\n */\nvoid bfq_weights_tree_add(struct bfq_queue *bfqq)\n{\n\tstruct rb_root_cached *root = &bfqq->bfqd->queue_weights_tree;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tbool leftmost = true;\n\n\t/*\n\t * Do not insert if the queue is already associated with a\n\t * counter, which happens if:\n\t *   1) a request arrival has caused the queue to become both\n\t *      non-weight-raised, and hence change its weight, and\n\t *      backlogged; in this respect, each of the two events\n\t *      causes an invocation of this function,\n\t *   2) this is the invocation of this function caused by the\n\t *      second event. This second invocation is actually useless,\n\t *      and we handle this fact by exiting immediately. More\n\t *      efficient or clearer solutions might possibly be adopted.\n\t */\n\tif (bfqq->weight_counter)\n\t\treturn;\n\n\twhile (*new) {\n\t\tstruct bfq_weight_counter *__counter = container_of(*new,\n\t\t\t\t\t\tstruct bfq_weight_counter,\n\t\t\t\t\t\tweights_node);\n\t\tparent = *new;\n\n\t\tif (entity->weight == __counter->weight) {\n\t\t\tbfqq->weight_counter = __counter;\n\t\t\tgoto inc_counter;\n\t\t}\n\t\tif (entity->weight < __counter->weight)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\tbfqq->weight_counter = kzalloc(sizeof(struct bfq_weight_counter),\n\t\t\t\t       GFP_ATOMIC);\n\n\t/*\n\t * In the unlucky event of an allocation failure, we just\n\t * exit. This will cause the weight of queue to not be\n\t * considered in bfq_asymmetric_scenario, which, in its turn,\n\t * causes the scenario to be deemed wrongly symmetric in case\n\t * bfqq's weight would have been the only weight making the\n\t * scenario asymmetric.  On the bright side, no unbalance will\n\t * however occur when bfqq becomes inactive again (the\n\t * invocation of this function is triggered by an activation\n\t * of queue).  In fact, bfq_weights_tree_remove does nothing\n\t * if !bfqq->weight_counter.\n\t */\n\tif (unlikely(!bfqq->weight_counter))\n\t\treturn;\n\n\tbfqq->weight_counter->weight = entity->weight;\n\trb_link_node(&bfqq->weight_counter->weights_node, parent, new);\n\trb_insert_color_cached(&bfqq->weight_counter->weights_node, root,\n\t\t\t\tleftmost);\n\ninc_counter:\n\tbfqq->weight_counter->num_active++;\n\tbfqq->ref++;\n}\n\n/*\n * Decrement the weight counter associated with the queue, and, if the\n * counter reaches 0, remove the counter from the tree.\n * See the comments to the function bfq_weights_tree_add() for considerations\n * about overhead.\n */\nvoid bfq_weights_tree_remove(struct bfq_queue *bfqq)\n{\n\tstruct rb_root_cached *root;\n\n\tif (!bfqq->weight_counter)\n\t\treturn;\n\n\troot = &bfqq->bfqd->queue_weights_tree;\n\tbfqq->weight_counter->num_active--;\n\tif (bfqq->weight_counter->num_active > 0)\n\t\tgoto reset_entity_pointer;\n\n\trb_erase_cached(&bfqq->weight_counter->weights_node, root);\n\tkfree(bfqq->weight_counter);\n\nreset_entity_pointer:\n\tbfqq->weight_counter = NULL;\n\tbfq_put_queue(bfqq);\n}\n\n/*\n * Return expired entry, or NULL to just start from scratch in rbtree.\n */\nstatic struct request *bfq_check_fifo(struct bfq_queue *bfqq,\n\t\t\t\t      struct request *last)\n{\n\tstruct request *rq;\n\n\tif (bfq_bfqq_fifo_expire(bfqq))\n\t\treturn NULL;\n\n\tbfq_mark_bfqq_fifo_expire(bfqq);\n\n\trq = rq_entry_fifo(bfqq->fifo.next);\n\n\tif (rq == last || ktime_get_ns() < rq->fifo_time)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"check_fifo: returned %p\", rq);\n\treturn rq;\n}\n\nstatic struct request *bfq_find_next_rq(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\tstruct request *last)\n{\n\tstruct rb_node *rbnext = rb_next(&last->rb_node);\n\tstruct rb_node *rbprev = rb_prev(&last->rb_node);\n\tstruct request *next, *prev = NULL;\n\n\t/* Follow expired path, else get first next available. */\n\tnext = bfq_check_fifo(bfqq, last);\n\tif (next)\n\t\treturn next;\n\n\tif (rbprev)\n\t\tprev = rb_entry_rq(rbprev);\n\n\tif (rbnext)\n\t\tnext = rb_entry_rq(rbnext);\n\telse {\n\t\trbnext = rb_first(&bfqq->sort_list);\n\t\tif (rbnext && rbnext != &last->rb_node)\n\t\t\tnext = rb_entry_rq(rbnext);\n\t}\n\n\treturn bfq_choose_req(bfqd, next, prev, blk_rq_pos(last));\n}\n\n/* see the definition of bfq_async_charge_factor for details */\nstatic unsigned long bfq_serv_to_charge(struct request *rq,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\tif (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||\n\t    bfq_asymmetric_scenario(bfqq->bfqd, bfqq))\n\t\treturn blk_rq_sectors(rq);\n\n\treturn blk_rq_sectors(rq) * bfq_async_charge_factor;\n}\n\n/**\n * bfq_updated_next_req - update the queue after a new next_rq selection.\n * @bfqd: the device data the queue belongs to.\n * @bfqq: the queue to update.\n *\n * If the first request of a queue changes we make sure that the queue\n * has enough budget to serve at least its first request (if the\n * request has grown).  We do this because if the queue has not enough\n * budget for its first request, it has to go through two dispatch\n * rounds to actually get it dispatched.\n */\nstatic void bfq_updated_next_req(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct request *next_rq = bfqq->next_rq;\n\tunsigned long new_budget;\n\n\tif (!next_rq)\n\t\treturn;\n\n\tif (bfqq == bfqd->in_service_queue)\n\t\t/*\n\t\t * In order not to break guarantees, budgets cannot be\n\t\t * changed after an entity has been selected.\n\t\t */\n\t\treturn;\n\n\tnew_budget = max_t(unsigned long,\n\t\t\t   max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t bfq_serv_to_charge(next_rq, bfqq)),\n\t\t\t   entity->service);\n\tif (entity->budget != new_budget) {\n\t\tentity->budget = new_budget;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"updated next rq: new budget %lu\",\n\t\t\t\t\t new_budget);\n\t\tbfq_requeue_bfqq(bfqd, bfqq, false);\n\t}\n}\n\nstatic unsigned int bfq_wr_duration(struct bfq_data *bfqd)\n{\n\tu64 dur;\n\n\tdur = bfqd->rate_dur_prod;\n\tdo_div(dur, bfqd->peak_rate);\n\n\t/*\n\t * Limit duration between 3 and 25 seconds. The upper limit\n\t * has been conservatively set after the following worst case:\n\t * on a QEMU/KVM virtual machine\n\t * - running in a slow PC\n\t * - with a virtual disk stacked on a slow low-end 5400rpm HDD\n\t * - serving a heavy I/O workload, such as the sequential reading\n\t *   of several files\n\t * mplayer took 23 seconds to start, if constantly weight-raised.\n\t *\n\t * As for higher values than that accommodating the above bad\n\t * scenario, tests show that higher values would often yield\n\t * the opposite of the desired result, i.e., would worsen\n\t * responsiveness by allowing non-interactive applications to\n\t * preserve weight raising for too long.\n\t *\n\t * On the other end, lower values than 3 seconds make it\n\t * difficult for most interactive tasks to complete their jobs\n\t * before weight-raising finishes.\n\t */\n\treturn clamp_val(dur, msecs_to_jiffies(3000), msecs_to_jiffies(25000));\n}\n\n/* switch back from soft real-time to interactive weight raising */\nstatic void switch_back_to_interactive_wr(struct bfq_queue *bfqq,\n\t\t\t\t\t  struct bfq_data *bfqd)\n{\n\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\tbfqq->last_wr_start_finish = bfqq->wr_start_at_switch_to_srt;\n}\n\nstatic void\nbfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,\n\t\t      struct bfq_io_cq *bic, bool bfq_already_existing)\n{\n\tunsigned int old_wr_coeff = 1;\n\tbool busy = bfq_already_existing && bfq_bfqq_busy(bfqq);\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\tif (bfqq_data->saved_has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\tif (bfqq_data->saved_IO_bound)\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\tbfqq->last_serv_time_ns = bfqq_data->saved_last_serv_time_ns;\n\tbfqq->inject_limit = bfqq_data->saved_inject_limit;\n\tbfqq->decrease_time_jif = bfqq_data->saved_decrease_time_jif;\n\n\tbfqq->entity.new_weight = bfqq_data->saved_weight;\n\tbfqq->ttime = bfqq_data->saved_ttime;\n\tbfqq->io_start_time = bfqq_data->saved_io_start_time;\n\tbfqq->tot_idle_time = bfqq_data->saved_tot_idle_time;\n\t/*\n\t * Restore weight coefficient only if low_latency is on\n\t */\n\tif (bfqd->low_latency) {\n\t\told_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq->wr_coeff = bfqq_data->saved_wr_coeff;\n\t}\n\tbfqq->service_from_wr = bfqq_data->saved_service_from_wr;\n\tbfqq->wr_start_at_switch_to_srt =\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt;\n\tbfqq->last_wr_start_finish = bfqq_data->saved_last_wr_start_finish;\n\tbfqq->wr_cur_max_time = bfqq_data->saved_wr_cur_max_time;\n\n\tif (bfqq->wr_coeff > 1 && (bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t   bfqq->wr_cur_max_time))) {\n\t\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t    time_is_after_eq_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t     bfq_wr_duration(bfqd))) {\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t} else {\n\t\t\tbfqq->wr_coeff = 1;\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t     \"resume state: switching off wr\");\n\t\t}\n\t}\n\n\t/* make sure weight will be updated, however we got here */\n\tbfqq->entity.prio_changed = 1;\n\n\tif (likely(!busy))\n\t\treturn;\n\n\tif (old_wr_coeff == 1 && bfqq->wr_coeff > 1)\n\t\tbfqd->wr_busy_queues++;\n\telse if (old_wr_coeff > 1 && bfqq->wr_coeff == 1)\n\t\tbfqd->wr_busy_queues--;\n}\n\nstatic int bfqq_process_refs(struct bfq_queue *bfqq)\n{\n\treturn bfqq->ref - bfqq->entity.allocated -\n\t\tbfqq->entity.on_st_or_in_serv -\n\t\t(bfqq->weight_counter != NULL) - bfqq->stable_ref;\n}\n\n/* Empty burst list and add just bfqq (see comments on bfq_handle_burst) */\nstatic void bfq_reset_burst_list(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(item, n, &bfqd->burst_list, burst_list_node)\n\t\thlist_del_init(&item->burst_list_node);\n\n\t/*\n\t * Start the creation of a new burst list only if there is no\n\t * active queue. See comments on the conditional invocation of\n\t * bfq_handle_burst().\n\t */\n\tif (bfq_tot_busy_queues(bfqd) == 0) {\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n\t\tbfqd->burst_size = 1;\n\t} else\n\t\tbfqd->burst_size = 0;\n\n\tbfqd->burst_parent_entity = bfqq->entity.parent;\n}\n\n/* Add bfqq to the list of queues in current burst (see bfq_handle_burst) */\nstatic void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/* Increment burst size to take into account also bfqq */\n\tbfqd->burst_size++;\n\n\tif (bfqd->burst_size == bfqd->bfq_large_burst_thresh) {\n\t\tstruct bfq_queue *pos, *bfqq_item;\n\t\tstruct hlist_node *n;\n\n\t\t/*\n\t\t * Enough queues have been activated shortly after each\n\t\t * other to consider this burst as large.\n\t\t */\n\t\tbfqd->large_burst = true;\n\n\t\t/*\n\t\t * We can now mark all queues in the burst list as\n\t\t * belonging to a large burst.\n\t\t */\n\t\thlist_for_each_entry(bfqq_item, &bfqd->burst_list,\n\t\t\t\t     burst_list_node)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq_item);\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\n\t\t/*\n\t\t * From now on, and until the current burst finishes, any\n\t\t * new queue being activated shortly after the last queue\n\t\t * was inserted in the burst can be immediately marked as\n\t\t * belonging to a large burst. So the burst list is not\n\t\t * needed any more. Remove it.\n\t\t */\n\t\thlist_for_each_entry_safe(pos, n, &bfqd->burst_list,\n\t\t\t\t\t  burst_list_node)\n\t\t\thlist_del_init(&pos->burst_list_node);\n\t} else /*\n\t\t* Burst not yet large: add bfqq to the burst list. Do\n\t\t* not increment the ref counter for bfqq, because bfqq\n\t\t* is removed from the burst list before freeing bfqq\n\t\t* in put_queue.\n\t\t*/\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n}\n\n/*\n * If many queues belonging to the same group happen to be created\n * shortly after each other, then the processes associated with these\n * queues have typically a common goal. In particular, bursts of queue\n * creations are usually caused by services or applications that spawn\n * many parallel threads/processes. Examples are systemd during boot,\n * or git grep. To help these processes get their job done as soon as\n * possible, it is usually better to not grant either weight-raising\n * or device idling to their queues, unless these queues must be\n * protected from the I/O flowing through other active queues.\n *\n * In this comment we describe, firstly, the reasons why this fact\n * holds, and, secondly, the next function, which implements the main\n * steps needed to properly mark these queues so that they can then be\n * treated in a different way.\n *\n * The above services or applications benefit mostly from a high\n * throughput: the quicker the requests of the activated queues are\n * cumulatively served, the sooner the target job of these queues gets\n * completed. As a consequence, weight-raising any of these queues,\n * which also implies idling the device for it, is almost always\n * counterproductive, unless there are other active queues to isolate\n * these new queues from. If there no other active queues, then\n * weight-raising these new queues just lowers throughput in most\n * cases.\n *\n * On the other hand, a burst of queue creations may be caused also by\n * the start of an application that does not consist of a lot of\n * parallel I/O-bound threads. In fact, with a complex application,\n * several short processes may need to be executed to start-up the\n * application. In this respect, to start an application as quickly as\n * possible, the best thing to do is in any case to privilege the I/O\n * related to the application with respect to all other\n * I/O. Therefore, the best strategy to start as quickly as possible\n * an application that causes a burst of queue creations is to\n * weight-raise all the queues created during the burst. This is the\n * exact opposite of the best strategy for the other type of bursts.\n *\n * In the end, to take the best action for each of the two cases, the\n * two types of bursts need to be distinguished. Fortunately, this\n * seems relatively easy, by looking at the sizes of the bursts. In\n * particular, we found a threshold such that only bursts with a\n * larger size than that threshold are apparently caused by\n * services or commands such as systemd or git grep. For brevity,\n * hereafter we call just 'large' these bursts. BFQ *does not*\n * weight-raise queues whose creation occurs in a large burst. In\n * addition, for each of these queues BFQ performs or does not perform\n * idling depending on which choice boosts the throughput more. The\n * exact choice depends on the device and request pattern at\n * hand.\n *\n * Unfortunately, false positives may occur while an interactive task\n * is starting (e.g., an application is being started). The\n * consequence is that the queues associated with the task do not\n * enjoy weight raising as expected. Fortunately these false positives\n * are very rare. They typically occur if some service happens to\n * start doing I/O exactly when the interactive task starts.\n *\n * Turning back to the next function, it is invoked only if there are\n * no active queues (apart from active queues that would belong to the\n * same, possible burst bfqq would belong to), and it implements all\n * the steps needed to detect the occurrence of a large burst and to\n * properly mark all the queues belonging to it (so that they can then\n * be treated in a different way). This goal is achieved by\n * maintaining a \"burst list\" that holds, temporarily, the queues that\n * belong to the burst in progress. The list is then used to mark\n * these queues as belonging to a large burst if the burst does become\n * large. The main steps are the following.\n *\n * . when the very first queue is created, the queue is inserted into the\n *   list (as it could be the first queue in a possible burst)\n *\n * . if the current burst has not yet become large, and a queue Q that does\n *   not yet belong to the burst is activated shortly after the last time\n *   at which a new queue entered the burst list, then the function appends\n *   Q to the burst list\n *\n * . if, as a consequence of the previous step, the burst size reaches\n *   the large-burst threshold, then\n *\n *     . all the queues in the burst list are marked as belonging to a\n *       large burst\n *\n *     . the burst list is deleted; in fact, the burst list already served\n *       its purpose (keeping temporarily track of the queues in a burst,\n *       so as to be able to mark them as belonging to a large burst in the\n *       previous sub-step), and now is not needed any more\n *\n *     . the device enters a large-burst mode\n *\n * . if a queue Q that does not belong to the burst is created while\n *   the device is in large-burst mode and shortly after the last time\n *   at which a queue either entered the burst list or was marked as\n *   belonging to the current large burst, then Q is immediately marked\n *   as belonging to a large burst.\n *\n * . if a queue Q that does not belong to the burst is created a while\n *   later, i.e., not shortly after, than the last time at which a queue\n *   either entered the burst list or was marked as belonging to the\n *   current large burst, then the current burst is deemed as finished and:\n *\n *        . the large-burst mode is reset if set\n *\n *        . the burst list is emptied\n *\n *        . Q is inserted in the burst list, as Q may be the first queue\n *          in a possible new burst (then the burst list contains just Q\n *          after this step).\n */\nstatic void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq is already in the burst list or is part of a large\n\t * burst, or finally has just been split, then there is\n\t * nothing else to do.\n\t */\n\tif (!hlist_unhashed(&bfqq->burst_list_node) ||\n\t    bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     msecs_to_jiffies(10)))\n\t\treturn;\n\n\t/*\n\t * If bfqq's creation happens late enough, or bfqq belongs to\n\t * a different group than the burst group, then the current\n\t * burst is finished, and related data structures must be\n\t * reset.\n\t *\n\t * In this respect, consider the special case where bfqq is\n\t * the very first queue created after BFQ is selected for this\n\t * device. In this case, last_ins_in_burst and\n\t * burst_parent_entity are not yet significant when we get\n\t * here. But it is easy to verify that, whether or not the\n\t * following condition is true, bfqq will end up being\n\t * inserted into the burst list. In particular the list will\n\t * happen to contain only bfqq. And this is exactly what has\n\t * to happen, as bfqq may be the first queue of the first\n\t * burst.\n\t */\n\tif (time_is_before_jiffies(bfqd->last_ins_in_burst +\n\t    bfqd->bfq_burst_interval) ||\n\t    bfqq->entity.parent != bfqd->burst_parent_entity) {\n\t\tbfqd->large_burst = false;\n\t\tbfq_reset_burst_list(bfqd, bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then bfqq is being activated shortly after the\n\t * last queue. So, if the current burst is also large, we can mark\n\t * bfqq as belonging to this large burst immediately.\n\t */\n\tif (bfqd->large_burst) {\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then a large-burst state has not yet been\n\t * reached, but bfqq is being activated shortly after the last\n\t * queue. Then we add bfqq to the burst.\n\t */\n\tbfq_add_to_burst(bfqd, bfqq);\nend:\n\t/*\n\t * At this point, bfqq either has been added to the current\n\t * burst or has caused the current burst to terminate and a\n\t * possible new burst to start. In particular, in the second\n\t * case, bfqq has become the first queue in the possible new\n\t * burst.  In both cases last_ins_in_burst needs to be moved\n\t * forward.\n\t */\n\tbfqd->last_ins_in_burst = jiffies;\n}\n\nstatic int bfq_bfqq_budget_left(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\treturn entity->budget - entity->service;\n}\n\n/*\n * If enough samples have been computed, return the current max budget\n * stored in bfqd, which is dynamically updated according to the\n * estimated disk peak rate; otherwise return the default max budget\n */\nstatic int bfq_max_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget;\n\telse\n\t\treturn bfqd->bfq_max_budget;\n}\n\n/*\n * Return min budget, which is a fraction of the current or default\n * max budget (trying with 1/32)\n */\nstatic int bfq_min_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget / 32;\n\telse\n\t\treturn bfqd->bfq_max_budget / 32;\n}\n\n/*\n * The next function, invoked after the input queue bfqq switches from\n * idle to busy, updates the budget of bfqq. The function also tells\n * whether the in-service queue should be expired, by returning\n * true. The purpose of expiring the in-service queue is to give bfqq\n * the chance to possibly preempt the in-service queue, and the reason\n * for preempting the in-service queue is to achieve one of the two\n * goals below.\n *\n * 1. Guarantee to bfqq its reserved bandwidth even if bfqq has\n * expired because it has remained idle. In particular, bfqq may have\n * expired for one of the following two reasons:\n *\n * - BFQQE_NO_MORE_REQUESTS bfqq did not enjoy any device idling\n *   and did not make it to issue a new request before its last\n *   request was served;\n *\n * - BFQQE_TOO_IDLE bfqq did enjoy device idling, but did not issue\n *   a new request before the expiration of the idling-time.\n *\n * Even if bfqq has expired for one of the above reasons, the process\n * associated with the queue may be however issuing requests greedily,\n * and thus be sensitive to the bandwidth it receives (bfqq may have\n * remained idle for other reasons: CPU high load, bfqq not enjoying\n * idling, I/O throttling somewhere in the path from the process to\n * the I/O scheduler, ...). But if, after every expiration for one of\n * the above two reasons, bfqq has to wait for the service of at least\n * one full budget of another queue before being served again, then\n * bfqq is likely to get a much lower bandwidth or resource time than\n * its reserved ones. To address this issue, two countermeasures need\n * to be taken.\n *\n * First, the budget and the timestamps of bfqq need to be updated in\n * a special way on bfqq reactivation: they need to be updated as if\n * bfqq did not remain idle and did not expire. In fact, if they are\n * computed as if bfqq expired and remained idle until reactivation,\n * then the process associated with bfqq is treated as if, instead of\n * being greedy, it stopped issuing requests when bfqq remained idle,\n * and restarts issuing requests only on this reactivation. In other\n * words, the scheduler does not help the process recover the \"service\n * hole\" between bfqq expiration and reactivation. As a consequence,\n * the process receives a lower bandwidth than its reserved one. In\n * contrast, to recover this hole, the budget must be updated as if\n * bfqq was not expired at all before this reactivation, i.e., it must\n * be set to the value of the remaining budget when bfqq was\n * expired. Along the same line, timestamps need to be assigned the\n * value they had the last time bfqq was selected for service, i.e.,\n * before last expiration. Thus timestamps need to be back-shifted\n * with respect to their normal computation (see [1] for more details\n * on this tricky aspect).\n *\n * Secondly, to allow the process to recover the hole, the in-service\n * queue must be expired too, to give bfqq the chance to preempt it\n * immediately. In fact, if bfqq has to wait for a full budget of the\n * in-service queue to be completed, then it may become impossible to\n * let the process recover the hole, even if the back-shifted\n * timestamps of bfqq are lower than those of the in-service queue. If\n * this happens for most or all of the holes, then the process may not\n * receive its reserved bandwidth. In this respect, it is worth noting\n * that, being the service of outstanding requests unpreemptible, a\n * little fraction of the holes may however be unrecoverable, thereby\n * causing a little loss of bandwidth.\n *\n * The last important point is detecting whether bfqq does need this\n * bandwidth recovery. In this respect, the next function deems the\n * process associated with bfqq greedy, and thus allows it to recover\n * the hole, if: 1) the process is waiting for the arrival of a new\n * request (which implies that bfqq expired for one of the above two\n * reasons), and 2) such a request has arrived soon. The first\n * condition is controlled through the flag non_blocking_wait_rq,\n * while the second through the flag arrived_in_time. If both\n * conditions hold, then the function computes the budget in the\n * above-described special way, and signals that the in-service queue\n * should be expired. Timestamp back-shifting is done later in\n * __bfq_activate_entity.\n *\n * 2. Reduce latency. Even if timestamps are not backshifted to let\n * the process associated with bfqq recover a service hole, bfqq may\n * however happen to have, after being (re)activated, a lower finish\n * timestamp than the in-service queue.\t That is, the next budget of\n * bfqq may have to be completed before the one of the in-service\n * queue. If this is the case, then preempting the in-service queue\n * allows this goal to be achieved, apart from the unpreemptible,\n * outstanding requests mentioned above.\n *\n * Unfortunately, regardless of which of the above two goals one wants\n * to achieve, service trees need first to be updated to know whether\n * the in-service queue must be preempted. To have service trees\n * correctly updated, the in-service queue must be expired and\n * rescheduled, and bfqq must be scheduled too. This is one of the\n * most costly operations (in future versions, the scheduling\n * mechanism may be re-designed in such a way to make it possible to\n * know whether preemption is needed without needing to update service\n * trees). In addition, queue preemptions almost always cause random\n * I/O, which may in turn cause loss of throughput. Finally, there may\n * even be no in-service queue when the next function is invoked (so,\n * no queue to compare timestamps with). Because of these facts, the\n * next function adopts the following simple scheme to avoid costly\n * operations, too frequent preemptions and too many dependencies on\n * the state of the scheduler: it requests the expiration of the\n * in-service queue (unconditionally) only for queues that need to\n * recover a hole. Then it delegates to other parts of the code the\n * responsibility of handling the above case 2.\n */\nstatic bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\t\tbool arrived_in_time)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * In the next compound condition, we check also whether there\n\t * is some budget left, because otherwise there is no point in\n\t * trying to go on serving bfqq with this same budget: bfqq\n\t * would be expired immediately after being selected for\n\t * service. This would only cause useless overhead.\n\t */\n\tif (bfq_bfqq_non_blocking_wait_rq(bfqq) && arrived_in_time &&\n\t    bfq_bfqq_budget_left(bfqq) > 0) {\n\t\t/*\n\t\t * We do not clear the flag non_blocking_wait_rq here, as\n\t\t * the latter is used in bfq_activate_bfqq to signal\n\t\t * that timestamps need to be back-shifted (and is\n\t\t * cleared right after).\n\t\t */\n\n\t\t/*\n\t\t * In next assignment we rely on that either\n\t\t * entity->service or entity->budget are not updated\n\t\t * on expiration if bfqq is empty (see\n\t\t * __bfq_bfqq_recalc_budget). Thus both quantities\n\t\t * remain unchanged after such an expiration, and the\n\t\t * following statement therefore assigns to\n\t\t * entity->budget the remaining budget on such an\n\t\t * expiration.\n\t\t */\n\t\tentity->budget = min_t(unsigned long,\n\t\t\t\t       bfq_bfqq_budget_left(bfqq),\n\t\t\t\t       bfqq->max_budget);\n\n\t\t/*\n\t\t * At this point, we have used entity->service to get\n\t\t * the budget left (needed for updating\n\t\t * entity->budget). Thus we finally can, and have to,\n\t\t * reset entity->service. The latter must be reset\n\t\t * because bfqq would otherwise be charged again for\n\t\t * the service it has received during its previous\n\t\t * service slot(s).\n\t\t */\n\t\tentity->service = 0;\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * We can finally complete expiration, by setting service to 0.\n\t */\n\tentity->service = 0;\n\tentity->budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t       bfq_serv_to_charge(bfqq->next_rq, bfqq));\n\tbfq_clear_bfqq_non_blocking_wait_rq(bfqq);\n\treturn false;\n}\n\n/*\n * Return the farthest past time instant according to jiffies\n * macros.\n */\nstatic unsigned long bfq_smallest_from_now(void)\n{\n\treturn jiffies - MAX_JIFFY_OFFSET;\n}\n\nstatic void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     unsigned int old_wr_coeff,\n\t\t\t\t\t     bool wr_or_deserves_wr,\n\t\t\t\t\t     bool interactive,\n\t\t\t\t\t     bool in_burst,\n\t\t\t\t\t     bool soft_rt)\n{\n\tif (old_wr_coeff == 1 && wr_or_deserves_wr) {\n\t\t/* start a weight-raising period */\n\t\tif (interactive) {\n\t\t\tbfqq->service_from_wr = 0;\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else {\n\t\t\t/*\n\t\t\t * No interactive weight raising in progress\n\t\t\t * here: assign minus infinity to\n\t\t\t * wr_start_at_switch_to_srt, to make sure\n\t\t\t * that, at the end of the soft-real-time\n\t\t\t * weight raising periods that is starting\n\t\t\t * now, no interactive weight-raising period\n\t\t\t * may be wrongly considered as still in\n\t\t\t * progress (and thus actually started by\n\t\t\t * mistake).\n\t\t\t */\n\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\tbfq_smallest_from_now();\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t}\n\n\t\t/*\n\t\t * If needed, further reduce budget to make sure it is\n\t\t * close to bfqq's backlog, so as to reduce the\n\t\t * scheduling-error component due to a too large\n\t\t * budget. Do not care about throughput consequences,\n\t\t * but only about latency. Finally, do not assign a\n\t\t * too small budget either, to avoid increasing\n\t\t * latency by causing too frequent expirations.\n\t\t */\n\t\tbfqq->entity.budget = min_t(unsigned long,\n\t\t\t\t\t    bfqq->entity.budget,\n\t\t\t\t\t    2 * bfq_min_budget(bfqd));\n\t} else if (old_wr_coeff > 1) {\n\t\tif (interactive) { /* update wr coeff and duration */\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else if (in_burst)\n\t\t\tbfqq->wr_coeff = 1;\n\t\telse if (soft_rt) {\n\t\t\t/*\n\t\t\t * The application is now or still meeting the\n\t\t\t * requirements for being deemed soft rt.  We\n\t\t\t * can then correctly and safely (re)charge\n\t\t\t * the weight-raising duration for the\n\t\t\t * application with the weight-raising\n\t\t\t * duration for soft rt applications.\n\t\t\t *\n\t\t\t * In particular, doing this recharge now, i.e.,\n\t\t\t * before the weight-raising period for the\n\t\t\t * application finishes, reduces the probability\n\t\t\t * of the following negative scenario:\n\t\t\t * 1) the weight of a soft rt application is\n\t\t\t *    raised at startup (as for any newly\n\t\t\t *    created application),\n\t\t\t * 2) since the application is not interactive,\n\t\t\t *    at a certain time weight-raising is\n\t\t\t *    stopped for the application,\n\t\t\t * 3) at that time the application happens to\n\t\t\t *    still have pending requests, and hence\n\t\t\t *    is destined to not have a chance to be\n\t\t\t *    deemed soft rt before these requests are\n\t\t\t *    completed (see the comments to the\n\t\t\t *    function bfq_bfqq_softrt_next_start()\n\t\t\t *    for details on soft rt detection),\n\t\t\t * 4) these pending requests experience a high\n\t\t\t *    latency because the application is not\n\t\t\t *    weight-raised while they are pending.\n\t\t\t */\n\t\t\tif (bfqq->wr_cur_max_time !=\n\t\t\t\tbfqd->bfq_wr_rt_max_time) {\n\t\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\t\tbfqq->last_wr_start_finish;\n\n\t\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\t}\n\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\t}\n}\n\nstatic bool bfq_bfqq_idle_for_long_time(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn bfqq->dispatched == 0 &&\n\t\ttime_is_before_jiffies(\n\t\t\tbfqq->budget_timeout +\n\t\t\tbfqd->bfq_wr_min_idle_time);\n}\n\n\n/*\n * Return true if bfqq is in a higher priority class, or has a higher\n * weight than the in-service queue.\n */\nstatic bool bfq_bfqq_higher_class_or_weight(struct bfq_queue *bfqq,\n\t\t\t\t\t    struct bfq_queue *in_serv_bfqq)\n{\n\tint bfqq_weight, in_serv_weight;\n\n\tif (bfqq->ioprio_class < in_serv_bfqq->ioprio_class)\n\t\treturn true;\n\n\tif (in_serv_bfqq->entity.parent == bfqq->entity.parent) {\n\t\tbfqq_weight = bfqq->entity.weight;\n\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t} else {\n\t\tif (bfqq->entity.parent)\n\t\t\tbfqq_weight = bfqq->entity.parent->weight;\n\t\telse\n\t\t\tbfqq_weight = bfqq->entity.weight;\n\t\tif (in_serv_bfqq->entity.parent)\n\t\t\tin_serv_weight = in_serv_bfqq->entity.parent->weight;\n\t\telse\n\t\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t}\n\n\treturn bfqq_weight > in_serv_weight;\n}\n\n/*\n * Get the index of the actuator that will serve bio.\n */\nstatic unsigned int bfq_actuator_index(struct bfq_data *bfqd, struct bio *bio)\n{\n\tunsigned int i;\n\tsector_t end;\n\n\t/* no search needed if one or zero ranges present */\n\tif (bfqd->num_actuators == 1)\n\t\treturn 0;\n\n\t/* bio_end_sector(bio) gives the sector after the last one */\n\tend = bio_end_sector(bio) - 1;\n\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tif (end >= bfqd->sector[i] &&\n\t\t    end < bfqd->sector[i] + bfqd->nr_sectors[i])\n\t\t\treturn i;\n\t}\n\n\tWARN_ONCE(true,\n\t\t  \"bfq_actuator_index: bio sector out of ranges: end=%llu\\n\",\n\t\t  end);\n\treturn 0;\n}\n\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq);\n\nstatic void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     int old_wr_coeff,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     bool *interactive)\n{\n\tbool soft_rt, in_burst,\twr_or_deserves_wr,\n\t\tbfqq_wants_to_preempt,\n\t\tidle_for_long_time = bfq_bfqq_idle_for_long_time(bfqd, bfqq),\n\t\t/*\n\t\t * See the comments on\n\t\t * bfq_bfqq_update_budg_for_activation for\n\t\t * details on the usage of the next variable.\n\t\t */\n\t\tarrived_in_time =  ktime_get_ns() <=\n\t\t\tbfqq->ttime.last_end_request +\n\t\t\tbfqd->bfq_slice_idle * 3;\n\tunsigned int act_idx = bfq_actuator_index(bfqd, rq->bio);\n\tbool bfqq_non_merged_or_stably_merged =\n\t\tbfqq->bic || RQ_BIC(rq)->bfqq_data[act_idx].stably_merged;\n\n\t/*\n\t * bfqq deserves to be weight-raised if:\n\t * - it is sync,\n\t * - it does not belong to a large burst,\n\t * - it has been idle for enough time or is soft real-time,\n\t * - is linked to a bfq_io_cq (it is not shared in any sense),\n\t * - has a default weight (otherwise we assume the user wanted\n\t *   to control its weight explicitly)\n\t */\n\tin_burst = bfq_bfqq_in_large_burst(bfqq);\n\tsoft_rt = bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t\t!BFQQ_TOTALLY_SEEKY(bfqq) &&\n\t\t!in_burst &&\n\t\ttime_is_before_jiffies(bfqq->soft_rt_next_start) &&\n\t\tbfqq->dispatched == 0 &&\n\t\tbfqq->entity.new_weight == 40;\n\t*interactive = !in_burst && idle_for_long_time &&\n\t\tbfqq->entity.new_weight == 40;\n\t/*\n\t * Merged bfq_queues are kept out of weight-raising\n\t * (low-latency) mechanisms. The reason is that these queues\n\t * are usually created for non-interactive and\n\t * non-soft-real-time tasks. Yet this is not the case for\n\t * stably-merged queues. These queues are merged just because\n\t * they are created shortly after each other. So they may\n\t * easily serve the I/O of an interactive or soft-real time\n\t * application, if the application happens to spawn multiple\n\t * processes. So let also stably-merged queued enjoy weight\n\t * raising.\n\t */\n\twr_or_deserves_wr = bfqd->low_latency &&\n\t\t(bfqq->wr_coeff > 1 ||\n\t\t (bfq_bfqq_sync(bfqq) && bfqq_non_merged_or_stably_merged &&\n\t\t  (*interactive || soft_rt)));\n\n\t/*\n\t * Using the last flag, update budget and check whether bfqq\n\t * may want to preempt the in-service queue.\n\t */\n\tbfqq_wants_to_preempt =\n\t\tbfq_bfqq_update_budg_for_activation(bfqd, bfqq,\n\t\t\t\t\t\t    arrived_in_time);\n\n\t/*\n\t * If bfqq happened to be activated in a burst, but has been\n\t * idle for much more than an interactive queue, then we\n\t * assume that, in the overall I/O initiated in the burst, the\n\t * I/O associated with bfqq is finished. So bfqq does not need\n\t * to be treated as a queue belonging to a burst\n\t * anymore. Accordingly, we reset bfqq's in_large_burst flag\n\t * if set, and remove bfqq from the burst list if it's\n\t * there. We do not decrement burst_size, because the fact\n\t * that bfqq does not need to belong to the burst list any\n\t * more does not invalidate the fact that bfqq was created in\n\t * a burst.\n\t */\n\tif (likely(!bfq_bfqq_just_created(bfqq)) &&\n\t    idle_for_long_time &&\n\t    time_is_before_jiffies(\n\t\t    bfqq->budget_timeout +\n\t\t    msecs_to_jiffies(10000))) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t}\n\n\tbfq_clear_bfqq_just_created(bfqq);\n\n\tif (bfqd->low_latency) {\n\t\tif (unlikely(time_is_after_jiffies(bfqq->split_time)))\n\t\t\t/* wraparound */\n\t\t\tbfqq->split_time =\n\t\t\t\tjiffies - bfqd->bfq_wr_min_idle_time - 1;\n\n\t\tif (time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t   bfqd->bfq_wr_min_idle_time)) {\n\t\t\tbfq_update_bfqq_wr_on_rq_arrival(bfqd, bfqq,\n\t\t\t\t\t\t\t old_wr_coeff,\n\t\t\t\t\t\t\t wr_or_deserves_wr,\n\t\t\t\t\t\t\t *interactive,\n\t\t\t\t\t\t\t in_burst,\n\t\t\t\t\t\t\t soft_rt);\n\n\t\t\tif (old_wr_coeff != bfqq->wr_coeff)\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n\n\tbfqq->last_idle_bklogged = jiffies;\n\tbfqq->service_from_backlogged = 0;\n\tbfq_clear_bfqq_softrt_update(bfqq);\n\n\tbfq_add_bfqq_busy(bfqq);\n\n\t/*\n\t * Expire in-service queue if preemption may be needed for\n\t * guarantees or throughput. As for guarantees, we care\n\t * explicitly about two cases. The first is that bfqq has to\n\t * recover a service hole, as explained in the comments on\n\t * bfq_bfqq_update_budg_for_activation(), i.e., that\n\t * bfqq_wants_to_preempt is true. However, if bfqq does not\n\t * carry time-critical I/O, then bfqq's bandwidth is less\n\t * important than that of queues that carry time-critical I/O.\n\t * So, as a further constraint, we consider this case only if\n\t * bfqq is at least as weight-raised, i.e., at least as time\n\t * critical, as the in-service queue.\n\t *\n\t * The second case is that bfqq is in a higher priority class,\n\t * or has a higher weight than the in-service queue. If this\n\t * condition does not hold, we don't care because, even if\n\t * bfqq does not start to be served immediately, the resulting\n\t * delay for bfqq's I/O is however lower or much lower than\n\t * the ideal completion time to be guaranteed to bfqq's I/O.\n\t *\n\t * In both cases, preemption is needed only if, according to\n\t * the timestamps of both bfqq and of the in-service queue,\n\t * bfqq actually is the next queue to serve. So, to reduce\n\t * useless preemptions, the return value of\n\t * next_queue_may_preempt() is considered in the next compound\n\t * condition too. Yet next_queue_may_preempt() just checks a\n\t * simple, necessary condition for bfqq to be the next queue\n\t * to serve. In fact, to evaluate a sufficient condition, the\n\t * timestamps of the in-service queue would need to be\n\t * updated, and this operation is quite costly (see the\n\t * comments on bfq_bfqq_update_budg_for_activation()).\n\t *\n\t * As for throughput, we ask bfq_better_to_idle() whether we\n\t * still need to plug I/O dispatching. If bfq_better_to_idle()\n\t * says no, then plugging is not needed any longer, either to\n\t * boost throughput or to perserve service guarantees. Then\n\t * the best option is to stop plugging I/O, as not doing so\n\t * would certainly lower throughput. We may end up in this\n\t * case if: (1) upon a dispatch attempt, we detected that it\n\t * was better to plug I/O dispatch, and to wait for a new\n\t * request to arrive for the currently in-service queue, but\n\t * (2) this switch of bfqq to busy changes the scenario.\n\t */\n\tif (bfqd->in_service_queue &&\n\t    ((bfqq_wants_to_preempt &&\n\t      bfqq->wr_coeff >= bfqd->in_service_queue->wr_coeff) ||\n\t     bfq_bfqq_higher_class_or_weight(bfqq, bfqd->in_service_queue) ||\n\t     !bfq_better_to_idle(bfqd->in_service_queue)) &&\n\t    next_queue_may_preempt(bfqd))\n\t\tbfq_bfqq_expire(bfqd, bfqd->in_service_queue,\n\t\t\t\tfalse, BFQQE_PREEMPTED);\n}\n\nstatic void bfq_reset_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\t/* invalidate baseline total service time */\n\tbfqq->last_serv_time_ns = 0;\n\n\t/*\n\t * Reset pointer in case we are waiting for\n\t * some request completion.\n\t */\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * If bfqq has a short think time, then start by setting the\n\t * inject limit to 0 prudentially, because the service time of\n\t * an injected I/O request may be higher than the think time\n\t * of bfqq, and therefore, if one request was injected when\n\t * bfqq remains empty, this injected request might delay the\n\t * service of the next I/O request for bfqq significantly. In\n\t * case bfqq can actually tolerate some injection, then the\n\t * adaptive update will however raise the limit soon. This\n\t * lucky circumstance holds exactly because bfqq has a short\n\t * think time, and thus, after remaining empty, is likely to\n\t * get new I/O enqueued---and then completed---before being\n\t * expired. This is the very pattern that gives the\n\t * limit-update algorithm the chance to measure the effect of\n\t * injection on request service times, and then to update the\n\t * limit accordingly.\n\t *\n\t * However, in the following special case, the inject limit is\n\t * left to 1 even if the think time is short: bfqq's I/O is\n\t * synchronized with that of some other queue, i.e., bfqq may\n\t * receive new I/O only after the I/O of the other queue is\n\t * completed. Keeping the inject limit to 1 allows the\n\t * blocking I/O to be served while bfqq is in service. And\n\t * this is very convenient both for bfqq and for overall\n\t * throughput, as explained in detail in the comments in\n\t * bfq_update_has_short_ttime().\n\t *\n\t * On the opposite end, if bfqq has a long think time, then\n\t * start directly by 1, because:\n\t * a) on the bright side, keeping at most one request in\n\t * service in the drive is unlikely to cause any harm to the\n\t * latency of bfqq's requests, as the service time of a single\n\t * request is likely to be lower than the think time of bfqq;\n\t * b) on the downside, after becoming empty, bfqq is likely to\n\t * expire before getting its next request. With this request\n\t * arrival pattern, it is very hard to sample total service\n\t * times and update the inject limit accordingly (see comments\n\t * on bfq_update_inject_limit()). So the limit is likely to be\n\t * never, or at least seldom, updated.  As a consequence, by\n\t * setting the limit to 1, we avoid that no injection ever\n\t * occurs with bfqq. On the downside, this proactive step\n\t * further reduces chances to actually compute the baseline\n\t * total service time. Thus it reduces chances to execute the\n\t * limit-update algorithm and possibly raise the limit to more\n\t * than 1.\n\t */\n\tif (bfq_bfqq_has_short_ttime(bfqq))\n\t\tbfqq->inject_limit = 0;\n\telse\n\t\tbfqq->inject_limit = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)\n{\n\tu64 tot_io_time = now_ns - bfqq->io_start_time;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) && bfqq->dispatched == 0)\n\t\tbfqq->tot_idle_time +=\n\t\t\tnow_ns - bfqq->ttime.last_end_request;\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq)))\n\t\treturn;\n\n\t/*\n\t * Must be busy for at least about 80% of the time to be\n\t * considered I/O bound.\n\t */\n\tif (bfqq->tot_idle_time * 5 > tot_io_time)\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * Keep an observation window of at most 200 ms in the past\n\t * from now.\n\t */\n\tif (tot_io_time > 200 * NSEC_PER_MSEC) {\n\t\tbfqq->io_start_time = now_ns - (tot_io_time>>1);\n\t\tbfqq->tot_idle_time >>= 1;\n\t}\n}\n\n/*\n * Detect whether bfqq's I/O seems synchronized with that of some\n * other queue, i.e., whether bfqq, after remaining empty, happens to\n * receive new I/O only right after some I/O request of the other\n * queue has been completed. We call waker queue the other queue, and\n * we assume, for simplicity, that bfqq may have at most one waker\n * queue.\n *\n * A remarkable throughput boost can be reached by unconditionally\n * injecting the I/O of the waker queue, every time a new\n * bfq_dispatch_request happens to be invoked while I/O is being\n * plugged for bfqq.  In addition to boosting throughput, this\n * unblocks bfqq's I/O, thereby improving bandwidth and latency for\n * bfqq. Note that these same results may be achieved with the general\n * injection mechanism, but less effectively. For details on this\n * aspect, see the comments on the choice of the queue for injection\n * in bfq_select_queue().\n *\n * Turning back to the detection of a waker queue, a queue Q is deemed as a\n * waker queue for bfqq if, for three consecutive times, bfqq happens to become\n * non empty right after a request of Q has been completed within given\n * timeout. In this respect, even if bfqq is empty, we do not check for a waker\n * if it still has some in-flight I/O. In fact, in this case bfqq is actually\n * still being served by the drive, and may receive new I/O on the completion\n * of some of the in-flight requests. In particular, on the first time, Q is\n * tentatively set as a candidate waker queue, while on the third consecutive\n * time that Q is detected, the field waker_bfqq is set to Q, to confirm that Q\n * is a waker queue for bfqq. These detection steps are performed only if bfqq\n * has a long think time, so as to make it more likely that bfqq's I/O is\n * actually being blocked by a synchronization. This last filter, plus the\n * above three-times requirement and time limit for detection, make false\n * positives less likely.\n *\n * NOTE\n *\n * The sooner a waker queue is detected, the sooner throughput can be\n * boosted by injecting I/O from the waker queue. Fortunately,\n * detection is likely to be actually fast, for the following\n * reasons. While blocked by synchronization, bfqq has a long think\n * time. This implies that bfqq's inject limit is at least equal to 1\n * (see the comments in bfq_update_inject_limit()). So, thanks to\n * injection, the waker queue is likely to be served during the very\n * first I/O-plugging time interval for bfqq. This triggers the first\n * step of the detection mechanism. Thanks again to injection, the\n * candidate waker queue is then likely to be confirmed no later than\n * during the next I/O-plugging interval for bfqq.\n *\n * ISSUE\n *\n * On queue merging all waker information is lost.\n */\nstatic void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    u64 now_ns)\n{\n\tchar waker_name[MAX_BFQQ_NAME_LENGTH];\n\n\tif (!bfqd->last_completed_rq_bfqq ||\n\t    bfqd->last_completed_rq_bfqq == bfqq ||\n\t    bfq_bfqq_has_short_ttime(bfqq) ||\n\t    now_ns - bfqd->last_completion >= 4 * NSEC_PER_MSEC ||\n\t    bfqd->last_completed_rq_bfqq == &bfqd->oom_bfqq ||\n\t    bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t/*\n\t * We reset waker detection logic also if too much time has passed\n \t * since the first detection. If wakeups are rare, pointless idling\n\t * doesn't hurt throughput that much. The condition below makes sure\n\t * we do not uselessly idle blocking waker in more than 1/64 cases.\n\t */\n\tif (bfqd->last_completed_rq_bfqq !=\n\t    bfqq->tentative_waker_bfqq ||\n\t    now_ns > bfqq->waker_detection_started +\n\t\t\t\t\t128 * (u64)bfqd->bfq_slice_idle) {\n\t\t/*\n\t\t * First synchronization detected with a\n\t\t * candidate waker queue, or with a different\n\t\t * candidate waker queue from the current one.\n\t\t */\n\t\tbfqq->tentative_waker_bfqq =\n\t\t\tbfqd->last_completed_rq_bfqq;\n\t\tbfqq->num_waker_detections = 1;\n\t\tbfqq->waker_detection_started = now_ns;\n\t\tbfq_bfqq_name(bfqq->tentative_waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set tentative waker %s\", waker_name);\n\t} else /* Same tentative waker queue detected again */\n\t\tbfqq->num_waker_detections++;\n\n\tif (bfqq->num_waker_detections == 3) {\n\t\tbfqq->waker_bfqq = bfqd->last_completed_rq_bfqq;\n\t\tbfqq->tentative_waker_bfqq = NULL;\n\t\tbfq_bfqq_name(bfqq->waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set waker %s\", waker_name);\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * bfqq->waker_bfqq must be reset. To\n\t\t * this goal, we maintain in each\n\t\t * waker queue a list, woken_list, of\n\t\t * all the queues that reference the\n\t\t * waker queue through their\n\t\t * waker_bfqq pointer. When the waker\n\t\t * queue exits, the waker_bfqq pointer\n\t\t * of all the queues in the woken_list\n\t\t * is reset.\n\t\t *\n\t\t * In addition, if bfqq is already in\n\t\t * the woken_list of a waker queue,\n\t\t * then, before being inserted into\n\t\t * the woken_list of a new waker\n\t\t * queue, bfqq must be removed from\n\t\t * the woken_list of the old waker\n\t\t * queue.\n\t\t */\n\t\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\t\thlist_del_init(&bfqq->woken_list_node);\n\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t       &bfqd->last_completed_rq_bfqq->woken_list);\n\t}\n}\n\nstatic void bfq_add_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct request *next_rq, *prev;\n\tunsigned int old_wr_coeff = bfqq->wr_coeff;\n\tbool interactive = false;\n\tu64 now_ns = ktime_get_ns();\n\n\tbfq_log_bfqq(bfqd, bfqq, \"add_request %d\", rq_is_sync(rq));\n\tbfqq->queued[rq_is_sync(rq)]++;\n\t/*\n\t * Updating of 'bfqd->queued' is protected by 'bfqd->lock', however, it\n\t * may be read without holding the lock in bfq_has_work().\n\t */\n\tWRITE_ONCE(bfqd->queued, bfqd->queued + 1);\n\n\tif (bfq_bfqq_sync(bfqq) && RQ_BIC(rq)->requests <= 1) {\n\t\tbfq_check_waker(bfqd, bfqq, now_ns);\n\n\t\t/*\n\t\t * Periodically reset inject limit, to make sure that\n\t\t * the latter eventually drops in case workload\n\t\t * changes, see step (3) in the comments on\n\t\t * bfq_update_inject_limit().\n\t\t */\n\t\tif (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t     msecs_to_jiffies(1000)))\n\t\t\tbfq_reset_inject_limit(bfqd, bfqq);\n\n\t\t/*\n\t\t * The following conditions must hold to setup a new\n\t\t * sampling of total service time, and then a new\n\t\t * update of the inject limit:\n\t\t * - bfqq is in service, because the total service\n\t\t *   time is evaluated only for the I/O requests of\n\t\t *   the queues in service;\n\t\t * - this is the right occasion to compute or to\n\t\t *   lower the baseline total service time, because\n\t\t *   there are actually no requests in the drive,\n\t\t *   or\n\t\t *   the baseline total service time is available, and\n\t\t *   this is the right occasion to compute the other\n\t\t *   quantity needed to update the inject limit, i.e.,\n\t\t *   the total service time caused by the amount of\n\t\t *   injection allowed by the current value of the\n\t\t *   limit. It is the right occasion because injection\n\t\t *   has actually been performed during the service\n\t\t *   hole, and there are still in-flight requests,\n\t\t *   which are very likely to be exactly the injected\n\t\t *   requests, or part of them;\n\t\t * - the minimum interval for sampling the total\n\t\t *   service time and updating the inject limit has\n\t\t *   elapsed.\n\t\t */\n\t\tif (bfqq == bfqd->in_service_queue &&\n\t\t    (bfqd->tot_rq_in_driver == 0 ||\n\t\t     (bfqq->last_serv_time_ns > 0 &&\n\t\t      bfqd->rqs_injected && bfqd->tot_rq_in_driver > 0)) &&\n\t\t    time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t      msecs_to_jiffies(10))) {\n\t\t\tbfqd->last_empty_occupied_ns = ktime_get_ns();\n\t\t\t/*\n\t\t\t * Start the state machine for measuring the\n\t\t\t * total service time of rq: setting\n\t\t\t * wait_dispatch will cause bfqd->waited_rq to\n\t\t\t * be set when rq will be dispatched.\n\t\t\t */\n\t\t\tbfqd->wait_dispatch = true;\n\t\t\t/*\n\t\t\t * If there is no I/O in service in the drive,\n\t\t\t * then possible injection occurred before the\n\t\t\t * arrival of rq will not affect the total\n\t\t\t * service time of rq. So the injection limit\n\t\t\t * must not be updated as a function of such\n\t\t\t * total service time, unless new injection\n\t\t\t * occurs before rq is completed. To have the\n\t\t\t * injection limit updated only in the latter\n\t\t\t * case, reset rqs_injected here (rqs_injected\n\t\t\t * will be set in case injection is performed\n\t\t\t * on bfqq before rq is completed).\n\t\t\t */\n\t\t\tif (bfqd->tot_rq_in_driver == 0)\n\t\t\t\tbfqd->rqs_injected = false;\n\t\t}\n\t}\n\n\tif (bfq_bfqq_sync(bfqq))\n\t\tbfq_update_io_intensity(bfqq, now_ns);\n\n\telv_rb_add(&bfqq->sort_list, rq);\n\n\t/*\n\t * Check if this request is a better next-serve candidate.\n\t */\n\tprev = bfqq->next_rq;\n\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, rq, bfqd->last_position);\n\tbfqq->next_rq = next_rq;\n\n\t/*\n\t * Adjust priority tree position, if next_rq changes.\n\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing && prev != bfqq->next_rq))\n\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\n\tif (!bfq_bfqq_busy(bfqq)) /* switching to busy ... */\n\t\tbfq_bfqq_handle_idle_busy_switch(bfqd, bfqq, old_wr_coeff,\n\t\t\t\t\t\t rq, &interactive);\n\telse {\n\t\tif (bfqd->low_latency && old_wr_coeff == 1 && !rq_is_sync(rq) &&\n\t\t    time_is_before_jiffies(\n\t\t\t\tbfqq->last_wr_start_finish +\n\t\t\t\tbfqd->bfq_wr_min_inter_arr_async)) {\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\n\t\t\tbfqd->wr_busy_queues++;\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t\tif (prev != bfqq->next_rq)\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\t/*\n\t * Assign jiffies to last_wr_start_finish in the following\n\t * cases:\n\t *\n\t * . if bfqq is not going to be weight-raised, because, for\n\t *   non weight-raised queues, last_wr_start_finish stores the\n\t *   arrival time of the last request; as of now, this piece\n\t *   of information is used only for deciding whether to\n\t *   weight-raise async queues\n\t *\n\t * . if bfqq is not weight-raised, because, if bfqq is now\n\t *   switching to weight-raised, then last_wr_start_finish\n\t *   stores the time when weight-raising starts\n\t *\n\t * . if bfqq is interactive, because, regardless of whether\n\t *   bfqq is currently weight-raised, the weight-raising\n\t *   period must start or restart (this case is considered\n\t *   separately because it is not detected by the above\n\t *   conditions, if bfqq is already weight-raised)\n\t *\n\t * last_wr_start_finish has to be updated also if bfqq is soft\n\t * real-time, because the weight-raising period is constantly\n\t * restarted on idle-to-busy transitions for these queues, but\n\t * this is already done in bfq_bfqq_handle_idle_busy_switch if\n\t * needed.\n\t */\n\tif (bfqd->low_latency &&\n\t\t(old_wr_coeff == 1 || bfqq->wr_coeff == 1 || interactive))\n\t\tbfqq->last_wr_start_finish = jiffies;\n}\n\nstatic struct request *bfq_find_rq_fmerge(struct bfq_data *bfqd,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq;\n\n\n\tif (bfqq)\n\t\treturn elv_rb_find(&bfqq->sort_list, bio_end_sector(bio));\n\n\treturn NULL;\n}\n\nstatic sector_t get_sdist(sector_t last_pos, struct request *rq)\n{\n\tif (last_pos)\n\t\treturn abs(blk_rq_pos(rq) - last_pos);\n\n\treturn 0;\n}\n\nstatic void bfq_remove_request(struct request_queue *q,\n\t\t\t       struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tconst int sync = rq_is_sync(rq);\n\n\tif (bfqq->next_rq == rq) {\n\t\tbfqq->next_rq = bfq_find_next_rq(bfqd, bfqq, rq);\n\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\tif (rq->queuelist.prev != &rq->queuelist)\n\t\tlist_del_init(&rq->queuelist);\n\tbfqq->queued[sync]--;\n\t/*\n\t * Updating of 'bfqd->queued' is protected by 'bfqd->lock', however, it\n\t * may be read without holding the lock in bfq_has_work().\n\t */\n\tWRITE_ONCE(bfqd->queued, bfqd->queued - 1);\n\telv_rb_del(&bfqq->sort_list, rq);\n\n\telv_rqhash_del(q, rq);\n\tif (q->last_merge == rq)\n\t\tq->last_merge = NULL;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\tbfqq->next_rq = NULL;\n\n\t\tif (bfq_bfqq_busy(bfqq) && bfqq != bfqd->in_service_queue) {\n\t\t\tbfq_del_bfqq_busy(bfqq, false);\n\t\t\t/*\n\t\t\t * bfqq emptied. In normal operation, when\n\t\t\t * bfqq is empty, bfqq->entity.service and\n\t\t\t * bfqq->entity.budget must contain,\n\t\t\t * respectively, the service received and the\n\t\t\t * budget used last time bfqq emptied. These\n\t\t\t * facts do not hold in this case, as at least\n\t\t\t * this last removal occurred while bfqq is\n\t\t\t * not in service. To avoid inconsistencies,\n\t\t\t * reset both bfqq->entity.service and\n\t\t\t * bfqq->entity.budget, if bfqq has still a\n\t\t\t * process that may issue I/O requests to it.\n\t\t\t */\n\t\t\tbfqq->entity.budget = bfqq->entity.service = 0;\n\t\t}\n\n\t\t/*\n\t\t * Remove queue from request-position tree as it is empty.\n\t\t */\n\t\tif (bfqq->pos_root) {\n\t\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\t\tbfqq->pos_root = NULL;\n\t\t}\n\t} else {\n\t\t/* see comments on bfq_pos_tree_add_move() for the unlikely() */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending--;\n\n}\n\nstatic bool bfq_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *free = NULL;\n\t/*\n\t * bfq_bic_lookup grabs the queue_lock: invoke it now and\n\t * store its return value for later use, to avoid nesting\n\t * queue_lock inside the bfqd->lock. We assume that the bic\n\t * returned by bfq_bic_lookup does not go away before\n\t * bfqd->lock is taken.\n\t */\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(q);\n\tbool ret;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tif (bic) {\n\t\t/*\n\t\t * Make sure cgroup info is uptodate for current process before\n\t\t * considering the merge.\n\t\t */\n\t\tbfq_bic_update_cgroup(bic, bio);\n\n\t\tbfqd->bio_bfqq = bic_to_bfqq(bic, op_is_sync(bio->bi_opf),\n\t\t\t\t\t     bfq_actuator_index(bfqd, bio));\n\t} else {\n\t\tbfqd->bio_bfqq = NULL;\n\t}\n\tbfqd->bio_bic = bic;\n\n\tret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);\n\n\tspin_unlock_irq(&bfqd->lock);\n\tif (free)\n\t\tblk_mq_free_request(free);\n\n\treturn ret;\n}\n\nstatic int bfq_request_merge(struct request_queue *q, struct request **req,\n\t\t\t     struct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *__rq;\n\n\t__rq = bfq_find_rq_fmerge(bfqd, bio, q);\n\tif (__rq && elv_bio_merge_ok(__rq, bio)) {\n\t\t*req = __rq;\n\n\t\tif (blk_discard_mergable(__rq))\n\t\t\treturn ELEVATOR_DISCARD_MERGE;\n\t\treturn ELEVATOR_FRONT_MERGE;\n\t}\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\nstatic void bfq_request_merged(struct request_queue *q, struct request *req,\n\t\t\t       enum elv_merge type)\n{\n\tif (type == ELEVATOR_FRONT_MERGE &&\n\t    rb_prev(&req->rb_node) &&\n\t    blk_rq_pos(req) <\n\t    blk_rq_pos(container_of(rb_prev(&req->rb_node),\n\t\t\t\t    struct request, rb_node))) {\n\t\tstruct bfq_queue *bfqq = RQ_BFQQ(req);\n\t\tstruct bfq_data *bfqd;\n\t\tstruct request *prev, *next_rq;\n\n\t\tif (!bfqq)\n\t\t\treturn;\n\n\t\tbfqd = bfqq->bfqd;\n\n\t\t/* Reposition request in its sort_list */\n\t\telv_rb_del(&bfqq->sort_list, req);\n\t\telv_rb_add(&bfqq->sort_list, req);\n\n\t\t/* Choose next request to be served for bfqq */\n\t\tprev = bfqq->next_rq;\n\t\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, req,\n\t\t\t\t\t bfqd->last_position);\n\t\tbfqq->next_rq = next_rq;\n\t\t/*\n\t\t * If next_rq changes, update both the queue's budget to\n\t\t * fit the new request and the queue's position in its\n\t\t * rq_pos_tree.\n\t\t */\n\t\tif (prev != bfqq->next_rq) {\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t\t\t/*\n\t\t\t * See comments on bfq_pos_tree_add_move() for\n\t\t\t * the unlikely().\n\t\t\t */\n\t\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t\t}\n\t}\n}\n\n/*\n * This function is called to notify the scheduler that the requests\n * rq and 'next' have been merged, with 'next' going away.  BFQ\n * exploits this hook to address the following issue: if 'next' has a\n * fifo_time lower that rq, then the fifo_time of rq must be set to\n * the value of 'next', to not forget the greater age of 'next'.\n *\n * NOTE: in this function we assume that rq is in a bfq_queue, basing\n * on that rq is picked from the hash table q->elevator->hash, which,\n * in its turn, is filled only with I/O requests present in\n * bfq_queues, while BFQ is in use for the request queue q. In fact,\n * the function that fills this hash table (elv_rqhash_add) is called\n * only by bfq_insert_request.\n */\nstatic void bfq_requests_merged(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*next_bfqq = RQ_BFQQ(next);\n\n\tif (!bfqq)\n\t\tgoto remove;\n\n\t/*\n\t * If next and rq belong to the same bfq_queue and next is older\n\t * than rq, then reposition rq in the fifo (by substituting next\n\t * with rq). Otherwise, if next and rq belong to different\n\t * bfq_queues, never reposition rq: in fact, we would have to\n\t * reposition it with respect to next's position in its own fifo,\n\t * which would most certainly be too expensive with respect to\n\t * the benefits.\n\t */\n\tif (bfqq == next_bfqq &&\n\t    !list_empty(&rq->queuelist) && !list_empty(&next->queuelist) &&\n\t    next->fifo_time < rq->fifo_time) {\n\t\tlist_del_init(&rq->queuelist);\n\t\tlist_replace_init(&next->queuelist, &rq->queuelist);\n\t\trq->fifo_time = next->fifo_time;\n\t}\n\n\tif (bfqq->next_rq == next)\n\t\tbfqq->next_rq = rq;\n\n\tbfqg_stats_update_io_merged(bfqq_group(bfqq), next->cmd_flags);\nremove:\n\t/* Merged request may be in the IO scheduler. Remove it. */\n\tif (!RB_EMPTY_NODE(&next->rb_node)) {\n\t\tbfq_remove_request(next->q, next);\n\t\tif (next_bfqq)\n\t\t\tbfqg_stats_update_io_remove(bfqq_group(next_bfqq),\n\t\t\t\t\t\t    next->cmd_flags);\n\t}\n}\n\n/* Must be called with bfqq != NULL */\nstatic void bfq_bfqq_end_wr(struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq has been enjoying interactive weight-raising, then\n\t * reset soft_rt_next_start. We do it for the following\n\t * reason. bfqq may have been conveying the I/O needed to load\n\t * a soft real-time application. Such an application actually\n\t * exhibits a soft real-time I/O pattern after it finishes\n\t * loading, and finally starts doing its job. But, if bfqq has\n\t * been receiving a lot of bandwidth so far (likely to happen\n\t * on a fast device), then soft_rt_next_start now contains a\n\t * high value that. So, without this reset, bfqq would be\n\t * prevented from being possibly considered as soft_rt for a\n\t * very long time.\n\t */\n\n\tif (bfqq->wr_cur_max_time !=\n\t    bfqq->bfqd->bfq_wr_rt_max_time)\n\t\tbfqq->soft_rt_next_start = jiffies;\n\n\tif (bfq_bfqq_busy(bfqq))\n\t\tbfqq->bfqd->wr_busy_queues--;\n\tbfqq->wr_coeff = 1;\n\tbfqq->wr_cur_max_time = 0;\n\tbfqq->last_wr_start_finish = jiffies;\n\t/*\n\t * Trigger a weight change on the next invocation of\n\t * __bfq_entity_update_weight_prio.\n\t */\n\tbfqq->entity.prio_changed = 1;\n}\n\nvoid bfq_end_wr_async_queues(struct bfq_data *bfqd,\n\t\t\t     struct bfq_group *bfqg)\n{\n\tint i, j, k;\n\n\tfor (k = 0; k < bfqd->num_actuators; k++) {\n\t\tfor (i = 0; i < 2; i++)\n\t\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t\tif (bfqg->async_bfqq[i][j][k])\n\t\t\t\t\tbfq_bfqq_end_wr(bfqg->async_bfqq[i][j][k]);\n\t\tif (bfqg->async_idle_bfqq[k])\n\t\t\tbfq_bfqq_end_wr(bfqg->async_idle_bfqq[k]);\n\t}\n}\n\nstatic void bfq_end_wr(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\tint i;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tlist_for_each_entry(bfqq, &bfqd->active_list[i], bfqq_list)\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t}\n\tlist_for_each_entry(bfqq, &bfqd->idle_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tbfq_end_wr_async(bfqd);\n\n\tspin_unlock_irq(&bfqd->lock);\n}\n\nstatic sector_t bfq_io_struct_pos(void *io_struct, bool request)\n{\n\tif (request)\n\t\treturn blk_rq_pos(io_struct);\n\telse\n\t\treturn ((struct bio *)io_struct)->bi_iter.bi_sector;\n}\n\nstatic int bfq_rq_close_to_sector(void *io_struct, bool request,\n\t\t\t\t  sector_t sector)\n{\n\treturn abs(bfq_io_struct_pos(io_struct, request) - sector) <=\n\t       BFQQ_CLOSE_THR;\n}\n\nstatic struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_queue *bfqq,\n\t\t\t\t\t sector_t sector)\n{\n\tstruct rb_root *root = &bfqq_group(bfqq)->rq_pos_tree;\n\tstruct rb_node *parent, *node;\n\tstruct bfq_queue *__bfqq;\n\n\tif (RB_EMPTY_ROOT(root))\n\t\treturn NULL;\n\n\t/*\n\t * First, if we find a request starting at the end of the last\n\t * request, choose it.\n\t */\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, root, sector, &parent, NULL);\n\tif (__bfqq)\n\t\treturn __bfqq;\n\n\t/*\n\t * If the exact sector wasn't found, the parent of the NULL leaf\n\t * will contain the closest sector (rq_pos_tree sorted by\n\t * next_request position).\n\t */\n\t__bfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\tif (blk_rq_pos(__bfqq->next_rq) < sector)\n\t\tnode = rb_next(&__bfqq->pos_node);\n\telse\n\t\tnode = rb_prev(&__bfqq->pos_node);\n\tif (!node)\n\t\treturn NULL;\n\n\t__bfqq = rb_entry(node, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_find_close_cooperator(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_queue *cur_bfqq,\n\t\t\t\t\t\t   sector_t sector)\n{\n\tstruct bfq_queue *bfqq;\n\n\t/*\n\t * We shall notice if some of the queues are cooperating,\n\t * e.g., working closely on the same area of the device. In\n\t * that case, we can group them together and: 1) don't waste\n\t * time idling, and 2) serve the union of their requests in\n\t * the best possible order for throughput.\n\t */\n\tbfqq = bfqq_find_close(bfqd, cur_bfqq, sector);\n\tif (!bfqq || bfqq == cur_bfqq)\n\t\treturn NULL;\n\n\treturn bfqq;\n}\n\nstatic struct bfq_queue *\nbfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)\n{\n\tint process_refs, new_process_refs;\n\tstruct bfq_queue *__bfqq;\n\n\t/*\n\t * If there are no process references on the new_bfqq, then it is\n\t * unsafe to follow the ->new_bfqq chain as other bfqq's in the chain\n\t * may have dropped their last reference (not just their last process\n\t * reference).\n\t */\n\tif (!bfqq_process_refs(new_bfqq))\n\t\treturn NULL;\n\n\t/* Avoid a circular list and skip interim queue merges. */\n\twhile ((__bfqq = new_bfqq->new_bfqq)) {\n\t\tif (__bfqq == bfqq)\n\t\t\treturn NULL;\n\t\tnew_bfqq = __bfqq;\n\t}\n\n\tprocess_refs = bfqq_process_refs(bfqq);\n\tnew_process_refs = bfqq_process_refs(new_bfqq);\n\t/*\n\t * If the process for the bfqq has gone away, there is no\n\t * sense in merging the queues.\n\t */\n\tif (process_refs == 0 || new_process_refs == 0)\n\t\treturn NULL;\n\n\t/*\n\t * Make sure merged queues belong to the same parent. Parents could\n\t * have changed since the time we decided the two queues are suitable\n\t * for merging.\n\t */\n\tif (new_bfqq->entity.parent != bfqq->entity.parent)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"scheduling merge with queue %d\",\n\t\tnew_bfqq->pid);\n\n\t/*\n\t * Merging is just a redirection: the requests of the process\n\t * owning one of the two queues are redirected to the other queue.\n\t * The latter queue, in its turn, is set as shared if this is the\n\t * first time that the requests of some process are redirected to\n\t * it.\n\t *\n\t * We redirect bfqq to new_bfqq and not the opposite, because\n\t * we are in the context of the process owning bfqq, thus we\n\t * have the io_cq of this process. So we can immediately\n\t * configure this io_cq to redirect the requests of the\n\t * process to new_bfqq. In contrast, the io_cq of new_bfqq is\n\t * not available any more (new_bfqq->bic == NULL).\n\t *\n\t * Anyway, even in case new_bfqq coincides with the in-service\n\t * queue, redirecting requests the in-service queue is the\n\t * best option, as we feed the in-service queue with new\n\t * requests close to the last request served and, by doing so,\n\t * are likely to increase the throughput.\n\t */\n\tbfqq->new_bfqq = new_bfqq;\n\t/*\n\t * The above assignment schedules the following redirections:\n\t * each time some I/O for bfqq arrives, the process that\n\t * generated that I/O is disassociated from bfqq and\n\t * associated with new_bfqq. Here we increases new_bfqq->ref\n\t * in advance, adding the number of processes that are\n\t * expected to be associated with new_bfqq as they happen to\n\t * issue I/O.\n\t */\n\tnew_bfqq->ref += process_refs;\n\treturn new_bfqq;\n}\n\nstatic bool bfq_may_be_close_cooperator(struct bfq_queue *bfqq,\n\t\t\t\t\tstruct bfq_queue *new_bfqq)\n{\n\tif (bfq_too_late_for_merging(new_bfqq))\n\t\treturn false;\n\n\tif (bfq_class_idle(bfqq) || bfq_class_idle(new_bfqq) ||\n\t    (bfqq->ioprio_class != new_bfqq->ioprio_class))\n\t\treturn false;\n\n\t/*\n\t * If either of the queues has already been detected as seeky,\n\t * then merging it with the other queue is unlikely to lead to\n\t * sequential I/O.\n\t */\n\tif (BFQQ_SEEKY(bfqq) || BFQQ_SEEKY(new_bfqq))\n\t\treturn false;\n\n\t/*\n\t * Interleaved I/O is known to be done by (some) applications\n\t * only for reads, so it does not make sense to merge async\n\t * queues.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || !bfq_bfqq_sync(new_bfqq))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq);\n\nstatic struct bfq_queue *\nbfq_setup_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct bfq_queue *stable_merge_bfqq,\n\t\t       struct bfq_iocq_bfqq_data *bfqq_data)\n{\n\tint proc_ref = min(bfqq_process_refs(bfqq),\n\t\t\t   bfqq_process_refs(stable_merge_bfqq));\n\tstruct bfq_queue *new_bfqq = NULL;\n\n\tbfqq_data->stable_merge_bfqq = NULL;\n\tif (idling_boosts_thr_without_issues(bfqd, bfqq) || proc_ref == 0)\n\t\tgoto out;\n\n\t/* next function will take at least one ref */\n\tnew_bfqq = bfq_setup_merge(bfqq, stable_merge_bfqq);\n\n\tif (new_bfqq) {\n\t\tbfqq_data->stably_merged = true;\n\t\tif (new_bfqq->bic) {\n\t\t\tunsigned int new_a_idx = new_bfqq->actuator_idx;\n\t\t\tstruct bfq_iocq_bfqq_data *new_bfqq_data =\n\t\t\t\t&new_bfqq->bic->bfqq_data[new_a_idx];\n\n\t\t\tnew_bfqq_data->stably_merged = true;\n\t\t}\n\t}\n\nout:\n\t/* deschedule stable merge, because done or aborted here */\n\tbfq_put_stable_ref(stable_merge_bfqq);\n\n\treturn new_bfqq;\n}\n\n/*\n * Attempt to schedule a merge of bfqq with the currently in-service\n * queue or with a close queue among the scheduled queues.  Return\n * NULL if no merge was scheduled, a pointer to the shared bfq_queue\n * structure otherwise.\n *\n * The OOM queue is not allowed to participate to cooperation: in fact, since\n * the requests temporarily redirected to the OOM queue could be redirected\n * again to dedicated queues at any time, the state needed to correctly\n * handle merging with the OOM queue would be quite complex and expensive\n * to maintain. Besides, in such a critical condition as an out of memory,\n * the benefits of queue merging may be little relevant, or even negligible.\n *\n * WARNING: queue merging may impair fairness among non-weight raised\n * queues, for at least two reasons: 1) the original weight of a\n * merged queue may change during the merged state, 2) even being the\n * weight the same, a merged queue may be bloated with many more\n * requests than the ones produced by its originally-associated\n * process.\n */\nstatic struct bfq_queue *\nbfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t     void *io_struct, bool request, struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue *in_service_bfqq, *new_bfqq;\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\t/* if a merge has already been setup, then proceed with that first */\n\tnew_bfqq = bfqq->new_bfqq;\n\tif (new_bfqq) {\n\t\twhile (new_bfqq->new_bfqq)\n\t\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t\treturn new_bfqq;\n\t}\n\n\t/*\n\t * Check delayed stable merge for rotational or non-queueing\n\t * devs. For this branch to be executed, bfqq must not be\n\t * currently merged with some other queue (i.e., bfqq->bic\n\t * must be non null). If we considered also merged queues,\n\t * then we should also check whether bfqq has already been\n\t * merged with bic->stable_merge_bfqq. But this would be\n\t * costly and complicated.\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing)) {\n\t\t/*\n\t\t * Make sure also that bfqq is sync, because\n\t\t * bic->stable_merge_bfqq may point to some queue (for\n\t\t * stable merging) also if bic is associated with a\n\t\t * sync queue, but this bfqq is async\n\t\t */\n\t\tif (bfq_bfqq_sync(bfqq) && bfqq_data->stable_merge_bfqq &&\n\t\t    !bfq_bfqq_just_created(bfqq) &&\n\t\t    time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t  msecs_to_jiffies(bfq_late_stable_merging)) &&\n\t\t    time_is_before_jiffies(bfqq->creation_time +\n\t\t\t\t\t   msecs_to_jiffies(bfq_late_stable_merging))) {\n\t\t\tstruct bfq_queue *stable_merge_bfqq =\n\t\t\t\tbfqq_data->stable_merge_bfqq;\n\n\t\t\treturn bfq_setup_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t      stable_merge_bfqq,\n\t\t\t\t\t\t      bfqq_data);\n\t\t}\n\t}\n\n\t/*\n\t * Do not perform queue merging if the device is non\n\t * rotational and performs internal queueing. In fact, such a\n\t * device reaches a high speed through internal parallelism\n\t * and pipelining. This means that, to reach a high\n\t * throughput, it must have many requests enqueued at the same\n\t * time. But, in this configuration, the internal scheduling\n\t * algorithm of the device does exactly the job of queue\n\t * merging: it reorders requests so as to obtain as much as\n\t * possible a sequential I/O pattern. As a consequence, with\n\t * the workload generated by processes doing interleaved I/O,\n\t * the throughput reached by the device is likely to be the\n\t * same, with and without queue merging.\n\t *\n\t * Disabling merging also provides a remarkable benefit in\n\t * terms of throughput. Merging tends to make many workloads\n\t * artificially more uneven, because of shared queues\n\t * remaining non empty for incomparably more time than\n\t * non-merged queues. This may accentuate workload\n\t * asymmetries. For example, if one of the queues in a set of\n\t * merged queues has a higher weight than a normal queue, then\n\t * the shared queue may inherit such a high weight and, by\n\t * staying almost always active, may force BFQ to perform I/O\n\t * plugging most of the time. This evidently makes it harder\n\t * for BFQ to let the device reach a high throughput.\n\t *\n\t * Finally, the likely() macro below is not used because one\n\t * of the two branches is more likely than the other, but to\n\t * have the code path after the following if() executed as\n\t * fast as possible for the case of a non rotational device\n\t * with queueing. We want it because this is the fastest kind\n\t * of device. On the opposite end, the likely() may lengthen\n\t * the execution time of BFQ for the case of slower devices\n\t * (rotational or at least without queueing). But in this case\n\t * the execution time of BFQ matters very little, if not at\n\t * all.\n\t */\n\tif (likely(bfqd->nonrot_with_queueing))\n\t\treturn NULL;\n\n\t/*\n\t * Prevent bfqq from being merged if it has been created too\n\t * long ago. The idea is that true cooperating processes, and\n\t * thus their associated bfq_queues, are supposed to be\n\t * created shortly after each other. This is the case, e.g.,\n\t * for KVM/QEMU and dump I/O threads. Basing on this\n\t * assumption, the following filtering greatly reduces the\n\t * probability that two non-cooperating processes, which just\n\t * happen to do close I/O for some short time interval, have\n\t * their queues merged by mistake.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn NULL;\n\n\tif (!io_struct || unlikely(bfqq == &bfqd->oom_bfqq))\n\t\treturn NULL;\n\n\t/* If there is only one backlogged queue, don't search. */\n\tif (bfq_tot_busy_queues(bfqd) == 1)\n\t\treturn NULL;\n\n\tin_service_bfqq = bfqd->in_service_queue;\n\n\tif (in_service_bfqq && in_service_bfqq != bfqq &&\n\t    likely(in_service_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_rq_close_to_sector(io_struct, request,\n\t\t\t\t   bfqd->in_serv_last_pos) &&\n\t    bfqq->entity.parent == in_service_bfqq->entity.parent &&\n\t    bfq_may_be_close_cooperator(bfqq, in_service_bfqq)) {\n\t\tnew_bfqq = bfq_setup_merge(bfqq, in_service_bfqq);\n\t\tif (new_bfqq)\n\t\t\treturn new_bfqq;\n\t}\n\t/*\n\t * Check whether there is a cooperator among currently scheduled\n\t * queues. The only thing we need is that the bio/request is not\n\t * NULL, as we need it to establish whether a cooperator exists.\n\t */\n\tnew_bfqq = bfq_find_close_cooperator(bfqd, bfqq,\n\t\t\tbfq_io_struct_pos(io_struct, request));\n\n\tif (new_bfqq && likely(new_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_may_be_close_cooperator(bfqq, new_bfqq))\n\t\treturn bfq_setup_merge(bfqq, new_bfqq);\n\n\treturn NULL;\n}\n\nstatic void bfq_bfqq_save_state(struct bfq_queue *bfqq)\n{\n\tstruct bfq_io_cq *bic = bfqq->bic;\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\t/*\n\t * If !bfqq->bic, the queue is already shared or its requests\n\t * have already been redirected to a shared queue; both idle window\n\t * and weight raising state have already been saved. Do nothing.\n\t */\n\tif (!bic)\n\t\treturn;\n\n\tbfqq_data->saved_last_serv_time_ns = bfqq->last_serv_time_ns;\n\tbfqq_data->saved_inject_limit =\tbfqq->inject_limit;\n\tbfqq_data->saved_decrease_time_jif = bfqq->decrease_time_jif;\n\n\tbfqq_data->saved_weight = bfqq->entity.orig_weight;\n\tbfqq_data->saved_ttime = bfqq->ttime;\n\tbfqq_data->saved_has_short_ttime =\n\t\tbfq_bfqq_has_short_ttime(bfqq);\n\tbfqq_data->saved_IO_bound = bfq_bfqq_IO_bound(bfqq);\n\tbfqq_data->saved_io_start_time = bfqq->io_start_time;\n\tbfqq_data->saved_tot_idle_time = bfqq->tot_idle_time;\n\tbfqq_data->saved_in_large_burst = bfq_bfqq_in_large_burst(bfqq);\n\tbfqq_data->was_in_burst_list =\n\t\t!hlist_unhashed(&bfqq->burst_list_node);\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t     bfqq->bfqd->low_latency)) {\n\t\t/*\n\t\t * bfqq being merged right after being created: bfqq\n\t\t * would have deserved interactive weight raising, but\n\t\t * did not make it to be set in a weight-raised state,\n\t\t * because of this early merge.\tStore directly the\n\t\t * weight-raising state that would have been assigned\n\t\t * to bfqq, so that to avoid that bfqq unjustly fails\n\t\t * to enjoy weight raising if split soon.\n\t\t */\n\t\tbfqq_data->saved_wr_coeff = bfqq->bfqd->bfq_wr_coeff;\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt =\n\t\t\tbfq_smallest_from_now();\n\t\tbfqq_data->saved_wr_cur_max_time =\n\t\t\tbfq_wr_duration(bfqq->bfqd);\n\t\tbfqq_data->saved_last_wr_start_finish = jiffies;\n\t} else {\n\t\tbfqq_data->saved_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tbfqq_data->saved_service_from_wr =\n\t\t\tbfqq->service_from_wr;\n\t\tbfqq_data->saved_last_wr_start_finish =\n\t\t\tbfqq->last_wr_start_finish;\n\t\tbfqq_data->saved_wr_cur_max_time = bfqq->wr_cur_max_time;\n\t}\n}\n\n\nstatic void\nbfq_reassign_last_bfqq(struct bfq_queue *cur_bfqq, struct bfq_queue *new_bfqq)\n{\n\tif (cur_bfqq->entity.parent &&\n\t    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->entity.parent->last_bfqq_created = new_bfqq;\n\telse if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->bfqd->last_bfqq_created = new_bfqq;\n}\n\nvoid bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * To prevent bfqq's service guarantees from being violated,\n\t * bfqq may be left busy, i.e., queued for service, even if\n\t * empty (see comments in __bfq_bfqq_expire() for\n\t * details). But, if no process will send requests to bfqq any\n\t * longer, then there is no point in keeping bfqq queued for\n\t * service. In addition, keeping bfqq queued for service, but\n\t * with no process ref any longer, may have caused bfqq to be\n\t * freed when dequeued from service. But this is assumed to\n\t * never happen.\n\t */\n\tif (bfq_bfqq_busy(bfqq) && RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq != bfqd->in_service_queue)\n\t\tbfq_del_bfqq_busy(bfqq, false);\n\n\tbfq_reassign_last_bfqq(bfqq, NULL);\n\n\tbfq_put_queue(bfqq);\n}\n\nstatic struct bfq_queue *bfq_merge_bfqqs(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_io_cq *bic,\n\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"merging with queue %lu\",\n\t\t(unsigned long)new_bfqq->pid);\n\t/* Save weight raising and idle window of the merged queues */\n\tbfq_bfqq_save_state(bfqq);\n\tbfq_bfqq_save_state(new_bfqq);\n\tif (bfq_bfqq_IO_bound(bfqq))\n\t\tbfq_mark_bfqq_IO_bound(new_bfqq);\n\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * The processes associated with bfqq are cooperators of the\n\t * processes associated with new_bfqq. So, if bfqq has a\n\t * waker, then assume that all these processes will be happy\n\t * to let bfqq's waker freely inject I/O when they have no\n\t * I/O.\n\t */\n\tif (bfqq->waker_bfqq && !new_bfqq->waker_bfqq &&\n\t    bfqq->waker_bfqq != new_bfqq) {\n\t\tnew_bfqq->waker_bfqq = bfqq->waker_bfqq;\n\t\tnew_bfqq->tentative_waker_bfqq = NULL;\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * new_bfqq->waker_bfqq must be reset. So insert\n\t\t * new_bfqq into the woken_list of the waker. See\n\t\t * bfq_check_waker for details.\n\t\t */\n\t\thlist_add_head(&new_bfqq->woken_list_node,\n\t\t\t       &new_bfqq->waker_bfqq->woken_list);\n\n\t}\n\n\t/*\n\t * If bfqq is weight-raised, then let new_bfqq inherit\n\t * weight-raising. To reduce false positives, neglect the case\n\t * where bfqq has just been created, but has not yet made it\n\t * to be weight-raised (which may happen because EQM may merge\n\t * bfqq even before bfq_add_request is executed for the first\n\t * time for bfqq). Handling this case would however be very\n\t * easy, thanks to the flag just_created.\n\t */\n\tif (new_bfqq->wr_coeff == 1 && bfqq->wr_coeff > 1) {\n\t\tnew_bfqq->wr_coeff = bfqq->wr_coeff;\n\t\tnew_bfqq->wr_cur_max_time = bfqq->wr_cur_max_time;\n\t\tnew_bfqq->last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tnew_bfqq->wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tif (bfq_bfqq_busy(new_bfqq))\n\t\t\tbfqd->wr_busy_queues++;\n\t\tnew_bfqq->entity.prio_changed = 1;\n\t}\n\n\tif (bfqq->wr_coeff > 1) { /* bfqq has given its wr to new_bfqq */\n\t\tbfqq->wr_coeff = 1;\n\t\tbfqq->entity.prio_changed = 1;\n\t\tif (bfq_bfqq_busy(bfqq))\n\t\t\tbfqd->wr_busy_queues--;\n\t}\n\n\tbfq_log_bfqq(bfqd, new_bfqq, \"merge_bfqqs: wr_busy %d\",\n\t\t     bfqd->wr_busy_queues);\n\n\t/*\n\t * Merge queues (that is, let bic redirect its requests to new_bfqq)\n\t */\n\tbic_set_bfqq(bic, new_bfqq, true, bfqq->actuator_idx);\n\tbfq_mark_bfqq_coop(new_bfqq);\n\t/*\n\t * new_bfqq now belongs to at least two bics (it is a shared queue):\n\t * set new_bfqq->bic to NULL. bfqq either:\n\t * - does not belong to any bic any more, and hence bfqq->bic must\n\t *   be set to NULL, or\n\t * - is a queue whose owning bics have already been redirected to a\n\t *   different queue, hence the queue is destined to not belong to\n\t *   any bic soon and bfqq->bic is already NULL (therefore the next\n\t *   assignment causes no harm).\n\t */\n\tnew_bfqq->bic = NULL;\n\t/*\n\t * If the queue is shared, the pid is the pid of one of the associated\n\t * processes. Which pid depends on the exact sequence of merge events\n\t * the queue underwent. So printing such a pid is useless and confusing\n\t * because it reports a random pid between those of the associated\n\t * processes.\n\t * We mark such a queue with a pid -1, and then print SHARED instead of\n\t * a pid in logging messages.\n\t */\n\tnew_bfqq->pid = -1;\n\tbfqq->bic = NULL;\n\n\tbfq_reassign_last_bfqq(bfqq, new_bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n\n\treturn new_bfqq;\n}\n\nstatic bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tbool is_sync = op_is_sync(bio->bi_opf);\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq, *new_bfqq;\n\n\t/*\n\t * Disallow merge of a sync bio into an async request.\n\t */\n\tif (is_sync && !rq_is_sync(rq))\n\t\treturn false;\n\n\t/*\n\t * Lookup the bfqq that this bio will be queued with. Allow\n\t * merge only if rq is queued there.\n\t */\n\tif (!bfqq)\n\t\treturn false;\n\n\t/*\n\t * We take advantage of this function to perform an early merge\n\t * of the queues of possible cooperating processes.\n\t */\n\tnew_bfqq = bfq_setup_cooperator(bfqd, bfqq, bio, false, bfqd->bio_bic);\n\tif (new_bfqq) {\n\t\t/*\n\t\t * bic still points to bfqq, then it has not yet been\n\t\t * redirected to some other bfq_queue, and a queue\n\t\t * merge between bfqq and new_bfqq can be safely\n\t\t * fulfilled, i.e., bic can be redirected to new_bfqq\n\t\t * and bfqq can be put.\n\t\t */\n\t\twhile (bfqq != new_bfqq)\n\t\t\tbfqq = bfq_merge_bfqqs(bfqd, bfqd->bio_bic, bfqq);\n\n\t\t/*\n\t\t * Change also bqfd->bio_bfqq, as\n\t\t * bfqd->bio_bic now points to new_bfqq, and\n\t\t * this function may be invoked again (and then may\n\t\t * use again bqfd->bio_bfqq).\n\t\t */\n\t\tbfqd->bio_bfqq = bfqq;\n\t}\n\n\treturn bfqq == RQ_BFQQ(rq);\n}\n\n/*\n * Set the maximum time for the in-service queue to consume its\n * budget. This prevents seeky processes from lowering the throughput.\n * In practice, a time-slice service scheme is used with seeky\n * processes.\n */\nstatic void bfq_set_budget_timeout(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tunsigned int timeout_coeff;\n\n\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time)\n\t\ttimeout_coeff = 1;\n\telse\n\t\ttimeout_coeff = bfqq->entity.weight / bfqq->entity.orig_weight;\n\n\tbfqd->last_budget_start = ktime_get();\n\n\tbfqq->budget_timeout = jiffies +\n\t\tbfqd->bfq_timeout * timeout_coeff;\n}\n\nstatic void __bfq_set_in_service_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq)\n{\n\tif (bfqq) {\n\t\tbfq_clear_bfqq_fifo_expire(bfqq);\n\n\t\tbfqd->budgets_assigned = (bfqd->budgets_assigned * 7 + 256) / 8;\n\n\t\tif (time_is_before_jiffies(bfqq->last_wr_start_finish) &&\n\t\t    bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    time_is_before_jiffies(bfqq->budget_timeout)) {\n\t\t\t/*\n\t\t\t * For soft real-time queues, move the start\n\t\t\t * of the weight-raising period forward by the\n\t\t\t * time the queue has not received any\n\t\t\t * service. Otherwise, a relatively long\n\t\t\t * service delay is likely to cause the\n\t\t\t * weight-raising period of the queue to end,\n\t\t\t * because of the short duration of the\n\t\t\t * weight-raising period of a soft real-time\n\t\t\t * queue.  It is worth noting that this move\n\t\t\t * is not so dangerous for the other queues,\n\t\t\t * because soft real-time queues are not\n\t\t\t * greedy.\n\t\t\t *\n\t\t\t * To not add a further variable, we use the\n\t\t\t * overloaded field budget_timeout to\n\t\t\t * determine for how long the queue has not\n\t\t\t * received service, i.e., how much time has\n\t\t\t * elapsed since the queue expired. However,\n\t\t\t * this is a little imprecise, because\n\t\t\t * budget_timeout is set to jiffies if bfqq\n\t\t\t * not only expires, but also remains with no\n\t\t\t * request.\n\t\t\t */\n\t\t\tif (time_after(bfqq->budget_timeout,\n\t\t\t\t       bfqq->last_wr_start_finish))\n\t\t\t\tbfqq->last_wr_start_finish +=\n\t\t\t\t\tjiffies - bfqq->budget_timeout;\n\t\t\telse\n\t\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\n\t\tbfq_set_budget_timeout(bfqd, bfqq);\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t     \"set_in_service_queue, cur-budget = %d\",\n\t\t\t     bfqq->entity.budget);\n\t}\n\n\tbfqd->in_service_queue = bfqq;\n\tbfqd->in_serv_last_pos = 0;\n}\n\n/*\n * Get and set a new queue for service.\n */\nstatic struct bfq_queue *bfq_set_in_service_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfq_get_next_queue(bfqd);\n\n\t__bfq_set_in_service_queue(bfqd, bfqq);\n\treturn bfqq;\n}\n\nstatic void bfq_arm_slice_timer(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\tu32 sl;\n\n\tbfq_mark_bfqq_wait_request(bfqq);\n\n\t/*\n\t * We don't want to idle for seeks, but we do want to allow\n\t * fair distribution of slice time for a process doing back-to-back\n\t * seeks. So allow a little bit of time for him to submit a new rq.\n\t */\n\tsl = bfqd->bfq_slice_idle;\n\t/*\n\t * Unless the queue is being weight-raised or the scenario is\n\t * asymmetric, grant only minimum idle time if the queue\n\t * is seeky. A long idling is preserved for a weight-raised\n\t * queue, or, more in general, in an asymmetric scenario,\n\t * because a long idling is needed for guaranteeing to a queue\n\t * its reserved share of the throughput (in particular, it is\n\t * needed if the queue has a higher weight than some other\n\t * queue).\n\t */\n\tif (BFQQ_SEEKY(bfqq) && bfqq->wr_coeff == 1 &&\n\t    !bfq_asymmetric_scenario(bfqd, bfqq))\n\t\tsl = min_t(u64, sl, BFQ_MIN_TT);\n\telse if (bfqq->wr_coeff > 1)\n\t\tsl = max_t(u32, sl, 20ULL * NSEC_PER_MSEC);\n\n\tbfqd->last_idling_start = ktime_get();\n\tbfqd->last_idling_start_jiffies = jiffies;\n\n\thrtimer_start(&bfqd->idle_slice_timer, ns_to_ktime(sl),\n\t\t      HRTIMER_MODE_REL);\n\tbfqg_stats_set_start_idle_time(bfqq_group(bfqq));\n}\n\n/*\n * In autotuning mode, max_budget is dynamically recomputed as the\n * amount of sectors transferred in timeout at the estimated peak\n * rate. This enables BFQ to utilize a full timeslice with a full\n * budget, even if the in-service queue is served at peak rate. And\n * this maximises throughput with sequential workloads.\n */\nstatic unsigned long bfq_calc_max_budget(struct bfq_data *bfqd)\n{\n\treturn (u64)bfqd->peak_rate * USEC_PER_MSEC *\n\t\tjiffies_to_msecs(bfqd->bfq_timeout)>>BFQ_RATE_SHIFT;\n}\n\n/*\n * Update parameters related to throughput and responsiveness, as a\n * function of the estimated peak rate. See comments on\n * bfq_calc_max_budget(), and on the ref_wr_duration array.\n */\nstatic void update_thr_responsiveness_params(struct bfq_data *bfqd)\n{\n\tif (bfqd->bfq_user_max_budget == 0) {\n\t\tbfqd->bfq_max_budget =\n\t\t\tbfq_calc_max_budget(bfqd);\n\t\tbfq_log(bfqd, \"new max_budget = %d\", bfqd->bfq_max_budget);\n\t}\n}\n\nstatic void bfq_reset_rate_computation(struct bfq_data *bfqd,\n\t\t\t\t       struct request *rq)\n{\n\tif (rq != NULL) { /* new rq dispatch now, reset accordingly */\n\t\tbfqd->last_dispatch = bfqd->first_dispatch = ktime_get_ns();\n\t\tbfqd->peak_rate_samples = 1;\n\t\tbfqd->sequential_samples = 0;\n\t\tbfqd->tot_sectors_dispatched = bfqd->last_rq_max_size =\n\t\t\tblk_rq_sectors(rq);\n\t} else /* no new rq dispatched, just reset the number of samples */\n\t\tbfqd->peak_rate_samples = 0; /* full re-init on next disp. */\n\n\tbfq_log(bfqd,\n\t\t\"reset_rate_computation at end, sample %u/%u tot_sects %llu\",\n\t\tbfqd->peak_rate_samples, bfqd->sequential_samples,\n\t\tbfqd->tot_sectors_dispatched);\n}\n\nstatic void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)\n{\n\tu32 rate, weight, divisor;\n\n\t/*\n\t * For the convergence property to hold (see comments on\n\t * bfq_update_peak_rate()) and for the assessment to be\n\t * reliable, a minimum number of samples must be present, and\n\t * a minimum amount of time must have elapsed. If not so, do\n\t * not compute new rate. Just reset parameters, to get ready\n\t * for a new evaluation attempt.\n\t */\n\tif (bfqd->peak_rate_samples < BFQ_RATE_MIN_SAMPLES ||\n\t    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL)\n\t\tgoto reset_computation;\n\n\t/*\n\t * If a new request completion has occurred after last\n\t * dispatch, then, to approximate the rate at which requests\n\t * have been served by the device, it is more precise to\n\t * extend the observation interval to the last completion.\n\t */\n\tbfqd->delta_from_first =\n\t\tmax_t(u64, bfqd->delta_from_first,\n\t\t      bfqd->last_completion - bfqd->first_dispatch);\n\n\t/*\n\t * Rate computed in sects/usec, and not sects/nsec, for\n\t * precision issues.\n\t */\n\trate = div64_ul(bfqd->tot_sectors_dispatched<<BFQ_RATE_SHIFT,\n\t\t\tdiv_u64(bfqd->delta_from_first, NSEC_PER_USEC));\n\n\t/*\n\t * Peak rate not updated if:\n\t * - the percentage of sequential dispatches is below 3/4 of the\n\t *   total, and rate is below the current estimated peak rate\n\t * - rate is unreasonably high (> 20M sectors/sec)\n\t */\n\tif ((bfqd->sequential_samples < (3 * bfqd->peak_rate_samples)>>2 &&\n\t     rate <= bfqd->peak_rate) ||\n\t\trate > 20<<BFQ_RATE_SHIFT)\n\t\tgoto reset_computation;\n\n\t/*\n\t * We have to update the peak rate, at last! To this purpose,\n\t * we use a low-pass filter. We compute the smoothing constant\n\t * of the filter as a function of the 'weight' of the new\n\t * measured rate.\n\t *\n\t * As can be seen in next formulas, we define this weight as a\n\t * quantity proportional to how sequential the workload is,\n\t * and to how long the observation time interval is.\n\t *\n\t * The weight runs from 0 to 8. The maximum value of the\n\t * weight, 8, yields the minimum value for the smoothing\n\t * constant. At this minimum value for the smoothing constant,\n\t * the measured rate contributes for half of the next value of\n\t * the estimated peak rate.\n\t *\n\t * So, the first step is to compute the weight as a function\n\t * of how sequential the workload is. Note that the weight\n\t * cannot reach 9, because bfqd->sequential_samples cannot\n\t * become equal to bfqd->peak_rate_samples, which, in its\n\t * turn, holds true because bfqd->sequential_samples is not\n\t * incremented for the first sample.\n\t */\n\tweight = (9 * bfqd->sequential_samples) / bfqd->peak_rate_samples;\n\n\t/*\n\t * Second step: further refine the weight as a function of the\n\t * duration of the observation interval.\n\t */\n\tweight = min_t(u32, 8,\n\t\t       div_u64(weight * bfqd->delta_from_first,\n\t\t\t       BFQ_RATE_REF_INTERVAL));\n\n\t/*\n\t * Divisor ranging from 10, for minimum weight, to 2, for\n\t * maximum weight.\n\t */\n\tdivisor = 10 - weight;\n\n\t/*\n\t * Finally, update peak rate:\n\t *\n\t * peak_rate = peak_rate * (divisor-1) / divisor  +  rate / divisor\n\t */\n\tbfqd->peak_rate *= divisor-1;\n\tbfqd->peak_rate /= divisor;\n\trate /= divisor; /* smoothing constant alpha = 1/divisor */\n\n\tbfqd->peak_rate += rate;\n\n\t/*\n\t * For a very slow device, bfqd->peak_rate can reach 0 (see\n\t * the minimum representable values reported in the comments\n\t * on BFQ_RATE_SHIFT). Push to 1 if this happens, to avoid\n\t * divisions by zero where bfqd->peak_rate is used as a\n\t * divisor.\n\t */\n\tbfqd->peak_rate = max_t(u32, 1, bfqd->peak_rate);\n\n\tupdate_thr_responsiveness_params(bfqd);\n\nreset_computation:\n\tbfq_reset_rate_computation(bfqd, rq);\n}\n\n/*\n * Update the read/write peak rate (the main quantity used for\n * auto-tuning, see update_thr_responsiveness_params()).\n *\n * It is not trivial to estimate the peak rate (correctly): because of\n * the presence of sw and hw queues between the scheduler and the\n * device components that finally serve I/O requests, it is hard to\n * say exactly when a given dispatched request is served inside the\n * device, and for how long. As a consequence, it is hard to know\n * precisely at what rate a given set of requests is actually served\n * by the device.\n *\n * On the opposite end, the dispatch time of any request is trivially\n * available, and, from this piece of information, the \"dispatch rate\"\n * of requests can be immediately computed. So, the idea in the next\n * function is to use what is known, namely request dispatch times\n * (plus, when useful, request completion times), to estimate what is\n * unknown, namely in-device request service rate.\n *\n * The main issue is that, because of the above facts, the rate at\n * which a certain set of requests is dispatched over a certain time\n * interval can vary greatly with respect to the rate at which the\n * same requests are then served. But, since the size of any\n * intermediate queue is limited, and the service scheme is lossless\n * (no request is silently dropped), the following obvious convergence\n * property holds: the number of requests dispatched MUST become\n * closer and closer to the number of requests completed as the\n * observation interval grows. This is the key property used in\n * the next function to estimate the peak service rate as a function\n * of the observed dispatch rate. The function assumes to be invoked\n * on every request dispatch.\n */\nstatic void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tif (bfqd->peak_rate_samples == 0) { /* first dispatch */\n\t\tbfq_log(bfqd, \"update_peak_rate: goto reset, samples %d\",\n\t\t\tbfqd->peak_rate_samples);\n\t\tbfq_reset_rate_computation(bfqd, rq);\n\t\tgoto update_last_values; /* will add one sample */\n\t}\n\n\t/*\n\t * Device idle for very long: the observation interval lasting\n\t * up to this dispatch cannot be a valid observation interval\n\t * for computing a new peak rate (similarly to the late-\n\t * completion event in bfq_completed_request()). Go to\n\t * update_rate_and_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - start a new observation interval with this dispatch\n\t */\n\tif (now_ns - bfqd->last_dispatch > 100*NSEC_PER_MSEC &&\n\t    bfqd->tot_rq_in_driver == 0)\n\t\tgoto update_rate_and_reset;\n\n\t/* Update sampling information */\n\tbfqd->peak_rate_samples++;\n\n\tif ((bfqd->tot_rq_in_driver > 0 ||\n\t\tnow_ns - bfqd->last_completion < BFQ_MIN_TT)\n\t    && !BFQ_RQ_SEEKY(bfqd, bfqd->last_position, rq))\n\t\tbfqd->sequential_samples++;\n\n\tbfqd->tot_sectors_dispatched += blk_rq_sectors(rq);\n\n\t/* Reset max observed rq size every 32 dispatches */\n\tif (likely(bfqd->peak_rate_samples % 32))\n\t\tbfqd->last_rq_max_size =\n\t\t\tmax_t(u32, blk_rq_sectors(rq), bfqd->last_rq_max_size);\n\telse\n\t\tbfqd->last_rq_max_size = blk_rq_sectors(rq);\n\n\tbfqd->delta_from_first = now_ns - bfqd->first_dispatch;\n\n\t/* Target observation interval not yet reached, go on sampling */\n\tif (bfqd->delta_from_first < BFQ_RATE_REF_INTERVAL)\n\t\tgoto update_last_values;\n\nupdate_rate_and_reset:\n\tbfq_update_rate_reset(bfqd, rq);\nupdate_last_values:\n\tbfqd->last_position = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\tif (RQ_BFQQ(rq) == bfqd->in_service_queue)\n\t\tbfqd->in_serv_last_pos = bfqd->last_position;\n\tbfqd->last_dispatch = now_ns;\n}\n\n/*\n * Remove request from internal lists.\n */\nstatic void bfq_dispatch_remove(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\n\t/*\n\t * For consistency, the next instruction should have been\n\t * executed after removing the request from the queue and\n\t * dispatching it.  We execute instead this instruction before\n\t * bfq_remove_request() (and hence introduce a temporary\n\t * inconsistency), for efficiency.  In fact, should this\n\t * dispatch occur for a non in-service bfqq, this anticipated\n\t * increment prevents two counters related to bfqq->dispatched\n\t * from risking to be, first, uselessly decremented, and then\n\t * incremented again when the (new) value of bfqq->dispatched\n\t * happens to be taken into account.\n\t */\n\tbfqq->dispatched++;\n\tbfq_update_peak_rate(q->elevator->elevator_data, rq);\n\n\tbfq_remove_request(q, rq);\n}\n\n/*\n * There is a case where idling does not have to be performed for\n * throughput concerns, but to preserve the throughput share of\n * the process associated with bfqq.\n *\n * To introduce this case, we can note that allowing the drive\n * to enqueue more than one request at a time, and hence\n * delegating de facto final scheduling decisions to the\n * drive's internal scheduler, entails loss of control on the\n * actual request service order. In particular, the critical\n * situation is when requests from different processes happen\n * to be present, at the same time, in the internal queue(s)\n * of the drive. In such a situation, the drive, by deciding\n * the service order of the internally-queued requests, does\n * determine also the actual throughput distribution among\n * these processes. But the drive typically has no notion or\n * concern about per-process throughput distribution, and\n * makes its decisions only on a per-request basis. Therefore,\n * the service distribution enforced by the drive's internal\n * scheduler is likely to coincide with the desired throughput\n * distribution only in a completely symmetric, or favorably\n * skewed scenario where:\n * (i-a) each of these processes must get the same throughput as\n *\t the others,\n * (i-b) in case (i-a) does not hold, it holds that the process\n *       associated with bfqq must receive a lower or equal\n *\t throughput than any of the other processes;\n * (ii)  the I/O of each process has the same properties, in\n *       terms of locality (sequential or random), direction\n *       (reads or writes), request sizes, greediness\n *       (from I/O-bound to sporadic), and so on;\n\n * In fact, in such a scenario, the drive tends to treat the requests\n * of each process in about the same way as the requests of the\n * others, and thus to provide each of these processes with about the\n * same throughput.  This is exactly the desired throughput\n * distribution if (i-a) holds, or, if (i-b) holds instead, this is an\n * even more convenient distribution for (the process associated with)\n * bfqq.\n *\n * In contrast, in any asymmetric or unfavorable scenario, device\n * idling (I/O-dispatch plugging) is certainly needed to guarantee\n * that bfqq receives its assigned fraction of the device throughput\n * (see [1] for details).\n *\n * The problem is that idling may significantly reduce throughput with\n * certain combinations of types of I/O and devices. An important\n * example is sync random I/O on flash storage with command\n * queueing. So, unless bfqq falls in cases where idling also boosts\n * throughput, it is important to check conditions (i-a), i(-b) and\n * (ii) accurately, so as to avoid idling when not strictly needed for\n * service guarantees.\n *\n * Unfortunately, it is extremely difficult to thoroughly check\n * condition (ii). And, in case there are active groups, it becomes\n * very difficult to check conditions (i-a) and (i-b) too.  In fact,\n * if there are active groups, then, for conditions (i-a) or (i-b) to\n * become false 'indirectly', it is enough that an active group\n * contains more active processes or sub-groups than some other active\n * group. More precisely, for conditions (i-a) or (i-b) to become\n * false because of such a group, it is not even necessary that the\n * group is (still) active: it is sufficient that, even if the group\n * has become inactive, some of its descendant processes still have\n * some request already dispatched but still waiting for\n * completion. In fact, requests have still to be guaranteed their\n * share of the throughput even after being dispatched. In this\n * respect, it is easy to show that, if a group frequently becomes\n * inactive while still having in-flight requests, and if, when this\n * happens, the group is not considered in the calculation of whether\n * the scenario is asymmetric, then the group may fail to be\n * guaranteed its fair share of the throughput (basically because\n * idling may not be performed for the descendant processes of the\n * group, but it had to be).  We address this issue with the following\n * bi-modal behavior, implemented in the function\n * bfq_asymmetric_scenario().\n *\n * If there are groups with requests waiting for completion\n * (as commented above, some of these groups may even be\n * already inactive), then the scenario is tagged as\n * asymmetric, conservatively, without checking any of the\n * conditions (i-a), (i-b) or (ii). So the device is idled for bfqq.\n * This behavior matches also the fact that groups are created\n * exactly if controlling I/O is a primary concern (to\n * preserve bandwidth and latency guarantees).\n *\n * On the opposite end, if there are no groups with requests waiting\n * for completion, then only conditions (i-a) and (i-b) are actually\n * controlled, i.e., provided that conditions (i-a) or (i-b) holds,\n * idling is not performed, regardless of whether condition (ii)\n * holds.  In other words, only if conditions (i-a) and (i-b) do not\n * hold, then idling is allowed, and the device tends to be prevented\n * from queueing many requests, possibly of several processes. Since\n * there are no groups with requests waiting for completion, then, to\n * control conditions (i-a) and (i-b) it is enough to check just\n * whether all the queues with requests waiting for completion also\n * have the same weight.\n *\n * Not checking condition (ii) evidently exposes bfqq to the\n * risk of getting less throughput than its fair share.\n * However, for queues with the same weight, a further\n * mechanism, preemption, mitigates or even eliminates this\n * problem. And it does so without consequences on overall\n * throughput. This mechanism and its benefits are explained\n * in the next three paragraphs.\n *\n * Even if a queue, say Q, is expired when it remains idle, Q\n * can still preempt the new in-service queue if the next\n * request of Q arrives soon (see the comments on\n * bfq_bfqq_update_budg_for_activation). If all queues and\n * groups have the same weight, this form of preemption,\n * combined with the hole-recovery heuristic described in the\n * comments on function bfq_bfqq_update_budg_for_activation,\n * are enough to preserve a correct bandwidth distribution in\n * the mid term, even without idling. In fact, even if not\n * idling allows the internal queues of the device to contain\n * many requests, and thus to reorder requests, we can rather\n * safely assume that the internal scheduler still preserves a\n * minimum of mid-term fairness.\n *\n * More precisely, this preemption-based, idleless approach\n * provides fairness in terms of IOPS, and not sectors per\n * second. This can be seen with a simple example. Suppose\n * that there are two queues with the same weight, but that\n * the first queue receives requests of 8 sectors, while the\n * second queue receives requests of 1024 sectors. In\n * addition, suppose that each of the two queues contains at\n * most one request at a time, which implies that each queue\n * always remains idle after it is served. Finally, after\n * remaining idle, each queue receives very quickly a new\n * request. It follows that the two queues are served\n * alternatively, preempting each other if needed. This\n * implies that, although both queues have the same weight,\n * the queue with large requests receives a service that is\n * 1024/8 times as high as the service received by the other\n * queue.\n *\n * The motivation for using preemption instead of idling (for\n * queues with the same weight) is that, by not idling,\n * service guarantees are preserved (completely or at least in\n * part) without minimally sacrificing throughput. And, if\n * there is no active group, then the primary expectation for\n * this device is probably a high throughput.\n *\n * We are now left only with explaining the two sub-conditions in the\n * additional compound condition that is checked below for deciding\n * whether the scenario is asymmetric. To explain the first\n * sub-condition, we need to add that the function\n * bfq_asymmetric_scenario checks the weights of only\n * non-weight-raised queues, for efficiency reasons (see comments on\n * bfq_weights_tree_add()). Then the fact that bfqq is weight-raised\n * is checked explicitly here. More precisely, the compound condition\n * below takes into account also the fact that, even if bfqq is being\n * weight-raised, the scenario is still symmetric if all queues with\n * requests waiting for completion happen to be\n * weight-raised. Actually, we should be even more precise here, and\n * differentiate between interactive weight raising and soft real-time\n * weight raising.\n *\n * The second sub-condition checked in the compound condition is\n * whether there is a fair amount of already in-flight I/O not\n * belonging to bfqq. If so, I/O dispatching is to be plugged, for the\n * following reason. The drive may decide to serve in-flight\n * non-bfqq's I/O requests before bfqq's ones, thereby delaying the\n * arrival of new I/O requests for bfqq (recall that bfqq is sync). If\n * I/O-dispatching is not plugged, then, while bfqq remains empty, a\n * basically uncontrolled amount of I/O from other queues may be\n * dispatched too, possibly causing the service of bfqq's I/O to be\n * delayed even longer in the drive. This problem gets more and more\n * serious as the speed and the queue depth of the drive grow,\n * because, as these two quantities grow, the probability to find no\n * queue busy but many requests in flight grows too. By contrast,\n * plugging I/O dispatching minimizes the delay induced by already\n * in-flight I/O, and enables bfqq to recover the bandwidth it may\n * lose because of this delay.\n *\n * As a side note, it is worth considering that the above\n * device-idling countermeasures may however fail in the following\n * unlucky scenario: if I/O-dispatch plugging is (correctly) disabled\n * in a time period during which all symmetry sub-conditions hold, and\n * therefore the device is allowed to enqueue many requests, but at\n * some later point in time some sub-condition stops to hold, then it\n * may become impossible to make requests be served in the desired\n * order until all the requests already queued in the device have been\n * served. The last sub-condition commented above somewhat mitigates\n * this problem for weight-raised queues.\n *\n * However, as an additional mitigation for this problem, we preserve\n * plugging for a special symmetric case that may suddenly turn into\n * asymmetric: the case where only bfqq is busy. In this case, not\n * expiring bfqq does not cause any harm to any other queues in terms\n * of service guarantees. In contrast, it avoids the following unlucky\n * sequence of events: (1) bfqq is expired, (2) a new queue with a\n * lower weight than bfqq becomes busy (or more queues), (3) the new\n * queue is served until a new request arrives for bfqq, (4) when bfqq\n * is finally served, there are so many requests of the new queue in\n * the drive that the pending requests for bfqq take a lot of time to\n * be served. In particular, event (2) may case even already\n * dispatched requests of bfqq to be delayed, inside the drive. So, to\n * avoid this series of events, the scenario is preventively declared\n * as asymmetric also if bfqq is the only busy queues\n */\nstatic bool idling_needed_for_service_guarantees(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tint tot_busy_queues = bfq_tot_busy_queues(bfqd);\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\treturn (bfqq->wr_coeff > 1 &&\n\t\t(bfqd->wr_busy_queues < tot_busy_queues ||\n\t\t bfqd->tot_rq_in_driver >= bfqq->dispatched + 4)) ||\n\t\tbfq_asymmetric_scenario(bfqd, bfqq) ||\n\t\ttot_busy_queues == 1;\n}\n\nstatic bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t      enum bfqq_expiration reason)\n{\n\t/*\n\t * If this bfqq is shared between multiple processes, check\n\t * to make sure that those processes are still issuing I/Os\n\t * within the mean seek distance. If not, it may be time to\n\t * break the queues apart again.\n\t */\n\tif (bfq_bfqq_coop(bfqq) && BFQQ_SEEKY(bfqq))\n\t\tbfq_mark_bfqq_split_coop(bfqq);\n\n\t/*\n\t * Consider queues with a higher finish virtual time than\n\t * bfqq. If idling_needed_for_service_guarantees(bfqq) returns\n\t * true, then bfqq's bandwidth would be violated if an\n\t * uncontrolled amount of I/O from these queues were\n\t * dispatched while bfqq is waiting for its new I/O to\n\t * arrive. This is exactly what may happen if this is a forced\n\t * expiration caused by a preemption attempt, and if bfqq is\n\t * not re-scheduled. To prevent this from happening, re-queue\n\t * bfqq if it needs I/O-dispatch plugging, even if it is\n\t * empty. By doing so, bfqq is granted to be served before the\n\t * above queues (provided that bfqq is of course eligible).\n\t */\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    !(reason == BFQQE_PREEMPTED &&\n\t      idling_needed_for_service_guarantees(bfqd, bfqq))) {\n\t\tif (bfqq->dispatched == 0)\n\t\t\t/*\n\t\t\t * Overloading budget_timeout field to store\n\t\t\t * the time at which the queue remains with no\n\t\t\t * backlog and no outstanding request; used by\n\t\t\t * the weight-raising mechanism.\n\t\t\t */\n\t\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_busy(bfqq, true);\n\t} else {\n\t\tbfq_requeue_bfqq(bfqd, bfqq, true);\n\t\t/*\n\t\t * Resort priority tree of potential close cooperators.\n\t\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t\t */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing &&\n\t\t\t     !RB_EMPTY_ROOT(&bfqq->sort_list)))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\t/*\n\t * All in-service entities must have been properly deactivated\n\t * or requeued before executing the next function, which\n\t * resets all in-service entities as no more in service. This\n\t * may cause bfqq to be freed. If this happens, the next\n\t * function returns true.\n\t */\n\treturn __bfq_bfqd_reset_in_service(bfqd);\n}\n\n/**\n * __bfq_bfqq_recalc_budget - try to adapt the budget to the @bfqq behavior.\n * @bfqd: device data.\n * @bfqq: queue to update.\n * @reason: reason for expiration.\n *\n * Handle the feedback on @bfqq budget at queue expiration.\n * See the body for detailed comments.\n */\nstatic void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,\n\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t     enum bfqq_expiration reason)\n{\n\tstruct request *next_rq;\n\tint budget, min_budget;\n\n\tmin_budget = bfq_min_budget(bfqd);\n\n\tif (bfqq->wr_coeff == 1)\n\t\tbudget = bfqq->max_budget;\n\telse /*\n\t      * Use a constant, low budget for weight-raised queues,\n\t      * to help achieve a low latency. Keep it slightly higher\n\t      * than the minimum possible budget, to cause a little\n\t      * bit fewer expirations.\n\t      */\n\t\tbudget = 2 * min_budget;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last budg %d, budg left %d\",\n\t\tbfqq->entity.budget, bfq_bfqq_budget_left(bfqq));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last max_budg %d, min budg %d\",\n\t\tbudget, bfq_min_budget(bfqd));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: sync %d, seeky %d\",\n\t\tbfq_bfqq_sync(bfqq), BFQQ_SEEKY(bfqd->in_service_queue));\n\n\tif (bfq_bfqq_sync(bfqq) && bfqq->wr_coeff == 1) {\n\t\tswitch (reason) {\n\t\t/*\n\t\t * Caveat: in all the following cases we trade latency\n\t\t * for throughput.\n\t\t */\n\t\tcase BFQQE_TOO_IDLE:\n\t\t\t/*\n\t\t\t * This is the only case where we may reduce\n\t\t\t * the budget: if there is no request of the\n\t\t\t * process still waiting for completion, then\n\t\t\t * we assume (tentatively) that the timer has\n\t\t\t * expired because the batch of requests of\n\t\t\t * the process could have been served with a\n\t\t\t * smaller budget.  Hence, betting that\n\t\t\t * process will behave in the same way when it\n\t\t\t * becomes backlogged again, we reduce its\n\t\t\t * next budget.  As long as we guess right,\n\t\t\t * this budget cut reduces the latency\n\t\t\t * experienced by the process.\n\t\t\t *\n\t\t\t * However, if there are still outstanding\n\t\t\t * requests, then the process may have not yet\n\t\t\t * issued its next request just because it is\n\t\t\t * still waiting for the completion of some of\n\t\t\t * the still outstanding ones.  So in this\n\t\t\t * subcase we do not reduce its budget, on the\n\t\t\t * contrary we increase it to possibly boost\n\t\t\t * the throughput, as discussed in the\n\t\t\t * comments to the BUDGET_TIMEOUT case.\n\t\t\t */\n\t\t\tif (bfqq->dispatched > 0) /* still outstanding reqs */\n\t\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\telse {\n\t\t\t\tif (budget > 5 * min_budget)\n\t\t\t\t\tbudget -= 4 * min_budget;\n\t\t\t\telse\n\t\t\t\t\tbudget = min_budget;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_TIMEOUT:\n\t\t\t/*\n\t\t\t * We double the budget here because it gives\n\t\t\t * the chance to boost the throughput if this\n\t\t\t * is not a seeky process (and has bumped into\n\t\t\t * this timeout because of, e.g., ZBR).\n\t\t\t */\n\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_EXHAUSTED:\n\t\t\t/*\n\t\t\t * The process still has backlog, and did not\n\t\t\t * let either the budget timeout or the disk\n\t\t\t * idling timeout expire. Hence it is not\n\t\t\t * seeky, has a short thinktime and may be\n\t\t\t * happy with a higher budget too. So\n\t\t\t * definitely increase the budget of this good\n\t\t\t * candidate to boost the disk throughput.\n\t\t\t */\n\t\t\tbudget = min(budget * 4, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_NO_MORE_REQUESTS:\n\t\t\t/*\n\t\t\t * For queues that expire for this reason, it\n\t\t\t * is particularly important to keep the\n\t\t\t * budget close to the actual service they\n\t\t\t * need. Doing so reduces the timestamp\n\t\t\t * misalignment problem described in the\n\t\t\t * comments in the body of\n\t\t\t * __bfq_activate_entity. In fact, suppose\n\t\t\t * that a queue systematically expires for\n\t\t\t * BFQQE_NO_MORE_REQUESTS and presents a\n\t\t\t * new request in time to enjoy timestamp\n\t\t\t * back-shifting. The larger the budget of the\n\t\t\t * queue is with respect to the service the\n\t\t\t * queue actually requests in each service\n\t\t\t * slot, the more times the queue can be\n\t\t\t * reactivated with the same virtual finish\n\t\t\t * time. It follows that, even if this finish\n\t\t\t * time is pushed to the system virtual time\n\t\t\t * to reduce the consequent timestamp\n\t\t\t * misalignment, the queue unjustly enjoys for\n\t\t\t * many re-activations a lower finish time\n\t\t\t * than all newly activated queues.\n\t\t\t *\n\t\t\t * The service needed by bfqq is measured\n\t\t\t * quite precisely by bfqq->entity.service.\n\t\t\t * Since bfqq does not enjoy device idling,\n\t\t\t * bfqq->entity.service is equal to the number\n\t\t\t * of sectors that the process associated with\n\t\t\t * bfqq requested to read/write before waiting\n\t\t\t * for request completions, or blocking for\n\t\t\t * other reasons.\n\t\t\t */\n\t\t\tbudget = max_t(int, bfqq->entity.service, min_budget);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else if (!bfq_bfqq_sync(bfqq)) {\n\t\t/*\n\t\t * Async queues get always the maximum possible\n\t\t * budget, as for them we do not care about latency\n\t\t * (in addition, their ability to dispatch is limited\n\t\t * by the charging factor).\n\t\t */\n\t\tbudget = bfqd->bfq_max_budget;\n\t}\n\n\tbfqq->max_budget = budget;\n\n\tif (bfqd->budgets_assigned >= bfq_stats_min_budgets &&\n\t    !bfqd->bfq_user_max_budget)\n\t\tbfqq->max_budget = min(bfqq->max_budget, bfqd->bfq_max_budget);\n\n\t/*\n\t * If there is still backlog, then assign a new budget, making\n\t * sure that it is large enough for the next request.  Since\n\t * the finish time of bfqq must be kept in sync with the\n\t * budget, be sure to call __bfq_bfqq_expire() *after* this\n\t * update.\n\t *\n\t * If there is no backlog, then no need to update the budget;\n\t * it will be updated on the arrival of a new request.\n\t */\n\tnext_rq = bfqq->next_rq;\n\tif (next_rq)\n\t\tbfqq->entity.budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t\t    bfq_serv_to_charge(next_rq, bfqq));\n\n\tbfq_log_bfqq(bfqd, bfqq, \"head sect: %u, new budget %d\",\n\t\t\tnext_rq ? blk_rq_sectors(next_rq) : 0,\n\t\t\tbfqq->entity.budget);\n}\n\n/*\n * Return true if the process associated with bfqq is \"slow\". The slow\n * flag is used, in addition to the budget timeout, to reduce the\n * amount of service provided to seeky processes, and thus reduce\n * their chances to lower the throughput. More details in the comments\n * on the function bfq_bfqq_expire().\n *\n * An important observation is in order: as discussed in the comments\n * on the function bfq_update_peak_rate(), with devices with internal\n * queues, it is hard if ever possible to know when and for how long\n * an I/O request is processed by the device (apart from the trivial\n * I/O pattern where a new request is dispatched only after the\n * previous one has been completed). This makes it hard to evaluate\n * the real rate at which the I/O requests of each bfq_queue are\n * served.  In fact, for an I/O scheduler like BFQ, serving a\n * bfq_queue means just dispatching its requests during its service\n * slot (i.e., until the budget of the queue is exhausted, or the\n * queue remains idle, or, finally, a timeout fires). But, during the\n * service slot of a bfq_queue, around 100 ms at most, the device may\n * be even still processing requests of bfq_queues served in previous\n * service slots. On the opposite end, the requests of the in-service\n * bfq_queue may be completed after the service slot of the queue\n * finishes.\n *\n * Anyway, unless more sophisticated solutions are used\n * (where possible), the sum of the sizes of the requests dispatched\n * during the service slot of a bfq_queue is probably the only\n * approximation available for the service received by the bfq_queue\n * during its service slot. And this sum is the quantity used in this\n * function to evaluate the I/O speed of a process.\n */\nstatic bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t\t bool compensate, unsigned long *delta_ms)\n{\n\tktime_t delta_ktime;\n\tu32 delta_usecs;\n\tbool slow = BFQQ_SEEKY(bfqq); /* if delta too short, use seekyness */\n\n\tif (!bfq_bfqq_sync(bfqq))\n\t\treturn false;\n\n\tif (compensate)\n\t\tdelta_ktime = bfqd->last_idling_start;\n\telse\n\t\tdelta_ktime = ktime_get();\n\tdelta_ktime = ktime_sub(delta_ktime, bfqd->last_budget_start);\n\tdelta_usecs = ktime_to_us(delta_ktime);\n\n\t/* don't use too short time intervals */\n\tif (delta_usecs < 1000) {\n\t\tif (blk_queue_nonrot(bfqd->queue))\n\t\t\t /*\n\t\t\t  * give same worst-case guarantees as idling\n\t\t\t  * for seeky\n\t\t\t  */\n\t\t\t*delta_ms = BFQ_MIN_TT / NSEC_PER_MSEC;\n\t\telse /* charge at least one seek */\n\t\t\t*delta_ms = bfq_slice_idle / NSEC_PER_MSEC;\n\n\t\treturn slow;\n\t}\n\n\t*delta_ms = delta_usecs / USEC_PER_MSEC;\n\n\t/*\n\t * Use only long (> 20ms) intervals to filter out excessive\n\t * spikes in service rate estimation.\n\t */\n\tif (delta_usecs > 20000) {\n\t\t/*\n\t\t * Caveat for rotational devices: processes doing I/O\n\t\t * in the slower disk zones tend to be slow(er) even\n\t\t * if not seeky. In this respect, the estimated peak\n\t\t * rate is likely to be an average over the disk\n\t\t * surface. Accordingly, to not be too harsh with\n\t\t * unlucky processes, a process is deemed slow only if\n\t\t * its rate has been lower than half of the estimated\n\t\t * peak rate.\n\t\t */\n\t\tslow = bfqq->entity.service < bfqd->bfq_max_budget / 2;\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"bfq_bfqq_is_slow: slow %d\", slow);\n\n\treturn slow;\n}\n\n/*\n * To be deemed as soft real-time, an application must meet two\n * requirements. First, the application must not require an average\n * bandwidth higher than the approximate bandwidth required to playback or\n * record a compressed high-definition video.\n * The next function is invoked on the completion of the last request of a\n * batch, to compute the next-start time instant, soft_rt_next_start, such\n * that, if the next request of the application does not arrive before\n * soft_rt_next_start, then the above requirement on the bandwidth is met.\n *\n * The second requirement is that the request pattern of the application is\n * isochronous, i.e., that, after issuing a request or a batch of requests,\n * the application stops issuing new requests until all its pending requests\n * have been completed. After that, the application may issue a new batch,\n * and so on.\n * For this reason the next function is invoked to compute\n * soft_rt_next_start only for applications that meet this requirement,\n * whereas soft_rt_next_start is set to infinity for applications that do\n * not.\n *\n * Unfortunately, even a greedy (i.e., I/O-bound) application may\n * happen to meet, occasionally or systematically, both the above\n * bandwidth and isochrony requirements. This may happen at least in\n * the following circumstances. First, if the CPU load is high. The\n * application may stop issuing requests while the CPUs are busy\n * serving other processes, then restart, then stop again for a while,\n * and so on. The other circumstances are related to the storage\n * device: the storage device is highly loaded or reaches a low-enough\n * throughput with the I/O of the application (e.g., because the I/O\n * is random and/or the device is slow). In all these cases, the\n * I/O of the application may be simply slowed down enough to meet\n * the bandwidth and isochrony requirements. To reduce the probability\n * that greedy applications are deemed as soft real-time in these\n * corner cases, a further rule is used in the computation of\n * soft_rt_next_start: the return value of this function is forced to\n * be higher than the maximum between the following two quantities.\n *\n * (a) Current time plus: (1) the maximum time for which the arrival\n *     of a request is waited for when a sync queue becomes idle,\n *     namely bfqd->bfq_slice_idle, and (2) a few extra jiffies. We\n *     postpone for a moment the reason for adding a few extra\n *     jiffies; we get back to it after next item (b).  Lower-bounding\n *     the return value of this function with the current time plus\n *     bfqd->bfq_slice_idle tends to filter out greedy applications,\n *     because the latter issue their next request as soon as possible\n *     after the last one has been completed. In contrast, a soft\n *     real-time application spends some time processing data, after a\n *     batch of its requests has been completed.\n *\n * (b) Current value of bfqq->soft_rt_next_start. As pointed out\n *     above, greedy applications may happen to meet both the\n *     bandwidth and isochrony requirements under heavy CPU or\n *     storage-device load. In more detail, in these scenarios, these\n *     applications happen, only for limited time periods, to do I/O\n *     slowly enough to meet all the requirements described so far,\n *     including the filtering in above item (a). These slow-speed\n *     time intervals are usually interspersed between other time\n *     intervals during which these applications do I/O at a very high\n *     speed. Fortunately, exactly because of the high speed of the\n *     I/O in the high-speed intervals, the values returned by this\n *     function happen to be so high, near the end of any such\n *     high-speed interval, to be likely to fall *after* the end of\n *     the low-speed time interval that follows. These high values are\n *     stored in bfqq->soft_rt_next_start after each invocation of\n *     this function. As a consequence, if the last value of\n *     bfqq->soft_rt_next_start is constantly used to lower-bound the\n *     next value that this function may return, then, from the very\n *     beginning of a low-speed interval, bfqq->soft_rt_next_start is\n *     likely to be constantly kept so high that any I/O request\n *     issued during the low-speed interval is considered as arriving\n *     to soon for the application to be deemed as soft\n *     real-time. Then, in the high-speed interval that follows, the\n *     application will not be deemed as soft real-time, just because\n *     it will do I/O at a high speed. And so on.\n *\n * Getting back to the filtering in item (a), in the following two\n * cases this filtering might be easily passed by a greedy\n * application, if the reference quantity was just\n * bfqd->bfq_slice_idle:\n * 1) HZ is so low that the duration of a jiffy is comparable to or\n *    higher than bfqd->bfq_slice_idle. This happens, e.g., on slow\n *    devices with HZ=100. The time granularity may be so coarse\n *    that the approximation, in jiffies, of bfqd->bfq_slice_idle\n *    is rather lower than the exact value.\n * 2) jiffies, instead of increasing at a constant rate, may stop increasing\n *    for a while, then suddenly 'jump' by several units to recover the lost\n *    increments. This seems to happen, e.g., inside virtual machines.\n * To address this issue, in the filtering in (a) we do not use as a\n * reference time interval just bfqd->bfq_slice_idle, but\n * bfqd->bfq_slice_idle plus a few jiffies. In particular, we add the\n * minimum number of jiffies for which the filter seems to be quite\n * precise also in embedded systems and KVM/QEMU virtual machines.\n */\nstatic unsigned long bfq_bfqq_softrt_next_start(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn max3(bfqq->soft_rt_next_start,\n\t\t    bfqq->last_idle_bklogged +\n\t\t    HZ * bfqq->service_from_backlogged /\n\t\t    bfqd->bfq_wr_max_softrt_rate,\n\t\t    jiffies + nsecs_to_jiffies(bfqq->bfqd->bfq_slice_idle) + 4);\n}\n\n/**\n * bfq_bfqq_expire - expire a queue.\n * @bfqd: device owning the queue.\n * @bfqq: the queue to expire.\n * @compensate: if true, compensate for the time spent idling.\n * @reason: the reason causing the expiration.\n *\n * If the process associated with bfqq does slow I/O (e.g., because it\n * issues random requests), we charge bfqq with the time it has been\n * in service instead of the service it has received (see\n * bfq_bfqq_charge_time for details on how this goal is achieved). As\n * a consequence, bfqq will typically get higher timestamps upon\n * reactivation, and hence it will be rescheduled as if it had\n * received more service than what it has actually received. In the\n * end, bfqq receives less service in proportion to how slowly its\n * associated process consumes its budgets (and hence how seriously it\n * tends to lower the throughput). In addition, this time-charging\n * strategy guarantees time fairness among slow processes. In\n * contrast, if the process associated with bfqq is not slow, we\n * charge bfqq exactly with the service it has received.\n *\n * Charging time to the first type of queues and the exact service to\n * the other has the effect of using the WF2Q+ policy to schedule the\n * former on a timeslice basis, without violating service domain\n * guarantees among the latter.\n */\nvoid bfq_bfqq_expire(struct bfq_data *bfqd,\n\t\t     struct bfq_queue *bfqq,\n\t\t     bool compensate,\n\t\t     enum bfqq_expiration reason)\n{\n\tbool slow;\n\tunsigned long delta = 0;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * Check whether the process is slow (see bfq_bfqq_is_slow).\n\t */\n\tslow = bfq_bfqq_is_slow(bfqd, bfqq, compensate, &delta);\n\n\t/*\n\t * As above explained, charge slow (typically seeky) and\n\t * timed-out queues with the time and not the service\n\t * received, to favor sequential workloads.\n\t *\n\t * Processes doing I/O in the slower disk zones will tend to\n\t * be slow(er) even if not seeky. Therefore, since the\n\t * estimated peak rate is actually an average over the disk\n\t * surface, these processes may timeout just for bad luck. To\n\t * avoid punishing them, do not charge time to processes that\n\t * succeeded in consuming at least 2/3 of their budget. This\n\t * allows BFQ to preserve enough elasticity to still perform\n\t * bandwidth, and not time, distribution with little unlucky\n\t * or quasi-sequential processes.\n\t */\n\tif (bfqq->wr_coeff == 1 &&\n\t    (slow ||\n\t     (reason == BFQQE_BUDGET_TIMEOUT &&\n\t      bfq_bfqq_budget_left(bfqq) >=  entity->budget / 3)))\n\t\tbfq_bfqq_charge_time(bfqd, bfqq, delta);\n\n\tif (bfqd->low_latency && bfqq->wr_coeff == 1)\n\t\tbfqq->last_wr_start_finish = jiffies;\n\n\tif (bfqd->low_latency && bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\t/*\n\t\t * If we get here, and there are no outstanding\n\t\t * requests, then the request pattern is isochronous\n\t\t * (see the comments on the function\n\t\t * bfq_bfqq_softrt_next_start()). Therefore we can\n\t\t * compute soft_rt_next_start.\n\t\t *\n\t\t * If, instead, the queue still has outstanding\n\t\t * requests, then we have to wait for the completion\n\t\t * of all the outstanding requests to discover whether\n\t\t * the request pattern is actually isochronous.\n\t\t */\n\t\tif (bfqq->dispatched == 0)\n\t\t\tbfqq->soft_rt_next_start =\n\t\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\t\telse if (bfqq->dispatched > 0) {\n\t\t\t/*\n\t\t\t * Schedule an update of soft_rt_next_start to when\n\t\t\t * the task may be discovered to be isochronous.\n\t\t\t */\n\t\t\tbfq_mark_bfqq_softrt_update(bfqq);\n\t\t}\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\"expire (%d, slow %d, num_disp %d, short_ttime %d)\", reason,\n\t\tslow, bfqq->dispatched, bfq_bfqq_has_short_ttime(bfqq));\n\n\t/*\n\t * bfqq expired, so no total service time needs to be computed\n\t * any longer: reset state machine for measuring total service\n\t * times.\n\t */\n\tbfqd->rqs_injected = bfqd->wait_dispatch = false;\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * Increase, decrease or leave budget unchanged according to\n\t * reason.\n\t */\n\t__bfq_bfqq_recalc_budget(bfqd, bfqq, reason);\n\tif (__bfq_bfqq_expire(bfqd, bfqq, reason))\n\t\t/* bfqq is gone, no more actions on it */\n\t\treturn;\n\n\t/* mark bfqq as waiting a request only if a bic still points to it */\n\tif (!bfq_bfqq_busy(bfqq) &&\n\t    reason != BFQQE_BUDGET_TIMEOUT &&\n\t    reason != BFQQE_BUDGET_EXHAUSTED) {\n\t\tbfq_mark_bfqq_non_blocking_wait_rq(bfqq);\n\t\t/*\n\t\t * Not setting service to 0, because, if the next rq\n\t\t * arrives in time, the queue will go on receiving\n\t\t * service with this same budget (as if it never expired)\n\t\t */\n\t} else\n\t\tentity->service = 0;\n\n\t/*\n\t * Reset the received-service counter for every parent entity.\n\t * Differently from what happens with bfqq->entity.service,\n\t * the resetting of this counter never needs to be postponed\n\t * for parent entities. In fact, in case bfqq may have a\n\t * chance to go on being served using the last, partially\n\t * consumed budget, bfqq->entity.service needs to be kept,\n\t * because if bfqq then actually goes on being served using\n\t * the same budget, the last value of bfqq->entity.service is\n\t * needed to properly decrement bfqq->entity.budget by the\n\t * portion already consumed. In contrast, it is not necessary\n\t * to keep entity->service for parent entities too, because\n\t * the bubble up of the new value of bfqq->entity.budget will\n\t * make sure that the budgets of parent entities are correct,\n\t * even in case bfqq and thus parent entities go on receiving\n\t * service with the same budget.\n\t */\n\tentity = entity->parent;\n\tfor_each_entity(entity)\n\t\tentity->service = 0;\n}\n\n/*\n * Budget timeout is not implemented through a dedicated timer, but\n * just checked on request arrivals and completions, as well as on\n * idle timer expirations.\n */\nstatic bool bfq_bfqq_budget_timeout(struct bfq_queue *bfqq)\n{\n\treturn time_is_before_eq_jiffies(bfqq->budget_timeout);\n}\n\n/*\n * If we expire a queue that is actively waiting (i.e., with the\n * device idled) for the arrival of a new request, then we may incur\n * the timestamp misalignment problem described in the body of the\n * function __bfq_activate_entity. Hence we return true only if this\n * condition does not hold, or if the queue is slow enough to deserve\n * only to be kicked off for preserving a high throughput.\n */\nstatic bool bfq_may_expire_for_budg_timeout(struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\"may_budget_timeout: wait_request %d left %d timeout %d\",\n\t\tbfq_bfqq_wait_request(bfqq),\n\t\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3,\n\t\tbfq_bfqq_budget_timeout(bfqq));\n\n\treturn (!bfq_bfqq_wait_request(bfqq) ||\n\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3)\n\t\t&&\n\t\tbfq_bfqq_budget_timeout(bfqq);\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq)\n{\n\tbool rot_without_queueing =\n\t\t!blk_queue_nonrot(bfqd->queue) && !bfqd->hw_tag,\n\t\tbfqq_sequential_and_IO_bound,\n\t\tidling_boosts_thr;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tbfqq_sequential_and_IO_bound = !BFQQ_SEEKY(bfqq) &&\n\t\tbfq_bfqq_IO_bound(bfqq) && bfq_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * The next variable takes into account the cases where idling\n\t * boosts the throughput.\n\t *\n\t * The value of the variable is computed considering, first, that\n\t * idling is virtually always beneficial for the throughput if:\n\t * (a) the device is not NCQ-capable and rotational, or\n\t * (b) regardless of the presence of NCQ, the device is rotational and\n\t *     the request pattern for bfqq is I/O-bound and sequential, or\n\t * (c) regardless of whether it is rotational, the device is\n\t *     not NCQ-capable and the request pattern for bfqq is\n\t *     I/O-bound and sequential.\n\t *\n\t * Secondly, and in contrast to the above item (b), idling an\n\t * NCQ-capable flash-based device would not boost the\n\t * throughput even with sequential I/O; rather it would lower\n\t * the throughput in proportion to how fast the device\n\t * is. Accordingly, the next variable is true if any of the\n\t * above conditions (a), (b) or (c) is true, and, in\n\t * particular, happens to be false if bfqd is an NCQ-capable\n\t * flash-based device.\n\t */\n\tidling_boosts_thr = rot_without_queueing ||\n\t\t((!blk_queue_nonrot(bfqd->queue) || !bfqd->hw_tag) &&\n\t\t bfqq_sequential_and_IO_bound);\n\n\t/*\n\t * The return value of this function is equal to that of\n\t * idling_boosts_thr, unless a special case holds. In this\n\t * special case, described below, idling may cause problems to\n\t * weight-raised queues.\n\t *\n\t * When the request pool is saturated (e.g., in the presence\n\t * of write hogs), if the processes associated with\n\t * non-weight-raised queues ask for requests at a lower rate,\n\t * then processes associated with weight-raised queues have a\n\t * higher probability to get a request from the pool\n\t * immediately (or at least soon) when they need one. Thus\n\t * they have a higher probability to actually get a fraction\n\t * of the device throughput proportional to their high\n\t * weight. This is especially true with NCQ-capable drives,\n\t * which enqueue several requests in advance, and further\n\t * reorder internally-queued requests.\n\t *\n\t * For this reason, we force to false the return value if\n\t * there are weight-raised busy queues. In this case, and if\n\t * bfqq is not weight-raised, this guarantees that the device\n\t * is not idled for bfqq (if, instead, bfqq is weight-raised,\n\t * then idling will be guaranteed by another variable, see\n\t * below). Combined with the timestamping rules of BFQ (see\n\t * [1] for details), this behavior causes bfqq, and hence any\n\t * sync non-weight-raised queue, to get a lower number of\n\t * requests served, and thus to ask for a lower number of\n\t * requests from the request pool, before the busy\n\t * weight-raised queues get served again. This often mitigates\n\t * starvation problems in the presence of heavy write\n\t * workloads and NCQ, thereby guaranteeing a higher\n\t * application and system responsiveness in these hostile\n\t * scenarios.\n\t */\n\treturn idling_boosts_thr &&\n\t\tbfqd->wr_busy_queues == 0;\n}\n\n/*\n * For a queue that becomes empty, device idling is allowed only if\n * this function returns true for that queue. As a consequence, since\n * device idling plays a critical role for both throughput boosting\n * and service guarantees, the return value of this function plays a\n * critical role as well.\n *\n * In a nutshell, this function returns true only if idling is\n * beneficial for throughput or, even if detrimental for throughput,\n * idling is however necessary to preserve service guarantees (low\n * latency, desired throughput distribution, ...). In particular, on\n * NCQ-capable devices, this function tries to return false, so as to\n * help keep the drives' internal queues full, whenever this helps the\n * device boost the throughput without causing any service-guarantee\n * issue.\n *\n * Most of the issues taken into account to get the return value of\n * this function are not trivial. We discuss these issues in the two\n * functions providing the main pieces of information needed by this\n * function.\n */\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tbool idling_boosts_thr_with_no_issue, idling_needed_for_service_guar;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tif (unlikely(bfqd->strict_guarantees))\n\t\treturn true;\n\n\t/*\n\t * Idling is performed only if slice_idle > 0. In addition, we\n\t * do not idle if\n\t * (a) bfqq is async\n\t * (b) bfqq is in the idle io prio class: in this case we do\n\t * not idle because we want to minimize the bandwidth that\n\t * queues in this class can steal to higher-priority queues\n\t */\n\tif (bfqd->bfq_slice_idle == 0 || !bfq_bfqq_sync(bfqq) ||\n\t   bfq_class_idle(bfqq))\n\t\treturn false;\n\n\tidling_boosts_thr_with_no_issue =\n\t\tidling_boosts_thr_without_issues(bfqd, bfqq);\n\n\tidling_needed_for_service_guar =\n\t\tidling_needed_for_service_guarantees(bfqd, bfqq);\n\n\t/*\n\t * We have now the two components we need to compute the\n\t * return value of the function, which is true only if idling\n\t * either boosts the throughput (without issues), or is\n\t * necessary to preserve service guarantees.\n\t */\n\treturn idling_boosts_thr_with_no_issue ||\n\t\tidling_needed_for_service_guar;\n}\n\n/*\n * If the in-service queue is empty but the function bfq_better_to_idle\n * returns true, then:\n * 1) the queue must remain in service and cannot be expired, and\n * 2) the device must be idled to wait for the possible arrival of a new\n *    request for the queue.\n * See the comments on the function bfq_better_to_idle for the reasons\n * why performing device idling is the best choice to boost the throughput\n * and preserve service guarantees when bfq_better_to_idle itself\n * returns true.\n */\nstatic bool bfq_bfqq_must_idle(struct bfq_queue *bfqq)\n{\n\treturn RB_EMPTY_ROOT(&bfqq->sort_list) && bfq_better_to_idle(bfqq);\n}\n\n/*\n * This function chooses the queue from which to pick the next extra\n * I/O request to inject, if it finds a compatible queue. See the\n * comments on bfq_update_inject_limit() for details on the injection\n * mechanism, and for the definitions of the quantities mentioned\n * below.\n */\nstatic struct bfq_queue *\nbfq_choose_bfqq_for_injection(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *in_serv_bfqq = bfqd->in_service_queue;\n\tunsigned int limit = in_serv_bfqq->inject_limit;\n\tint i;\n\n\t/*\n\t * If\n\t * - bfqq is not weight-raised and therefore does not carry\n\t *   time-critical I/O,\n\t * or\n\t * - regardless of whether bfqq is weight-raised, bfqq has\n\t *   however a long think time, during which it can absorb the\n\t *   effect of an appropriate number of extra I/O requests\n\t *   from other queues (see bfq_update_inject_limit for\n\t *   details on the computation of this number);\n\t * then injection can be performed without restrictions.\n\t */\n\tbool in_serv_always_inject = in_serv_bfqq->wr_coeff == 1 ||\n\t\t!bfq_bfqq_has_short_ttime(in_serv_bfqq);\n\n\t/*\n\t * If\n\t * - the baseline total service time could not be sampled yet,\n\t *   so the inject limit happens to be still 0, and\n\t * - a lot of time has elapsed since the plugging of I/O\n\t *   dispatching started, so drive speed is being wasted\n\t *   significantly;\n\t * then temporarily raise inject limit to one request.\n\t */\n\tif (limit == 0 && in_serv_bfqq->last_serv_time_ns == 0 &&\n\t    bfq_bfqq_wait_request(in_serv_bfqq) &&\n\t    time_is_before_eq_jiffies(bfqd->last_idling_start_jiffies +\n\t\t\t\t      bfqd->bfq_slice_idle)\n\t\t)\n\t\tlimit = 1;\n\n\tif (bfqd->tot_rq_in_driver >= limit)\n\t\treturn NULL;\n\n\t/*\n\t * Linear search of the source queue for injection; but, with\n\t * a high probability, very few steps are needed to find a\n\t * candidate queue, i.e., a queue with enough budget left for\n\t * its next request. In fact:\n\t * - BFQ dynamically updates the budget of every queue so as\n\t *   to accommodate the expected backlog of the queue;\n\t * - if a queue gets all its requests dispatched as injected\n\t *   service, then the queue is removed from the active list\n\t *   (and re-added only if it gets new requests, but then it\n\t *   is assigned again enough budget for its new backlog).\n\t */\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tlist_for_each_entry(bfqq, &bfqd->active_list[i], bfqq_list)\n\t\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t\t(in_serv_always_inject || bfqq->wr_coeff > 1) &&\n\t\t\t\tbfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Allow for only one large in-flight request\n\t\t\t * on non-rotational devices, for the\n\t\t\t * following reason. On non-rotationl drives,\n\t\t\t * large requests take much longer than\n\t\t\t * smaller requests to be served. In addition,\n\t\t\t * the drive prefers to serve large requests\n\t\t\t * w.r.t. to small ones, if it can choose. So,\n\t\t\t * having more than one large requests queued\n\t\t\t * in the drive may easily make the next first\n\t\t\t * request of the in-service queue wait for so\n\t\t\t * long to break bfqq's service guarantees. On\n\t\t\t * the bright side, large requests let the\n\t\t\t * drive reach a very high throughput, even if\n\t\t\t * there is only one in-flight large request\n\t\t\t * at a time.\n\t\t\t */\n\t\t\tif (blk_queue_nonrot(bfqd->queue) &&\n\t\t\t    blk_rq_sectors(bfqq->next_rq) >=\n\t\t\t    BFQQ_SECT_THR_NONROT &&\n\t\t\t    bfqd->tot_rq_in_driver >= 1)\n\t\t\t\tcontinue;\n\t\t\telse {\n\t\t\t\tbfqd->rqs_injected = true;\n\t\t\t\treturn bfqq;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *\nbfq_find_active_bfqq_for_actuator(struct bfq_data *bfqd, int idx)\n{\n\tstruct bfq_queue *bfqq;\n\n\tif (bfqd->in_service_queue &&\n\t    bfqd->in_service_queue->actuator_idx == idx)\n\t\treturn bfqd->in_service_queue;\n\n\tlist_for_each_entry(bfqq, &bfqd->active_list[idx], bfqq_list) {\n\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\tbfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\treturn bfqq;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Perform a linear scan of each actuator, until an actuator is found\n * for which the following three conditions hold: the load of the\n * actuator is below the threshold (see comments on\n * actuator_load_threshold for details) and lower than that of the\n * next actuator (comments on this extra condition below), and there\n * is a queue that contains I/O for that actuator. On success, return\n * that queue.\n *\n * Performing a plain linear scan entails a prioritization among\n * actuators. The extra condition above breaks this prioritization and\n * tends to distribute injection uniformly across actuators.\n */\nstatic struct bfq_queue *\nbfq_find_bfqq_for_underused_actuator(struct bfq_data *bfqd)\n{\n\tint i;\n\n\tfor (i = 0 ; i < bfqd->num_actuators; i++) {\n\t\tif (bfqd->rq_in_driver[i] < bfqd->actuator_load_threshold &&\n\t\t    (i == bfqd->num_actuators - 1 ||\n\t\t     bfqd->rq_in_driver[i] < bfqd->rq_in_driver[i+1])) {\n\t\t\tstruct bfq_queue *bfqq =\n\t\t\t\tbfq_find_active_bfqq_for_actuator(bfqd, i);\n\n\t\t\tif (bfqq)\n\t\t\t\treturn bfqq;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n\n/*\n * Select a queue for service.  If we have a current queue in service,\n * check whether to continue servicing it, or retrieve and set a new one.\n */\nstatic struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *inject_bfqq;\n\tstruct request *next_rq;\n\tenum bfqq_expiration reason = BFQQE_BUDGET_TIMEOUT;\n\n\tbfqq = bfqd->in_service_queue;\n\tif (!bfqq)\n\t\tgoto new_queue;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: already in-service queue\");\n\n\t/*\n\t * Do not expire bfqq for budget timeout if bfqq may be about\n\t * to enjoy device idling. The reason why, in this case, we\n\t * prevent bfqq from expiring is the same as in the comments\n\t * on the case where bfq_bfqq_must_idle() returns true, in\n\t * bfq_completed_request().\n\t */\n\tif (bfq_may_expire_for_budg_timeout(bfqq) &&\n\t    !bfq_bfqq_must_idle(bfqq))\n\t\tgoto expire;\n\ncheck_queue:\n\t/*\n\t *  If some actuator is underutilized, but the in-service\n\t *  queue does not contain I/O for that actuator, then try to\n\t *  inject I/O for that actuator.\n\t */\n\tinject_bfqq = bfq_find_bfqq_for_underused_actuator(bfqd);\n\tif (inject_bfqq && inject_bfqq != bfqq)\n\t\treturn inject_bfqq;\n\n\t/*\n\t * This loop is rarely executed more than once. Even when it\n\t * happens, it is much more convenient to re-execute this loop\n\t * than to return NULL and trigger a new dispatch to get a\n\t * request served.\n\t */\n\tnext_rq = bfqq->next_rq;\n\t/*\n\t * If bfqq has requests queued and it has enough budget left to\n\t * serve them, keep the queue, otherwise expire it.\n\t */\n\tif (next_rq) {\n\t\tif (bfq_serv_to_charge(next_rq, bfqq) >\n\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Expire the queue for budget exhaustion,\n\t\t\t * which makes sure that the next budget is\n\t\t\t * enough to serve the next request, even if\n\t\t\t * it comes from the fifo expired path.\n\t\t\t */\n\t\t\treason = BFQQE_BUDGET_EXHAUSTED;\n\t\t\tgoto expire;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The idle timer may be pending because we may\n\t\t\t * not disable disk idling even when a new request\n\t\t\t * arrives.\n\t\t\t */\n\t\t\tif (bfq_bfqq_wait_request(bfqq)) {\n\t\t\t\t/*\n\t\t\t\t * If we get here: 1) at least a new request\n\t\t\t\t * has arrived but we have not disabled the\n\t\t\t\t * timer because the request was too small,\n\t\t\t\t * 2) then the block layer has unplugged\n\t\t\t\t * the device, causing the dispatch to be\n\t\t\t\t * invoked.\n\t\t\t\t *\n\t\t\t\t * Since the device is unplugged, now the\n\t\t\t\t * requests are probably large enough to\n\t\t\t\t * provide a reasonable throughput.\n\t\t\t\t * So we disable idling.\n\t\t\t\t */\n\t\t\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\t\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\t\t\t}\n\t\t\tgoto keep_queue;\n\t\t}\n\t}\n\n\t/*\n\t * No requests pending. However, if the in-service queue is idling\n\t * for a new request, or has requests waiting for a completion and\n\t * may idle after their completion, then keep it anyway.\n\t *\n\t * Yet, inject service from other queues if it boosts\n\t * throughput and is possible.\n\t */\n\tif (bfq_bfqq_wait_request(bfqq) ||\n\t    (bfqq->dispatched != 0 && bfq_better_to_idle(bfqq))) {\n\t\tunsigned int act_idx = bfqq->actuator_idx;\n\t\tstruct bfq_queue *async_bfqq = NULL;\n\t\tstruct bfq_queue *blocked_bfqq =\n\t\t\t!hlist_empty(&bfqq->woken_list) ?\n\t\t\tcontainer_of(bfqq->woken_list.first,\n\t\t\t\t     struct bfq_queue,\n\t\t\t\t     woken_list_node)\n\t\t\t: NULL;\n\n\t\tif (bfqq->bic && bfqq->bic->bfqq[0][act_idx] &&\n\t\t    bfq_bfqq_busy(bfqq->bic->bfqq[0][act_idx]) &&\n\t\t    bfqq->bic->bfqq[0][act_idx]->next_rq)\n\t\t\tasync_bfqq = bfqq->bic->bfqq[0][act_idx];\n\t\t/*\n\t\t * The next four mutually-exclusive ifs decide\n\t\t * whether to try injection, and choose the queue to\n\t\t * pick an I/O request from.\n\t\t *\n\t\t * The first if checks whether the process associated\n\t\t * with bfqq has also async I/O pending. If so, it\n\t\t * injects such I/O unconditionally. Injecting async\n\t\t * I/O from the same process can cause no harm to the\n\t\t * process. On the contrary, it can only increase\n\t\t * bandwidth and reduce latency for the process.\n\t\t *\n\t\t * The second if checks whether there happens to be a\n\t\t * non-empty waker queue for bfqq, i.e., a queue whose\n\t\t * I/O needs to be completed for bfqq to receive new\n\t\t * I/O. This happens, e.g., if bfqq is associated with\n\t\t * a process that does some sync. A sync generates\n\t\t * extra blocking I/O, which must be completed before\n\t\t * the process associated with bfqq can go on with its\n\t\t * I/O. If the I/O of the waker queue is not served,\n\t\t * then bfqq remains empty, and no I/O is dispatched,\n\t\t * until the idle timeout fires for bfqq. This is\n\t\t * likely to result in lower bandwidth and higher\n\t\t * latencies for bfqq, and in a severe loss of total\n\t\t * throughput. The best action to take is therefore to\n\t\t * serve the waker queue as soon as possible. So do it\n\t\t * (without relying on the third alternative below for\n\t\t * eventually serving waker_bfqq's I/O; see the last\n\t\t * paragraph for further details). This systematic\n\t\t * injection of I/O from the waker queue does not\n\t\t * cause any delay to bfqq's I/O. On the contrary,\n\t\t * next bfqq's I/O is brought forward dramatically,\n\t\t * for it is not blocked for milliseconds.\n\t\t *\n\t\t * The third if checks whether there is a queue woken\n\t\t * by bfqq, and currently with pending I/O. Such a\n\t\t * woken queue does not steal bandwidth from bfqq,\n\t\t * because it remains soon without I/O if bfqq is not\n\t\t * served. So there is virtually no risk of loss of\n\t\t * bandwidth for bfqq if this woken queue has I/O\n\t\t * dispatched while bfqq is waiting for new I/O.\n\t\t *\n\t\t * The fourth if checks whether bfqq is a queue for\n\t\t * which it is better to avoid injection. It is so if\n\t\t * bfqq delivers more throughput when served without\n\t\t * any further I/O from other queues in the middle, or\n\t\t * if the service times of bfqq's I/O requests both\n\t\t * count more than overall throughput, and may be\n\t\t * easily increased by injection (this happens if bfqq\n\t\t * has a short think time). If none of these\n\t\t * conditions holds, then a candidate queue for\n\t\t * injection is looked for through\n\t\t * bfq_choose_bfqq_for_injection(). Note that the\n\t\t * latter may return NULL (for example if the inject\n\t\t * limit for bfqq is currently 0).\n\t\t *\n\t\t * NOTE: motivation for the second alternative\n\t\t *\n\t\t * Thanks to the way the inject limit is updated in\n\t\t * bfq_update_has_short_ttime(), it is rather likely\n\t\t * that, if I/O is being plugged for bfqq and the\n\t\t * waker queue has pending I/O requests that are\n\t\t * blocking bfqq's I/O, then the fourth alternative\n\t\t * above lets the waker queue get served before the\n\t\t * I/O-plugging timeout fires. So one may deem the\n\t\t * second alternative superfluous. It is not, because\n\t\t * the fourth alternative may be way less effective in\n\t\t * case of a synchronization. For two main\n\t\t * reasons. First, throughput may be low because the\n\t\t * inject limit may be too low to guarantee the same\n\t\t * amount of injected I/O, from the waker queue or\n\t\t * other queues, that the second alternative\n\t\t * guarantees (the second alternative unconditionally\n\t\t * injects a pending I/O request of the waker queue\n\t\t * for each bfq_dispatch_request()). Second, with the\n\t\t * fourth alternative, the duration of the plugging,\n\t\t * i.e., the time before bfqq finally receives new I/O,\n\t\t * may not be minimized, because the waker queue may\n\t\t * happen to be served only after other queues.\n\t\t */\n\t\tif (async_bfqq &&\n\t\t    icq_to_bic(async_bfqq->next_rq->elv.icq) == bfqq->bic &&\n\t\t    bfq_serv_to_charge(async_bfqq->next_rq, async_bfqq) <=\n\t\t    bfq_bfqq_budget_left(async_bfqq))\n\t\t\tbfqq = async_bfqq;\n\t\telse if (bfqq->waker_bfqq &&\n\t\t\t   bfq_bfqq_busy(bfqq->waker_bfqq) &&\n\t\t\t   bfqq->waker_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(bfqq->waker_bfqq->next_rq,\n\t\t\t\t\t      bfqq->waker_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(bfqq->waker_bfqq)\n\t\t\t)\n\t\t\tbfqq = bfqq->waker_bfqq;\n\t\telse if (blocked_bfqq &&\n\t\t\t   bfq_bfqq_busy(blocked_bfqq) &&\n\t\t\t   blocked_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(blocked_bfqq->next_rq,\n\t\t\t\t\t      blocked_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(blocked_bfqq)\n\t\t\t)\n\t\t\tbfqq = blocked_bfqq;\n\t\telse if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t (bfqq->wr_coeff == 1 || bfqd->wr_busy_queues > 1 ||\n\t\t\t  !bfq_bfqq_has_short_ttime(bfqq)))\n\t\t\tbfqq = bfq_choose_bfqq_for_injection(bfqd);\n\t\telse\n\t\t\tbfqq = NULL;\n\n\t\tgoto keep_queue;\n\t}\n\n\treason = BFQQE_NO_MORE_REQUESTS;\nexpire:\n\tbfq_bfqq_expire(bfqd, bfqq, false, reason);\nnew_queue:\n\tbfqq = bfq_set_in_service_queue(bfqd);\n\tif (bfqq) {\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: checking new queue\");\n\t\tgoto check_queue;\n\t}\nkeep_queue:\n\tif (bfqq)\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: returned this queue\");\n\telse\n\t\tbfq_log(bfqd, \"select_queue: no queue returned\");\n\n\treturn bfqq;\n}\n\nstatic void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tif (bfqq->wr_coeff > 1) { /* queue is being weight-raised */\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t\"raising period dur %u/%u msec, old coeff %u, w %d(%d)\",\n\t\t\tjiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),\n\t\t\tjiffies_to_msecs(bfqq->wr_cur_max_time),\n\t\t\tbfqq->wr_coeff,\n\t\t\tbfqq->entity.weight, bfqq->entity.orig_weight);\n\n\t\tif (entity->prio_changed)\n\t\t\tbfq_log_bfqq(bfqd, bfqq, \"WARN: pending prio change\");\n\n\t\t/*\n\t\t * If the queue was activated in a burst, or too much\n\t\t * time has elapsed from the beginning of this\n\t\t * weight-raising period, then end weight raising.\n\t\t */\n\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\telse if (time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t\t\tbfqq->wr_cur_max_time)) {\n\t\t\tif (bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time ||\n\t\t\ttime_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t       bfq_wr_duration(bfqd))) {\n\t\t\t\t/*\n\t\t\t\t * Either in interactive weight\n\t\t\t\t * raising, or in soft_rt weight\n\t\t\t\t * raising with the\n\t\t\t\t * interactive-weight-raising period\n\t\t\t\t * elapsed (so no switch back to\n\t\t\t\t * interactive weight raising).\n\t\t\t\t */\n\t\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t\t} else { /*\n\t\t\t\t  * soft_rt finishing while still in\n\t\t\t\t  * interactive period, switch back to\n\t\t\t\t  * interactive weight raising\n\t\t\t\t  */\n\t\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t\t}\n\t\t}\n\t\tif (bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time &&\n\t\t    bfqq->service_from_wr > max_service_from_wr) {\n\t\t\t/* see comments on max_service_from_wr */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t}\n\t}\n\t/*\n\t * To improve latency (for this or other queues), immediately\n\t * update weight both if it must be raised and if it must be\n\t * lowered. Since, entity may be on some active tree here, and\n\t * might have a pending change of its ioprio class, invoke\n\t * next function with the last parameter unset (see the\n\t * comments on the function).\n\t */\n\tif ((entity->weight > entity->orig_weight) != (bfqq->wr_coeff > 1))\n\t\t__bfq_entity_update_weight_prio(bfq_entity_service_tree(entity),\n\t\t\t\t\t\tentity, false);\n}\n\n/*\n * Dispatch next request from bfqq.\n */\nstatic struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct request *rq = bfqq->next_rq;\n\tunsigned long service_to_charge;\n\n\tservice_to_charge = bfq_serv_to_charge(rq, bfqq);\n\n\tbfq_bfqq_served(bfqq, service_to_charge);\n\n\tif (bfqq == bfqd->in_service_queue && bfqd->wait_dispatch) {\n\t\tbfqd->wait_dispatch = false;\n\t\tbfqd->waited_rq = rq;\n\t}\n\n\tbfq_dispatch_remove(bfqd->queue, rq);\n\n\tif (bfqq != bfqd->in_service_queue)\n\t\treturn rq;\n\n\t/*\n\t * If weight raising has to terminate for bfqq, then next\n\t * function causes an immediate update of bfqq's weight,\n\t * without waiting for next activation. As a consequence, on\n\t * expiration, bfqq will be timestamped as if has never been\n\t * weight-raised during this service slot, even if it has\n\t * received part or even most of the service as a\n\t * weight-raised queue. This inflates bfqq's timestamps, which\n\t * is beneficial, as bfqq is then more willing to leave the\n\t * device immediately to possible other weight-raised queues.\n\t */\n\tbfq_update_wr_data(bfqd, bfqq);\n\n\t/*\n\t * Expire bfqq, pretending that its budget expired, if bfqq\n\t * belongs to CLASS_IDLE and other queues are waiting for\n\t * service.\n\t */\n\tif (bfq_tot_busy_queues(bfqd) > 1 && bfq_class_idle(bfqq))\n\t\tbfq_bfqq_expire(bfqd, bfqq, false, BFQQE_BUDGET_EXHAUSTED);\n\n\treturn rq;\n}\n\nstatic bool bfq_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\n\t/*\n\t * Avoiding lock: a race on bfqd->queued should cause at\n\t * most a call to dispatch for nothing\n\t */\n\treturn !list_empty_careful(&bfqd->dispatch) ||\n\t\tREAD_ONCE(bfqd->queued);\n}\n\nstatic struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq = NULL;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tif (!list_empty(&bfqd->dispatch)) {\n\t\trq = list_first_entry(&bfqd->dispatch, struct request,\n\t\t\t\t      queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (bfqq) {\n\t\t\t/*\n\t\t\t * Increment counters here, because this\n\t\t\t * dispatch does not follow the standard\n\t\t\t * dispatch flow (where counters are\n\t\t\t * incremented)\n\t\t\t */\n\t\t\tbfqq->dispatched++;\n\n\t\t\tgoto inc_in_driver_start_rq;\n\t\t}\n\n\t\t/*\n\t\t * We exploit the bfq_finish_requeue_request hook to\n\t\t * decrement tot_rq_in_driver, but\n\t\t * bfq_finish_requeue_request will not be invoked on\n\t\t * this request. So, to avoid unbalance, just start\n\t\t * this request, without incrementing tot_rq_in_driver. As\n\t\t * a negative consequence, tot_rq_in_driver is deceptively\n\t\t * lower than it should be while this request is in\n\t\t * service. This may cause bfq_schedule_dispatch to be\n\t\t * invoked uselessly.\n\t\t *\n\t\t * As for implementing an exact solution, the\n\t\t * bfq_finish_requeue_request hook, if defined, is\n\t\t * probably invoked also on this request. So, by\n\t\t * exploiting this hook, we could 1) increment\n\t\t * tot_rq_in_driver here, and 2) decrement it in\n\t\t * bfq_finish_requeue_request. Such a solution would\n\t\t * let the value of the counter be always accurate,\n\t\t * but it would entail using an extra interface\n\t\t * function. This cost seems higher than the benefit,\n\t\t * being the frequency of non-elevator-private\n\t\t * requests very low.\n\t\t */\n\t\tgoto start_rq;\n\t}\n\n\tbfq_log(bfqd, \"dispatch requests: %d busy queues\",\n\t\tbfq_tot_busy_queues(bfqd));\n\n\tif (bfq_tot_busy_queues(bfqd) == 0)\n\t\tgoto exit;\n\n\t/*\n\t * Force device to serve one request at a time if\n\t * strict_guarantees is true. Forcing this service scheme is\n\t * currently the ONLY way to guarantee that the request\n\t * service order enforced by the scheduler is respected by a\n\t * queueing device. Otherwise the device is free even to make\n\t * some unlucky request wait for as long as the device\n\t * wishes.\n\t *\n\t * Of course, serving one request at a time may cause loss of\n\t * throughput.\n\t */\n\tif (bfqd->strict_guarantees && bfqd->tot_rq_in_driver > 0)\n\t\tgoto exit;\n\n\tbfqq = bfq_select_queue(bfqd);\n\tif (!bfqq)\n\t\tgoto exit;\n\n\trq = bfq_dispatch_rq_from_bfqq(bfqd, bfqq);\n\n\tif (rq) {\ninc_in_driver_start_rq:\n\t\tbfqd->rq_in_driver[bfqq->actuator_idx]++;\n\t\tbfqd->tot_rq_in_driver++;\nstart_rq:\n\t\trq->rq_flags |= RQF_STARTED;\n\t}\nexit:\n\treturn rq;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t      struct request *rq,\n\t\t\t\t      struct bfq_queue *in_serv_queue,\n\t\t\t\t      bool idle_timer_disabled)\n{\n\tstruct bfq_queue *bfqq = rq ? RQ_BFQQ(rq) : NULL;\n\n\tif (!idle_timer_disabled && !bfqq)\n\t\treturn;\n\n\t/*\n\t * rq and bfqq are guaranteed to exist until this function\n\t * ends, for the following reasons. First, rq can be\n\t * dispatched to the device, and then can be completed and\n\t * freed, only after this function ends. Second, rq cannot be\n\t * merged (and thus freed because of a merge) any longer,\n\t * because it has already started. Thus rq cannot be freed\n\t * before this function ends, and, since rq has a reference to\n\t * bfqq, the same guarantee holds for bfqq too.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tif (idle_timer_disabled)\n\t\t/*\n\t\t * Since the idle timer has been disabled,\n\t\t * in_serv_queue contained some request when\n\t\t * __bfq_dispatch_request was invoked above, which\n\t\t * implies that rq was picked exactly from\n\t\t * in_serv_queue. Thus in_serv_queue == bfqq, and is\n\t\t * therefore guaranteed to exist because of the above\n\t\t * arguments.\n\t\t */\n\t\tbfqg_stats_update_idle_time(bfqq_group(in_serv_queue));\n\tif (bfqq) {\n\t\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\t\tbfqg_stats_update_avg_queue_size(bfqg);\n\t\tbfqg_stats_set_start_empty_time(bfqg);\n\t\tbfqg_stats_update_io_remove(bfqg, rq->cmd_flags);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     struct bfq_queue *in_serv_queue,\n\t\t\t\t\t     bool idle_timer_disabled) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq;\n\tstruct bfq_queue *in_serv_queue;\n\tbool waiting_rq, idle_timer_disabled = false;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tin_serv_queue = bfqd->in_service_queue;\n\twaiting_rq = in_serv_queue && bfq_bfqq_wait_request(in_serv_queue);\n\n\trq = __bfq_dispatch_request(hctx);\n\tif (in_serv_queue == bfqd->in_service_queue) {\n\t\tidle_timer_disabled =\n\t\t\twaiting_rq && !bfq_bfqq_wait_request(in_serv_queue);\n\t}\n\n\tspin_unlock_irq(&bfqd->lock);\n\tbfq_update_dispatch_stats(hctx->queue, rq,\n\t\t\tidle_timer_disabled ? in_serv_queue : NULL,\n\t\t\t\tidle_timer_disabled);\n\n\treturn rq;\n}\n\n/*\n * Task holds one reference to the queue, dropped when task exits.  Each rq\n * in-flight on this queue also holds a reference, dropped when rq is freed.\n *\n * Scheduler lock must be held here. Recall not to use bfqq after calling\n * this function on it.\n */\nvoid bfq_put_queue(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"put_queue: %p %d\", bfqq, bfqq->ref);\n\n\tbfqq->ref--;\n\tif (bfqq->ref)\n\t\treturn;\n\n\tif (!hlist_unhashed(&bfqq->burst_list_node)) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\t/*\n\t\t * Decrement also burst size after the removal, if the\n\t\t * process associated with bfqq is exiting, and thus\n\t\t * does not contribute to the burst any longer. This\n\t\t * decrement helps filter out false positives of large\n\t\t * bursts, when some short-lived process (often due to\n\t\t * the execution of commands by some service) happens\n\t\t * to start and exit while a complex application is\n\t\t * starting, and thus spawning several processes that\n\t\t * do I/O (and that *must not* be treated as a large\n\t\t * burst, see comments on bfq_handle_burst).\n\t\t *\n\t\t * In particular, the decrement is performed only if:\n\t\t * 1) bfqq is not a merged queue, because, if it is,\n\t\t * then this free of bfqq is not triggered by the exit\n\t\t * of the process bfqq is associated with, but exactly\n\t\t * by the fact that bfqq has just been merged.\n\t\t * 2) burst_size is greater than 0, to handle\n\t\t * unbalanced decrements. Unbalanced decrements may\n\t\t * happen in te following case: bfqq is inserted into\n\t\t * the current burst list--without incrementing\n\t\t * bust_size--because of a split, but the current\n\t\t * burst list is not the burst list bfqq belonged to\n\t\t * (see comments on the case of a split in\n\t\t * bfq_set_request).\n\t\t */\n\t\tif (bfqq->bic && bfqq->bfqd->burst_size > 0)\n\t\t\tbfqq->bfqd->burst_size--;\n\t}\n\n\t/*\n\t * bfqq does not exist any longer, so it cannot be woken by\n\t * any other queue, and cannot wake any other queue. Then bfqq\n\t * must be removed from the woken list of its possible waker\n\t * queue, and all queues in the woken list of bfqq must stop\n\t * having a waker queue. Strictly speaking, these updates\n\t * should be performed when bfqq remains with no I/O source\n\t * attached to it, which happens before bfqq gets freed. In\n\t * particular, this happens when the last process associated\n\t * with bfqq exits or gets associated with a different\n\t * queue. However, both events lead to bfqq being freed soon,\n\t * and dangling references would come out only after bfqq gets\n\t * freed. So these updates are done here, as a simple and safe\n\t * way to handle all cases.\n\t */\n\t/* remove bfqq from woken list */\n\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\thlist_del_init(&bfqq->woken_list_node);\n\n\t/* reset waker for all queues in woken list */\n\thlist_for_each_entry_safe(item, n, &bfqq->woken_list,\n\t\t\t\t  woken_list_node) {\n\t\titem->waker_bfqq = NULL;\n\t\thlist_del_init(&item->woken_list_node);\n\t}\n\n\tif (bfqq->bfqd->last_completed_rq_bfqq == bfqq)\n\t\tbfqq->bfqd->last_completed_rq_bfqq = NULL;\n\n\tWARN_ON_ONCE(!list_empty(&bfqq->fifo));\n\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&bfqq->sort_list));\n\tWARN_ON_ONCE(bfqq->dispatched);\n\n\tkmem_cache_free(bfq_pool, bfqq);\n\tbfqg_and_blkg_put(bfqg);\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq)\n{\n\tbfqq->stable_ref--;\n\tbfq_put_queue(bfqq);\n}\n\nvoid bfq_put_cooperator(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *__bfqq, *next;\n\n\t/*\n\t * If this queue was scheduled to merge with another queue, be\n\t * sure to drop the reference taken on that queue (and others in\n\t * the merge chain). See bfq_setup_merge and bfq_merge_bfqqs.\n\t */\n\t__bfqq = bfqq->new_bfqq;\n\twhile (__bfqq) {\n\t\tnext = __bfqq->new_bfqq;\n\t\tbfq_put_queue(__bfqq);\n\t\t__bfqq = next;\n\t}\n}\n\nstatic void bfq_exit_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tif (bfqq == bfqd->in_service_queue) {\n\t\t__bfq_bfqq_expire(bfqd, bfqq, BFQQE_BUDGET_TIMEOUT);\n\t\tbfq_schedule_dispatch(bfqd);\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"exit_bfqq: %p, %d\", bfqq, bfqq->ref);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n}\n\nstatic void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync,\n\t\t\t      unsigned int actuator_idx)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync, actuator_idx);\n\tstruct bfq_data *bfqd;\n\n\tif (bfqq)\n\t\tbfqd = bfqq->bfqd; /* NULL if scheduler already exited */\n\n\tif (bfqq && bfqd) {\n\t\tbic_set_bfqq(bic, NULL, is_sync, actuator_idx);\n\t\tbfq_exit_bfqq(bfqd, bfqq);\n\t}\n}\n\nstatic void bfq_exit_icq(struct io_cq *icq)\n{\n\tstruct bfq_io_cq *bic = icq_to_bic(icq);\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tunsigned long flags;\n\tunsigned int act_idx;\n\t/*\n\t * If bfqd and thus bfqd->num_actuators is not available any\n\t * longer, then cycle over all possible per-actuator bfqqs in\n\t * next loop. We rely on bic being zeroed on creation, and\n\t * therefore on its unused per-actuator fields being NULL.\n\t */\n\tunsigned int num_actuators = BFQ_MAX_ACTUATORS;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = bic->bfqq_data;\n\n\t/*\n\t * bfqd is NULL if scheduler already exited, and in that case\n\t * this is the last time these queues are accessed.\n\t */\n\tif (bfqd) {\n\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\tnum_actuators = bfqd->num_actuators;\n\t}\n\n\tfor (act_idx = 0; act_idx < num_actuators; act_idx++) {\n\t\tif (bfqq_data[act_idx].stable_merge_bfqq)\n\t\t\tbfq_put_stable_ref(bfqq_data[act_idx].stable_merge_bfqq);\n\n\t\tbfq_exit_icq_bfqq(bic, true, act_idx);\n\t\tbfq_exit_icq_bfqq(bic, false, act_idx);\n\t}\n\n\tif (bfqd)\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n/*\n * Update the entity prio values; note that the new values will not\n * be used until the next (re)activation.\n */\nstatic void\nbfq_set_next_ioprio_data(struct bfq_queue *bfqq, struct bfq_io_cq *bic)\n{\n\tstruct task_struct *tsk = current;\n\tint ioprio_class;\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\n\tif (!bfqd)\n\t\treturn;\n\n\tioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tswitch (ioprio_class) {\n\tdefault:\n\t\tpr_err(\"bdi %s: bfq: bad prio class %d\\n\",\n\t\t\tbdi_dev_name(bfqq->bfqd->queue->disk->bdi),\n\t\t\tioprio_class);\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_NONE:\n\t\t/*\n\t\t * No prio set, inherit CPU scheduling settings.\n\t\t */\n\t\tbfqq->new_ioprio = task_nice_ioprio(tsk);\n\t\tbfqq->new_ioprio_class = task_nice_ioclass(tsk);\n\t\tbreak;\n\tcase IOPRIO_CLASS_RT:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_RT;\n\t\tbreak;\n\tcase IOPRIO_CLASS_BE:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_BE;\n\t\tbreak;\n\tcase IOPRIO_CLASS_IDLE:\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_IDLE;\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t\tbreak;\n\t}\n\n\tif (bfqq->new_ioprio >= IOPRIO_NR_LEVELS) {\n\t\tpr_crit(\"bfq_set_next_ioprio_data: new_ioprio %d\\n\",\n\t\t\tbfqq->new_ioprio);\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t}\n\n\tbfqq->entity.new_weight = bfq_ioprio_to_weight(bfqq->new_ioprio);\n\tbfq_log_bfqq(bfqd, bfqq, \"new_ioprio %d new_weight %d\",\n\t\t     bfqq->new_ioprio, bfqq->entity.new_weight);\n\tbfqq->entity.prio_changed = 1;\n}\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn);\n\nstatic void bfq_check_ioprio_change(struct bfq_io_cq *bic, struct bio *bio)\n{\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tstruct bfq_queue *bfqq;\n\tint ioprio = bic->icq.ioc->ioprio;\n\n\t/*\n\t * This condition may trigger on a newly created bic, be sure to\n\t * drop the lock before returning.\n\t */\n\tif (unlikely(!bfqd) || likely(bic->ioprio == ioprio))\n\t\treturn;\n\n\tbic->ioprio = ioprio;\n\n\tbfqq = bic_to_bfqq(bic, false, bfq_actuator_index(bfqd, bio));\n\tif (bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\n\t\tbfqq = bfq_get_queue(bfqd, bio, false, bic, true);\n\t\tbic_set_bfqq(bic, bfqq, false, bfq_actuator_index(bfqd, bio));\n\t\tbfq_release_process_ref(bfqd, old_bfqq);\n\t}\n\n\tbfqq = bic_to_bfqq(bic, true, bfq_actuator_index(bfqd, bio));\n\tif (bfqq)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n}\n\nstatic void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic, pid_t pid, int is_sync,\n\t\t\t  unsigned int act_idx)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tbfqq->actuator_idx = act_idx;\n\tRB_CLEAR_NODE(&bfqq->entity.rb_node);\n\tINIT_LIST_HEAD(&bfqq->fifo);\n\tINIT_HLIST_NODE(&bfqq->burst_list_node);\n\tINIT_HLIST_NODE(&bfqq->woken_list_node);\n\tINIT_HLIST_HEAD(&bfqq->woken_list);\n\n\tbfqq->ref = 0;\n\tbfqq->bfqd = bfqd;\n\n\tif (bic)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n\n\tif (is_sync) {\n\t\t/*\n\t\t * No need to mark as has_short_ttime if in\n\t\t * idle_class, because no device idling is performed\n\t\t * for queues in idle class\n\t\t */\n\t\tif (!bfq_class_idle(bfqq))\n\t\t\t/* tentatively mark as has_short_ttime */\n\t\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\t\tbfq_mark_bfqq_sync(bfqq);\n\t\tbfq_mark_bfqq_just_created(bfqq);\n\t} else\n\t\tbfq_clear_bfqq_sync(bfqq);\n\n\t/* set end request to minus infinity from now */\n\tbfqq->ttime.last_end_request = now_ns + 1;\n\n\tbfqq->creation_time = jiffies;\n\n\tbfqq->io_start_time = now_ns;\n\n\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\tbfqq->pid = pid;\n\n\t/* Tentative initial value to trade off between thr and lat */\n\tbfqq->max_budget = (2 * bfq_max_budget(bfqd)) / 3;\n\tbfqq->budget_timeout = bfq_smallest_from_now();\n\n\tbfqq->wr_coeff = 1;\n\tbfqq->last_wr_start_finish = jiffies;\n\tbfqq->wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\tbfqq->split_time = bfq_smallest_from_now();\n\n\t/*\n\t * To not forget the possibly high bandwidth consumed by a\n\t * process/queue in the recent past,\n\t * bfq_bfqq_softrt_next_start() returns a value at least equal\n\t * to the current value of bfqq->soft_rt_next_start (see\n\t * comments on bfq_bfqq_softrt_next_start).  Set\n\t * soft_rt_next_start to now, to mean that bfqq has consumed\n\t * no bandwidth so far.\n\t */\n\tbfqq->soft_rt_next_start = jiffies;\n\n\t/* first request is almost certainly seeky */\n\tbfqq->seek_history = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic struct bfq_queue **bfq_async_queue_prio(struct bfq_data *bfqd,\n\t\t\t\t\t       struct bfq_group *bfqg,\n\t\t\t\t\t       int ioprio_class, int ioprio, int act_idx)\n{\n\tswitch (ioprio_class) {\n\tcase IOPRIO_CLASS_RT:\n\t\treturn &bfqg->async_bfqq[0][ioprio][act_idx];\n\tcase IOPRIO_CLASS_NONE:\n\t\tioprio = IOPRIO_BE_NORM;\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_BE:\n\t\treturn &bfqg->async_bfqq[1][ioprio][act_idx];\n\tcase IOPRIO_CLASS_IDLE:\n\t\treturn &bfqg->async_idle_bfqq[act_idx];\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bfq_queue *\nbfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic,\n\t\t\t  struct bfq_queue *last_bfqq_created)\n{\n\tunsigned int a_idx = last_bfqq_created->actuator_idx;\n\tstruct bfq_queue *new_bfqq =\n\t\tbfq_setup_merge(bfqq, last_bfqq_created);\n\n\tif (!new_bfqq)\n\t\treturn bfqq;\n\n\tif (new_bfqq->bic)\n\t\tnew_bfqq->bic->bfqq_data[a_idx].stably_merged = true;\n\tbic->bfqq_data[a_idx].stably_merged = true;\n\n\t/*\n\t * Reusing merge functions. This implies that\n\t * bfqq->bic must be set too, for\n\t * bfq_merge_bfqqs to correctly save bfqq's\n\t * state before killing it.\n\t */\n\tbfqq->bic = bic;\n\treturn bfq_merge_bfqqs(bfqd, bic, bfqq);\n}\n\n/*\n * Many throughput-sensitive workloads are made of several parallel\n * I/O flows, with all flows generated by the same application, or\n * more generically by the same task (e.g., system boot). The most\n * counterproductive action with these workloads is plugging I/O\n * dispatch when one of the bfq_queues associated with these flows\n * remains temporarily empty.\n *\n * To avoid this plugging, BFQ has been using a burst-handling\n * mechanism for years now. This mechanism has proven effective for\n * throughput, and not detrimental for service guarantees. The\n * following function pushes this mechanism a little bit further,\n * basing on the following two facts.\n *\n * First, all the I/O flows of a the same application or task\n * contribute to the execution/completion of that common application\n * or task. So the performance figures that matter are total\n * throughput of the flows and task-wide I/O latency.  In particular,\n * these flows do not need to be protected from each other, in terms\n * of individual bandwidth or latency.\n *\n * Second, the above fact holds regardless of the number of flows.\n *\n * Putting these two facts together, this commits merges stably the\n * bfq_queues associated with these I/O flows, i.e., with the\n * processes that generate these IO/ flows, regardless of how many the\n * involved processes are.\n *\n * To decide whether a set of bfq_queues is actually associated with\n * the I/O flows of a common application or task, and to merge these\n * queues stably, this function operates as follows: given a bfq_queue,\n * say Q2, currently being created, and the last bfq_queue, say Q1,\n * created before Q2, Q2 is merged stably with Q1 if\n * - very little time has elapsed since when Q1 was created\n * - Q2 has the same ioprio as Q1\n * - Q2 belongs to the same group as Q1\n *\n * Merging bfq_queues also reduces scheduling overhead. A fio test\n * with ten random readers on /dev/nullb shows a throughput boost of\n * 40%, with a quadcore. Since BFQ's execution time amounts to ~50% of\n * the total per-request processing time, the above throughput boost\n * implies that BFQ's overhead is reduced by more than 50%.\n *\n * This new mechanism most certainly obsoletes the current\n * burst-handling heuristics. We keep those heuristics for the moment.\n */\nstatic struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,\n\t\t\t\t\t\t      struct bfq_queue *bfqq,\n\t\t\t\t\t\t      struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue **source_bfqq = bfqq->entity.parent ?\n\t\t&bfqq->entity.parent->last_bfqq_created :\n\t\t&bfqd->last_bfqq_created;\n\n\tstruct bfq_queue *last_bfqq_created = *source_bfqq;\n\n\t/*\n\t * If last_bfqq_created has not been set yet, then init it. If\n\t * it has been set already, but too long ago, then move it\n\t * forward to bfqq. Finally, move also if bfqq belongs to a\n\t * different group than last_bfqq_created, or if bfqq has a\n\t * different ioprio, ioprio_class or actuator_idx. If none of\n\t * these conditions holds true, then try an early stable merge\n\t * or schedule a delayed stable merge. As for the condition on\n\t * actuator_idx, the reason is that, if queues associated with\n\t * different actuators are merged, then control is lost on\n\t * each actuator. Therefore some actuator may be\n\t * underutilized, and throughput may decrease.\n\t *\n\t * A delayed merge is scheduled (instead of performing an\n\t * early merge), in case bfqq might soon prove to be more\n\t * throughput-beneficial if not merged. Currently this is\n\t * possible only if bfqd is rotational with no queueing. For\n\t * such a drive, not merging bfqq is better for throughput if\n\t * bfqq happens to contain sequential I/O. So, we wait a\n\t * little bit for enough I/O to flow through bfqq. After that,\n\t * if such an I/O is sequential, then the merge is\n\t * canceled. Otherwise the merge is finally performed.\n\t */\n\tif (!last_bfqq_created ||\n\t    time_before(last_bfqq_created->creation_time +\n\t\t\tmsecs_to_jiffies(bfq_activation_stable_merging),\n\t\t\tbfqq->creation_time) ||\n\t\tbfqq->entity.parent != last_bfqq_created->entity.parent ||\n\t\tbfqq->ioprio != last_bfqq_created->ioprio ||\n\t\tbfqq->ioprio_class != last_bfqq_created->ioprio_class ||\n\t\tbfqq->actuator_idx != last_bfqq_created->actuator_idx)\n\t\t*source_bfqq = bfqq;\n\telse if (time_after_eq(last_bfqq_created->creation_time +\n\t\t\t\t bfqd->bfq_burst_interval,\n\t\t\t\t bfqq->creation_time)) {\n\t\tif (likely(bfqd->nonrot_with_queueing))\n\t\t\t/*\n\t\t\t * With this type of drive, leaving\n\t\t\t * bfqq alone may provide no\n\t\t\t * throughput benefits compared with\n\t\t\t * merging bfqq. So merge bfqq now.\n\t\t\t */\n\t\t\tbfqq = bfq_do_early_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t\t bic,\n\t\t\t\t\t\t\t last_bfqq_created);\n\t\telse { /* schedule tentative stable merge */\n\t\t\t/*\n\t\t\t * get reference on last_bfqq_created,\n\t\t\t * to prevent it from being freed,\n\t\t\t * until we decide whether to merge\n\t\t\t */\n\t\t\tlast_bfqq_created->ref++;\n\t\t\t/*\n\t\t\t * need to keep track of stable refs, to\n\t\t\t * compute process refs correctly\n\t\t\t */\n\t\t\tlast_bfqq_created->stable_ref++;\n\t\t\t/*\n\t\t\t * Record the bfqq to merge to.\n\t\t\t */\n\t\t\tbic->bfqq_data[last_bfqq_created->actuator_idx].stable_merge_bfqq =\n\t\t\t\tlast_bfqq_created;\n\t\t}\n\t}\n\n\treturn bfqq;\n}\n\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn)\n{\n\tconst int ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\tconst int ioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tstruct bfq_queue **async_bfqq = NULL;\n\tstruct bfq_queue *bfqq;\n\tstruct bfq_group *bfqg;\n\n\tbfqg = bfq_bio_bfqg(bfqd, bio);\n\tif (!is_sync) {\n\t\tasync_bfqq = bfq_async_queue_prio(bfqd, bfqg, ioprio_class,\n\t\t\t\t\t\t  ioprio,\n\t\t\t\t\t\t  bfq_actuator_index(bfqd, bio));\n\t\tbfqq = *async_bfqq;\n\t\tif (bfqq)\n\t\t\tgoto out;\n\t}\n\n\tbfqq = kmem_cache_alloc_node(bfq_pool,\n\t\t\t\t     GFP_NOWAIT | __GFP_ZERO | __GFP_NOWARN,\n\t\t\t\t     bfqd->queue->node);\n\n\tif (bfqq) {\n\t\tbfq_init_bfqq(bfqd, bfqq, bic, current->pid,\n\t\t\t      is_sync, bfq_actuator_index(bfqd, bio));\n\t\tbfq_init_entity(&bfqq->entity, bfqg);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"allocated\");\n\t} else {\n\t\tbfqq = &bfqd->oom_bfqq;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"using oom bfqq\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Pin the queue now that it's allocated, scheduler exit will\n\t * prune it.\n\t */\n\tif (async_bfqq) {\n\t\tbfqq->ref++; /*\n\t\t\t      * Extra group reference, w.r.t. sync\n\t\t\t      * queue. This extra reference is removed\n\t\t\t      * only if bfqq->bfqg disappears, to\n\t\t\t      * guarantee that this queue is not freed\n\t\t\t      * until its group goes away.\n\t\t\t      */\n\t\tbfq_log_bfqq(bfqd, bfqq, \"get_queue, bfqq not in async: %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\t*async_bfqq = bfqq;\n\t}\n\nout:\n\tbfqq->ref++; /* get a process reference to this queue */\n\n\tif (bfqq != &bfqd->oom_bfqq && is_sync && !respawn)\n\t\tbfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);\n\treturn bfqq;\n}\n\nstatic void bfq_update_io_thinktime(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tstruct bfq_ttime *ttime = &bfqq->ttime;\n\tu64 elapsed;\n\n\t/*\n\t * We are really interested in how long it takes for the queue to\n\t * become busy when there is no outstanding IO for this queue. So\n\t * ignore cases when the bfq queue has already IO queued.\n\t */\n\tif (bfqq->dispatched || bfq_bfqq_busy(bfqq))\n\t\treturn;\n\telapsed = ktime_get_ns() - bfqq->ttime.last_end_request;\n\telapsed = min_t(u64, elapsed, 2ULL * bfqd->bfq_slice_idle);\n\n\tttime->ttime_samples = (7*ttime->ttime_samples + 256) / 8;\n\tttime->ttime_total = div_u64(7*ttime->ttime_total + 256*elapsed,  8);\n\tttime->ttime_mean = div64_ul(ttime->ttime_total + 128,\n\t\t\t\t     ttime->ttime_samples);\n}\n\nstatic void\nbfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct request *rq)\n{\n\tbfqq->seek_history <<= 1;\n\tbfqq->seek_history |= BFQ_RQ_SEEKY(bfqd, bfqq->last_request_pos, rq);\n\n\tif (bfqq->wr_coeff > 1 &&\n\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t    BFQQ_TOTALLY_SEEKY(bfqq)) {\n\t\tif (time_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t   bfq_wr_duration(bfqd))) {\n\t\t\t/*\n\t\t\t * In soft_rt weight raising with the\n\t\t\t * interactive-weight-raising period\n\t\t\t * elapsed (so no switch back to\n\t\t\t * interactive weight raising).\n\t\t\t */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t} else { /*\n\t\t\t  * stopping soft_rt weight raising\n\t\t\t  * while still in interactive period,\n\t\t\t  * switch back to interactive weight\n\t\t\t  * raising\n\t\t\t  */\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n}\n\nstatic void bfq_update_has_short_ttime(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq,\n\t\t\t\t       struct bfq_io_cq *bic)\n{\n\tbool has_short_ttime = true, state_changed;\n\n\t/*\n\t * No need to update has_short_ttime if bfqq is async or in\n\t * idle io prio class, or if bfq_slice_idle is zero, because\n\t * no device idling is performed for bfqq in this case.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || bfq_class_idle(bfqq) ||\n\t    bfqd->bfq_slice_idle == 0)\n\t\treturn;\n\n\t/* Idle window just restored, statistics are meaningless. */\n\tif (time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     bfqd->bfq_wr_min_idle_time))\n\t\treturn;\n\n\t/* Think time is infinite if no process is linked to\n\t * bfqq. Otherwise check average think time to decide whether\n\t * to mark as has_short_ttime. To this goal, compare average\n\t * think time with half the I/O-plugging timeout.\n\t */\n\tif (atomic_read(&bic->icq.ioc->active_ref) == 0 ||\n\t    (bfq_sample_valid(bfqq->ttime.ttime_samples) &&\n\t     bfqq->ttime.ttime_mean > bfqd->bfq_slice_idle>>1))\n\t\thas_short_ttime = false;\n\n\tstate_changed = has_short_ttime != bfq_bfqq_has_short_ttime(bfqq);\n\n\tif (has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * Until the base value for the total service time gets\n\t * finally computed for bfqq, the inject limit does depend on\n\t * the think-time state (short|long). In particular, the limit\n\t * is 0 or 1 if the think time is deemed, respectively, as\n\t * short or long (details in the comments in\n\t * bfq_update_inject_limit()). Accordingly, the next\n\t * instructions reset the inject limit if the think-time state\n\t * has changed and the above base value is still to be\n\t * computed.\n\t *\n\t * However, the reset is performed only if more than 100 ms\n\t * have elapsed since the last update of the inject limit, or\n\t * (inclusive) if the change is from short to long think\n\t * time. The reason for this waiting is as follows.\n\t *\n\t * bfqq may have a long think time because of a\n\t * synchronization with some other queue, i.e., because the\n\t * I/O of some other queue may need to be completed for bfqq\n\t * to receive new I/O. Details in the comments on the choice\n\t * of the queue for injection in bfq_select_queue().\n\t *\n\t * As stressed in those comments, if such a synchronization is\n\t * actually in place, then, without injection on bfqq, the\n\t * blocking I/O cannot happen to served while bfqq is in\n\t * service. As a consequence, if bfqq is granted\n\t * I/O-dispatch-plugging, then bfqq remains empty, and no I/O\n\t * is dispatched, until the idle timeout fires. This is likely\n\t * to result in lower bandwidth and higher latencies for bfqq,\n\t * and in a severe loss of total throughput.\n\t *\n\t * On the opposite end, a non-zero inject limit may allow the\n\t * I/O that blocks bfqq to be executed soon, and therefore\n\t * bfqq to receive new I/O soon.\n\t *\n\t * But, if the blocking gets actually eliminated, then the\n\t * next think-time sample for bfqq may be very low. This in\n\t * turn may cause bfqq's think time to be deemed\n\t * short. Without the 100 ms barrier, this new state change\n\t * would cause the body of the next if to be executed\n\t * immediately. But this would set to 0 the inject\n\t * limit. Without injection, the blocking I/O would cause the\n\t * think time of bfqq to become long again, and therefore the\n\t * inject limit to be raised again, and so on. The only effect\n\t * of such a steady oscillation between the two think-time\n\t * states would be to prevent effective injection on bfqq.\n\t *\n\t * In contrast, if the inject limit is not reset during such a\n\t * long time interval as 100 ms, then the number of short\n\t * think time samples can grow significantly before the reset\n\t * is performed. As a consequence, the think time state can\n\t * become stable before the reset. Therefore there will be no\n\t * state change when the 100 ms elapse, and no reset of the\n\t * inject limit. The inject limit remains steadily equal to 1\n\t * both during and after the 100 ms. So injection can be\n\t * performed at all times, and throughput gets boosted.\n\t *\n\t * An inject limit equal to 1 is however in conflict, in\n\t * general, with the fact that the think time of bfqq is\n\t * short, because injection may be likely to delay bfqq's I/O\n\t * (as explained in the comments in\n\t * bfq_update_inject_limit()). But this does not happen in\n\t * this special case, because bfqq's low think time is due to\n\t * an effective handling of a synchronization, through\n\t * injection. In this special case, bfqq's I/O does not get\n\t * delayed by injection; on the contrary, bfqq's I/O is\n\t * brought forward, because it is not blocked for\n\t * milliseconds.\n\t *\n\t * In addition, serving the blocking I/O much sooner, and much\n\t * more frequently than once per I/O-plugging timeout, makes\n\t * it much quicker to detect a waker queue (the concept of\n\t * waker queue is defined in the comments in\n\t * bfq_add_request()). This makes it possible to start sooner\n\t * to boost throughput more effectively, by injecting the I/O\n\t * of the waker queue unconditionally on every\n\t * bfq_dispatch_request().\n\t *\n\t * One last, important benefit of not resetting the inject\n\t * limit before 100 ms is that, during this time interval, the\n\t * base value for the total service time is likely to get\n\t * finally computed for bfqq, freeing the inject limit from\n\t * its relation with the think time.\n\t */\n\tif (state_changed && bfqq->last_serv_time_ns == 0 &&\n\t    (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t      msecs_to_jiffies(100)) ||\n\t     !has_short_ttime))\n\t\tbfq_reset_inject_limit(bfqd, bfqq);\n}\n\n/*\n * Called when a new fs request (rq) is added to bfqq.  Check if there's\n * something we should do about it.\n */\nstatic void bfq_rq_enqueued(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending++;\n\n\tbfqq->last_request_pos = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\n\tif (bfqq == bfqd->in_service_queue && bfq_bfqq_wait_request(bfqq)) {\n\t\tbool small_req = bfqq->queued[rq_is_sync(rq)] == 1 &&\n\t\t\t\t blk_rq_sectors(rq) < 32;\n\t\tbool budget_timeout = bfq_bfqq_budget_timeout(bfqq);\n\n\t\t/*\n\t\t * There is just this request queued: if\n\t\t * - the request is small, and\n\t\t * - we are idling to boost throughput, and\n\t\t * - the queue is not to be expired,\n\t\t * then just exit.\n\t\t *\n\t\t * In this way, if the device is being idled to wait\n\t\t * for a new request from the in-service queue, we\n\t\t * avoid unplugging the device and committing the\n\t\t * device to serve just a small request. In contrast\n\t\t * we wait for the block layer to decide when to\n\t\t * unplug the device: hopefully, new requests will be\n\t\t * merged to this one quickly, then the device will be\n\t\t * unplugged and larger requests will be dispatched.\n\t\t */\n\t\tif (small_req && idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t    !budget_timeout)\n\t\t\treturn;\n\n\t\t/*\n\t\t * A large enough request arrived, or idling is being\n\t\t * performed to preserve service guarantees, or\n\t\t * finally the queue is to be expired: in all these\n\t\t * cases disk idling is to be stopped, so clear\n\t\t * wait_request flag and reset timer.\n\t\t */\n\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\n\t\t/*\n\t\t * The queue is not empty, because a new request just\n\t\t * arrived. Hence we can safely expire the queue, in\n\t\t * case of budget timeout, without risking that the\n\t\t * timestamps of the queue are not updated correctly.\n\t\t * See [1] for more details.\n\t\t */\n\t\tif (budget_timeout)\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t}\n}\n\nstatic void bfqq_request_allocated(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated++;\n}\n\nstatic void bfqq_request_freed(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated--;\n}\n\n/* returns true if it causes the idle timer to be disabled */\nstatic bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,\n\t\t\t\t\t\t RQ_BIC(rq));\n\tbool waiting, idle_timer_disabled = false;\n\n\tif (new_bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\t\t/*\n\t\t * Release the request's reference to the old bfqq\n\t\t * and make sure one is taken to the shared queue.\n\t\t */\n\t\tbfqq_request_allocated(new_bfqq);\n\t\tbfqq_request_freed(bfqq);\n\t\tnew_bfqq->ref++;\n\t\t/*\n\t\t * If the bic associated with the process\n\t\t * issuing this request still points to bfqq\n\t\t * (and thus has not been already redirected\n\t\t * to new_bfqq or even some other bfq_queue),\n\t\t * then complete the merge and redirect it to\n\t\t * new_bfqq.\n\t\t */\n\t\tif (bic_to_bfqq(RQ_BIC(rq), true,\n\t\t\t\tbfq_actuator_index(bfqd, rq->bio)) == bfqq) {\n\t\t\twhile (bfqq != new_bfqq)\n\t\t\t\tbfqq = bfq_merge_bfqqs(bfqd, RQ_BIC(rq), bfqq);\n\t\t}\n\n\t\tbfq_clear_bfqq_just_created(old_bfqq);\n\t\t/*\n\t\t * rq is about to be enqueued into new_bfqq,\n\t\t * release rq reference on bfqq\n\t\t */\n\t\tbfq_put_queue(old_bfqq);\n\t\trq->elv.priv[1] = new_bfqq;\n\t}\n\n\tbfq_update_io_thinktime(bfqd, bfqq);\n\tbfq_update_has_short_ttime(bfqd, bfqq, RQ_BIC(rq));\n\tbfq_update_io_seektime(bfqd, bfqq, rq);\n\n\twaiting = bfqq && bfq_bfqq_wait_request(bfqq);\n\tbfq_add_request(rq);\n\tidle_timer_disabled = waiting && !bfq_bfqq_wait_request(bfqq);\n\n\trq->fifo_time = ktime_get_ns() + bfqd->bfq_fifo_expire[rq_is_sync(rq)];\n\tlist_add_tail(&rq->queuelist, &bfqq->fifo);\n\n\tbfq_rq_enqueued(bfqd, bfqq, rq);\n\n\treturn idle_timer_disabled;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t    struct bfq_queue *bfqq,\n\t\t\t\t    bool idle_timer_disabled,\n\t\t\t\t    blk_opf_t cmd_flags)\n{\n\tif (!bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq still exists, because it can disappear only after\n\t * either it is merged with another queue, or the process it\n\t * is associated with exits. But both actions must be taken by\n\t * the same process currently executing this flow of\n\t * instructions.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tbfqg_stats_update_io_add(bfqq_group(bfqq), bfqq, cmd_flags);\n\tif (idle_timer_disabled)\n\t\tbfqg_stats_update_idle_time(bfqq_group(bfqq));\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t\t   struct bfq_queue *bfqq,\n\t\t\t\t\t   bool idle_timer_disabled,\n\t\t\t\t\t   blk_opf_t cmd_flags) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct bfq_queue *bfq_init_rq(struct request *rq);\n\nstatic void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t       blk_insert_t flags)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_queue *bfqq;\n\tbool idle_timer_disabled = false;\n\tblk_opf_t cmd_flags;\n\tLIST_HEAD(free);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)\n\t\tbfqg_stats_update_legacy_io(q, rq);\n#endif\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bfq_init_rq(rq);\n\tif (blk_mq_sched_try_insert_merge(q, rq, &free)) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tblk_mq_free_requests(&free);\n\t\treturn;\n\t}\n\n\ttrace_block_rq_insert(rq);\n\n\tif (flags & BLK_MQ_INSERT_AT_HEAD) {\n\t\tlist_add(&rq->queuelist, &bfqd->dispatch);\n\t} else if (!bfqq) {\n\t\tlist_add_tail(&rq->queuelist, &bfqd->dispatch);\n\t} else {\n\t\tidle_timer_disabled = __bfq_insert_request(bfqd, rq);\n\t\t/*\n\t\t * Update bfqq, because, if a queue merge has occurred\n\t\t * in __bfq_insert_request, then rq has been\n\t\t * redirected into a new queue.\n\t\t */\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (rq_mergeable(rq)) {\n\t\t\telv_rqhash_add(q, rq);\n\t\t\tif (!q->last_merge)\n\t\t\t\tq->last_merge = rq;\n\t\t}\n\t}\n\n\t/*\n\t * Cache cmd_flags before releasing scheduler lock, because rq\n\t * may disappear afterwards (for example, because of a request\n\t * merge).\n\t */\n\tcmd_flags = rq->cmd_flags;\n\tspin_unlock_irq(&bfqd->lock);\n\n\tbfq_update_insert_stats(q, bfqq, idle_timer_disabled,\n\t\t\t\tcmd_flags);\n}\n\nstatic void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t\tstruct list_head *list,\n\t\t\t\tblk_insert_t flags)\n{\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tbfq_insert_request(hctx, rq, flags);\n\t}\n}\n\nstatic void bfq_update_hw_tag(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\tbfqd->max_rq_in_driver = max_t(int, bfqd->max_rq_in_driver,\n\t\t\t\t       bfqd->tot_rq_in_driver);\n\n\tif (bfqd->hw_tag == 1)\n\t\treturn;\n\n\t/*\n\t * This sample is valid if the number of outstanding requests\n\t * is large enough to allow a queueing behavior.  Note that the\n\t * sum is not exact, as it's not taking into account deactivated\n\t * requests.\n\t */\n\tif (bfqd->tot_rq_in_driver + bfqd->queued <= BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\t/*\n\t * If active queue hasn't enough requests and can idle, bfq might not\n\t * dispatch sufficient requests to hardware. Don't zero hw_tag in this\n\t * case\n\t */\n\tif (bfqq && bfq_bfqq_has_short_ttime(bfqq) &&\n\t    bfqq->dispatched + bfqq->queued[0] + bfqq->queued[1] <\n\t    BFQ_HW_QUEUE_THRESHOLD &&\n\t    bfqd->tot_rq_in_driver < BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\tif (bfqd->hw_tag_samples++ < BFQ_HW_QUEUE_SAMPLES)\n\t\treturn;\n\n\tbfqd->hw_tag = bfqd->max_rq_in_driver > BFQ_HW_QUEUE_THRESHOLD;\n\tbfqd->max_rq_in_driver = 0;\n\tbfqd->hw_tag_samples = 0;\n\n\tbfqd->nonrot_with_queueing =\n\t\tblk_queue_nonrot(bfqd->queue) && bfqd->hw_tag;\n}\n\nstatic void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)\n{\n\tu64 now_ns;\n\tu32 delta_us;\n\n\tbfq_update_hw_tag(bfqd);\n\n\tbfqd->rq_in_driver[bfqq->actuator_idx]--;\n\tbfqd->tot_rq_in_driver--;\n\tbfqq->dispatched--;\n\n\tif (!bfqq->dispatched && !bfq_bfqq_busy(bfqq)) {\n\t\t/*\n\t\t * Set budget_timeout (which we overload to store the\n\t\t * time at which the queue remains with no backlog and\n\t\t * no outstanding request; used by the weight-raising\n\t\t * mechanism).\n\t\t */\n\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_in_groups_with_pending_reqs(bfqq);\n\t\tbfq_weights_tree_remove(bfqq);\n\t}\n\n\tnow_ns = ktime_get_ns();\n\n\tbfqq->ttime.last_end_request = now_ns;\n\n\t/*\n\t * Using us instead of ns, to get a reasonable precision in\n\t * computing rate in next check.\n\t */\n\tdelta_us = div_u64(now_ns - bfqd->last_completion, NSEC_PER_USEC);\n\n\t/*\n\t * If the request took rather long to complete, and, according\n\t * to the maximum request size recorded, this completion latency\n\t * implies that the request was certainly served at a very low\n\t * rate (less than 1M sectors/sec), then the whole observation\n\t * interval that lasts up to this time instant cannot be a\n\t * valid time interval for computing a new peak rate.  Invoke\n\t * bfq_update_rate_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - reset to zero samples, which will trigger a proper\n\t *   re-initialization of the observation interval on next\n\t *   dispatch\n\t */\n\tif (delta_us > BFQ_MIN_TT/NSEC_PER_USEC &&\n\t   (bfqd->last_rq_max_size<<BFQ_RATE_SHIFT)/delta_us <\n\t\t\t1UL<<(BFQ_RATE_SHIFT - 10))\n\t\tbfq_update_rate_reset(bfqd, NULL);\n\tbfqd->last_completion = now_ns;\n\t/*\n\t * Shared queues are likely to receive I/O at a high\n\t * rate. This may deceptively let them be considered as wakers\n\t * of other queues. But a false waker will unjustly steal\n\t * bandwidth to its supposedly woken queue. So considering\n\t * also shared queues in the waking mechanism may cause more\n\t * control troubles than throughput benefits. Then reset\n\t * last_completed_rq_bfqq if bfqq is a shared queue.\n\t */\n\tif (!bfq_bfqq_coop(bfqq))\n\t\tbfqd->last_completed_rq_bfqq = bfqq;\n\telse\n\t\tbfqd->last_completed_rq_bfqq = NULL;\n\n\t/*\n\t * If we are waiting to discover whether the request pattern\n\t * of the task associated with the queue is actually\n\t * isochronous, and both requisites for this condition to hold\n\t * are now satisfied, then compute soft_rt_next_start (see the\n\t * comments on the function bfq_bfqq_softrt_next_start()). We\n\t * do not compute soft_rt_next_start if bfqq is in interactive\n\t * weight raising (see the comments in bfq_bfqq_expire() for\n\t * an explanation). We schedule this delayed update when bfqq\n\t * expires, if it still has in-flight requests.\n\t */\n\tif (bfq_bfqq_softrt_update(bfqq) && bfqq->dispatched == 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq->wr_coeff != bfqd->bfq_wr_coeff)\n\t\tbfqq->soft_rt_next_start =\n\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\n\t/*\n\t * If this is the in-service queue, check if it needs to be expired,\n\t * or if we want to idle in case it has no pending requests.\n\t */\n\tif (bfqd->in_service_queue == bfqq) {\n\t\tif (bfq_bfqq_must_idle(bfqq)) {\n\t\t\tif (bfqq->dispatched == 0)\n\t\t\t\tbfq_arm_slice_timer(bfqd);\n\t\t\t/*\n\t\t\t * If we get here, we do not expire bfqq, even\n\t\t\t * if bfqq was in budget timeout or had no\n\t\t\t * more requests (as controlled in the next\n\t\t\t * conditional instructions). The reason for\n\t\t\t * not expiring bfqq is as follows.\n\t\t\t *\n\t\t\t * Here bfqq->dispatched > 0 holds, but\n\t\t\t * bfq_bfqq_must_idle() returned true. This\n\t\t\t * implies that, even if no request arrives\n\t\t\t * for bfqq before bfqq->dispatched reaches 0,\n\t\t\t * bfqq will, however, not be expired on the\n\t\t\t * completion event that causes bfqq->dispatch\n\t\t\t * to reach zero. In contrast, on this event,\n\t\t\t * bfqq will start enjoying device idling\n\t\t\t * (I/O-dispatch plugging).\n\t\t\t *\n\t\t\t * But, if we expired bfqq here, bfqq would\n\t\t\t * not have the chance to enjoy device idling\n\t\t\t * when bfqq->dispatched finally reaches\n\t\t\t * zero. This would expose bfqq to violation\n\t\t\t * of its reserved service guarantees.\n\t\t\t */\n\t\t\treturn;\n\t\t} else if (bfq_may_expire_for_budg_timeout(bfqq))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t\telse if (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t (bfqq->dispatched == 0 ||\n\t\t\t  !bfq_better_to_idle(bfqq)))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_NO_MORE_REQUESTS);\n\t}\n\n\tif (!bfqd->tot_rq_in_driver)\n\t\tbfq_schedule_dispatch(bfqd);\n}\n\n/*\n * The processes associated with bfqq may happen to generate their\n * cumulative I/O at a lower rate than the rate at which the device\n * could serve the same I/O. This is rather probable, e.g., if only\n * one process is associated with bfqq and the device is an SSD. It\n * results in bfqq becoming often empty while in service. In this\n * respect, if BFQ is allowed to switch to another queue when bfqq\n * remains empty, then the device goes on being fed with I/O requests,\n * and the throughput is not affected. In contrast, if BFQ is not\n * allowed to switch to another queue---because bfqq is sync and\n * I/O-dispatch needs to be plugged while bfqq is temporarily\n * empty---then, during the service of bfqq, there will be frequent\n * \"service holes\", i.e., time intervals during which bfqq gets empty\n * and the device can only consume the I/O already queued in its\n * hardware queues. During service holes, the device may even get to\n * remaining idle. In the end, during the service of bfqq, the device\n * is driven at a lower speed than the one it can reach with the kind\n * of I/O flowing through bfqq.\n *\n * To counter this loss of throughput, BFQ implements a \"request\n * injection mechanism\", which tries to fill the above service holes\n * with I/O requests taken from other queues. The hard part in this\n * mechanism is finding the right amount of I/O to inject, so as to\n * both boost throughput and not break bfqq's bandwidth and latency\n * guarantees. In this respect, the mechanism maintains a per-queue\n * inject limit, computed as below. While bfqq is empty, the injection\n * mechanism dispatches extra I/O requests only until the total number\n * of I/O requests in flight---i.e., already dispatched but not yet\n * completed---remains lower than this limit.\n *\n * A first definition comes in handy to introduce the algorithm by\n * which the inject limit is computed.  We define as first request for\n * bfqq, an I/O request for bfqq that arrives while bfqq is in\n * service, and causes bfqq to switch from empty to non-empty. The\n * algorithm updates the limit as a function of the effect of\n * injection on the service times of only the first requests of\n * bfqq. The reason for this restriction is that these are the\n * requests whose service time is affected most, because they are the\n * first to arrive after injection possibly occurred.\n *\n * To evaluate the effect of injection, the algorithm measures the\n * \"total service time\" of first requests. We define as total service\n * time of an I/O request, the time that elapses since when the\n * request is enqueued into bfqq, to when it is completed. This\n * quantity allows the whole effect of injection to be measured. It is\n * easy to see why. Suppose that some requests of other queues are\n * actually injected while bfqq is empty, and that a new request R\n * then arrives for bfqq. If the device does start to serve all or\n * part of the injected requests during the service hole, then,\n * because of this extra service, it may delay the next invocation of\n * the dispatch hook of BFQ. Then, even after R gets eventually\n * dispatched, the device may delay the actual service of R if it is\n * still busy serving the extra requests, or if it decides to serve,\n * before R, some extra request still present in its queues. As a\n * conclusion, the cumulative extra delay caused by injection can be\n * easily evaluated by just comparing the total service time of first\n * requests with and without injection.\n *\n * The limit-update algorithm works as follows. On the arrival of a\n * first request of bfqq, the algorithm measures the total time of the\n * request only if one of the three cases below holds, and, for each\n * case, it updates the limit as described below:\n *\n * (1) If there is no in-flight request. This gives a baseline for the\n *     total service time of the requests of bfqq. If the baseline has\n *     not been computed yet, then, after computing it, the limit is\n *     set to 1, to start boosting throughput, and to prepare the\n *     ground for the next case. If the baseline has already been\n *     computed, then it is updated, in case it results to be lower\n *     than the previous value.\n *\n * (2) If the limit is higher than 0 and there are in-flight\n *     requests. By comparing the total service time in this case with\n *     the above baseline, it is possible to know at which extent the\n *     current value of the limit is inflating the total service\n *     time. If the inflation is below a certain threshold, then bfqq\n *     is assumed to be suffering from no perceivable loss of its\n *     service guarantees, and the limit is even tentatively\n *     increased. If the inflation is above the threshold, then the\n *     limit is decreased. Due to the lack of any hysteresis, this\n *     logic makes the limit oscillate even in steady workload\n *     conditions. Yet we opted for it, because it is fast in reaching\n *     the best value for the limit, as a function of the current I/O\n *     workload. To reduce oscillations, this step is disabled for a\n *     short time interval after the limit happens to be decreased.\n *\n * (3) Periodically, after resetting the limit, to make sure that the\n *     limit eventually drops in case the workload changes. This is\n *     needed because, after the limit has gone safely up for a\n *     certain workload, it is impossible to guess whether the\n *     baseline total service time may have changed, without measuring\n *     it again without injection. A more effective version of this\n *     step might be to just sample the baseline, by interrupting\n *     injection only once, and then to reset/lower the limit only if\n *     the total service time with the current limit does happen to be\n *     too large.\n *\n * More details on each step are provided in the comments on the\n * pieces of code that implement these steps: the branch handling the\n * transition from empty to non empty in bfq_add_request(), the branch\n * handling injection in bfq_select_queue(), and the function\n * bfq_choose_bfqq_for_injection(). These comments also explain some\n * exceptions, made by the injection mechanism in some special cases.\n */\nstatic void bfq_update_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tu64 tot_time_ns = ktime_get_ns() - bfqd->last_empty_occupied_ns;\n\tunsigned int old_limit = bfqq->inject_limit;\n\n\tif (bfqq->last_serv_time_ns > 0 && bfqd->rqs_injected) {\n\t\tu64 threshold = (bfqq->last_serv_time_ns * 3)>>1;\n\n\t\tif (tot_time_ns >= threshold && old_limit > 0) {\n\t\t\tbfqq->inject_limit--;\n\t\t\tbfqq->decrease_time_jif = jiffies;\n\t\t} else if (tot_time_ns < threshold &&\n\t\t\t   old_limit <= bfqd->max_rq_in_driver)\n\t\t\tbfqq->inject_limit++;\n\t}\n\n\t/*\n\t * Either we still have to compute the base value for the\n\t * total service time, and there seem to be the right\n\t * conditions to do it, or we can lower the last base value\n\t * computed.\n\t *\n\t * NOTE: (bfqd->tot_rq_in_driver == 1) means that there is no I/O\n\t * request in flight, because this function is in the code\n\t * path that handles the completion of a request of bfqq, and,\n\t * in particular, this function is executed before\n\t * bfqd->tot_rq_in_driver is decremented in such a code path.\n\t */\n\tif ((bfqq->last_serv_time_ns == 0 && bfqd->tot_rq_in_driver == 1) ||\n\t    tot_time_ns < bfqq->last_serv_time_ns) {\n\t\tif (bfqq->last_serv_time_ns == 0) {\n\t\t\t/*\n\t\t\t * Now we certainly have a base value: make sure we\n\t\t\t * start trying injection.\n\t\t\t */\n\t\t\tbfqq->inject_limit = max_t(unsigned int, 1, old_limit);\n\t\t}\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\t} else if (!bfqd->rqs_injected && bfqd->tot_rq_in_driver == 1)\n\t\t/*\n\t\t * No I/O injected and no request still in service in\n\t\t * the drive: these are the exact conditions for\n\t\t * computing the base value of the total service time\n\t\t * for bfqq. So let's update this value, because it is\n\t\t * rather variable. For example, it varies if the size\n\t\t * or the spatial locality of the I/O requests in bfqq\n\t\t * change.\n\t\t */\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\n\n\t/* update complete, not waiting for any request completion any longer */\n\tbfqd->waited_rq = NULL;\n\tbfqd->rqs_injected = false;\n}\n\n/*\n * Handle either a requeue or a finish for rq. The things to do are\n * the same in both cases: all references to rq are to be dropped. In\n * particular, rq is considered completed from the point of view of\n * the scheduler.\n */\nstatic void bfq_finish_requeue_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd;\n\tunsigned long flags;\n\n\t/*\n\t * rq either is not associated with any icq, or is an already\n\t * requeued request that has not (yet) been re-inserted into\n\t * a bfq_queue.\n\t */\n\tif (!rq->elv.icq || !bfqq)\n\t\treturn;\n\n\tbfqd = bfqq->bfqd;\n\n\tif (rq->rq_flags & RQF_STARTED)\n\t\tbfqg_stats_update_completion(bfqq_group(bfqq),\n\t\t\t\t\t     rq->start_time_ns,\n\t\t\t\t\t     rq->io_start_time_ns,\n\t\t\t\t\t     rq->cmd_flags);\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\tif (likely(rq->rq_flags & RQF_STARTED)) {\n\t\tif (rq == bfqd->waited_rq)\n\t\t\tbfq_update_inject_limit(bfqd, bfqq);\n\n\t\tbfq_completed_request(bfqq, bfqd);\n\t}\n\tbfqq_request_freed(bfqq);\n\tbfq_put_queue(bfqq);\n\tRQ_BIC(rq)->requests--;\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\n\t/*\n\t * Reset private fields. In case of a requeue, this allows\n\t * this function to correctly do nothing if it is spuriously\n\t * invoked again on this same request (see the check at the\n\t * beginning of the function). Probably, a better general\n\t * design would be to prevent blk-mq from invoking the requeue\n\t * or finish hooks of an elevator, for a request that is not\n\t * referred by that elevator.\n\t *\n\t * Resetting the following fields would break the\n\t * request-insertion logic if rq is re-inserted into a bfq\n\t * internal queue, without a re-preparation. Here we assume\n\t * that re-insertions of requeued requests, without\n\t * re-preparation, can happen only for pass_through or at_head\n\t * requests (which are not re-inserted into bfq internal\n\t * queues).\n\t */\n\trq->elv.priv[0] = NULL;\n\trq->elv.priv[1] = NULL;\n}\n\nstatic void bfq_finish_request(struct request *rq)\n{\n\tbfq_finish_requeue_request(rq);\n\n\tif (rq->elv.icq) {\n\t\tput_io_context(rq->elv.icq->ioc);\n\t\trq->elv.icq = NULL;\n\t}\n}\n\n/*\n * Removes the association between the current task and bfqq, assuming\n * that bic points to the bfq iocontext of the task.\n * Returns NULL if a new bfqq should be allocated, or the old bfqq if this\n * was the last process referring to that bfqq.\n */\nstatic struct bfq_queue *\nbfq_split_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"splitting queue\");\n\n\tif (bfqq_process_refs(bfqq) == 1 && !bfqq->new_bfqq) {\n\t\tbfqq->pid = current->pid;\n\t\tbfq_clear_bfqq_coop(bfqq);\n\t\tbfq_clear_bfqq_split_coop(bfqq);\n\t\treturn bfqq;\n\t}\n\n\tbic_set_bfqq(bic, NULL, true, bfqq->actuator_idx);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqq->bfqd, bfqq);\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_get_bfqq_handle_split(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_io_cq *bic,\n\t\t\t\t\t\t   struct bio *bio,\n\t\t\t\t\t\t   bool split, bool is_sync,\n\t\t\t\t\t\t   bool *new_queue)\n{\n\tunsigned int act_idx = bfq_actuator_index(bfqd, bio);\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync, act_idx);\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[act_idx];\n\n\tif (likely(bfqq && bfqq != &bfqd->oom_bfqq))\n\t\treturn bfqq;\n\n\tif (new_queue)\n\t\t*new_queue = true;\n\n\tif (bfqq)\n\t\tbfq_put_queue(bfqq);\n\tbfqq = bfq_get_queue(bfqd, bio, is_sync, bic, split);\n\n\tbic_set_bfqq(bic, bfqq, is_sync, act_idx);\n\tif (split && is_sync) {\n\t\tif ((bfqq_data->was_in_burst_list && bfqd->large_burst) ||\n\t\t    bfqq_data->saved_in_large_burst)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\telse {\n\t\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t\t\tif (bfqq_data->was_in_burst_list)\n\t\t\t\t/*\n\t\t\t\t * If bfqq was in the current\n\t\t\t\t * burst list before being\n\t\t\t\t * merged, then we have to add\n\t\t\t\t * it back. And we do not need\n\t\t\t\t * to increase burst_size, as\n\t\t\t\t * we did not decrement\n\t\t\t\t * burst_size when we removed\n\t\t\t\t * bfqq from the burst list as\n\t\t\t\t * a consequence of a merge\n\t\t\t\t * (see comments in\n\t\t\t\t * bfq_put_queue). In this\n\t\t\t\t * respect, it would be rather\n\t\t\t\t * costly to know whether the\n\t\t\t\t * current burst list is still\n\t\t\t\t * the same burst list from\n\t\t\t\t * which bfqq was removed on\n\t\t\t\t * the merge. To avoid this\n\t\t\t\t * cost, if bfqq was in a\n\t\t\t\t * burst list, then we add\n\t\t\t\t * bfqq to the current burst\n\t\t\t\t * list without any further\n\t\t\t\t * check. This can cause\n\t\t\t\t * inappropriate insertions,\n\t\t\t\t * but rarely enough to not\n\t\t\t\t * harm the detection of large\n\t\t\t\t * bursts significantly.\n\t\t\t\t */\n\t\t\t\thlist_add_head(&bfqq->burst_list_node,\n\t\t\t\t\t       &bfqd->burst_list);\n\t\t}\n\t\tbfqq->split_time = jiffies;\n\t}\n\n\treturn bfqq;\n}\n\n/*\n * Only reset private fields. The actual request preparation will be\n * performed by bfq_init_rq, when rq is either inserted or merged. See\n * comments on bfq_init_rq for the reason behind this delayed\n * preparation.\n */\nstatic void bfq_prepare_request(struct request *rq)\n{\n\trq->elv.icq = ioc_find_get_icq(rq->q);\n\n\t/*\n\t * Regardless of whether we have an icq attached, we have to\n\t * clear the scheduler pointers, as they might point to\n\t * previously allocated bic/bfqq structs.\n\t */\n\trq->elv.priv[0] = rq->elv.priv[1] = NULL;\n}\n\nstatic struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\tstruct bfq_queue *waker_bfqq = bfqq->waker_bfqq;\n\n\tif (!waker_bfqq)\n\t\treturn NULL;\n\n\twhile (new_bfqq) {\n\t\tif (new_bfqq == waker_bfqq) {\n\t\t\t/*\n\t\t\t * If waker_bfqq is in the merge chain, and current\n\t\t\t * is the only procress.\n\t\t\t */\n\t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n\t\t\t\treturn NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t}\n\n\treturn waker_bfqq;\n}\n\n/*\n * If needed, init rq, allocate bfq data structures associated with\n * rq, and increment reference counters in the destination bfq_queue\n * for rq. Return the destination bfq_queue for rq, or NULL is rq is\n * not associated with any bfq_queue.\n *\n * This function is invoked by the functions that perform rq insertion\n * or merging. One may have expected the above preparation operations\n * to be performed in bfq_prepare_request, and not delayed to when rq\n * is inserted or merged. The rationale behind this delayed\n * preparation is that, after the prepare_request hook is invoked for\n * rq, rq may still be transformed into a request with no icq, i.e., a\n * request not associated with any queue. No bfq hook is invoked to\n * signal this transformation. As a consequence, should these\n * preparation operations be performed when the prepare_request hook\n * is invoked, and should rq be transformed one moment later, bfq\n * would end up in an inconsistent state, because it would have\n * incremented some queue counters for an rq destined to\n * transformation, without any chance to correctly lower these\n * counters back. In contrast, no transformation can still happen for\n * rq after rq has been inserted or merged. So, it is safe to execute\n * these preparation operations when rq is finally inserted or merged.\n */\nstatic struct bfq_queue *bfq_init_rq(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio = rq->bio;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic;\n\tconst int is_sync = rq_is_sync(rq);\n\tstruct bfq_queue *bfqq;\n\tbool new_queue = false;\n\tbool bfqq_already_existing = false, split = false;\n\tunsigned int a_idx = bfq_actuator_index(bfqd, bio);\n\n\tif (unlikely(!rq->elv.icq))\n\t\treturn NULL;\n\n\t/*\n\t * Assuming that RQ_BFQQ(rq) is set only if everything is set\n\t * for this rq. This holds true, because this function is\n\t * invoked only for insertion or merging, and, after such\n\t * events, a request cannot be manipulated any longer before\n\t * being removed from bfq.\n\t */\n\tif (RQ_BFQQ(rq))\n\t\treturn RQ_BFQQ(rq);\n\n\tbic = icq_to_bic(rq->elv.icq);\n\n\tbfq_check_ioprio_change(bic, bio);\n\n\tbfq_bic_update_cgroup(bic, bio);\n\n\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync,\n\t\t\t\t\t &new_queue);\n\n\tif (likely(!new_queue)) {\n\t\t/* If the queue was seeky for too long, break it apart. */\n\t\tif (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) &&\n\t\t\t!bic->bfqq_data[a_idx].stably_merged) {\n\t\t\tstruct bfq_queue *waker_bfqq = bfq_waker_bfqq(bfqq);\n\n\t\t\t/* Update bic before losing reference to bfqq */\n\t\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\t\tbic->bfqq_data[a_idx].saved_in_large_burst =\n\t\t\t\t\ttrue;\n\n\t\t\tbfqq = bfq_split_bfqq(bic, bfqq);\n\t\t\tsplit = true;\n\n\t\t\tif (!bfqq) {\n\t\t\t\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio,\n\t\t\t\t\t\t\t\t true, is_sync,\n\t\t\t\t\t\t\t\t NULL);\n\t\t\t\tif (unlikely(bfqq == &bfqd->oom_bfqq))\n\t\t\t\t\tbfqq_already_existing = true;\n\t\t\t} else\n\t\t\t\tbfqq_already_existing = true;\n\n\t\t\tif (!bfqq_already_existing) {\n\t\t\t\tbfqq->waker_bfqq = waker_bfqq;\n\t\t\t\tbfqq->tentative_waker_bfqq = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * If the waker queue disappears, then\n\t\t\t\t * new_bfqq->waker_bfqq must be\n\t\t\t\t * reset. So insert new_bfqq into the\n\t\t\t\t * woken_list of the waker. See\n\t\t\t\t * bfq_check_waker for details.\n\t\t\t\t */\n\t\t\t\tif (waker_bfqq)\n\t\t\t\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t\t\t\t       &bfqq->waker_bfqq->woken_list);\n\t\t\t}\n\t\t}\n\t}\n\n\tbfqq_request_allocated(bfqq);\n\tbfqq->ref++;\n\tbic->requests++;\n\tbfq_log_bfqq(bfqd, bfqq, \"get_request %p: bfqq %p, %d\",\n\t\t     rq, bfqq, bfqq->ref);\n\n\trq->elv.priv[0] = bic;\n\trq->elv.priv[1] = bfqq;\n\n\t/*\n\t * If a bfq_queue has only one process reference, it is owned\n\t * by only this bic: we can then set bfqq->bic = bic. in\n\t * addition, if the queue has also just been split, we have to\n\t * resume its state.\n\t */\n\tif (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq &&\n\t    bfqq_process_refs(bfqq) == 1) {\n\t\tbfqq->bic = bic;\n\t\tif (split) {\n\t\t\t/*\n\t\t\t * The queue has just been split from a shared\n\t\t\t * queue: restore the idle window and the\n\t\t\t * possible weight raising period.\n\t\t\t */\n\t\t\tbfq_bfqq_resume_state(bfqq, bfqd, bic,\n\t\t\t\t\t      bfqq_already_existing);\n\t\t}\n\t}\n\n\t/*\n\t * Consider bfqq as possibly belonging to a burst of newly\n\t * created queues only if:\n\t * 1) A burst is actually happening (bfqd->burst_size > 0)\n\t * or\n\t * 2) There is no other active queue. In fact, if, in\n\t *    contrast, there are active queues not belonging to the\n\t *    possible burst bfqq may belong to, then there is no gain\n\t *    in considering bfqq as belonging to a burst, and\n\t *    therefore in not weight-raising bfqq. See comments on\n\t *    bfq_handle_burst().\n\t *\n\t * This filtering also helps eliminating false positives,\n\t * occurring when bfqq does not belong to an actual large\n\t * burst, but some background task (e.g., a service) happens\n\t * to trigger the creation of new queues very close to when\n\t * bfqq and its possible companion queues are created. See\n\t * comments on bfq_handle_burst() for further details also on\n\t * this issue.\n\t */\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     (bfqd->burst_size > 0 ||\n\t\t      bfq_tot_busy_queues(bfqd) == 0)))\n\t\tbfq_handle_burst(bfqd, bfqq);\n\n\treturn bfqq;\n}\n\nstatic void\nbfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\n\t/*\n\t * Considering that bfqq may be in race, we should firstly check\n\t * whether bfqq is in service before doing something on it. If\n\t * the bfqq in race is not in service, it has already been expired\n\t * through __bfq_bfqq_expire func and its wait_request flags has\n\t * been cleared in __bfq_bfqd_reset_in_service func.\n\t */\n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t/*\n\t\t * Also here the queue can be safely expired\n\t\t * for budget timeout without wasting\n\t\t * guarantees\n\t\t */\n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t/*\n\t\t * The queue may not be empty upon timer expiration,\n\t\t * because we may not disable the timer when the\n\t\t * first request of the in-service queue arrives\n\t\t * during disk idling.\n\t\t */\n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tbfq_schedule_dispatch(bfqd);\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n/*\n * Handler of the expiration of the timer running if the in-service queue\n * is idling inside its time slice.\n */\nstatic enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t/*\n\t * Theoretical race here: the in-service queue can be NULL or\n\t * different from the queue that was idling if a new request\n\t * arrives for the current queue and there is a full dispatch\n\t * cycle that changes the in-service queue.  This can hardly\n\t * happen, but in the worst case we just expire a queue too\n\t * early.\n\t */\n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __bfq_put_async_bfqq(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue **bfqq_ptr)\n{\n\tstruct bfq_queue *bfqq = *bfqq_ptr;\n\n\tbfq_log(bfqd, \"put_async_bfqq: %p\", bfqq);\n\tif (bfqq) {\n\t\tbfq_bfqq_move(bfqd, bfqq, bfqd->root_group);\n\n\t\tbfq_log_bfqq(bfqd, bfqq, \"put_async_bfqq: putting %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\tbfq_put_queue(bfqq);\n\t\t*bfqq_ptr = NULL;\n\t}\n}\n\n/*\n * Release all the bfqg references to its async queues.  If we are\n * deallocating the group these queues may still contain requests, so\n * we reparent them to the root cgroup (i.e., the only one that will\n * exist for sure until all the requests on a device are gone).\n */\nvoid bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)\n{\n\tint i, j, k;\n\n\tfor (k = 0; k < bfqd->num_actuators; k++) {\n\t\tfor (i = 0; i < 2; i++)\n\t\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_bfqq[i][j][k]);\n\n\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_idle_bfqq[k]);\n\t}\n}\n\n/*\n * See the comments on bfq_limit_depth for the purpose of\n * the depths set in the function. Return minimum shallow depth we'll use.\n */\nstatic void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)\n{\n\tunsigned int depth = 1U << bt->sb.shift;\n\n\tbfqd->full_depth_shift = bt->sb.shift;\n\t/*\n\t * In-word depths if no bfq_queue is being weight-raised:\n\t * leaving 25% of tags only for sync reads.\n\t *\n\t * In next formulas, right-shift the value\n\t * (1U<<bt->sb.shift), instead of computing directly\n\t * (1U<<(bt->sb.shift - something)), to be robust against\n\t * any possible value of bt->sb.shift, without having to\n\t * limit 'something'.\n\t */\n\t/* no more than 50% of tags for async I/O */\n\tbfqd->word_depths[0][0] = max(depth >> 1, 1U);\n\t/*\n\t * no more than 75% of tags for sync writes (25% extra tags\n\t * w.r.t. async I/O, to prevent async I/O from starving sync\n\t * writes)\n\t */\n\tbfqd->word_depths[0][1] = max((depth * 3) >> 2, 1U);\n\n\t/*\n\t * In-word depths in case some bfq_queue is being weight-\n\t * raised: leaving ~63% of tags for sync reads. This is the\n\t * highest percentage for which, in our tests, application\n\t * start-up times didn't suffer from any regression due to tag\n\t * shortage.\n\t */\n\t/* no more than ~18% of tags for async I/O */\n\tbfqd->word_depths[1][0] = max((depth * 3) >> 4, 1U);\n\t/* no more than ~37% of tags for sync writes (~20% extra tags) */\n\tbfqd->word_depths[1][1] = max((depth * 6) >> 4, 1U);\n}\n\nstatic void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\n\tbfq_update_depths(bfqd, &tags->bitmap_tags);\n\tsbitmap_queue_min_shallow_depth(&tags->bitmap_tags, 1);\n}\n\nstatic int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)\n{\n\tbfq_depth_updated(hctx);\n\treturn 0;\n}\n\nstatic void bfq_exit_queue(struct elevator_queue *e)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tstruct bfq_queue *bfqq, *n;\n\tunsigned int actuator;\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\tspin_lock_irq(&bfqd->lock);\n\tlist_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)\n\t\tbfq_deactivate_bfqq(bfqd, bfqq, false, false);\n\tspin_unlock_irq(&bfqd->lock);\n\n\tfor (actuator = 0; actuator < bfqd->num_actuators; actuator++)\n\t\tWARN_ON_ONCE(bfqd->rq_in_driver[actuator]);\n\tWARN_ON_ONCE(bfqd->tot_rq_in_driver);\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\t/* release oom-queue reference to root group */\n\tbfqg_and_blkg_put(bfqd->root_group);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_deactivate_policy(bfqd->queue->disk, &blkcg_policy_bfq);\n#else\n\tspin_lock_irq(&bfqd->lock);\n\tbfq_put_async_queues(bfqd, bfqd->root_group);\n\tkfree(bfqd->root_group);\n\tspin_unlock_irq(&bfqd->lock);\n#endif\n\n\tblk_stat_disable_accounting(bfqd->queue);\n\tclear_bit(ELEVATOR_FLAG_DISABLE_WBT, &e->flags);\n\twbt_enable_default(bfqd->queue->disk);\n\n\tkfree(bfqd);\n}\n\nstatic void bfq_init_root_group(struct bfq_group *root_group,\n\t\t\t\tstruct bfq_data *bfqd)\n{\n\tint i;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\troot_group->entity.parent = NULL;\n\troot_group->my_entity = NULL;\n\troot_group->bfqd = bfqd;\n#endif\n\troot_group->rq_pos_tree = RB_ROOT;\n\tfor (i = 0; i < BFQ_IOPRIO_CLASSES; i++)\n\t\troot_group->sched_data.service_tree[i] = BFQ_SERVICE_TREE_INIT;\n\troot_group->sched_data.bfq_class_idle_last_service = jiffies;\n}\n\nstatic int bfq_init_queue(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct bfq_data *bfqd;\n\tstruct elevator_queue *eq;\n\tunsigned int i;\n\tstruct blk_independent_access_ranges *ia_ranges = q->disk->ia_ranges;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn -ENOMEM;\n\n\tbfqd = kzalloc_node(sizeof(*bfqd), GFP_KERNEL, q->node);\n\tif (!bfqd) {\n\t\tkobject_put(&eq->kobj);\n\t\treturn -ENOMEM;\n\t}\n\teq->elevator_data = bfqd;\n\n\tspin_lock_irq(&q->queue_lock);\n\tq->elevator = eq;\n\tspin_unlock_irq(&q->queue_lock);\n\n\t/*\n\t * Our fallback bfqq if bfq_find_alloc_queue() runs into OOM issues.\n\t * Grab a permanent reference to it, so that the normal code flow\n\t * will not attempt to free it.\n\t * Set zero as actuator index: we will pretend that\n\t * all I/O requests are for the same actuator.\n\t */\n\tbfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0, 0);\n\tbfqd->oom_bfqq.ref++;\n\tbfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;\n\tbfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;\n\tbfqd->oom_bfqq.entity.new_weight =\n\t\tbfq_ioprio_to_weight(bfqd->oom_bfqq.new_ioprio);\n\n\t/* oom_bfqq does not participate to bursts */\n\tbfq_clear_bfqq_just_created(&bfqd->oom_bfqq);\n\n\t/*\n\t * Trigger weight initialization, according to ioprio, at the\n\t * oom_bfqq's first activation. The oom_bfqq's ioprio and ioprio\n\t * class won't be changed any more.\n\t */\n\tbfqd->oom_bfqq.entity.prio_changed = 1;\n\n\tbfqd->queue = q;\n\n\tbfqd->num_actuators = 1;\n\t/*\n\t * If the disk supports multiple actuators, copy independent\n\t * access ranges from the request queue structure.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tif (ia_ranges) {\n\t\t/*\n\t\t * Check if the disk ia_ranges size exceeds the current bfq\n\t\t * actuator limit.\n\t\t */\n\t\tif (ia_ranges->nr_ia_ranges > BFQ_MAX_ACTUATORS) {\n\t\t\tpr_crit(\"nr_ia_ranges higher than act limit: iars=%d, max=%d.\\n\",\n\t\t\t\tia_ranges->nr_ia_ranges, BFQ_MAX_ACTUATORS);\n\t\t\tpr_crit(\"Falling back to single actuator mode.\\n\");\n\t\t} else {\n\t\t\tbfqd->num_actuators = ia_ranges->nr_ia_ranges;\n\n\t\t\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\t\t\tbfqd->sector[i] = ia_ranges->ia_range[i].sector;\n\t\t\t\tbfqd->nr_sectors[i] =\n\t\t\t\t\tia_ranges->ia_range[i].nr_sectors;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Otherwise use single-actuator dev info */\n\tif (bfqd->num_actuators == 1) {\n\t\tbfqd->sector[0] = 0;\n\t\tbfqd->nr_sectors[0] = get_capacity(q->disk);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n\n\tINIT_LIST_HEAD(&bfqd->dispatch);\n\n\thrtimer_init(&bfqd->idle_slice_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL);\n\tbfqd->idle_slice_timer.function = bfq_idle_slice_timer;\n\n\tbfqd->queue_weights_tree = RB_ROOT_CACHED;\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tbfqd->num_groups_with_pending_reqs = 0;\n#endif\n\n\tINIT_LIST_HEAD(&bfqd->active_list[0]);\n\tINIT_LIST_HEAD(&bfqd->active_list[1]);\n\tINIT_LIST_HEAD(&bfqd->idle_list);\n\tINIT_HLIST_HEAD(&bfqd->burst_list);\n\n\tbfqd->hw_tag = -1;\n\tbfqd->nonrot_with_queueing = blk_queue_nonrot(bfqd->queue);\n\n\tbfqd->bfq_max_budget = bfq_default_max_budget;\n\n\tbfqd->bfq_fifo_expire[0] = bfq_fifo_expire[0];\n\tbfqd->bfq_fifo_expire[1] = bfq_fifo_expire[1];\n\tbfqd->bfq_back_max = bfq_back_max;\n\tbfqd->bfq_back_penalty = bfq_back_penalty;\n\tbfqd->bfq_slice_idle = bfq_slice_idle;\n\tbfqd->bfq_timeout = bfq_timeout;\n\n\tbfqd->bfq_large_burst_thresh = 8;\n\tbfqd->bfq_burst_interval = msecs_to_jiffies(180);\n\n\tbfqd->low_latency = true;\n\n\t/*\n\t * Trade-off between responsiveness and fairness.\n\t */\n\tbfqd->bfq_wr_coeff = 30;\n\tbfqd->bfq_wr_rt_max_time = msecs_to_jiffies(300);\n\tbfqd->bfq_wr_min_idle_time = msecs_to_jiffies(2000);\n\tbfqd->bfq_wr_min_inter_arr_async = msecs_to_jiffies(500);\n\tbfqd->bfq_wr_max_softrt_rate = 7000; /*\n\t\t\t\t\t      * Approximate rate required\n\t\t\t\t\t      * to playback or record a\n\t\t\t\t\t      * high-definition compressed\n\t\t\t\t\t      * video.\n\t\t\t\t\t      */\n\tbfqd->wr_busy_queues = 0;\n\n\t/*\n\t * Begin by assuming, optimistically, that the device peak\n\t * rate is equal to 2/3 of the highest reference rate.\n\t */\n\tbfqd->rate_dur_prod = ref_rate[blk_queue_nonrot(bfqd->queue)] *\n\t\tref_wr_duration[blk_queue_nonrot(bfqd->queue)];\n\tbfqd->peak_rate = ref_rate[blk_queue_nonrot(bfqd->queue)] * 2 / 3;\n\n\t/* see comments on the definition of next field inside bfq_data */\n\tbfqd->actuator_load_threshold = 4;\n\n\tspin_lock_init(&bfqd->lock);\n\n\t/*\n\t * The invocation of the next bfq_create_group_hierarchy\n\t * function is the head of a chain of function calls\n\t * (bfq_create_group_hierarchy->blkcg_activate_policy->\n\t * blk_mq_freeze_queue) that may lead to the invocation of the\n\t * has_work hook function. For this reason,\n\t * bfq_create_group_hierarchy is invoked only after all\n\t * scheduler data has been initialized, apart from the fields\n\t * that can be initialized only after invoking\n\t * bfq_create_group_hierarchy. This, in particular, enables\n\t * has_work to correctly return false. Of course, to avoid\n\t * other inconsistencies, the blk-mq stack must then refrain\n\t * from invoking further scheduler hooks before this init\n\t * function is finished.\n\t */\n\tbfqd->root_group = bfq_create_group_hierarchy(bfqd, q->node);\n\tif (!bfqd->root_group)\n\t\tgoto out_free;\n\tbfq_init_root_group(bfqd->root_group, bfqd);\n\tbfq_init_entity(&bfqd->oom_bfqq.entity, bfqd->root_group);\n\n\t/* We dispatch from request queue wide instead of hw queue */\n\tblk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);\n\n\tset_bit(ELEVATOR_FLAG_DISABLE_WBT, &eq->flags);\n\twbt_disable_default(q->disk);\n\tblk_stat_enable_accounting(q);\n\n\treturn 0;\n\nout_free:\n\tkfree(bfqd);\n\tkobject_put(&eq->kobj);\n\treturn -ENOMEM;\n}\n\nstatic void bfq_slab_kill(void)\n{\n\tkmem_cache_destroy(bfq_pool);\n}\n\nstatic int __init bfq_slab_setup(void)\n{\n\tbfq_pool = KMEM_CACHE(bfq_queue, 0);\n\tif (!bfq_pool)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic ssize_t bfq_var_show(unsigned int var, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", var);\n}\n\nstatic int bfq_var_store(unsigned long *var, const char *page)\n{\n\tunsigned long new_val;\n\tint ret = kstrtoul(page, 10, &new_val);\n\n\tif (ret)\n\t\treturn ret;\n\t*var = new_val;\n\treturn 0;\n}\n\n#define SHOW_FUNCTION(__FUNC, __VAR, __CONV)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t__data = jiffies_to_msecs(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t__data = div_u64(__data, NSEC_PER_MSEC);\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nSHOW_FUNCTION(bfq_fifo_expire_sync_show, bfqd->bfq_fifo_expire[1], 2);\nSHOW_FUNCTION(bfq_fifo_expire_async_show, bfqd->bfq_fifo_expire[0], 2);\nSHOW_FUNCTION(bfq_back_seek_max_show, bfqd->bfq_back_max, 0);\nSHOW_FUNCTION(bfq_back_seek_penalty_show, bfqd->bfq_back_penalty, 0);\nSHOW_FUNCTION(bfq_slice_idle_show, bfqd->bfq_slice_idle, 2);\nSHOW_FUNCTION(bfq_max_budget_show, bfqd->bfq_user_max_budget, 0);\nSHOW_FUNCTION(bfq_timeout_sync_show, bfqd->bfq_timeout, 1);\nSHOW_FUNCTION(bfq_strict_guarantees_show, bfqd->strict_guarantees, 0);\nSHOW_FUNCTION(bfq_low_latency_show, bfqd->low_latency, 0);\n#undef SHOW_FUNCTION\n\n#define USEC_SHOW_FUNCTION(__FUNC, __VAR)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\t__data = div_u64(__data, NSEC_PER_USEC);\t\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nUSEC_SHOW_FUNCTION(bfq_slice_idle_us_show, bfqd->bfq_slice_idle);\n#undef USEC_SHOW_FUNCTION\n\n#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX, __CONV)\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n__FUNC(struct elevator_queue *e, const char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t*(__PTR) = msecs_to_jiffies(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t*(__PTR) = (u64)__data * NSEC_PER_MSEC;\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t*(__PTR) = __data;\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nSTORE_FUNCTION(bfq_fifo_expire_sync_store, &bfqd->bfq_fifo_expire[1], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_fifo_expire_async_store, &bfqd->bfq_fifo_expire[0], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_back_seek_max_store, &bfqd->bfq_back_max, 0, INT_MAX, 0);\nSTORE_FUNCTION(bfq_back_seek_penalty_store, &bfqd->bfq_back_penalty, 1,\n\t\tINT_MAX, 0);\nSTORE_FUNCTION(bfq_slice_idle_store, &bfqd->bfq_slice_idle, 0, INT_MAX, 2);\n#undef STORE_FUNCTION\n\n#define USEC_STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\t*(__PTR) = (u64)__data * NSEC_PER_USEC;\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nUSEC_STORE_FUNCTION(bfq_slice_idle_us_store, &bfqd->bfq_slice_idle, 0,\n\t\t    UINT_MAX);\n#undef USEC_STORE_FUNCTION\n\nstatic ssize_t bfq_max_budget_store(struct elevator_queue *e,\n\t\t\t\t    const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\telse {\n\t\tif (__data > INT_MAX)\n\t\t\t__data = INT_MAX;\n\t\tbfqd->bfq_max_budget = __data;\n\t}\n\n\tbfqd->bfq_user_max_budget = __data;\n\n\treturn count;\n}\n\n/*\n * Leaving this name to preserve name compatibility with cfq\n * parameters, but this timeout is used for both sync and async.\n */\nstatic ssize_t bfq_timeout_sync_store(struct elevator_queue *e,\n\t\t\t\t      const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data < 1)\n\t\t__data = 1;\n\telse if (__data > INT_MAX)\n\t\t__data = INT_MAX;\n\n\tbfqd->bfq_timeout = msecs_to_jiffies(__data);\n\tif (bfqd->bfq_user_max_budget == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\n\treturn count;\n}\n\nstatic ssize_t bfq_strict_guarantees_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (!bfqd->strict_guarantees && __data == 1\n\t    && bfqd->bfq_slice_idle < 8 * NSEC_PER_MSEC)\n\t\tbfqd->bfq_slice_idle = 8 * NSEC_PER_MSEC;\n\n\tbfqd->strict_guarantees = __data;\n\n\treturn count;\n}\n\nstatic ssize_t bfq_low_latency_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (__data == 0 && bfqd->low_latency != 0)\n\t\tbfq_end_wr(bfqd);\n\tbfqd->low_latency = __data;\n\n\treturn count;\n}\n\n#define BFQ_ATTR(name) \\\n\t__ATTR(name, 0644, bfq_##name##_show, bfq_##name##_store)\n\nstatic struct elv_fs_entry bfq_attrs[] = {\n\tBFQ_ATTR(fifo_expire_sync),\n\tBFQ_ATTR(fifo_expire_async),\n\tBFQ_ATTR(back_seek_max),\n\tBFQ_ATTR(back_seek_penalty),\n\tBFQ_ATTR(slice_idle),\n\tBFQ_ATTR(slice_idle_us),\n\tBFQ_ATTR(max_budget),\n\tBFQ_ATTR(timeout_sync),\n\tBFQ_ATTR(strict_guarantees),\n\tBFQ_ATTR(low_latency),\n\t__ATTR_NULL\n};\n\nstatic struct elevator_type iosched_bfq_mq = {\n\t.ops = {\n\t\t.limit_depth\t\t= bfq_limit_depth,\n\t\t.prepare_request\t= bfq_prepare_request,\n\t\t.requeue_request        = bfq_finish_requeue_request,\n\t\t.finish_request\t\t= bfq_finish_request,\n\t\t.exit_icq\t\t= bfq_exit_icq,\n\t\t.insert_requests\t= bfq_insert_requests,\n\t\t.dispatch_request\t= bfq_dispatch_request,\n\t\t.next_request\t\t= elv_rb_latter_request,\n\t\t.former_request\t\t= elv_rb_former_request,\n\t\t.allow_merge\t\t= bfq_allow_bio_merge,\n\t\t.bio_merge\t\t= bfq_bio_merge,\n\t\t.request_merge\t\t= bfq_request_merge,\n\t\t.requests_merged\t= bfq_requests_merged,\n\t\t.request_merged\t\t= bfq_request_merged,\n\t\t.has_work\t\t= bfq_has_work,\n\t\t.depth_updated\t\t= bfq_depth_updated,\n\t\t.init_hctx\t\t= bfq_init_hctx,\n\t\t.init_sched\t\t= bfq_init_queue,\n\t\t.exit_sched\t\t= bfq_exit_queue,\n\t},\n\n\t.icq_size =\t\tsizeof(struct bfq_io_cq),\n\t.icq_align =\t\t__alignof__(struct bfq_io_cq),\n\t.elevator_attrs =\tbfq_attrs,\n\t.elevator_name =\t\"bfq\",\n\t.elevator_owner =\tTHIS_MODULE,\n};\nMODULE_ALIAS(\"bfq-iosched\");\n\nstatic int __init bfq_init(void)\n{\n\tint ret;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tret = blkcg_policy_register(&blkcg_policy_bfq);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = -ENOMEM;\n\tif (bfq_slab_setup())\n\t\tgoto err_pol_unreg;\n\n\t/*\n\t * Times to load large popular applications for the typical\n\t * systems installed on the reference devices (see the\n\t * comments before the definition of the next\n\t * array). Actually, we use slightly lower values, as the\n\t * estimated peak rate tends to be smaller than the actual\n\t * peak rate.  The reason for this last fact is that estimates\n\t * are computed over much shorter time intervals than the long\n\t * intervals typically used for benchmarking. Why? First, to\n\t * adapt more quickly to variations. Second, because an I/O\n\t * scheduler cannot rely on a peak-rate-evaluation workload to\n\t * be run for a long time.\n\t */\n\tref_wr_duration[0] = msecs_to_jiffies(7000); /* actually 8 sec */\n\tref_wr_duration[1] = msecs_to_jiffies(2500); /* actually 3 sec */\n\n\tret = elv_register(&iosched_bfq_mq);\n\tif (ret)\n\t\tgoto slab_kill;\n\n\treturn 0;\n\nslab_kill:\n\tbfq_slab_kill();\nerr_pol_unreg:\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\treturn ret;\n}\n\nstatic void __exit bfq_exit(void)\n{\n\telv_unregister(&iosched_bfq_mq);\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\tbfq_slab_kill();\n}\n\nmodule_init(bfq_init);\nmodule_exit(bfq_exit);\n\nMODULE_AUTHOR(\"Paolo Valente\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MQ Budget Fair Queueing I/O Scheduler\");\n"
                            }
                        },
                        {
                            "downstream_patch": "bc2aeb35ff167e0c6b0cedf0c96a5c41e6cba1ed",
                            "downstream_commit": "42268d885e44af875a6474f7bba519cc6cea6a9d",
                            "commit_date": "2025-01-17 13:40:59 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file block/bfq-iosched.c",
                            "downstream_patch_content": "commit bc2aeb35ff167e0c6b0cedf0c96a5c41e6cba1ed\nAuthor: Yu Kuai <yukuai3@huawei.com>\nDate:   Wed Jan 8 16:41:48 2025 +0800\n\n    block, bfq: fix waker_bfqq UAF after bfq_split_bfqq()\n    \n    [ Upstream commit fcede1f0a043ccefe9bc6ad57f12718e42f63f1d ]\n    \n    Our syzkaller report a following UAF for v6.6:\n    \n    BUG: KASAN: slab-use-after-free in bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n    Read of size 8 at addr ffff8881b57147d8 by task fsstress/232726\n    \n    CPU: 2 PID: 232726 Comm: fsstress Not tainted 6.6.0-g3629d1885222 #39\n    Call Trace:\n     <TASK>\n     __dump_stack lib/dump_stack.c:88 [inline]\n     dump_stack_lvl+0x91/0xf0 lib/dump_stack.c:106\n     print_address_description.constprop.0+0x66/0x300 mm/kasan/report.c:364\n     print_report+0x3e/0x70 mm/kasan/report.c:475\n     kasan_report+0xb8/0xf0 mm/kasan/report.c:588\n     hlist_add_head include/linux/list.h:1023 [inline]\n     bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Allocated by task 232719:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     __kasan_slab_alloc+0x87/0x90 mm/kasan/common.c:328\n     kasan_slab_alloc include/linux/kasan.h:188 [inline]\n     slab_post_alloc_hook mm/slab.h:768 [inline]\n     slab_alloc_node mm/slub.c:3492 [inline]\n     kmem_cache_alloc_node+0x1b8/0x6f0 mm/slub.c:3537\n     bfq_get_queue+0x215/0x1f00 block/bfq-iosched.c:5869\n     bfq_get_bfqq_handle_split+0x167/0x5f0 block/bfq-iosched.c:6776\n     bfq_init_rq+0x13a4/0x17a0 block/bfq-iosched.c:6938\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh_nowait+0x15a/0x240 fs/ext4/super.c:217\n     ext4_read_bh_lock+0xac/0xd0 fs/ext4/super.c:242\n     ext4_bread_batch+0x268/0x500 fs/ext4/inode.c:958\n     __ext4_find_entry+0x448/0x10f0 fs/ext4/namei.c:1671\n     ext4_lookup_entry fs/ext4/namei.c:1774 [inline]\n     ext4_lookup.part.0+0x359/0x6f0 fs/ext4/namei.c:1842\n     ext4_lookup+0x72/0x90 fs/ext4/namei.c:1839\n     __lookup_slow+0x257/0x480 fs/namei.c:1696\n     lookup_slow fs/namei.c:1713 [inline]\n     walk_component+0x454/0x5c0 fs/namei.c:2004\n     link_path_walk.part.0+0x773/0xda0 fs/namei.c:2331\n     link_path_walk fs/namei.c:3826 [inline]\n     path_openat+0x1b9/0x520 fs/namei.c:3826\n     do_filp_open+0x1b7/0x400 fs/namei.c:3857\n     do_sys_openat2+0x5dc/0x6e0 fs/open.c:1428\n     do_sys_open fs/open.c:1443 [inline]\n     __do_sys_openat fs/open.c:1459 [inline]\n     __se_sys_openat fs/open.c:1454 [inline]\n     __x64_sys_openat+0x148/0x200 fs/open.c:1454\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Freed by task 232726:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     kasan_save_free_info+0x2b/0x50 mm/kasan/generic.c:522\n     ____kasan_slab_free mm/kasan/common.c:236 [inline]\n     __kasan_slab_free+0x12a/0x1b0 mm/kasan/common.c:244\n     kasan_slab_free include/linux/kasan.h:164 [inline]\n     slab_free_hook mm/slub.c:1827 [inline]\n     slab_free_freelist_hook mm/slub.c:1853 [inline]\n     slab_free mm/slub.c:3820 [inline]\n     kmem_cache_free+0x110/0x760 mm/slub.c:3842\n     bfq_put_queue+0x6a7/0xfb0 block/bfq-iosched.c:5428\n     bfq_forget_entity block/bfq-wf2q.c:634 [inline]\n     bfq_put_idle_entity+0x142/0x240 block/bfq-wf2q.c:645\n     bfq_forget_idle+0x189/0x1e0 block/bfq-wf2q.c:671\n     bfq_update_vtime block/bfq-wf2q.c:1280 [inline]\n     __bfq_lookup_next_entity block/bfq-wf2q.c:1374 [inline]\n     bfq_lookup_next_entity+0x350/0x480 block/bfq-wf2q.c:1433\n     bfq_update_next_in_service+0x1c0/0x4f0 block/bfq-wf2q.c:128\n     bfq_deactivate_entity+0x10a/0x240 block/bfq-wf2q.c:1188\n     bfq_deactivate_bfqq block/bfq-wf2q.c:1592 [inline]\n     bfq_del_bfqq_busy+0x2e8/0xad0 block/bfq-wf2q.c:1659\n     bfq_release_process_ref+0x1cc/0x220 block/bfq-iosched.c:3139\n     bfq_split_bfqq+0x481/0xdf0 block/bfq-iosched.c:6754\n     bfq_init_rq+0xf29/0x17a0 block/bfq-iosched.c:6934\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    commit 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after\n    splitting\") fix the problem that if waker_bfqq is in the merge chain,\n    and current is the only procress, waker_bfqq can be freed from\n    bfq_split_bfqq(). However, the case that waker_bfqq is not in the merge\n    chain is missed, and if the procress reference of waker_bfqq is 0,\n    waker_bfqq can be freed as well.\n    \n    Fix the problem by checking procress reference if waker_bfqq is not in\n    the merge_chain.\n    \n    Fixes: 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after splitting\")\n    Signed-off-by: Hou Tao <houtao1@huawei.com>\n    Signed-off-by: Yu Kuai <yukuai3@huawei.com>\n    Reviewed-by: Jan Kara <jack@suse.cz>\n    Link: https://lore.kernel.org/r/20250108084148.1549973-1-yukuai1@huaweicloud.com\n    Signed-off-by: Jens Axboe <axboe@kernel.dk>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/block/bfq-iosched.c b/block/bfq-iosched.c\nindex 95dd7b795935..cad16c163611 100644\n--- a/block/bfq-iosched.c\n+++ b/block/bfq-iosched.c\n@@ -6844,16 +6844,24 @@ static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n \t\tif (new_bfqq == waker_bfqq) {\n \t\t\t/*\n \t\t\t * If waker_bfqq is in the merge chain, and current\n-\t\t\t * is the only procress.\n+\t\t\t * is the only process, waker_bfqq can be freed.\n \t\t\t */\n \t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n \t\t\t\treturn NULL;\n-\t\t\tbreak;\n+\n+\t\t\treturn waker_bfqq;\n \t\t}\n \n \t\tnew_bfqq = new_bfqq->new_bfqq;\n \t}\n \n+\t/*\n+\t * If waker_bfqq is not in the merge chain, and it's procress reference\n+\t * is 0, waker_bfqq can be freed.\n+\t */\n+\tif (bfqq_process_refs(waker_bfqq) == 0)\n+\t\treturn NULL;\n+\n \treturn waker_bfqq;\n }\n \n",
                            "downstream_file_content": {
                                "block/bfq-iosched.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Budget Fair Queueing (BFQ) I/O scheduler.\n *\n * Based on ideas and code from CFQ:\n * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>\n *\n * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it>\n *\t\t      Paolo Valente <paolo.valente@unimore.it>\n *\n * Copyright (C) 2010 Paolo Valente <paolo.valente@unimore.it>\n *                    Arianna Avanzini <avanzini@google.com>\n *\n * Copyright (C) 2017 Paolo Valente <paolo.valente@linaro.org>\n *\n * BFQ is a proportional-share I/O scheduler, with some extra\n * low-latency capabilities. BFQ also supports full hierarchical\n * scheduling through cgroups. Next paragraphs provide an introduction\n * on BFQ inner workings. Details on BFQ benefits, usage and\n * limitations can be found in Documentation/block/bfq-iosched.rst.\n *\n * BFQ is a proportional-share storage-I/O scheduling algorithm based\n * on the slice-by-slice service scheme of CFQ. But BFQ assigns\n * budgets, measured in number of sectors, to processes instead of\n * time slices. The device is not granted to the in-service process\n * for a given time slice, but until it has exhausted its assigned\n * budget. This change from the time to the service domain enables BFQ\n * to distribute the device throughput among processes as desired,\n * without any distortion due to throughput fluctuations, or to device\n * internal queueing. BFQ uses an ad hoc internal scheduler, called\n * B-WF2Q+, to schedule processes according to their budgets. More\n * precisely, BFQ schedules queues associated with processes. Each\n * process/queue is assigned a user-configurable weight, and B-WF2Q+\n * guarantees that each queue receives a fraction of the throughput\n * proportional to its weight. Thanks to the accurate policy of\n * B-WF2Q+, BFQ can afford to assign high budgets to I/O-bound\n * processes issuing sequential requests (to boost the throughput),\n * and yet guarantee a low latency to interactive and soft real-time\n * applications.\n *\n * In particular, to provide these low-latency guarantees, BFQ\n * explicitly privileges the I/O of two classes of time-sensitive\n * applications: interactive and soft real-time. In more detail, BFQ\n * behaves this way if the low_latency parameter is set (default\n * configuration). This feature enables BFQ to provide applications in\n * these classes with a very low latency.\n *\n * To implement this feature, BFQ constantly tries to detect whether\n * the I/O requests in a bfq_queue come from an interactive or a soft\n * real-time application. For brevity, in these cases, the queue is\n * said to be interactive or soft real-time. In both cases, BFQ\n * privileges the service of the queue, over that of non-interactive\n * and non-soft-real-time queues. This privileging is performed,\n * mainly, by raising the weight of the queue. So, for brevity, we\n * call just weight-raising periods the time periods during which a\n * queue is privileged, because deemed interactive or soft real-time.\n *\n * The detection of soft real-time queues/applications is described in\n * detail in the comments on the function\n * bfq_bfqq_softrt_next_start. On the other hand, the detection of an\n * interactive queue works as follows: a queue is deemed interactive\n * if it is constantly non empty only for a limited time interval,\n * after which it does become empty. The queue may be deemed\n * interactive again (for a limited time), if it restarts being\n * constantly non empty, provided that this happens only after the\n * queue has remained empty for a given minimum idle time.\n *\n * By default, BFQ computes automatically the above maximum time\n * interval, i.e., the time interval after which a constantly\n * non-empty queue stops being deemed interactive. Since a queue is\n * weight-raised while it is deemed interactive, this maximum time\n * interval happens to coincide with the (maximum) duration of the\n * weight-raising for interactive queues.\n *\n * Finally, BFQ also features additional heuristics for\n * preserving both a low latency and a high throughput on NCQ-capable,\n * rotational or flash-based devices, and to get the job done quickly\n * for applications consisting in many I/O-bound processes.\n *\n * NOTE: if the main or only goal, with a given device, is to achieve\n * the maximum-possible throughput at all times, then do switch off\n * all low-latency heuristics for that device, by setting low_latency\n * to 0.\n *\n * BFQ is described in [1], where also a reference to the initial,\n * more theoretical paper on BFQ can be found. The interested reader\n * can find in the latter paper full details on the main algorithm, as\n * well as formulas of the guarantees and formal proofs of all the\n * properties.  With respect to the version of BFQ presented in these\n * papers, this implementation adds a few more heuristics, such as the\n * ones that guarantee a low latency to interactive and soft real-time\n * applications, and a hierarchical extension based on H-WF2Q+.\n *\n * B-WF2Q+ is based on WF2Q+, which is described in [2], together with\n * H-WF2Q+, while the augmented tree used here to implement B-WF2Q+\n * with O(log N) complexity derives from the one introduced with EEVDF\n * in [3].\n *\n * [1] P. Valente, A. Avanzini, \"Evolution of the BFQ Storage I/O\n *     Scheduler\", Proceedings of the First Workshop on Mobile System\n *     Technologies (MST-2015), May 2015.\n *     http://algogroup.unimore.it/people/paolo/disk_sched/mst-2015.pdf\n *\n * [2] Jon C.R. Bennett and H. Zhang, \"Hierarchical Packet Fair Queueing\n *     Algorithms\", IEEE/ACM Transactions on Networking, 5(5):675-689,\n *     Oct 1997.\n *\n * http://www.cs.cmu.edu/~hzhang/papers/TON-97-Oct.ps.gz\n *\n * [3] I. Stoica and H. Abdel-Wahab, \"Earliest Eligible Virtual Deadline\n *     First: A Flexible and Accurate Mechanism for Proportional Share\n *     Resource Allocation\", technical report.\n *\n * http://www.cs.berkeley.edu/~istoica/papers/eevdf-tr-95.pdf\n */\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/cgroup.h>\n#include <linux/ktime.h>\n#include <linux/rbtree.h>\n#include <linux/ioprio.h>\n#include <linux/sbitmap.h>\n#include <linux/delay.h>\n#include <linux/backing-dev.h>\n\n#include <trace/events/block.h>\n\n#include \"elevator.h\"\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n#include \"bfq-iosched.h\"\n#include \"blk-wbt.h\"\n\n#define BFQ_BFQQ_FNS(name)\t\t\t\t\t\t\\\nvoid bfq_mark_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__set_bit(BFQQF_##name, &(bfqq)->flags);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nvoid bfq_clear_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__clear_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nint bfq_bfqq_##name(const struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\n\nBFQ_BFQQ_FNS(just_created);\nBFQ_BFQQ_FNS(busy);\nBFQ_BFQQ_FNS(wait_request);\nBFQ_BFQQ_FNS(non_blocking_wait_rq);\nBFQ_BFQQ_FNS(fifo_expire);\nBFQ_BFQQ_FNS(has_short_ttime);\nBFQ_BFQQ_FNS(sync);\nBFQ_BFQQ_FNS(IO_bound);\nBFQ_BFQQ_FNS(in_large_burst);\nBFQ_BFQQ_FNS(coop);\nBFQ_BFQQ_FNS(split_coop);\nBFQ_BFQQ_FNS(softrt_update);\n#undef BFQ_BFQQ_FNS\t\t\t\t\t\t\\\n\n/* Expiration time of async (0) and sync (1) requests, in ns. */\nstatic const u64 bfq_fifo_expire[2] = { NSEC_PER_SEC / 4, NSEC_PER_SEC / 8 };\n\n/* Maximum backwards seek (magic number lifted from CFQ), in KiB. */\nstatic const int bfq_back_max = 16 * 1024;\n\n/* Penalty of a backwards seek, in number of sectors. */\nstatic const int bfq_back_penalty = 2;\n\n/* Idling period duration, in ns. */\nstatic u64 bfq_slice_idle = NSEC_PER_SEC / 125;\n\n/* Minimum number of assigned budgets for which stats are safe to compute. */\nstatic const int bfq_stats_min_budgets = 194;\n\n/* Default maximum budget values, in sectors and number of requests. */\nstatic const int bfq_default_max_budget = 16 * 1024;\n\n/*\n * When a sync request is dispatched, the queue that contains that\n * request, and all the ancestor entities of that queue, are charged\n * with the number of sectors of the request. In contrast, if the\n * request is async, then the queue and its ancestor entities are\n * charged with the number of sectors of the request, multiplied by\n * the factor below. This throttles the bandwidth for async I/O,\n * w.r.t. to sync I/O, and it is done to counter the tendency of async\n * writes to steal I/O throughput to reads.\n *\n * The current value of this parameter is the result of a tuning with\n * several hardware and software configurations. We tried to find the\n * lowest value for which writes do not cause noticeable problems to\n * reads. In fact, the lower this parameter, the stabler I/O control,\n * in the following respect.  The lower this parameter is, the less\n * the bandwidth enjoyed by a group decreases\n * - when the group does writes, w.r.t. to when it does reads;\n * - when other groups do reads, w.r.t. to when they do writes.\n */\nstatic const int bfq_async_charge_factor = 3;\n\n/* Default timeout values, in jiffies, approximating CFQ defaults. */\nconst int bfq_timeout = HZ / 8;\n\n/*\n * Time limit for merging (see comments in bfq_setup_cooperator). Set\n * to the slowest value that, in our tests, proved to be effective in\n * removing false positives, while not causing true positives to miss\n * queue merging.\n *\n * As can be deduced from the low time limit below, queue merging, if\n * successful, happens at the very beginning of the I/O of the involved\n * cooperating processes, as a consequence of the arrival of the very\n * first requests from each cooperator.  After that, there is very\n * little chance to find cooperators.\n */\nstatic const unsigned long bfq_merge_time_limit = HZ/10;\n\nstatic struct kmem_cache *bfq_pool;\n\n/* Below this threshold (in ns), we consider thinktime immediate. */\n#define BFQ_MIN_TT\t\t(2 * NSEC_PER_MSEC)\n\n/* hw_tag detection: parallel requests threshold and min samples needed. */\n#define BFQ_HW_QUEUE_THRESHOLD\t3\n#define BFQ_HW_QUEUE_SAMPLES\t32\n\n#define BFQQ_SEEK_THR\t\t(sector_t)(8 * 100)\n#define BFQQ_SECT_THR_NONROT\t(sector_t)(2 * 32)\n#define BFQ_RQ_SEEKY(bfqd, last_pos, rq) \\\n\t(get_sdist(last_pos, rq) >\t\t\t\\\n\t BFQQ_SEEK_THR &&\t\t\t\t\\\n\t (!blk_queue_nonrot(bfqd->queue) ||\t\t\\\n\t  blk_rq_sectors(rq) < BFQQ_SECT_THR_NONROT))\n#define BFQQ_CLOSE_THR\t\t(sector_t)(8 * 1024)\n#define BFQQ_SEEKY(bfqq)\t(hweight32(bfqq->seek_history) > 19)\n/*\n * Sync random I/O is likely to be confused with soft real-time I/O,\n * because it is characterized by limited throughput and apparently\n * isochronous arrival pattern. To avoid false positives, queues\n * containing only random (seeky) I/O are prevented from being tagged\n * as soft real-time.\n */\n#define BFQQ_TOTALLY_SEEKY(bfqq)\t(bfqq->seek_history == -1)\n\n/* Min number of samples required to perform peak-rate update */\n#define BFQ_RATE_MIN_SAMPLES\t32\n/* Min observation time interval required to perform a peak-rate update (ns) */\n#define BFQ_RATE_MIN_INTERVAL\t(300*NSEC_PER_MSEC)\n/* Target observation time interval for a peak-rate update (ns) */\n#define BFQ_RATE_REF_INTERVAL\tNSEC_PER_SEC\n\n/*\n * Shift used for peak-rate fixed precision calculations.\n * With\n * - the current shift: 16 positions\n * - the current type used to store rate: u32\n * - the current unit of measure for rate: [sectors/usec], or, more precisely,\n *   [(sectors/usec) / 2^BFQ_RATE_SHIFT] to take into account the shift,\n * the range of rates that can be stored is\n * [1 / 2^BFQ_RATE_SHIFT, 2^(32 - BFQ_RATE_SHIFT)] sectors/usec =\n * [1 / 2^16, 2^16] sectors/usec = [15e-6, 65536] sectors/usec =\n * [15, 65G] sectors/sec\n * Which, assuming a sector size of 512B, corresponds to a range of\n * [7.5K, 33T] B/sec\n */\n#define BFQ_RATE_SHIFT\t\t16\n\n/*\n * When configured for computing the duration of the weight-raising\n * for interactive queues automatically (see the comments at the\n * beginning of this file), BFQ does it using the following formula:\n * duration = (ref_rate / r) * ref_wr_duration,\n * where r is the peak rate of the device, and ref_rate and\n * ref_wr_duration are two reference parameters.  In particular,\n * ref_rate is the peak rate of the reference storage device (see\n * below), and ref_wr_duration is about the maximum time needed, with\n * BFQ and while reading two files in parallel, to load typical large\n * applications on the reference device (see the comments on\n * max_service_from_wr below, for more details on how ref_wr_duration\n * is obtained).  In practice, the slower/faster the device at hand\n * is, the more/less it takes to load applications with respect to the\n * reference device.  Accordingly, the longer/shorter BFQ grants\n * weight raising to interactive applications.\n *\n * BFQ uses two different reference pairs (ref_rate, ref_wr_duration),\n * depending on whether the device is rotational or non-rotational.\n *\n * In the following definitions, ref_rate[0] and ref_wr_duration[0]\n * are the reference values for a rotational device, whereas\n * ref_rate[1] and ref_wr_duration[1] are the reference values for a\n * non-rotational device. The reference rates are not the actual peak\n * rates of the devices used as a reference, but slightly lower\n * values. The reason for using slightly lower values is that the\n * peak-rate estimator tends to yield slightly lower values than the\n * actual peak rate (it can yield the actual peak rate only if there\n * is only one process doing I/O, and the process does sequential\n * I/O).\n *\n * The reference peak rates are measured in sectors/usec, left-shifted\n * by BFQ_RATE_SHIFT.\n */\nstatic int ref_rate[2] = {14000, 33000};\n/*\n * To improve readability, a conversion function is used to initialize\n * the following array, which entails that the array can be\n * initialized only in a function.\n */\nstatic int ref_wr_duration[2];\n\n/*\n * BFQ uses the above-detailed, time-based weight-raising mechanism to\n * privilege interactive tasks. This mechanism is vulnerable to the\n * following false positives: I/O-bound applications that will go on\n * doing I/O for much longer than the duration of weight\n * raising. These applications have basically no benefit from being\n * weight-raised at the beginning of their I/O. On the opposite end,\n * while being weight-raised, these applications\n * a) unjustly steal throughput to applications that may actually need\n * low latency;\n * b) make BFQ uselessly perform device idling; device idling results\n * in loss of device throughput with most flash-based storage, and may\n * increase latencies when used purposelessly.\n *\n * BFQ tries to reduce these problems, by adopting the following\n * countermeasure. To introduce this countermeasure, we need first to\n * finish explaining how the duration of weight-raising for\n * interactive tasks is computed.\n *\n * For a bfq_queue deemed as interactive, the duration of weight\n * raising is dynamically adjusted, as a function of the estimated\n * peak rate of the device, so as to be equal to the time needed to\n * execute the 'largest' interactive task we benchmarked so far. By\n * largest task, we mean the task for which each involved process has\n * to do more I/O than for any of the other tasks we benchmarked. This\n * reference interactive task is the start-up of LibreOffice Writer,\n * and in this task each process/bfq_queue needs to have at most ~110K\n * sectors transferred.\n *\n * This last piece of information enables BFQ to reduce the actual\n * duration of weight-raising for at least one class of I/O-bound\n * applications: those doing sequential or quasi-sequential I/O. An\n * example is file copy. In fact, once started, the main I/O-bound\n * processes of these applications usually consume the above 110K\n * sectors in much less time than the processes of an application that\n * is starting, because these I/O-bound processes will greedily devote\n * almost all their CPU cycles only to their target,\n * throughput-friendly I/O operations. This is even more true if BFQ\n * happens to be underestimating the device peak rate, and thus\n * overestimating the duration of weight raising. But, according to\n * our measurements, once transferred 110K sectors, these processes\n * have no right to be weight-raised any longer.\n *\n * Basing on the last consideration, BFQ ends weight-raising for a\n * bfq_queue if the latter happens to have received an amount of\n * service at least equal to the following constant. The constant is\n * set to slightly more than 110K, to have a minimum safety margin.\n *\n * This early ending of weight-raising reduces the amount of time\n * during which interactive false positives cause the two problems\n * described at the beginning of these comments.\n */\nstatic const unsigned long max_service_from_wr = 120000;\n\n/*\n * Maximum time between the creation of two queues, for stable merge\n * to be activated (in ms)\n */\nstatic const unsigned long bfq_activation_stable_merging = 600;\n/*\n * Minimum time to be waited before evaluating delayed stable merge (in ms)\n */\nstatic const unsigned long bfq_late_stable_merging = 600;\n\n#define RQ_BIC(rq)\t\t((struct bfq_io_cq *)((rq)->elv.priv[0]))\n#define RQ_BFQQ(rq)\t\t((rq)->elv.priv[1])\n\nstruct bfq_queue *bic_to_bfqq(struct bfq_io_cq *bic, bool is_sync,\n\t\t\t      unsigned int actuator_idx)\n{\n\tif (is_sync)\n\t\treturn bic->bfqq[1][actuator_idx];\n\n\treturn bic->bfqq[0][actuator_idx];\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq);\n\nvoid bic_set_bfqq(struct bfq_io_cq *bic,\n\t\t  struct bfq_queue *bfqq,\n\t\t  bool is_sync,\n\t\t  unsigned int actuator_idx)\n{\n\tstruct bfq_queue *old_bfqq = bic->bfqq[is_sync][actuator_idx];\n\n\t/*\n\t * If bfqq != NULL, then a non-stable queue merge between\n\t * bic->bfqq and bfqq is happening here. This causes troubles\n\t * in the following case: bic->bfqq has also been scheduled\n\t * for a possible stable merge with bic->stable_merge_bfqq,\n\t * and bic->stable_merge_bfqq == bfqq happens to\n\t * hold. Troubles occur because bfqq may then undergo a split,\n\t * thereby becoming eligible for a stable merge. Yet, if\n\t * bic->stable_merge_bfqq points exactly to bfqq, then bfqq\n\t * would be stably merged with itself. To avoid this anomaly,\n\t * we cancel the stable merge if\n\t * bic->stable_merge_bfqq == bfqq.\n\t */\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[actuator_idx];\n\n\t/* Clear bic pointer if bfqq is detached from this bic */\n\tif (old_bfqq && old_bfqq->bic == bic)\n\t\told_bfqq->bic = NULL;\n\n\tif (is_sync)\n\t\tbic->bfqq[1][actuator_idx] = bfqq;\n\telse\n\t\tbic->bfqq[0][actuator_idx] = bfqq;\n\n\tif (bfqq && bfqq_data->stable_merge_bfqq == bfqq) {\n\t\t/*\n\t\t * Actually, these same instructions are executed also\n\t\t * in bfq_setup_cooperator, in case of abort or actual\n\t\t * execution of a stable merge. We could avoid\n\t\t * repeating these instructions there too, but if we\n\t\t * did so, we would nest even more complexity in this\n\t\t * function.\n\t\t */\n\t\tbfq_put_stable_ref(bfqq_data->stable_merge_bfqq);\n\n\t\tbfqq_data->stable_merge_bfqq = NULL;\n\t}\n}\n\nstruct bfq_data *bic_to_bfqd(struct bfq_io_cq *bic)\n{\n\treturn bic->icq.q->elevator->elevator_data;\n}\n\n/**\n * icq_to_bic - convert iocontext queue structure to bfq_io_cq.\n * @icq: the iocontext queue.\n */\nstatic struct bfq_io_cq *icq_to_bic(struct io_cq *icq)\n{\n\t/* bic->icq is the first member, %NULL will convert to %NULL */\n\treturn container_of(icq, struct bfq_io_cq, icq);\n}\n\n/**\n * bfq_bic_lookup - search into @ioc a bic associated to @bfqd.\n * @q: the request queue.\n */\nstatic struct bfq_io_cq *bfq_bic_lookup(struct request_queue *q)\n{\n\tstruct bfq_io_cq *icq;\n\tunsigned long flags;\n\n\tif (!current->io_context)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\ticq = icq_to_bic(ioc_lookup_icq(q));\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\n\treturn icq;\n}\n\n/*\n * Scheduler run of queue, if there are requests pending and no one in the\n * driver that will restart queueing.\n */\nvoid bfq_schedule_dispatch(struct bfq_data *bfqd)\n{\n\tlockdep_assert_held(&bfqd->lock);\n\n\tif (bfqd->queued != 0) {\n\t\tbfq_log(bfqd, \"schedule dispatch\");\n\t\tblk_mq_run_hw_queues(bfqd->queue, true);\n\t}\n}\n\n#define bfq_class_idle(bfqq)\t((bfqq)->ioprio_class == IOPRIO_CLASS_IDLE)\n\n#define bfq_sample_valid(samples)\t((samples) > 80)\n\n/*\n * Lifted from AS - choose which of rq1 and rq2 that is best served now.\n * We choose the request that is closer to the head right now.  Distance\n * behind the head is penalized and only allowed to a certain extent.\n */\nstatic struct request *bfq_choose_req(struct bfq_data *bfqd,\n\t\t\t\t      struct request *rq1,\n\t\t\t\t      struct request *rq2,\n\t\t\t\t      sector_t last)\n{\n\tsector_t s1, s2, d1 = 0, d2 = 0;\n\tunsigned long back_max;\n#define BFQ_RQ1_WRAP\t0x01 /* request 1 wraps */\n#define BFQ_RQ2_WRAP\t0x02 /* request 2 wraps */\n\tunsigned int wrap = 0; /* bit mask: requests behind the disk head? */\n\n\tif (!rq1 || rq1 == rq2)\n\t\treturn rq2;\n\tif (!rq2)\n\t\treturn rq1;\n\n\tif (rq_is_sync(rq1) && !rq_is_sync(rq2))\n\t\treturn rq1;\n\telse if (rq_is_sync(rq2) && !rq_is_sync(rq1))\n\t\treturn rq2;\n\tif ((rq1->cmd_flags & REQ_META) && !(rq2->cmd_flags & REQ_META))\n\t\treturn rq1;\n\telse if ((rq2->cmd_flags & REQ_META) && !(rq1->cmd_flags & REQ_META))\n\t\treturn rq2;\n\n\ts1 = blk_rq_pos(rq1);\n\ts2 = blk_rq_pos(rq2);\n\n\t/*\n\t * By definition, 1KiB is 2 sectors.\n\t */\n\tback_max = bfqd->bfq_back_max * 2;\n\n\t/*\n\t * Strict one way elevator _except_ in the case where we allow\n\t * short backward seeks which are biased as twice the cost of a\n\t * similar forward seek.\n\t */\n\tif (s1 >= last)\n\t\td1 = s1 - last;\n\telse if (s1 + back_max >= last)\n\t\td1 = (last - s1) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ1_WRAP;\n\n\tif (s2 >= last)\n\t\td2 = s2 - last;\n\telse if (s2 + back_max >= last)\n\t\td2 = (last - s2) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ2_WRAP;\n\n\t/* Found required data */\n\n\t/*\n\t * By doing switch() on the bit mask \"wrap\" we avoid having to\n\t * check two variables for all permutations: --> faster!\n\t */\n\tswitch (wrap) {\n\tcase 0: /* common case for CFQ: rq1 and rq2 not wrapped */\n\t\tif (d1 < d2)\n\t\t\treturn rq1;\n\t\telse if (d2 < d1)\n\t\t\treturn rq2;\n\n\t\tif (s1 >= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\n\tcase BFQ_RQ2_WRAP:\n\t\treturn rq1;\n\tcase BFQ_RQ1_WRAP:\n\t\treturn rq2;\n\tcase BFQ_RQ1_WRAP|BFQ_RQ2_WRAP: /* both rqs wrapped */\n\tdefault:\n\t\t/*\n\t\t * Since both rqs are wrapped,\n\t\t * start with the one that's further behind head\n\t\t * (--> only *one* back seek required),\n\t\t * since back seek takes more time than forward.\n\t\t */\n\t\tif (s1 <= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\t}\n}\n\n#define BFQ_LIMIT_INLINE_DEPTH 16\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\nstatic bool bfqq_request_over_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_io_cq *bic, blk_opf_t opf,\n\t\t\t\t    unsigned int act_idx, int limit)\n{\n\tstruct bfq_entity *inline_entities[BFQ_LIMIT_INLINE_DEPTH];\n\tstruct bfq_entity **entities = inline_entities;\n\tint alloc_depth = BFQ_LIMIT_INLINE_DEPTH;\n\tstruct bfq_sched_data *sched_data;\n\tstruct bfq_entity *entity;\n\tstruct bfq_queue *bfqq;\n\tunsigned long wsum;\n\tbool ret = false;\n\tint depth;\n\tint level;\n\nretry:\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bic_to_bfqq(bic, op_is_sync(opf), act_idx);\n\tif (!bfqq)\n\t\tgoto out;\n\n\tentity = &bfqq->entity;\n\tif (!entity->on_st_or_in_serv)\n\t\tgoto out;\n\n\t/* +1 for bfqq entity, root cgroup not included */\n\tdepth = bfqg_to_blkg(bfqq_group(bfqq))->blkcg->css.cgroup->level + 1;\n\tif (depth > alloc_depth) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tif (entities != inline_entities)\n\t\t\tkfree(entities);\n\t\tentities = kmalloc_array(depth, sizeof(*entities), GFP_NOIO);\n\t\tif (!entities)\n\t\t\treturn false;\n\t\talloc_depth = depth;\n\t\tgoto retry;\n\t}\n\n\tsched_data = entity->sched_data;\n\t/* Gather our ancestors as we need to traverse them in reverse order */\n\tlevel = 0;\n\tfor_each_entity(entity) {\n\t\t/*\n\t\t * If at some level entity is not even active, allow request\n\t\t * queueing so that BFQ knows there's work to do and activate\n\t\t * entities.\n\t\t */\n\t\tif (!entity->on_st_or_in_serv)\n\t\t\tgoto out;\n\t\t/* Uh, more parents than cgroup subsystem thinks? */\n\t\tif (WARN_ON_ONCE(level >= depth))\n\t\t\tbreak;\n\t\tentities[level++] = entity;\n\t}\n\tWARN_ON_ONCE(level != depth);\n\tfor (level--; level >= 0; level--) {\n\t\tentity = entities[level];\n\t\tif (level > 0) {\n\t\t\twsum = bfq_entity_service_tree(entity)->wsum;\n\t\t} else {\n\t\t\tint i;\n\t\t\t/*\n\t\t\t * For bfqq itself we take into account service trees\n\t\t\t * of all higher priority classes and multiply their\n\t\t\t * weights so that low prio queue from higher class\n\t\t\t * gets more requests than high prio queue from lower\n\t\t\t * class.\n\t\t\t */\n\t\t\twsum = 0;\n\t\t\tfor (i = 0; i <= bfqq->ioprio_class - 1; i++) {\n\t\t\t\twsum = wsum * IOPRIO_BE_NR +\n\t\t\t\t\tsched_data->service_tree[i].wsum;\n\t\t\t}\n\t\t}\n\t\tif (!wsum)\n\t\t\tcontinue;\n\t\tlimit = DIV_ROUND_CLOSEST(limit * entity->weight, wsum);\n\t\tif (entity->allocated >= limit) {\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t\"too many requests: allocated %d limit %d level %d\",\n\t\t\t\tentity->allocated, limit, level);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tspin_unlock_irq(&bfqd->lock);\n\tif (entities != inline_entities)\n\t\tkfree(entities);\n\treturn ret;\n}\n#else\nstatic bool bfqq_request_over_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_io_cq *bic, blk_opf_t opf,\n\t\t\t\t    unsigned int act_idx, int limit)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Async I/O can easily starve sync I/O (both sync reads and sync\n * writes), by consuming all tags. Similarly, storms of sync writes,\n * such as those that sync(2) may trigger, can starve sync reads.\n * Limit depths of async I/O and sync writes so as to counter both\n * problems.\n *\n * Also if a bfq queue or its parent cgroup consume more tags than would be\n * appropriate for their weight, we trim the available tag depth to 1. This\n * avoids a situation where one cgroup can starve another cgroup from tags and\n * thus block service differentiation among cgroups. Note that because the\n * queue / cgroup already has many requests allocated and queued, this does not\n * significantly affect service guarantees coming from the BFQ scheduling\n * algorithm.\n */\nstatic void bfq_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data)\n{\n\tstruct bfq_data *bfqd = data->q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(data->q);\n\tint depth;\n\tunsigned limit = data->q->nr_requests;\n\tunsigned int act_idx;\n\n\t/* Sync reads have full depth available */\n\tif (op_is_sync(opf) && !op_is_write(opf)) {\n\t\tdepth = 0;\n\t} else {\n\t\tdepth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(opf)];\n\t\tlimit = (limit * depth) >> bfqd->full_depth_shift;\n\t}\n\n\tfor (act_idx = 0; bic && act_idx < bfqd->num_actuators; act_idx++) {\n\t\t/* Fast path to check if bfqq is already allocated. */\n\t\tif (!bic_to_bfqq(bic, op_is_sync(opf), act_idx))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Does queue (or any parent entity) exceed number of\n\t\t * requests that should be available to it? Heavily\n\t\t * limit depth so that it cannot consume more\n\t\t * available requests and thus starve other entities.\n\t\t */\n\t\tif (bfqq_request_over_limit(bfqd, bic, opf, act_idx, limit)) {\n\t\t\tdepth = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbfq_log(bfqd, \"[%s] wr_busy %d sync %d depth %u\",\n\t\t__func__, bfqd->wr_busy_queues, op_is_sync(opf), depth);\n\tif (depth)\n\t\tdata->shallow_depth = depth;\n}\n\nstatic struct bfq_queue *\nbfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,\n\t\t     sector_t sector, struct rb_node **ret_parent,\n\t\t     struct rb_node ***rb_link)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tparent = NULL;\n\tp = &root->rb_node;\n\twhile (*p) {\n\t\tstruct rb_node **n;\n\n\t\tparent = *p;\n\t\tbfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\n\t\t/*\n\t\t * Sort strictly based on sector. Smallest to the left,\n\t\t * largest to the right.\n\t\t */\n\t\tif (sector > blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_right;\n\t\telse if (sector < blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_left;\n\t\telse\n\t\t\tbreak;\n\t\tp = n;\n\t\tbfqq = NULL;\n\t}\n\n\t*ret_parent = parent;\n\tif (rb_link)\n\t\t*rb_link = p;\n\n\tbfq_log(bfqd, \"rq_pos_tree_lookup %llu: returning %d\",\n\t\t(unsigned long long)sector,\n\t\tbfqq ? bfqq->pid : 0);\n\n\treturn bfqq;\n}\n\nstatic bool bfq_too_late_for_merging(struct bfq_queue *bfqq)\n{\n\treturn bfqq->service_from_backlogged > 0 &&\n\t\ttime_is_before_jiffies(bfqq->first_IO_time +\n\t\t\t\t       bfq_merge_time_limit);\n}\n\n/*\n * The following function is not marked as __cold because it is\n * actually cold, but for the same performance goal described in the\n * comments on the likely() at the beginning of\n * bfq_setup_cooperator(). Unexpectedly, to reach an even lower\n * execution time for the case where this function is not invoked, we\n * had to add an unlikely() in each involved if().\n */\nvoid __cold\nbfq_pos_tree_add_move(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *__bfqq;\n\n\tif (bfqq->pos_root) {\n\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\tbfqq->pos_root = NULL;\n\t}\n\n\t/* oom_bfqq does not participate in queue merging */\n\tif (bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq cannot be merged any longer (see comments in\n\t * bfq_setup_cooperator): no point in adding bfqq into the\n\t * position tree.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn;\n\n\tif (bfq_class_idle(bfqq))\n\t\treturn;\n\tif (!bfqq->next_rq)\n\t\treturn;\n\n\tbfqq->pos_root = &bfqq_group(bfqq)->rq_pos_tree;\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, bfqq->pos_root,\n\t\t\tblk_rq_pos(bfqq->next_rq), &parent, &p);\n\tif (!__bfqq) {\n\t\trb_link_node(&bfqq->pos_node, parent, p);\n\t\trb_insert_color(&bfqq->pos_node, bfqq->pos_root);\n\t} else\n\t\tbfqq->pos_root = NULL;\n}\n\n/*\n * The following function returns false either if every active queue\n * must receive the same share of the throughput (symmetric scenario),\n * or, as a special case, if bfqq must receive a share of the\n * throughput lower than or equal to the share that every other active\n * queue must receive.  If bfqq does sync I/O, then these are the only\n * two cases where bfqq happens to be guaranteed its share of the\n * throughput even if I/O dispatching is not plugged when bfqq remains\n * temporarily empty (for more details, see the comments in the\n * function bfq_better_to_idle()). For this reason, the return value\n * of this function is used to check whether I/O-dispatch plugging can\n * be avoided.\n *\n * The above first case (symmetric scenario) occurs when:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) all active groups at the same level in the groups tree have the same\n *    weight,\n * 4) all active groups at the same level in the groups tree have the same\n *    number of children.\n *\n * Unfortunately, keeping the necessary state for evaluating exactly\n * the last two symmetry sub-conditions above would be quite complex\n * and time consuming. Therefore this function evaluates, instead,\n * only the following stronger three sub-conditions, for which it is\n * much easier to maintain the needed state:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) there is at most one active group.\n * In particular, the last condition is always true if hierarchical\n * support or the cgroups interface are not enabled, thus no state\n * needs to be maintained in this case.\n */\nstatic bool bfq_asymmetric_scenario(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tbool smallest_weight = bfqq &&\n\t\tbfqq->weight_counter &&\n\t\tbfqq->weight_counter ==\n\t\tcontainer_of(\n\t\t\trb_first_cached(&bfqd->queue_weights_tree),\n\t\t\tstruct bfq_weight_counter,\n\t\t\tweights_node);\n\n\t/*\n\t * For queue weights to differ, queue_weights_tree must contain\n\t * at least two nodes.\n\t */\n\tbool varied_queue_weights = !smallest_weight &&\n\t\t!RB_EMPTY_ROOT(&bfqd->queue_weights_tree.rb_root) &&\n\t\t(bfqd->queue_weights_tree.rb_root.rb_node->rb_left ||\n\t\t bfqd->queue_weights_tree.rb_root.rb_node->rb_right);\n\n\tbool multiple_classes_busy =\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[1]) ||\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[2]) ||\n\t\t(bfqd->busy_queues[1] && bfqd->busy_queues[2]);\n\n\treturn varied_queue_weights || multiple_classes_busy\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\t       || bfqd->num_groups_with_pending_reqs > 1\n#endif\n\t\t;\n}\n\n/*\n * If the weight-counter tree passed as input contains no counter for\n * the weight of the input queue, then add that counter; otherwise just\n * increment the existing counter.\n *\n * Note that weight-counter trees contain few nodes in mostly symmetric\n * scenarios. For example, if all queues have the same weight, then the\n * weight-counter tree for the queues may contain at most one node.\n * This holds even if low_latency is on, because weight-raised queues\n * are not inserted in the tree.\n * In most scenarios, the rate at which nodes are created/destroyed\n * should be low too.\n */\nvoid bfq_weights_tree_add(struct bfq_queue *bfqq)\n{\n\tstruct rb_root_cached *root = &bfqq->bfqd->queue_weights_tree;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tbool leftmost = true;\n\n\t/*\n\t * Do not insert if the queue is already associated with a\n\t * counter, which happens if:\n\t *   1) a request arrival has caused the queue to become both\n\t *      non-weight-raised, and hence change its weight, and\n\t *      backlogged; in this respect, each of the two events\n\t *      causes an invocation of this function,\n\t *   2) this is the invocation of this function caused by the\n\t *      second event. This second invocation is actually useless,\n\t *      and we handle this fact by exiting immediately. More\n\t *      efficient or clearer solutions might possibly be adopted.\n\t */\n\tif (bfqq->weight_counter)\n\t\treturn;\n\n\twhile (*new) {\n\t\tstruct bfq_weight_counter *__counter = container_of(*new,\n\t\t\t\t\t\tstruct bfq_weight_counter,\n\t\t\t\t\t\tweights_node);\n\t\tparent = *new;\n\n\t\tif (entity->weight == __counter->weight) {\n\t\t\tbfqq->weight_counter = __counter;\n\t\t\tgoto inc_counter;\n\t\t}\n\t\tif (entity->weight < __counter->weight)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\tbfqq->weight_counter = kzalloc(sizeof(struct bfq_weight_counter),\n\t\t\t\t       GFP_ATOMIC);\n\n\t/*\n\t * In the unlucky event of an allocation failure, we just\n\t * exit. This will cause the weight of queue to not be\n\t * considered in bfq_asymmetric_scenario, which, in its turn,\n\t * causes the scenario to be deemed wrongly symmetric in case\n\t * bfqq's weight would have been the only weight making the\n\t * scenario asymmetric.  On the bright side, no unbalance will\n\t * however occur when bfqq becomes inactive again (the\n\t * invocation of this function is triggered by an activation\n\t * of queue).  In fact, bfq_weights_tree_remove does nothing\n\t * if !bfqq->weight_counter.\n\t */\n\tif (unlikely(!bfqq->weight_counter))\n\t\treturn;\n\n\tbfqq->weight_counter->weight = entity->weight;\n\trb_link_node(&bfqq->weight_counter->weights_node, parent, new);\n\trb_insert_color_cached(&bfqq->weight_counter->weights_node, root,\n\t\t\t\tleftmost);\n\ninc_counter:\n\tbfqq->weight_counter->num_active++;\n\tbfqq->ref++;\n}\n\n/*\n * Decrement the weight counter associated with the queue, and, if the\n * counter reaches 0, remove the counter from the tree.\n * See the comments to the function bfq_weights_tree_add() for considerations\n * about overhead.\n */\nvoid bfq_weights_tree_remove(struct bfq_queue *bfqq)\n{\n\tstruct rb_root_cached *root;\n\n\tif (!bfqq->weight_counter)\n\t\treturn;\n\n\troot = &bfqq->bfqd->queue_weights_tree;\n\tbfqq->weight_counter->num_active--;\n\tif (bfqq->weight_counter->num_active > 0)\n\t\tgoto reset_entity_pointer;\n\n\trb_erase_cached(&bfqq->weight_counter->weights_node, root);\n\tkfree(bfqq->weight_counter);\n\nreset_entity_pointer:\n\tbfqq->weight_counter = NULL;\n\tbfq_put_queue(bfqq);\n}\n\n/*\n * Return expired entry, or NULL to just start from scratch in rbtree.\n */\nstatic struct request *bfq_check_fifo(struct bfq_queue *bfqq,\n\t\t\t\t      struct request *last)\n{\n\tstruct request *rq;\n\n\tif (bfq_bfqq_fifo_expire(bfqq))\n\t\treturn NULL;\n\n\tbfq_mark_bfqq_fifo_expire(bfqq);\n\n\trq = rq_entry_fifo(bfqq->fifo.next);\n\n\tif (rq == last || blk_time_get_ns() < rq->fifo_time)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"check_fifo: returned %p\", rq);\n\treturn rq;\n}\n\nstatic struct request *bfq_find_next_rq(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\tstruct request *last)\n{\n\tstruct rb_node *rbnext = rb_next(&last->rb_node);\n\tstruct rb_node *rbprev = rb_prev(&last->rb_node);\n\tstruct request *next, *prev = NULL;\n\n\t/* Follow expired path, else get first next available. */\n\tnext = bfq_check_fifo(bfqq, last);\n\tif (next)\n\t\treturn next;\n\n\tif (rbprev)\n\t\tprev = rb_entry_rq(rbprev);\n\n\tif (rbnext)\n\t\tnext = rb_entry_rq(rbnext);\n\telse {\n\t\trbnext = rb_first(&bfqq->sort_list);\n\t\tif (rbnext && rbnext != &last->rb_node)\n\t\t\tnext = rb_entry_rq(rbnext);\n\t}\n\n\treturn bfq_choose_req(bfqd, next, prev, blk_rq_pos(last));\n}\n\n/* see the definition of bfq_async_charge_factor for details */\nstatic unsigned long bfq_serv_to_charge(struct request *rq,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\tif (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||\n\t    bfq_asymmetric_scenario(bfqq->bfqd, bfqq))\n\t\treturn blk_rq_sectors(rq);\n\n\treturn blk_rq_sectors(rq) * bfq_async_charge_factor;\n}\n\n/**\n * bfq_updated_next_req - update the queue after a new next_rq selection.\n * @bfqd: the device data the queue belongs to.\n * @bfqq: the queue to update.\n *\n * If the first request of a queue changes we make sure that the queue\n * has enough budget to serve at least its first request (if the\n * request has grown).  We do this because if the queue has not enough\n * budget for its first request, it has to go through two dispatch\n * rounds to actually get it dispatched.\n */\nstatic void bfq_updated_next_req(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct request *next_rq = bfqq->next_rq;\n\tunsigned long new_budget;\n\n\tif (!next_rq)\n\t\treturn;\n\n\tif (bfqq == bfqd->in_service_queue)\n\t\t/*\n\t\t * In order not to break guarantees, budgets cannot be\n\t\t * changed after an entity has been selected.\n\t\t */\n\t\treturn;\n\n\tnew_budget = max_t(unsigned long,\n\t\t\t   max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t bfq_serv_to_charge(next_rq, bfqq)),\n\t\t\t   entity->service);\n\tif (entity->budget != new_budget) {\n\t\tentity->budget = new_budget;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"updated next rq: new budget %lu\",\n\t\t\t\t\t new_budget);\n\t\tbfq_requeue_bfqq(bfqd, bfqq, false);\n\t}\n}\n\nstatic unsigned int bfq_wr_duration(struct bfq_data *bfqd)\n{\n\tu64 dur;\n\n\tdur = bfqd->rate_dur_prod;\n\tdo_div(dur, bfqd->peak_rate);\n\n\t/*\n\t * Limit duration between 3 and 25 seconds. The upper limit\n\t * has been conservatively set after the following worst case:\n\t * on a QEMU/KVM virtual machine\n\t * - running in a slow PC\n\t * - with a virtual disk stacked on a slow low-end 5400rpm HDD\n\t * - serving a heavy I/O workload, such as the sequential reading\n\t *   of several files\n\t * mplayer took 23 seconds to start, if constantly weight-raised.\n\t *\n\t * As for higher values than that accommodating the above bad\n\t * scenario, tests show that higher values would often yield\n\t * the opposite of the desired result, i.e., would worsen\n\t * responsiveness by allowing non-interactive applications to\n\t * preserve weight raising for too long.\n\t *\n\t * On the other end, lower values than 3 seconds make it\n\t * difficult for most interactive tasks to complete their jobs\n\t * before weight-raising finishes.\n\t */\n\treturn clamp_val(dur, msecs_to_jiffies(3000), msecs_to_jiffies(25000));\n}\n\n/* switch back from soft real-time to interactive weight raising */\nstatic void switch_back_to_interactive_wr(struct bfq_queue *bfqq,\n\t\t\t\t\t  struct bfq_data *bfqd)\n{\n\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\tbfqq->last_wr_start_finish = bfqq->wr_start_at_switch_to_srt;\n}\n\nstatic void\nbfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,\n\t\t      struct bfq_io_cq *bic, bool bfq_already_existing)\n{\n\tunsigned int old_wr_coeff = 1;\n\tbool busy = bfq_already_existing && bfq_bfqq_busy(bfqq);\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\tif (bfqq_data->saved_has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\tif (bfqq_data->saved_IO_bound)\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\tbfqq->last_serv_time_ns = bfqq_data->saved_last_serv_time_ns;\n\tbfqq->inject_limit = bfqq_data->saved_inject_limit;\n\tbfqq->decrease_time_jif = bfqq_data->saved_decrease_time_jif;\n\n\tbfqq->entity.new_weight = bfqq_data->saved_weight;\n\tbfqq->ttime = bfqq_data->saved_ttime;\n\tbfqq->io_start_time = bfqq_data->saved_io_start_time;\n\tbfqq->tot_idle_time = bfqq_data->saved_tot_idle_time;\n\t/*\n\t * Restore weight coefficient only if low_latency is on\n\t */\n\tif (bfqd->low_latency) {\n\t\told_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq->wr_coeff = bfqq_data->saved_wr_coeff;\n\t}\n\tbfqq->service_from_wr = bfqq_data->saved_service_from_wr;\n\tbfqq->wr_start_at_switch_to_srt =\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt;\n\tbfqq->last_wr_start_finish = bfqq_data->saved_last_wr_start_finish;\n\tbfqq->wr_cur_max_time = bfqq_data->saved_wr_cur_max_time;\n\n\tif (bfqq->wr_coeff > 1 && (bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t   bfqq->wr_cur_max_time))) {\n\t\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t    time_is_after_eq_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t     bfq_wr_duration(bfqd))) {\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t} else {\n\t\t\tbfqq->wr_coeff = 1;\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t     \"resume state: switching off wr\");\n\t\t}\n\t}\n\n\t/* make sure weight will be updated, however we got here */\n\tbfqq->entity.prio_changed = 1;\n\n\tif (likely(!busy))\n\t\treturn;\n\n\tif (old_wr_coeff == 1 && bfqq->wr_coeff > 1)\n\t\tbfqd->wr_busy_queues++;\n\telse if (old_wr_coeff > 1 && bfqq->wr_coeff == 1)\n\t\tbfqd->wr_busy_queues--;\n}\n\nstatic int bfqq_process_refs(struct bfq_queue *bfqq)\n{\n\treturn bfqq->ref - bfqq->entity.allocated -\n\t\tbfqq->entity.on_st_or_in_serv -\n\t\t(bfqq->weight_counter != NULL) - bfqq->stable_ref;\n}\n\n/* Empty burst list and add just bfqq (see comments on bfq_handle_burst) */\nstatic void bfq_reset_burst_list(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(item, n, &bfqd->burst_list, burst_list_node)\n\t\thlist_del_init(&item->burst_list_node);\n\n\t/*\n\t * Start the creation of a new burst list only if there is no\n\t * active queue. See comments on the conditional invocation of\n\t * bfq_handle_burst().\n\t */\n\tif (bfq_tot_busy_queues(bfqd) == 0) {\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n\t\tbfqd->burst_size = 1;\n\t} else\n\t\tbfqd->burst_size = 0;\n\n\tbfqd->burst_parent_entity = bfqq->entity.parent;\n}\n\n/* Add bfqq to the list of queues in current burst (see bfq_handle_burst) */\nstatic void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/* Increment burst size to take into account also bfqq */\n\tbfqd->burst_size++;\n\n\tif (bfqd->burst_size == bfqd->bfq_large_burst_thresh) {\n\t\tstruct bfq_queue *pos, *bfqq_item;\n\t\tstruct hlist_node *n;\n\n\t\t/*\n\t\t * Enough queues have been activated shortly after each\n\t\t * other to consider this burst as large.\n\t\t */\n\t\tbfqd->large_burst = true;\n\n\t\t/*\n\t\t * We can now mark all queues in the burst list as\n\t\t * belonging to a large burst.\n\t\t */\n\t\thlist_for_each_entry(bfqq_item, &bfqd->burst_list,\n\t\t\t\t     burst_list_node)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq_item);\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\n\t\t/*\n\t\t * From now on, and until the current burst finishes, any\n\t\t * new queue being activated shortly after the last queue\n\t\t * was inserted in the burst can be immediately marked as\n\t\t * belonging to a large burst. So the burst list is not\n\t\t * needed any more. Remove it.\n\t\t */\n\t\thlist_for_each_entry_safe(pos, n, &bfqd->burst_list,\n\t\t\t\t\t  burst_list_node)\n\t\t\thlist_del_init(&pos->burst_list_node);\n\t} else /*\n\t\t* Burst not yet large: add bfqq to the burst list. Do\n\t\t* not increment the ref counter for bfqq, because bfqq\n\t\t* is removed from the burst list before freeing bfqq\n\t\t* in put_queue.\n\t\t*/\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n}\n\n/*\n * If many queues belonging to the same group happen to be created\n * shortly after each other, then the processes associated with these\n * queues have typically a common goal. In particular, bursts of queue\n * creations are usually caused by services or applications that spawn\n * many parallel threads/processes. Examples are systemd during boot,\n * or git grep. To help these processes get their job done as soon as\n * possible, it is usually better to not grant either weight-raising\n * or device idling to their queues, unless these queues must be\n * protected from the I/O flowing through other active queues.\n *\n * In this comment we describe, firstly, the reasons why this fact\n * holds, and, secondly, the next function, which implements the main\n * steps needed to properly mark these queues so that they can then be\n * treated in a different way.\n *\n * The above services or applications benefit mostly from a high\n * throughput: the quicker the requests of the activated queues are\n * cumulatively served, the sooner the target job of these queues gets\n * completed. As a consequence, weight-raising any of these queues,\n * which also implies idling the device for it, is almost always\n * counterproductive, unless there are other active queues to isolate\n * these new queues from. If there no other active queues, then\n * weight-raising these new queues just lowers throughput in most\n * cases.\n *\n * On the other hand, a burst of queue creations may be caused also by\n * the start of an application that does not consist of a lot of\n * parallel I/O-bound threads. In fact, with a complex application,\n * several short processes may need to be executed to start-up the\n * application. In this respect, to start an application as quickly as\n * possible, the best thing to do is in any case to privilege the I/O\n * related to the application with respect to all other\n * I/O. Therefore, the best strategy to start as quickly as possible\n * an application that causes a burst of queue creations is to\n * weight-raise all the queues created during the burst. This is the\n * exact opposite of the best strategy for the other type of bursts.\n *\n * In the end, to take the best action for each of the two cases, the\n * two types of bursts need to be distinguished. Fortunately, this\n * seems relatively easy, by looking at the sizes of the bursts. In\n * particular, we found a threshold such that only bursts with a\n * larger size than that threshold are apparently caused by\n * services or commands such as systemd or git grep. For brevity,\n * hereafter we call just 'large' these bursts. BFQ *does not*\n * weight-raise queues whose creation occurs in a large burst. In\n * addition, for each of these queues BFQ performs or does not perform\n * idling depending on which choice boosts the throughput more. The\n * exact choice depends on the device and request pattern at\n * hand.\n *\n * Unfortunately, false positives may occur while an interactive task\n * is starting (e.g., an application is being started). The\n * consequence is that the queues associated with the task do not\n * enjoy weight raising as expected. Fortunately these false positives\n * are very rare. They typically occur if some service happens to\n * start doing I/O exactly when the interactive task starts.\n *\n * Turning back to the next function, it is invoked only if there are\n * no active queues (apart from active queues that would belong to the\n * same, possible burst bfqq would belong to), and it implements all\n * the steps needed to detect the occurrence of a large burst and to\n * properly mark all the queues belonging to it (so that they can then\n * be treated in a different way). This goal is achieved by\n * maintaining a \"burst list\" that holds, temporarily, the queues that\n * belong to the burst in progress. The list is then used to mark\n * these queues as belonging to a large burst if the burst does become\n * large. The main steps are the following.\n *\n * . when the very first queue is created, the queue is inserted into the\n *   list (as it could be the first queue in a possible burst)\n *\n * . if the current burst has not yet become large, and a queue Q that does\n *   not yet belong to the burst is activated shortly after the last time\n *   at which a new queue entered the burst list, then the function appends\n *   Q to the burst list\n *\n * . if, as a consequence of the previous step, the burst size reaches\n *   the large-burst threshold, then\n *\n *     . all the queues in the burst list are marked as belonging to a\n *       large burst\n *\n *     . the burst list is deleted; in fact, the burst list already served\n *       its purpose (keeping temporarily track of the queues in a burst,\n *       so as to be able to mark them as belonging to a large burst in the\n *       previous sub-step), and now is not needed any more\n *\n *     . the device enters a large-burst mode\n *\n * . if a queue Q that does not belong to the burst is created while\n *   the device is in large-burst mode and shortly after the last time\n *   at which a queue either entered the burst list or was marked as\n *   belonging to the current large burst, then Q is immediately marked\n *   as belonging to a large burst.\n *\n * . if a queue Q that does not belong to the burst is created a while\n *   later, i.e., not shortly after, than the last time at which a queue\n *   either entered the burst list or was marked as belonging to the\n *   current large burst, then the current burst is deemed as finished and:\n *\n *        . the large-burst mode is reset if set\n *\n *        . the burst list is emptied\n *\n *        . Q is inserted in the burst list, as Q may be the first queue\n *          in a possible new burst (then the burst list contains just Q\n *          after this step).\n */\nstatic void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq is already in the burst list or is part of a large\n\t * burst, or finally has just been split, then there is\n\t * nothing else to do.\n\t */\n\tif (!hlist_unhashed(&bfqq->burst_list_node) ||\n\t    bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     msecs_to_jiffies(10)))\n\t\treturn;\n\n\t/*\n\t * If bfqq's creation happens late enough, or bfqq belongs to\n\t * a different group than the burst group, then the current\n\t * burst is finished, and related data structures must be\n\t * reset.\n\t *\n\t * In this respect, consider the special case where bfqq is\n\t * the very first queue created after BFQ is selected for this\n\t * device. In this case, last_ins_in_burst and\n\t * burst_parent_entity are not yet significant when we get\n\t * here. But it is easy to verify that, whether or not the\n\t * following condition is true, bfqq will end up being\n\t * inserted into the burst list. In particular the list will\n\t * happen to contain only bfqq. And this is exactly what has\n\t * to happen, as bfqq may be the first queue of the first\n\t * burst.\n\t */\n\tif (time_is_before_jiffies(bfqd->last_ins_in_burst +\n\t    bfqd->bfq_burst_interval) ||\n\t    bfqq->entity.parent != bfqd->burst_parent_entity) {\n\t\tbfqd->large_burst = false;\n\t\tbfq_reset_burst_list(bfqd, bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then bfqq is being activated shortly after the\n\t * last queue. So, if the current burst is also large, we can mark\n\t * bfqq as belonging to this large burst immediately.\n\t */\n\tif (bfqd->large_burst) {\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then a large-burst state has not yet been\n\t * reached, but bfqq is being activated shortly after the last\n\t * queue. Then we add bfqq to the burst.\n\t */\n\tbfq_add_to_burst(bfqd, bfqq);\nend:\n\t/*\n\t * At this point, bfqq either has been added to the current\n\t * burst or has caused the current burst to terminate and a\n\t * possible new burst to start. In particular, in the second\n\t * case, bfqq has become the first queue in the possible new\n\t * burst.  In both cases last_ins_in_burst needs to be moved\n\t * forward.\n\t */\n\tbfqd->last_ins_in_burst = jiffies;\n}\n\nstatic int bfq_bfqq_budget_left(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\treturn entity->budget - entity->service;\n}\n\n/*\n * If enough samples have been computed, return the current max budget\n * stored in bfqd, which is dynamically updated according to the\n * estimated disk peak rate; otherwise return the default max budget\n */\nstatic int bfq_max_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget;\n\telse\n\t\treturn bfqd->bfq_max_budget;\n}\n\n/*\n * Return min budget, which is a fraction of the current or default\n * max budget (trying with 1/32)\n */\nstatic int bfq_min_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget / 32;\n\telse\n\t\treturn bfqd->bfq_max_budget / 32;\n}\n\n/*\n * The next function, invoked after the input queue bfqq switches from\n * idle to busy, updates the budget of bfqq. The function also tells\n * whether the in-service queue should be expired, by returning\n * true. The purpose of expiring the in-service queue is to give bfqq\n * the chance to possibly preempt the in-service queue, and the reason\n * for preempting the in-service queue is to achieve one of the two\n * goals below.\n *\n * 1. Guarantee to bfqq its reserved bandwidth even if bfqq has\n * expired because it has remained idle. In particular, bfqq may have\n * expired for one of the following two reasons:\n *\n * - BFQQE_NO_MORE_REQUESTS bfqq did not enjoy any device idling\n *   and did not make it to issue a new request before its last\n *   request was served;\n *\n * - BFQQE_TOO_IDLE bfqq did enjoy device idling, but did not issue\n *   a new request before the expiration of the idling-time.\n *\n * Even if bfqq has expired for one of the above reasons, the process\n * associated with the queue may be however issuing requests greedily,\n * and thus be sensitive to the bandwidth it receives (bfqq may have\n * remained idle for other reasons: CPU high load, bfqq not enjoying\n * idling, I/O throttling somewhere in the path from the process to\n * the I/O scheduler, ...). But if, after every expiration for one of\n * the above two reasons, bfqq has to wait for the service of at least\n * one full budget of another queue before being served again, then\n * bfqq is likely to get a much lower bandwidth or resource time than\n * its reserved ones. To address this issue, two countermeasures need\n * to be taken.\n *\n * First, the budget and the timestamps of bfqq need to be updated in\n * a special way on bfqq reactivation: they need to be updated as if\n * bfqq did not remain idle and did not expire. In fact, if they are\n * computed as if bfqq expired and remained idle until reactivation,\n * then the process associated with bfqq is treated as if, instead of\n * being greedy, it stopped issuing requests when bfqq remained idle,\n * and restarts issuing requests only on this reactivation. In other\n * words, the scheduler does not help the process recover the \"service\n * hole\" between bfqq expiration and reactivation. As a consequence,\n * the process receives a lower bandwidth than its reserved one. In\n * contrast, to recover this hole, the budget must be updated as if\n * bfqq was not expired at all before this reactivation, i.e., it must\n * be set to the value of the remaining budget when bfqq was\n * expired. Along the same line, timestamps need to be assigned the\n * value they had the last time bfqq was selected for service, i.e.,\n * before last expiration. Thus timestamps need to be back-shifted\n * with respect to their normal computation (see [1] for more details\n * on this tricky aspect).\n *\n * Secondly, to allow the process to recover the hole, the in-service\n * queue must be expired too, to give bfqq the chance to preempt it\n * immediately. In fact, if bfqq has to wait for a full budget of the\n * in-service queue to be completed, then it may become impossible to\n * let the process recover the hole, even if the back-shifted\n * timestamps of bfqq are lower than those of the in-service queue. If\n * this happens for most or all of the holes, then the process may not\n * receive its reserved bandwidth. In this respect, it is worth noting\n * that, being the service of outstanding requests unpreemptible, a\n * little fraction of the holes may however be unrecoverable, thereby\n * causing a little loss of bandwidth.\n *\n * The last important point is detecting whether bfqq does need this\n * bandwidth recovery. In this respect, the next function deems the\n * process associated with bfqq greedy, and thus allows it to recover\n * the hole, if: 1) the process is waiting for the arrival of a new\n * request (which implies that bfqq expired for one of the above two\n * reasons), and 2) such a request has arrived soon. The first\n * condition is controlled through the flag non_blocking_wait_rq,\n * while the second through the flag arrived_in_time. If both\n * conditions hold, then the function computes the budget in the\n * above-described special way, and signals that the in-service queue\n * should be expired. Timestamp back-shifting is done later in\n * __bfq_activate_entity.\n *\n * 2. Reduce latency. Even if timestamps are not backshifted to let\n * the process associated with bfqq recover a service hole, bfqq may\n * however happen to have, after being (re)activated, a lower finish\n * timestamp than the in-service queue.\t That is, the next budget of\n * bfqq may have to be completed before the one of the in-service\n * queue. If this is the case, then preempting the in-service queue\n * allows this goal to be achieved, apart from the unpreemptible,\n * outstanding requests mentioned above.\n *\n * Unfortunately, regardless of which of the above two goals one wants\n * to achieve, service trees need first to be updated to know whether\n * the in-service queue must be preempted. To have service trees\n * correctly updated, the in-service queue must be expired and\n * rescheduled, and bfqq must be scheduled too. This is one of the\n * most costly operations (in future versions, the scheduling\n * mechanism may be re-designed in such a way to make it possible to\n * know whether preemption is needed without needing to update service\n * trees). In addition, queue preemptions almost always cause random\n * I/O, which may in turn cause loss of throughput. Finally, there may\n * even be no in-service queue when the next function is invoked (so,\n * no queue to compare timestamps with). Because of these facts, the\n * next function adopts the following simple scheme to avoid costly\n * operations, too frequent preemptions and too many dependencies on\n * the state of the scheduler: it requests the expiration of the\n * in-service queue (unconditionally) only for queues that need to\n * recover a hole. Then it delegates to other parts of the code the\n * responsibility of handling the above case 2.\n */\nstatic bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\t\tbool arrived_in_time)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * In the next compound condition, we check also whether there\n\t * is some budget left, because otherwise there is no point in\n\t * trying to go on serving bfqq with this same budget: bfqq\n\t * would be expired immediately after being selected for\n\t * service. This would only cause useless overhead.\n\t */\n\tif (bfq_bfqq_non_blocking_wait_rq(bfqq) && arrived_in_time &&\n\t    bfq_bfqq_budget_left(bfqq) > 0) {\n\t\t/*\n\t\t * We do not clear the flag non_blocking_wait_rq here, as\n\t\t * the latter is used in bfq_activate_bfqq to signal\n\t\t * that timestamps need to be back-shifted (and is\n\t\t * cleared right after).\n\t\t */\n\n\t\t/*\n\t\t * In next assignment we rely on that either\n\t\t * entity->service or entity->budget are not updated\n\t\t * on expiration if bfqq is empty (see\n\t\t * __bfq_bfqq_recalc_budget). Thus both quantities\n\t\t * remain unchanged after such an expiration, and the\n\t\t * following statement therefore assigns to\n\t\t * entity->budget the remaining budget on such an\n\t\t * expiration.\n\t\t */\n\t\tentity->budget = min_t(unsigned long,\n\t\t\t\t       bfq_bfqq_budget_left(bfqq),\n\t\t\t\t       bfqq->max_budget);\n\n\t\t/*\n\t\t * At this point, we have used entity->service to get\n\t\t * the budget left (needed for updating\n\t\t * entity->budget). Thus we finally can, and have to,\n\t\t * reset entity->service. The latter must be reset\n\t\t * because bfqq would otherwise be charged again for\n\t\t * the service it has received during its previous\n\t\t * service slot(s).\n\t\t */\n\t\tentity->service = 0;\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * We can finally complete expiration, by setting service to 0.\n\t */\n\tentity->service = 0;\n\tentity->budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t       bfq_serv_to_charge(bfqq->next_rq, bfqq));\n\tbfq_clear_bfqq_non_blocking_wait_rq(bfqq);\n\treturn false;\n}\n\n/*\n * Return the farthest past time instant according to jiffies\n * macros.\n */\nstatic unsigned long bfq_smallest_from_now(void)\n{\n\treturn jiffies - MAX_JIFFY_OFFSET;\n}\n\nstatic void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     unsigned int old_wr_coeff,\n\t\t\t\t\t     bool wr_or_deserves_wr,\n\t\t\t\t\t     bool interactive,\n\t\t\t\t\t     bool in_burst,\n\t\t\t\t\t     bool soft_rt)\n{\n\tif (old_wr_coeff == 1 && wr_or_deserves_wr) {\n\t\t/* start a weight-raising period */\n\t\tif (interactive) {\n\t\t\tbfqq->service_from_wr = 0;\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else {\n\t\t\t/*\n\t\t\t * No interactive weight raising in progress\n\t\t\t * here: assign minus infinity to\n\t\t\t * wr_start_at_switch_to_srt, to make sure\n\t\t\t * that, at the end of the soft-real-time\n\t\t\t * weight raising periods that is starting\n\t\t\t * now, no interactive weight-raising period\n\t\t\t * may be wrongly considered as still in\n\t\t\t * progress (and thus actually started by\n\t\t\t * mistake).\n\t\t\t */\n\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\tbfq_smallest_from_now();\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t}\n\n\t\t/*\n\t\t * If needed, further reduce budget to make sure it is\n\t\t * close to bfqq's backlog, so as to reduce the\n\t\t * scheduling-error component due to a too large\n\t\t * budget. Do not care about throughput consequences,\n\t\t * but only about latency. Finally, do not assign a\n\t\t * too small budget either, to avoid increasing\n\t\t * latency by causing too frequent expirations.\n\t\t */\n\t\tbfqq->entity.budget = min_t(unsigned long,\n\t\t\t\t\t    bfqq->entity.budget,\n\t\t\t\t\t    2 * bfq_min_budget(bfqd));\n\t} else if (old_wr_coeff > 1) {\n\t\tif (interactive) { /* update wr coeff and duration */\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else if (in_burst)\n\t\t\tbfqq->wr_coeff = 1;\n\t\telse if (soft_rt) {\n\t\t\t/*\n\t\t\t * The application is now or still meeting the\n\t\t\t * requirements for being deemed soft rt.  We\n\t\t\t * can then correctly and safely (re)charge\n\t\t\t * the weight-raising duration for the\n\t\t\t * application with the weight-raising\n\t\t\t * duration for soft rt applications.\n\t\t\t *\n\t\t\t * In particular, doing this recharge now, i.e.,\n\t\t\t * before the weight-raising period for the\n\t\t\t * application finishes, reduces the probability\n\t\t\t * of the following negative scenario:\n\t\t\t * 1) the weight of a soft rt application is\n\t\t\t *    raised at startup (as for any newly\n\t\t\t *    created application),\n\t\t\t * 2) since the application is not interactive,\n\t\t\t *    at a certain time weight-raising is\n\t\t\t *    stopped for the application,\n\t\t\t * 3) at that time the application happens to\n\t\t\t *    still have pending requests, and hence\n\t\t\t *    is destined to not have a chance to be\n\t\t\t *    deemed soft rt before these requests are\n\t\t\t *    completed (see the comments to the\n\t\t\t *    function bfq_bfqq_softrt_next_start()\n\t\t\t *    for details on soft rt detection),\n\t\t\t * 4) these pending requests experience a high\n\t\t\t *    latency because the application is not\n\t\t\t *    weight-raised while they are pending.\n\t\t\t */\n\t\t\tif (bfqq->wr_cur_max_time !=\n\t\t\t\tbfqd->bfq_wr_rt_max_time) {\n\t\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\t\tbfqq->last_wr_start_finish;\n\n\t\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\t}\n\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\t}\n}\n\nstatic bool bfq_bfqq_idle_for_long_time(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn bfqq->dispatched == 0 &&\n\t\ttime_is_before_jiffies(\n\t\t\tbfqq->budget_timeout +\n\t\t\tbfqd->bfq_wr_min_idle_time);\n}\n\n\n/*\n * Return true if bfqq is in a higher priority class, or has a higher\n * weight than the in-service queue.\n */\nstatic bool bfq_bfqq_higher_class_or_weight(struct bfq_queue *bfqq,\n\t\t\t\t\t    struct bfq_queue *in_serv_bfqq)\n{\n\tint bfqq_weight, in_serv_weight;\n\n\tif (bfqq->ioprio_class < in_serv_bfqq->ioprio_class)\n\t\treturn true;\n\n\tif (in_serv_bfqq->entity.parent == bfqq->entity.parent) {\n\t\tbfqq_weight = bfqq->entity.weight;\n\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t} else {\n\t\tif (bfqq->entity.parent)\n\t\t\tbfqq_weight = bfqq->entity.parent->weight;\n\t\telse\n\t\t\tbfqq_weight = bfqq->entity.weight;\n\t\tif (in_serv_bfqq->entity.parent)\n\t\t\tin_serv_weight = in_serv_bfqq->entity.parent->weight;\n\t\telse\n\t\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t}\n\n\treturn bfqq_weight > in_serv_weight;\n}\n\n/*\n * Get the index of the actuator that will serve bio.\n */\nstatic unsigned int bfq_actuator_index(struct bfq_data *bfqd, struct bio *bio)\n{\n\tunsigned int i;\n\tsector_t end;\n\n\t/* no search needed if one or zero ranges present */\n\tif (bfqd->num_actuators == 1)\n\t\treturn 0;\n\n\t/* bio_end_sector(bio) gives the sector after the last one */\n\tend = bio_end_sector(bio) - 1;\n\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tif (end >= bfqd->sector[i] &&\n\t\t    end < bfqd->sector[i] + bfqd->nr_sectors[i])\n\t\t\treturn i;\n\t}\n\n\tWARN_ONCE(true,\n\t\t  \"bfq_actuator_index: bio sector out of ranges: end=%llu\\n\",\n\t\t  end);\n\treturn 0;\n}\n\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq);\n\nstatic void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     int old_wr_coeff,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     bool *interactive)\n{\n\tbool soft_rt, in_burst,\twr_or_deserves_wr,\n\t\tbfqq_wants_to_preempt,\n\t\tidle_for_long_time = bfq_bfqq_idle_for_long_time(bfqd, bfqq),\n\t\t/*\n\t\t * See the comments on\n\t\t * bfq_bfqq_update_budg_for_activation for\n\t\t * details on the usage of the next variable.\n\t\t */\n\t\tarrived_in_time =  blk_time_get_ns() <=\n\t\t\tbfqq->ttime.last_end_request +\n\t\t\tbfqd->bfq_slice_idle * 3;\n\tunsigned int act_idx = bfq_actuator_index(bfqd, rq->bio);\n\tbool bfqq_non_merged_or_stably_merged =\n\t\tbfqq->bic || RQ_BIC(rq)->bfqq_data[act_idx].stably_merged;\n\n\t/*\n\t * bfqq deserves to be weight-raised if:\n\t * - it is sync,\n\t * - it does not belong to a large burst,\n\t * - it has been idle for enough time or is soft real-time,\n\t * - is linked to a bfq_io_cq (it is not shared in any sense),\n\t * - has a default weight (otherwise we assume the user wanted\n\t *   to control its weight explicitly)\n\t */\n\tin_burst = bfq_bfqq_in_large_burst(bfqq);\n\tsoft_rt = bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t\t!BFQQ_TOTALLY_SEEKY(bfqq) &&\n\t\t!in_burst &&\n\t\ttime_is_before_jiffies(bfqq->soft_rt_next_start) &&\n\t\tbfqq->dispatched == 0 &&\n\t\tbfqq->entity.new_weight == 40;\n\t*interactive = !in_burst && idle_for_long_time &&\n\t\tbfqq->entity.new_weight == 40;\n\t/*\n\t * Merged bfq_queues are kept out of weight-raising\n\t * (low-latency) mechanisms. The reason is that these queues\n\t * are usually created for non-interactive and\n\t * non-soft-real-time tasks. Yet this is not the case for\n\t * stably-merged queues. These queues are merged just because\n\t * they are created shortly after each other. So they may\n\t * easily serve the I/O of an interactive or soft-real time\n\t * application, if the application happens to spawn multiple\n\t * processes. So let also stably-merged queued enjoy weight\n\t * raising.\n\t */\n\twr_or_deserves_wr = bfqd->low_latency &&\n\t\t(bfqq->wr_coeff > 1 ||\n\t\t (bfq_bfqq_sync(bfqq) && bfqq_non_merged_or_stably_merged &&\n\t\t  (*interactive || soft_rt)));\n\n\t/*\n\t * Using the last flag, update budget and check whether bfqq\n\t * may want to preempt the in-service queue.\n\t */\n\tbfqq_wants_to_preempt =\n\t\tbfq_bfqq_update_budg_for_activation(bfqd, bfqq,\n\t\t\t\t\t\t    arrived_in_time);\n\n\t/*\n\t * If bfqq happened to be activated in a burst, but has been\n\t * idle for much more than an interactive queue, then we\n\t * assume that, in the overall I/O initiated in the burst, the\n\t * I/O associated with bfqq is finished. So bfqq does not need\n\t * to be treated as a queue belonging to a burst\n\t * anymore. Accordingly, we reset bfqq's in_large_burst flag\n\t * if set, and remove bfqq from the burst list if it's\n\t * there. We do not decrement burst_size, because the fact\n\t * that bfqq does not need to belong to the burst list any\n\t * more does not invalidate the fact that bfqq was created in\n\t * a burst.\n\t */\n\tif (likely(!bfq_bfqq_just_created(bfqq)) &&\n\t    idle_for_long_time &&\n\t    time_is_before_jiffies(\n\t\t    bfqq->budget_timeout +\n\t\t    msecs_to_jiffies(10000))) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t}\n\n\tbfq_clear_bfqq_just_created(bfqq);\n\n\tif (bfqd->low_latency) {\n\t\tif (unlikely(time_is_after_jiffies(bfqq->split_time)))\n\t\t\t/* wraparound */\n\t\t\tbfqq->split_time =\n\t\t\t\tjiffies - bfqd->bfq_wr_min_idle_time - 1;\n\n\t\tif (time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t   bfqd->bfq_wr_min_idle_time)) {\n\t\t\tbfq_update_bfqq_wr_on_rq_arrival(bfqd, bfqq,\n\t\t\t\t\t\t\t old_wr_coeff,\n\t\t\t\t\t\t\t wr_or_deserves_wr,\n\t\t\t\t\t\t\t *interactive,\n\t\t\t\t\t\t\t in_burst,\n\t\t\t\t\t\t\t soft_rt);\n\n\t\t\tif (old_wr_coeff != bfqq->wr_coeff)\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n\n\tbfqq->last_idle_bklogged = jiffies;\n\tbfqq->service_from_backlogged = 0;\n\tbfq_clear_bfqq_softrt_update(bfqq);\n\n\tbfq_add_bfqq_busy(bfqq);\n\n\t/*\n\t * Expire in-service queue if preemption may be needed for\n\t * guarantees or throughput. As for guarantees, we care\n\t * explicitly about two cases. The first is that bfqq has to\n\t * recover a service hole, as explained in the comments on\n\t * bfq_bfqq_update_budg_for_activation(), i.e., that\n\t * bfqq_wants_to_preempt is true. However, if bfqq does not\n\t * carry time-critical I/O, then bfqq's bandwidth is less\n\t * important than that of queues that carry time-critical I/O.\n\t * So, as a further constraint, we consider this case only if\n\t * bfqq is at least as weight-raised, i.e., at least as time\n\t * critical, as the in-service queue.\n\t *\n\t * The second case is that bfqq is in a higher priority class,\n\t * or has a higher weight than the in-service queue. If this\n\t * condition does not hold, we don't care because, even if\n\t * bfqq does not start to be served immediately, the resulting\n\t * delay for bfqq's I/O is however lower or much lower than\n\t * the ideal completion time to be guaranteed to bfqq's I/O.\n\t *\n\t * In both cases, preemption is needed only if, according to\n\t * the timestamps of both bfqq and of the in-service queue,\n\t * bfqq actually is the next queue to serve. So, to reduce\n\t * useless preemptions, the return value of\n\t * next_queue_may_preempt() is considered in the next compound\n\t * condition too. Yet next_queue_may_preempt() just checks a\n\t * simple, necessary condition for bfqq to be the next queue\n\t * to serve. In fact, to evaluate a sufficient condition, the\n\t * timestamps of the in-service queue would need to be\n\t * updated, and this operation is quite costly (see the\n\t * comments on bfq_bfqq_update_budg_for_activation()).\n\t *\n\t * As for throughput, we ask bfq_better_to_idle() whether we\n\t * still need to plug I/O dispatching. If bfq_better_to_idle()\n\t * says no, then plugging is not needed any longer, either to\n\t * boost throughput or to perserve service guarantees. Then\n\t * the best option is to stop plugging I/O, as not doing so\n\t * would certainly lower throughput. We may end up in this\n\t * case if: (1) upon a dispatch attempt, we detected that it\n\t * was better to plug I/O dispatch, and to wait for a new\n\t * request to arrive for the currently in-service queue, but\n\t * (2) this switch of bfqq to busy changes the scenario.\n\t */\n\tif (bfqd->in_service_queue &&\n\t    ((bfqq_wants_to_preempt &&\n\t      bfqq->wr_coeff >= bfqd->in_service_queue->wr_coeff) ||\n\t     bfq_bfqq_higher_class_or_weight(bfqq, bfqd->in_service_queue) ||\n\t     !bfq_better_to_idle(bfqd->in_service_queue)) &&\n\t    next_queue_may_preempt(bfqd))\n\t\tbfq_bfqq_expire(bfqd, bfqd->in_service_queue,\n\t\t\t\tfalse, BFQQE_PREEMPTED);\n}\n\nstatic void bfq_reset_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\t/* invalidate baseline total service time */\n\tbfqq->last_serv_time_ns = 0;\n\n\t/*\n\t * Reset pointer in case we are waiting for\n\t * some request completion.\n\t */\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * If bfqq has a short think time, then start by setting the\n\t * inject limit to 0 prudentially, because the service time of\n\t * an injected I/O request may be higher than the think time\n\t * of bfqq, and therefore, if one request was injected when\n\t * bfqq remains empty, this injected request might delay the\n\t * service of the next I/O request for bfqq significantly. In\n\t * case bfqq can actually tolerate some injection, then the\n\t * adaptive update will however raise the limit soon. This\n\t * lucky circumstance holds exactly because bfqq has a short\n\t * think time, and thus, after remaining empty, is likely to\n\t * get new I/O enqueued---and then completed---before being\n\t * expired. This is the very pattern that gives the\n\t * limit-update algorithm the chance to measure the effect of\n\t * injection on request service times, and then to update the\n\t * limit accordingly.\n\t *\n\t * However, in the following special case, the inject limit is\n\t * left to 1 even if the think time is short: bfqq's I/O is\n\t * synchronized with that of some other queue, i.e., bfqq may\n\t * receive new I/O only after the I/O of the other queue is\n\t * completed. Keeping the inject limit to 1 allows the\n\t * blocking I/O to be served while bfqq is in service. And\n\t * this is very convenient both for bfqq and for overall\n\t * throughput, as explained in detail in the comments in\n\t * bfq_update_has_short_ttime().\n\t *\n\t * On the opposite end, if bfqq has a long think time, then\n\t * start directly by 1, because:\n\t * a) on the bright side, keeping at most one request in\n\t * service in the drive is unlikely to cause any harm to the\n\t * latency of bfqq's requests, as the service time of a single\n\t * request is likely to be lower than the think time of bfqq;\n\t * b) on the downside, after becoming empty, bfqq is likely to\n\t * expire before getting its next request. With this request\n\t * arrival pattern, it is very hard to sample total service\n\t * times and update the inject limit accordingly (see comments\n\t * on bfq_update_inject_limit()). So the limit is likely to be\n\t * never, or at least seldom, updated.  As a consequence, by\n\t * setting the limit to 1, we avoid that no injection ever\n\t * occurs with bfqq. On the downside, this proactive step\n\t * further reduces chances to actually compute the baseline\n\t * total service time. Thus it reduces chances to execute the\n\t * limit-update algorithm and possibly raise the limit to more\n\t * than 1.\n\t */\n\tif (bfq_bfqq_has_short_ttime(bfqq))\n\t\tbfqq->inject_limit = 0;\n\telse\n\t\tbfqq->inject_limit = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)\n{\n\tu64 tot_io_time = now_ns - bfqq->io_start_time;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) && bfqq->dispatched == 0)\n\t\tbfqq->tot_idle_time +=\n\t\t\tnow_ns - bfqq->ttime.last_end_request;\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq)))\n\t\treturn;\n\n\t/*\n\t * Must be busy for at least about 80% of the time to be\n\t * considered I/O bound.\n\t */\n\tif (bfqq->tot_idle_time * 5 > tot_io_time)\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * Keep an observation window of at most 200 ms in the past\n\t * from now.\n\t */\n\tif (tot_io_time > 200 * NSEC_PER_MSEC) {\n\t\tbfqq->io_start_time = now_ns - (tot_io_time>>1);\n\t\tbfqq->tot_idle_time >>= 1;\n\t}\n}\n\n/*\n * Detect whether bfqq's I/O seems synchronized with that of some\n * other queue, i.e., whether bfqq, after remaining empty, happens to\n * receive new I/O only right after some I/O request of the other\n * queue has been completed. We call waker queue the other queue, and\n * we assume, for simplicity, that bfqq may have at most one waker\n * queue.\n *\n * A remarkable throughput boost can be reached by unconditionally\n * injecting the I/O of the waker queue, every time a new\n * bfq_dispatch_request happens to be invoked while I/O is being\n * plugged for bfqq.  In addition to boosting throughput, this\n * unblocks bfqq's I/O, thereby improving bandwidth and latency for\n * bfqq. Note that these same results may be achieved with the general\n * injection mechanism, but less effectively. For details on this\n * aspect, see the comments on the choice of the queue for injection\n * in bfq_select_queue().\n *\n * Turning back to the detection of a waker queue, a queue Q is deemed as a\n * waker queue for bfqq if, for three consecutive times, bfqq happens to become\n * non empty right after a request of Q has been completed within given\n * timeout. In this respect, even if bfqq is empty, we do not check for a waker\n * if it still has some in-flight I/O. In fact, in this case bfqq is actually\n * still being served by the drive, and may receive new I/O on the completion\n * of some of the in-flight requests. In particular, on the first time, Q is\n * tentatively set as a candidate waker queue, while on the third consecutive\n * time that Q is detected, the field waker_bfqq is set to Q, to confirm that Q\n * is a waker queue for bfqq. These detection steps are performed only if bfqq\n * has a long think time, so as to make it more likely that bfqq's I/O is\n * actually being blocked by a synchronization. This last filter, plus the\n * above three-times requirement and time limit for detection, make false\n * positives less likely.\n *\n * NOTE\n *\n * The sooner a waker queue is detected, the sooner throughput can be\n * boosted by injecting I/O from the waker queue. Fortunately,\n * detection is likely to be actually fast, for the following\n * reasons. While blocked by synchronization, bfqq has a long think\n * time. This implies that bfqq's inject limit is at least equal to 1\n * (see the comments in bfq_update_inject_limit()). So, thanks to\n * injection, the waker queue is likely to be served during the very\n * first I/O-plugging time interval for bfqq. This triggers the first\n * step of the detection mechanism. Thanks again to injection, the\n * candidate waker queue is then likely to be confirmed no later than\n * during the next I/O-plugging interval for bfqq.\n *\n * ISSUE\n *\n * On queue merging all waker information is lost.\n */\nstatic void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    u64 now_ns)\n{\n\tchar waker_name[MAX_BFQQ_NAME_LENGTH];\n\n\tif (!bfqd->last_completed_rq_bfqq ||\n\t    bfqd->last_completed_rq_bfqq == bfqq ||\n\t    bfq_bfqq_has_short_ttime(bfqq) ||\n\t    now_ns - bfqd->last_completion >= 4 * NSEC_PER_MSEC ||\n\t    bfqd->last_completed_rq_bfqq == &bfqd->oom_bfqq ||\n\t    bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t/*\n\t * We reset waker detection logic also if too much time has passed\n \t * since the first detection. If wakeups are rare, pointless idling\n\t * doesn't hurt throughput that much. The condition below makes sure\n\t * we do not uselessly idle blocking waker in more than 1/64 cases.\n\t */\n\tif (bfqd->last_completed_rq_bfqq !=\n\t    bfqq->tentative_waker_bfqq ||\n\t    now_ns > bfqq->waker_detection_started +\n\t\t\t\t\t128 * (u64)bfqd->bfq_slice_idle) {\n\t\t/*\n\t\t * First synchronization detected with a\n\t\t * candidate waker queue, or with a different\n\t\t * candidate waker queue from the current one.\n\t\t */\n\t\tbfqq->tentative_waker_bfqq =\n\t\t\tbfqd->last_completed_rq_bfqq;\n\t\tbfqq->num_waker_detections = 1;\n\t\tbfqq->waker_detection_started = now_ns;\n\t\tbfq_bfqq_name(bfqq->tentative_waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set tentative waker %s\", waker_name);\n\t} else /* Same tentative waker queue detected again */\n\t\tbfqq->num_waker_detections++;\n\n\tif (bfqq->num_waker_detections == 3) {\n\t\tbfqq->waker_bfqq = bfqd->last_completed_rq_bfqq;\n\t\tbfqq->tentative_waker_bfqq = NULL;\n\t\tbfq_bfqq_name(bfqq->waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set waker %s\", waker_name);\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * bfqq->waker_bfqq must be reset. To\n\t\t * this goal, we maintain in each\n\t\t * waker queue a list, woken_list, of\n\t\t * all the queues that reference the\n\t\t * waker queue through their\n\t\t * waker_bfqq pointer. When the waker\n\t\t * queue exits, the waker_bfqq pointer\n\t\t * of all the queues in the woken_list\n\t\t * is reset.\n\t\t *\n\t\t * In addition, if bfqq is already in\n\t\t * the woken_list of a waker queue,\n\t\t * then, before being inserted into\n\t\t * the woken_list of a new waker\n\t\t * queue, bfqq must be removed from\n\t\t * the woken_list of the old waker\n\t\t * queue.\n\t\t */\n\t\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\t\thlist_del_init(&bfqq->woken_list_node);\n\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t       &bfqd->last_completed_rq_bfqq->woken_list);\n\t}\n}\n\nstatic void bfq_add_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct request *next_rq, *prev;\n\tunsigned int old_wr_coeff = bfqq->wr_coeff;\n\tbool interactive = false;\n\tu64 now_ns = blk_time_get_ns();\n\n\tbfq_log_bfqq(bfqd, bfqq, \"add_request %d\", rq_is_sync(rq));\n\tbfqq->queued[rq_is_sync(rq)]++;\n\t/*\n\t * Updating of 'bfqd->queued' is protected by 'bfqd->lock', however, it\n\t * may be read without holding the lock in bfq_has_work().\n\t */\n\tWRITE_ONCE(bfqd->queued, bfqd->queued + 1);\n\n\tif (bfq_bfqq_sync(bfqq) && RQ_BIC(rq)->requests <= 1) {\n\t\tbfq_check_waker(bfqd, bfqq, now_ns);\n\n\t\t/*\n\t\t * Periodically reset inject limit, to make sure that\n\t\t * the latter eventually drops in case workload\n\t\t * changes, see step (3) in the comments on\n\t\t * bfq_update_inject_limit().\n\t\t */\n\t\tif (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t     msecs_to_jiffies(1000)))\n\t\t\tbfq_reset_inject_limit(bfqd, bfqq);\n\n\t\t/*\n\t\t * The following conditions must hold to setup a new\n\t\t * sampling of total service time, and then a new\n\t\t * update of the inject limit:\n\t\t * - bfqq is in service, because the total service\n\t\t *   time is evaluated only for the I/O requests of\n\t\t *   the queues in service;\n\t\t * - this is the right occasion to compute or to\n\t\t *   lower the baseline total service time, because\n\t\t *   there are actually no requests in the drive,\n\t\t *   or\n\t\t *   the baseline total service time is available, and\n\t\t *   this is the right occasion to compute the other\n\t\t *   quantity needed to update the inject limit, i.e.,\n\t\t *   the total service time caused by the amount of\n\t\t *   injection allowed by the current value of the\n\t\t *   limit. It is the right occasion because injection\n\t\t *   has actually been performed during the service\n\t\t *   hole, and there are still in-flight requests,\n\t\t *   which are very likely to be exactly the injected\n\t\t *   requests, or part of them;\n\t\t * - the minimum interval for sampling the total\n\t\t *   service time and updating the inject limit has\n\t\t *   elapsed.\n\t\t */\n\t\tif (bfqq == bfqd->in_service_queue &&\n\t\t    (bfqd->tot_rq_in_driver == 0 ||\n\t\t     (bfqq->last_serv_time_ns > 0 &&\n\t\t      bfqd->rqs_injected && bfqd->tot_rq_in_driver > 0)) &&\n\t\t    time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t      msecs_to_jiffies(10))) {\n\t\t\tbfqd->last_empty_occupied_ns = blk_time_get_ns();\n\t\t\t/*\n\t\t\t * Start the state machine for measuring the\n\t\t\t * total service time of rq: setting\n\t\t\t * wait_dispatch will cause bfqd->waited_rq to\n\t\t\t * be set when rq will be dispatched.\n\t\t\t */\n\t\t\tbfqd->wait_dispatch = true;\n\t\t\t/*\n\t\t\t * If there is no I/O in service in the drive,\n\t\t\t * then possible injection occurred before the\n\t\t\t * arrival of rq will not affect the total\n\t\t\t * service time of rq. So the injection limit\n\t\t\t * must not be updated as a function of such\n\t\t\t * total service time, unless new injection\n\t\t\t * occurs before rq is completed. To have the\n\t\t\t * injection limit updated only in the latter\n\t\t\t * case, reset rqs_injected here (rqs_injected\n\t\t\t * will be set in case injection is performed\n\t\t\t * on bfqq before rq is completed).\n\t\t\t */\n\t\t\tif (bfqd->tot_rq_in_driver == 0)\n\t\t\t\tbfqd->rqs_injected = false;\n\t\t}\n\t}\n\n\tif (bfq_bfqq_sync(bfqq))\n\t\tbfq_update_io_intensity(bfqq, now_ns);\n\n\telv_rb_add(&bfqq->sort_list, rq);\n\n\t/*\n\t * Check if this request is a better next-serve candidate.\n\t */\n\tprev = bfqq->next_rq;\n\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, rq, bfqd->last_position);\n\tbfqq->next_rq = next_rq;\n\n\t/*\n\t * Adjust priority tree position, if next_rq changes.\n\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing && prev != bfqq->next_rq))\n\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\n\tif (!bfq_bfqq_busy(bfqq)) /* switching to busy ... */\n\t\tbfq_bfqq_handle_idle_busy_switch(bfqd, bfqq, old_wr_coeff,\n\t\t\t\t\t\t rq, &interactive);\n\telse {\n\t\tif (bfqd->low_latency && old_wr_coeff == 1 && !rq_is_sync(rq) &&\n\t\t    time_is_before_jiffies(\n\t\t\t\tbfqq->last_wr_start_finish +\n\t\t\t\tbfqd->bfq_wr_min_inter_arr_async)) {\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\n\t\t\tbfqd->wr_busy_queues++;\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t\tif (prev != bfqq->next_rq)\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\t/*\n\t * Assign jiffies to last_wr_start_finish in the following\n\t * cases:\n\t *\n\t * . if bfqq is not going to be weight-raised, because, for\n\t *   non weight-raised queues, last_wr_start_finish stores the\n\t *   arrival time of the last request; as of now, this piece\n\t *   of information is used only for deciding whether to\n\t *   weight-raise async queues\n\t *\n\t * . if bfqq is not weight-raised, because, if bfqq is now\n\t *   switching to weight-raised, then last_wr_start_finish\n\t *   stores the time when weight-raising starts\n\t *\n\t * . if bfqq is interactive, because, regardless of whether\n\t *   bfqq is currently weight-raised, the weight-raising\n\t *   period must start or restart (this case is considered\n\t *   separately because it is not detected by the above\n\t *   conditions, if bfqq is already weight-raised)\n\t *\n\t * last_wr_start_finish has to be updated also if bfqq is soft\n\t * real-time, because the weight-raising period is constantly\n\t * restarted on idle-to-busy transitions for these queues, but\n\t * this is already done in bfq_bfqq_handle_idle_busy_switch if\n\t * needed.\n\t */\n\tif (bfqd->low_latency &&\n\t\t(old_wr_coeff == 1 || bfqq->wr_coeff == 1 || interactive))\n\t\tbfqq->last_wr_start_finish = jiffies;\n}\n\nstatic struct request *bfq_find_rq_fmerge(struct bfq_data *bfqd,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq;\n\n\n\tif (bfqq)\n\t\treturn elv_rb_find(&bfqq->sort_list, bio_end_sector(bio));\n\n\treturn NULL;\n}\n\nstatic sector_t get_sdist(sector_t last_pos, struct request *rq)\n{\n\tif (last_pos)\n\t\treturn abs(blk_rq_pos(rq) - last_pos);\n\n\treturn 0;\n}\n\nstatic void bfq_remove_request(struct request_queue *q,\n\t\t\t       struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tconst int sync = rq_is_sync(rq);\n\n\tif (bfqq->next_rq == rq) {\n\t\tbfqq->next_rq = bfq_find_next_rq(bfqd, bfqq, rq);\n\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\tif (rq->queuelist.prev != &rq->queuelist)\n\t\tlist_del_init(&rq->queuelist);\n\tbfqq->queued[sync]--;\n\t/*\n\t * Updating of 'bfqd->queued' is protected by 'bfqd->lock', however, it\n\t * may be read without holding the lock in bfq_has_work().\n\t */\n\tWRITE_ONCE(bfqd->queued, bfqd->queued - 1);\n\telv_rb_del(&bfqq->sort_list, rq);\n\n\telv_rqhash_del(q, rq);\n\tif (q->last_merge == rq)\n\t\tq->last_merge = NULL;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\tbfqq->next_rq = NULL;\n\n\t\tif (bfq_bfqq_busy(bfqq) && bfqq != bfqd->in_service_queue) {\n\t\t\tbfq_del_bfqq_busy(bfqq, false);\n\t\t\t/*\n\t\t\t * bfqq emptied. In normal operation, when\n\t\t\t * bfqq is empty, bfqq->entity.service and\n\t\t\t * bfqq->entity.budget must contain,\n\t\t\t * respectively, the service received and the\n\t\t\t * budget used last time bfqq emptied. These\n\t\t\t * facts do not hold in this case, as at least\n\t\t\t * this last removal occurred while bfqq is\n\t\t\t * not in service. To avoid inconsistencies,\n\t\t\t * reset both bfqq->entity.service and\n\t\t\t * bfqq->entity.budget, if bfqq has still a\n\t\t\t * process that may issue I/O requests to it.\n\t\t\t */\n\t\t\tbfqq->entity.budget = bfqq->entity.service = 0;\n\t\t}\n\n\t\t/*\n\t\t * Remove queue from request-position tree as it is empty.\n\t\t */\n\t\tif (bfqq->pos_root) {\n\t\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\t\tbfqq->pos_root = NULL;\n\t\t}\n\t} else {\n\t\t/* see comments on bfq_pos_tree_add_move() for the unlikely() */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending--;\n\n}\n\nstatic bool bfq_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *free = NULL;\n\t/*\n\t * bfq_bic_lookup grabs the queue_lock: invoke it now and\n\t * store its return value for later use, to avoid nesting\n\t * queue_lock inside the bfqd->lock. We assume that the bic\n\t * returned by bfq_bic_lookup does not go away before\n\t * bfqd->lock is taken.\n\t */\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(q);\n\tbool ret;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tif (bic) {\n\t\t/*\n\t\t * Make sure cgroup info is uptodate for current process before\n\t\t * considering the merge.\n\t\t */\n\t\tbfq_bic_update_cgroup(bic, bio);\n\n\t\tbfqd->bio_bfqq = bic_to_bfqq(bic, op_is_sync(bio->bi_opf),\n\t\t\t\t\t     bfq_actuator_index(bfqd, bio));\n\t} else {\n\t\tbfqd->bio_bfqq = NULL;\n\t}\n\tbfqd->bio_bic = bic;\n\n\tret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);\n\n\tspin_unlock_irq(&bfqd->lock);\n\tif (free)\n\t\tblk_mq_free_request(free);\n\n\treturn ret;\n}\n\nstatic int bfq_request_merge(struct request_queue *q, struct request **req,\n\t\t\t     struct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *__rq;\n\n\t__rq = bfq_find_rq_fmerge(bfqd, bio, q);\n\tif (__rq && elv_bio_merge_ok(__rq, bio)) {\n\t\t*req = __rq;\n\n\t\tif (blk_discard_mergable(__rq))\n\t\t\treturn ELEVATOR_DISCARD_MERGE;\n\t\treturn ELEVATOR_FRONT_MERGE;\n\t}\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\nstatic void bfq_request_merged(struct request_queue *q, struct request *req,\n\t\t\t       enum elv_merge type)\n{\n\tif (type == ELEVATOR_FRONT_MERGE &&\n\t    rb_prev(&req->rb_node) &&\n\t    blk_rq_pos(req) <\n\t    blk_rq_pos(container_of(rb_prev(&req->rb_node),\n\t\t\t\t    struct request, rb_node))) {\n\t\tstruct bfq_queue *bfqq = RQ_BFQQ(req);\n\t\tstruct bfq_data *bfqd;\n\t\tstruct request *prev, *next_rq;\n\n\t\tif (!bfqq)\n\t\t\treturn;\n\n\t\tbfqd = bfqq->bfqd;\n\n\t\t/* Reposition request in its sort_list */\n\t\telv_rb_del(&bfqq->sort_list, req);\n\t\telv_rb_add(&bfqq->sort_list, req);\n\n\t\t/* Choose next request to be served for bfqq */\n\t\tprev = bfqq->next_rq;\n\t\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, req,\n\t\t\t\t\t bfqd->last_position);\n\t\tbfqq->next_rq = next_rq;\n\t\t/*\n\t\t * If next_rq changes, update both the queue's budget to\n\t\t * fit the new request and the queue's position in its\n\t\t * rq_pos_tree.\n\t\t */\n\t\tif (prev != bfqq->next_rq) {\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t\t\t/*\n\t\t\t * See comments on bfq_pos_tree_add_move() for\n\t\t\t * the unlikely().\n\t\t\t */\n\t\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t\t}\n\t}\n}\n\n/*\n * This function is called to notify the scheduler that the requests\n * rq and 'next' have been merged, with 'next' going away.  BFQ\n * exploits this hook to address the following issue: if 'next' has a\n * fifo_time lower that rq, then the fifo_time of rq must be set to\n * the value of 'next', to not forget the greater age of 'next'.\n *\n * NOTE: in this function we assume that rq is in a bfq_queue, basing\n * on that rq is picked from the hash table q->elevator->hash, which,\n * in its turn, is filled only with I/O requests present in\n * bfq_queues, while BFQ is in use for the request queue q. In fact,\n * the function that fills this hash table (elv_rqhash_add) is called\n * only by bfq_insert_request.\n */\nstatic void bfq_requests_merged(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*next_bfqq = RQ_BFQQ(next);\n\n\tif (!bfqq)\n\t\tgoto remove;\n\n\t/*\n\t * If next and rq belong to the same bfq_queue and next is older\n\t * than rq, then reposition rq in the fifo (by substituting next\n\t * with rq). Otherwise, if next and rq belong to different\n\t * bfq_queues, never reposition rq: in fact, we would have to\n\t * reposition it with respect to next's position in its own fifo,\n\t * which would most certainly be too expensive with respect to\n\t * the benefits.\n\t */\n\tif (bfqq == next_bfqq &&\n\t    !list_empty(&rq->queuelist) && !list_empty(&next->queuelist) &&\n\t    next->fifo_time < rq->fifo_time) {\n\t\tlist_del_init(&rq->queuelist);\n\t\tlist_replace_init(&next->queuelist, &rq->queuelist);\n\t\trq->fifo_time = next->fifo_time;\n\t}\n\n\tif (bfqq->next_rq == next)\n\t\tbfqq->next_rq = rq;\n\n\tbfqg_stats_update_io_merged(bfqq_group(bfqq), next->cmd_flags);\nremove:\n\t/* Merged request may be in the IO scheduler. Remove it. */\n\tif (!RB_EMPTY_NODE(&next->rb_node)) {\n\t\tbfq_remove_request(next->q, next);\n\t\tif (next_bfqq)\n\t\t\tbfqg_stats_update_io_remove(bfqq_group(next_bfqq),\n\t\t\t\t\t\t    next->cmd_flags);\n\t}\n}\n\n/* Must be called with bfqq != NULL */\nstatic void bfq_bfqq_end_wr(struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq has been enjoying interactive weight-raising, then\n\t * reset soft_rt_next_start. We do it for the following\n\t * reason. bfqq may have been conveying the I/O needed to load\n\t * a soft real-time application. Such an application actually\n\t * exhibits a soft real-time I/O pattern after it finishes\n\t * loading, and finally starts doing its job. But, if bfqq has\n\t * been receiving a lot of bandwidth so far (likely to happen\n\t * on a fast device), then soft_rt_next_start now contains a\n\t * high value that. So, without this reset, bfqq would be\n\t * prevented from being possibly considered as soft_rt for a\n\t * very long time.\n\t */\n\n\tif (bfqq->wr_cur_max_time !=\n\t    bfqq->bfqd->bfq_wr_rt_max_time)\n\t\tbfqq->soft_rt_next_start = jiffies;\n\n\tif (bfq_bfqq_busy(bfqq))\n\t\tbfqq->bfqd->wr_busy_queues--;\n\tbfqq->wr_coeff = 1;\n\tbfqq->wr_cur_max_time = 0;\n\tbfqq->last_wr_start_finish = jiffies;\n\t/*\n\t * Trigger a weight change on the next invocation of\n\t * __bfq_entity_update_weight_prio.\n\t */\n\tbfqq->entity.prio_changed = 1;\n}\n\nvoid bfq_end_wr_async_queues(struct bfq_data *bfqd,\n\t\t\t     struct bfq_group *bfqg)\n{\n\tint i, j, k;\n\n\tfor (k = 0; k < bfqd->num_actuators; k++) {\n\t\tfor (i = 0; i < 2; i++)\n\t\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t\tif (bfqg->async_bfqq[i][j][k])\n\t\t\t\t\tbfq_bfqq_end_wr(bfqg->async_bfqq[i][j][k]);\n\t\tif (bfqg->async_idle_bfqq[k])\n\t\t\tbfq_bfqq_end_wr(bfqg->async_idle_bfqq[k]);\n\t}\n}\n\nstatic void bfq_end_wr(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\tint i;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tlist_for_each_entry(bfqq, &bfqd->active_list[i], bfqq_list)\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t}\n\tlist_for_each_entry(bfqq, &bfqd->idle_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tbfq_end_wr_async(bfqd);\n\n\tspin_unlock_irq(&bfqd->lock);\n}\n\nstatic sector_t bfq_io_struct_pos(void *io_struct, bool request)\n{\n\tif (request)\n\t\treturn blk_rq_pos(io_struct);\n\telse\n\t\treturn ((struct bio *)io_struct)->bi_iter.bi_sector;\n}\n\nstatic int bfq_rq_close_to_sector(void *io_struct, bool request,\n\t\t\t\t  sector_t sector)\n{\n\treturn abs(bfq_io_struct_pos(io_struct, request) - sector) <=\n\t       BFQQ_CLOSE_THR;\n}\n\nstatic struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_queue *bfqq,\n\t\t\t\t\t sector_t sector)\n{\n\tstruct rb_root *root = &bfqq_group(bfqq)->rq_pos_tree;\n\tstruct rb_node *parent, *node;\n\tstruct bfq_queue *__bfqq;\n\n\tif (RB_EMPTY_ROOT(root))\n\t\treturn NULL;\n\n\t/*\n\t * First, if we find a request starting at the end of the last\n\t * request, choose it.\n\t */\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, root, sector, &parent, NULL);\n\tif (__bfqq)\n\t\treturn __bfqq;\n\n\t/*\n\t * If the exact sector wasn't found, the parent of the NULL leaf\n\t * will contain the closest sector (rq_pos_tree sorted by\n\t * next_request position).\n\t */\n\t__bfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\tif (blk_rq_pos(__bfqq->next_rq) < sector)\n\t\tnode = rb_next(&__bfqq->pos_node);\n\telse\n\t\tnode = rb_prev(&__bfqq->pos_node);\n\tif (!node)\n\t\treturn NULL;\n\n\t__bfqq = rb_entry(node, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_find_close_cooperator(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_queue *cur_bfqq,\n\t\t\t\t\t\t   sector_t sector)\n{\n\tstruct bfq_queue *bfqq;\n\n\t/*\n\t * We shall notice if some of the queues are cooperating,\n\t * e.g., working closely on the same area of the device. In\n\t * that case, we can group them together and: 1) don't waste\n\t * time idling, and 2) serve the union of their requests in\n\t * the best possible order for throughput.\n\t */\n\tbfqq = bfqq_find_close(bfqd, cur_bfqq, sector);\n\tif (!bfqq || bfqq == cur_bfqq)\n\t\treturn NULL;\n\n\treturn bfqq;\n}\n\nstatic struct bfq_queue *\nbfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)\n{\n\tint process_refs, new_process_refs;\n\tstruct bfq_queue *__bfqq;\n\n\t/*\n\t * If there are no process references on the new_bfqq, then it is\n\t * unsafe to follow the ->new_bfqq chain as other bfqq's in the chain\n\t * may have dropped their last reference (not just their last process\n\t * reference).\n\t */\n\tif (!bfqq_process_refs(new_bfqq))\n\t\treturn NULL;\n\n\t/* Avoid a circular list and skip interim queue merges. */\n\twhile ((__bfqq = new_bfqq->new_bfqq)) {\n\t\tif (__bfqq == bfqq)\n\t\t\treturn NULL;\n\t\tnew_bfqq = __bfqq;\n\t}\n\n\tprocess_refs = bfqq_process_refs(bfqq);\n\tnew_process_refs = bfqq_process_refs(new_bfqq);\n\t/*\n\t * If the process for the bfqq has gone away, there is no\n\t * sense in merging the queues.\n\t */\n\tif (process_refs == 0 || new_process_refs == 0)\n\t\treturn NULL;\n\n\t/*\n\t * Make sure merged queues belong to the same parent. Parents could\n\t * have changed since the time we decided the two queues are suitable\n\t * for merging.\n\t */\n\tif (new_bfqq->entity.parent != bfqq->entity.parent)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"scheduling merge with queue %d\",\n\t\tnew_bfqq->pid);\n\n\t/*\n\t * Merging is just a redirection: the requests of the process\n\t * owning one of the two queues are redirected to the other queue.\n\t * The latter queue, in its turn, is set as shared if this is the\n\t * first time that the requests of some process are redirected to\n\t * it.\n\t *\n\t * We redirect bfqq to new_bfqq and not the opposite, because\n\t * we are in the context of the process owning bfqq, thus we\n\t * have the io_cq of this process. So we can immediately\n\t * configure this io_cq to redirect the requests of the\n\t * process to new_bfqq. In contrast, the io_cq of new_bfqq is\n\t * not available any more (new_bfqq->bic == NULL).\n\t *\n\t * Anyway, even in case new_bfqq coincides with the in-service\n\t * queue, redirecting requests the in-service queue is the\n\t * best option, as we feed the in-service queue with new\n\t * requests close to the last request served and, by doing so,\n\t * are likely to increase the throughput.\n\t */\n\tbfqq->new_bfqq = new_bfqq;\n\t/*\n\t * The above assignment schedules the following redirections:\n\t * each time some I/O for bfqq arrives, the process that\n\t * generated that I/O is disassociated from bfqq and\n\t * associated with new_bfqq. Here we increases new_bfqq->ref\n\t * in advance, adding the number of processes that are\n\t * expected to be associated with new_bfqq as they happen to\n\t * issue I/O.\n\t */\n\tnew_bfqq->ref += process_refs;\n\treturn new_bfqq;\n}\n\nstatic bool bfq_may_be_close_cooperator(struct bfq_queue *bfqq,\n\t\t\t\t\tstruct bfq_queue *new_bfqq)\n{\n\tif (bfq_too_late_for_merging(new_bfqq))\n\t\treturn false;\n\n\tif (bfq_class_idle(bfqq) || bfq_class_idle(new_bfqq) ||\n\t    (bfqq->ioprio_class != new_bfqq->ioprio_class))\n\t\treturn false;\n\n\t/*\n\t * If either of the queues has already been detected as seeky,\n\t * then merging it with the other queue is unlikely to lead to\n\t * sequential I/O.\n\t */\n\tif (BFQQ_SEEKY(bfqq) || BFQQ_SEEKY(new_bfqq))\n\t\treturn false;\n\n\t/*\n\t * Interleaved I/O is known to be done by (some) applications\n\t * only for reads, so it does not make sense to merge async\n\t * queues.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || !bfq_bfqq_sync(new_bfqq))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq);\n\nstatic struct bfq_queue *\nbfq_setup_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct bfq_queue *stable_merge_bfqq,\n\t\t       struct bfq_iocq_bfqq_data *bfqq_data)\n{\n\tint proc_ref = min(bfqq_process_refs(bfqq),\n\t\t\t   bfqq_process_refs(stable_merge_bfqq));\n\tstruct bfq_queue *new_bfqq = NULL;\n\n\tbfqq_data->stable_merge_bfqq = NULL;\n\tif (idling_boosts_thr_without_issues(bfqd, bfqq) || proc_ref == 0)\n\t\tgoto out;\n\n\t/* next function will take at least one ref */\n\tnew_bfqq = bfq_setup_merge(bfqq, stable_merge_bfqq);\n\n\tif (new_bfqq) {\n\t\tbfqq_data->stably_merged = true;\n\t\tif (new_bfqq->bic) {\n\t\t\tunsigned int new_a_idx = new_bfqq->actuator_idx;\n\t\t\tstruct bfq_iocq_bfqq_data *new_bfqq_data =\n\t\t\t\t&new_bfqq->bic->bfqq_data[new_a_idx];\n\n\t\t\tnew_bfqq_data->stably_merged = true;\n\t\t}\n\t}\n\nout:\n\t/* deschedule stable merge, because done or aborted here */\n\tbfq_put_stable_ref(stable_merge_bfqq);\n\n\treturn new_bfqq;\n}\n\n/*\n * Attempt to schedule a merge of bfqq with the currently in-service\n * queue or with a close queue among the scheduled queues.  Return\n * NULL if no merge was scheduled, a pointer to the shared bfq_queue\n * structure otherwise.\n *\n * The OOM queue is not allowed to participate to cooperation: in fact, since\n * the requests temporarily redirected to the OOM queue could be redirected\n * again to dedicated queues at any time, the state needed to correctly\n * handle merging with the OOM queue would be quite complex and expensive\n * to maintain. Besides, in such a critical condition as an out of memory,\n * the benefits of queue merging may be little relevant, or even negligible.\n *\n * WARNING: queue merging may impair fairness among non-weight raised\n * queues, for at least two reasons: 1) the original weight of a\n * merged queue may change during the merged state, 2) even being the\n * weight the same, a merged queue may be bloated with many more\n * requests than the ones produced by its originally-associated\n * process.\n */\nstatic struct bfq_queue *\nbfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t     void *io_struct, bool request, struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue *in_service_bfqq, *new_bfqq;\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\t/* if a merge has already been setup, then proceed with that first */\n\tnew_bfqq = bfqq->new_bfqq;\n\tif (new_bfqq) {\n\t\twhile (new_bfqq->new_bfqq)\n\t\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t\treturn new_bfqq;\n\t}\n\n\t/*\n\t * Check delayed stable merge for rotational or non-queueing\n\t * devs. For this branch to be executed, bfqq must not be\n\t * currently merged with some other queue (i.e., bfqq->bic\n\t * must be non null). If we considered also merged queues,\n\t * then we should also check whether bfqq has already been\n\t * merged with bic->stable_merge_bfqq. But this would be\n\t * costly and complicated.\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing)) {\n\t\t/*\n\t\t * Make sure also that bfqq is sync, because\n\t\t * bic->stable_merge_bfqq may point to some queue (for\n\t\t * stable merging) also if bic is associated with a\n\t\t * sync queue, but this bfqq is async\n\t\t */\n\t\tif (bfq_bfqq_sync(bfqq) && bfqq_data->stable_merge_bfqq &&\n\t\t    !bfq_bfqq_just_created(bfqq) &&\n\t\t    time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t  msecs_to_jiffies(bfq_late_stable_merging)) &&\n\t\t    time_is_before_jiffies(bfqq->creation_time +\n\t\t\t\t\t   msecs_to_jiffies(bfq_late_stable_merging))) {\n\t\t\tstruct bfq_queue *stable_merge_bfqq =\n\t\t\t\tbfqq_data->stable_merge_bfqq;\n\n\t\t\treturn bfq_setup_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t      stable_merge_bfqq,\n\t\t\t\t\t\t      bfqq_data);\n\t\t}\n\t}\n\n\t/*\n\t * Do not perform queue merging if the device is non\n\t * rotational and performs internal queueing. In fact, such a\n\t * device reaches a high speed through internal parallelism\n\t * and pipelining. This means that, to reach a high\n\t * throughput, it must have many requests enqueued at the same\n\t * time. But, in this configuration, the internal scheduling\n\t * algorithm of the device does exactly the job of queue\n\t * merging: it reorders requests so as to obtain as much as\n\t * possible a sequential I/O pattern. As a consequence, with\n\t * the workload generated by processes doing interleaved I/O,\n\t * the throughput reached by the device is likely to be the\n\t * same, with and without queue merging.\n\t *\n\t * Disabling merging also provides a remarkable benefit in\n\t * terms of throughput. Merging tends to make many workloads\n\t * artificially more uneven, because of shared queues\n\t * remaining non empty for incomparably more time than\n\t * non-merged queues. This may accentuate workload\n\t * asymmetries. For example, if one of the queues in a set of\n\t * merged queues has a higher weight than a normal queue, then\n\t * the shared queue may inherit such a high weight and, by\n\t * staying almost always active, may force BFQ to perform I/O\n\t * plugging most of the time. This evidently makes it harder\n\t * for BFQ to let the device reach a high throughput.\n\t *\n\t * Finally, the likely() macro below is not used because one\n\t * of the two branches is more likely than the other, but to\n\t * have the code path after the following if() executed as\n\t * fast as possible for the case of a non rotational device\n\t * with queueing. We want it because this is the fastest kind\n\t * of device. On the opposite end, the likely() may lengthen\n\t * the execution time of BFQ for the case of slower devices\n\t * (rotational or at least without queueing). But in this case\n\t * the execution time of BFQ matters very little, if not at\n\t * all.\n\t */\n\tif (likely(bfqd->nonrot_with_queueing))\n\t\treturn NULL;\n\n\t/*\n\t * Prevent bfqq from being merged if it has been created too\n\t * long ago. The idea is that true cooperating processes, and\n\t * thus their associated bfq_queues, are supposed to be\n\t * created shortly after each other. This is the case, e.g.,\n\t * for KVM/QEMU and dump I/O threads. Basing on this\n\t * assumption, the following filtering greatly reduces the\n\t * probability that two non-cooperating processes, which just\n\t * happen to do close I/O for some short time interval, have\n\t * their queues merged by mistake.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn NULL;\n\n\tif (!io_struct || unlikely(bfqq == &bfqd->oom_bfqq))\n\t\treturn NULL;\n\n\t/* If there is only one backlogged queue, don't search. */\n\tif (bfq_tot_busy_queues(bfqd) == 1)\n\t\treturn NULL;\n\n\tin_service_bfqq = bfqd->in_service_queue;\n\n\tif (in_service_bfqq && in_service_bfqq != bfqq &&\n\t    likely(in_service_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_rq_close_to_sector(io_struct, request,\n\t\t\t\t   bfqd->in_serv_last_pos) &&\n\t    bfqq->entity.parent == in_service_bfqq->entity.parent &&\n\t    bfq_may_be_close_cooperator(bfqq, in_service_bfqq)) {\n\t\tnew_bfqq = bfq_setup_merge(bfqq, in_service_bfqq);\n\t\tif (new_bfqq)\n\t\t\treturn new_bfqq;\n\t}\n\t/*\n\t * Check whether there is a cooperator among currently scheduled\n\t * queues. The only thing we need is that the bio/request is not\n\t * NULL, as we need it to establish whether a cooperator exists.\n\t */\n\tnew_bfqq = bfq_find_close_cooperator(bfqd, bfqq,\n\t\t\tbfq_io_struct_pos(io_struct, request));\n\n\tif (new_bfqq && likely(new_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_may_be_close_cooperator(bfqq, new_bfqq))\n\t\treturn bfq_setup_merge(bfqq, new_bfqq);\n\n\treturn NULL;\n}\n\nstatic void bfq_bfqq_save_state(struct bfq_queue *bfqq)\n{\n\tstruct bfq_io_cq *bic = bfqq->bic;\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\t/*\n\t * If !bfqq->bic, the queue is already shared or its requests\n\t * have already been redirected to a shared queue; both idle window\n\t * and weight raising state have already been saved. Do nothing.\n\t */\n\tif (!bic)\n\t\treturn;\n\n\tbfqq_data->saved_last_serv_time_ns = bfqq->last_serv_time_ns;\n\tbfqq_data->saved_inject_limit =\tbfqq->inject_limit;\n\tbfqq_data->saved_decrease_time_jif = bfqq->decrease_time_jif;\n\n\tbfqq_data->saved_weight = bfqq->entity.orig_weight;\n\tbfqq_data->saved_ttime = bfqq->ttime;\n\tbfqq_data->saved_has_short_ttime =\n\t\tbfq_bfqq_has_short_ttime(bfqq);\n\tbfqq_data->saved_IO_bound = bfq_bfqq_IO_bound(bfqq);\n\tbfqq_data->saved_io_start_time = bfqq->io_start_time;\n\tbfqq_data->saved_tot_idle_time = bfqq->tot_idle_time;\n\tbfqq_data->saved_in_large_burst = bfq_bfqq_in_large_burst(bfqq);\n\tbfqq_data->was_in_burst_list =\n\t\t!hlist_unhashed(&bfqq->burst_list_node);\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t     bfqq->bfqd->low_latency)) {\n\t\t/*\n\t\t * bfqq being merged right after being created: bfqq\n\t\t * would have deserved interactive weight raising, but\n\t\t * did not make it to be set in a weight-raised state,\n\t\t * because of this early merge.\tStore directly the\n\t\t * weight-raising state that would have been assigned\n\t\t * to bfqq, so that to avoid that bfqq unjustly fails\n\t\t * to enjoy weight raising if split soon.\n\t\t */\n\t\tbfqq_data->saved_wr_coeff = bfqq->bfqd->bfq_wr_coeff;\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt =\n\t\t\tbfq_smallest_from_now();\n\t\tbfqq_data->saved_wr_cur_max_time =\n\t\t\tbfq_wr_duration(bfqq->bfqd);\n\t\tbfqq_data->saved_last_wr_start_finish = jiffies;\n\t} else {\n\t\tbfqq_data->saved_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tbfqq_data->saved_service_from_wr =\n\t\t\tbfqq->service_from_wr;\n\t\tbfqq_data->saved_last_wr_start_finish =\n\t\t\tbfqq->last_wr_start_finish;\n\t\tbfqq_data->saved_wr_cur_max_time = bfqq->wr_cur_max_time;\n\t}\n}\n\n\nvoid bfq_reassign_last_bfqq(struct bfq_queue *cur_bfqq,\n\t\t\t    struct bfq_queue *new_bfqq)\n{\n\tif (cur_bfqq->entity.parent &&\n\t    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->entity.parent->last_bfqq_created = new_bfqq;\n\telse if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->bfqd->last_bfqq_created = new_bfqq;\n}\n\nvoid bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * To prevent bfqq's service guarantees from being violated,\n\t * bfqq may be left busy, i.e., queued for service, even if\n\t * empty (see comments in __bfq_bfqq_expire() for\n\t * details). But, if no process will send requests to bfqq any\n\t * longer, then there is no point in keeping bfqq queued for\n\t * service. In addition, keeping bfqq queued for service, but\n\t * with no process ref any longer, may have caused bfqq to be\n\t * freed when dequeued from service. But this is assumed to\n\t * never happen.\n\t */\n\tif (bfq_bfqq_busy(bfqq) && RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq != bfqd->in_service_queue)\n\t\tbfq_del_bfqq_busy(bfqq, false);\n\n\tbfq_reassign_last_bfqq(bfqq, NULL);\n\n\tbfq_put_queue(bfqq);\n}\n\nstatic struct bfq_queue *bfq_merge_bfqqs(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_io_cq *bic,\n\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"merging with queue %lu\",\n\t\t(unsigned long)new_bfqq->pid);\n\t/* Save weight raising and idle window of the merged queues */\n\tbfq_bfqq_save_state(bfqq);\n\tbfq_bfqq_save_state(new_bfqq);\n\tif (bfq_bfqq_IO_bound(bfqq))\n\t\tbfq_mark_bfqq_IO_bound(new_bfqq);\n\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * The processes associated with bfqq are cooperators of the\n\t * processes associated with new_bfqq. So, if bfqq has a\n\t * waker, then assume that all these processes will be happy\n\t * to let bfqq's waker freely inject I/O when they have no\n\t * I/O.\n\t */\n\tif (bfqq->waker_bfqq && !new_bfqq->waker_bfqq &&\n\t    bfqq->waker_bfqq != new_bfqq) {\n\t\tnew_bfqq->waker_bfqq = bfqq->waker_bfqq;\n\t\tnew_bfqq->tentative_waker_bfqq = NULL;\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * new_bfqq->waker_bfqq must be reset. So insert\n\t\t * new_bfqq into the woken_list of the waker. See\n\t\t * bfq_check_waker for details.\n\t\t */\n\t\thlist_add_head(&new_bfqq->woken_list_node,\n\t\t\t       &new_bfqq->waker_bfqq->woken_list);\n\n\t}\n\n\t/*\n\t * If bfqq is weight-raised, then let new_bfqq inherit\n\t * weight-raising. To reduce false positives, neglect the case\n\t * where bfqq has just been created, but has not yet made it\n\t * to be weight-raised (which may happen because EQM may merge\n\t * bfqq even before bfq_add_request is executed for the first\n\t * time for bfqq). Handling this case would however be very\n\t * easy, thanks to the flag just_created.\n\t */\n\tif (new_bfqq->wr_coeff == 1 && bfqq->wr_coeff > 1) {\n\t\tnew_bfqq->wr_coeff = bfqq->wr_coeff;\n\t\tnew_bfqq->wr_cur_max_time = bfqq->wr_cur_max_time;\n\t\tnew_bfqq->last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tnew_bfqq->wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tif (bfq_bfqq_busy(new_bfqq))\n\t\t\tbfqd->wr_busy_queues++;\n\t\tnew_bfqq->entity.prio_changed = 1;\n\t}\n\n\tif (bfqq->wr_coeff > 1) { /* bfqq has given its wr to new_bfqq */\n\t\tbfqq->wr_coeff = 1;\n\t\tbfqq->entity.prio_changed = 1;\n\t\tif (bfq_bfqq_busy(bfqq))\n\t\t\tbfqd->wr_busy_queues--;\n\t}\n\n\tbfq_log_bfqq(bfqd, new_bfqq, \"merge_bfqqs: wr_busy %d\",\n\t\t     bfqd->wr_busy_queues);\n\n\t/*\n\t * Merge queues (that is, let bic redirect its requests to new_bfqq)\n\t */\n\tbic_set_bfqq(bic, new_bfqq, true, bfqq->actuator_idx);\n\tbfq_mark_bfqq_coop(new_bfqq);\n\t/*\n\t * new_bfqq now belongs to at least two bics (it is a shared queue):\n\t * set new_bfqq->bic to NULL. bfqq either:\n\t * - does not belong to any bic any more, and hence bfqq->bic must\n\t *   be set to NULL, or\n\t * - is a queue whose owning bics have already been redirected to a\n\t *   different queue, hence the queue is destined to not belong to\n\t *   any bic soon and bfqq->bic is already NULL (therefore the next\n\t *   assignment causes no harm).\n\t */\n\tnew_bfqq->bic = NULL;\n\t/*\n\t * If the queue is shared, the pid is the pid of one of the associated\n\t * processes. Which pid depends on the exact sequence of merge events\n\t * the queue underwent. So printing such a pid is useless and confusing\n\t * because it reports a random pid between those of the associated\n\t * processes.\n\t * We mark such a queue with a pid -1, and then print SHARED instead of\n\t * a pid in logging messages.\n\t */\n\tnew_bfqq->pid = -1;\n\tbfqq->bic = NULL;\n\n\tbfq_reassign_last_bfqq(bfqq, new_bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n\n\treturn new_bfqq;\n}\n\nstatic bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tbool is_sync = op_is_sync(bio->bi_opf);\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq, *new_bfqq;\n\n\t/*\n\t * Disallow merge of a sync bio into an async request.\n\t */\n\tif (is_sync && !rq_is_sync(rq))\n\t\treturn false;\n\n\t/*\n\t * Lookup the bfqq that this bio will be queued with. Allow\n\t * merge only if rq is queued there.\n\t */\n\tif (!bfqq)\n\t\treturn false;\n\n\t/*\n\t * We take advantage of this function to perform an early merge\n\t * of the queues of possible cooperating processes.\n\t */\n\tnew_bfqq = bfq_setup_cooperator(bfqd, bfqq, bio, false, bfqd->bio_bic);\n\tif (new_bfqq) {\n\t\t/*\n\t\t * bic still points to bfqq, then it has not yet been\n\t\t * redirected to some other bfq_queue, and a queue\n\t\t * merge between bfqq and new_bfqq can be safely\n\t\t * fulfilled, i.e., bic can be redirected to new_bfqq\n\t\t * and bfqq can be put.\n\t\t */\n\t\twhile (bfqq != new_bfqq)\n\t\t\tbfqq = bfq_merge_bfqqs(bfqd, bfqd->bio_bic, bfqq);\n\n\t\t/*\n\t\t * Change also bqfd->bio_bfqq, as\n\t\t * bfqd->bio_bic now points to new_bfqq, and\n\t\t * this function may be invoked again (and then may\n\t\t * use again bqfd->bio_bfqq).\n\t\t */\n\t\tbfqd->bio_bfqq = bfqq;\n\t}\n\n\treturn bfqq == RQ_BFQQ(rq);\n}\n\n/*\n * Set the maximum time for the in-service queue to consume its\n * budget. This prevents seeky processes from lowering the throughput.\n * In practice, a time-slice service scheme is used with seeky\n * processes.\n */\nstatic void bfq_set_budget_timeout(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tunsigned int timeout_coeff;\n\n\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time)\n\t\ttimeout_coeff = 1;\n\telse\n\t\ttimeout_coeff = bfqq->entity.weight / bfqq->entity.orig_weight;\n\n\tbfqd->last_budget_start = blk_time_get();\n\n\tbfqq->budget_timeout = jiffies +\n\t\tbfqd->bfq_timeout * timeout_coeff;\n}\n\nstatic void __bfq_set_in_service_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq)\n{\n\tif (bfqq) {\n\t\tbfq_clear_bfqq_fifo_expire(bfqq);\n\n\t\tbfqd->budgets_assigned = (bfqd->budgets_assigned * 7 + 256) / 8;\n\n\t\tif (time_is_before_jiffies(bfqq->last_wr_start_finish) &&\n\t\t    bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    time_is_before_jiffies(bfqq->budget_timeout)) {\n\t\t\t/*\n\t\t\t * For soft real-time queues, move the start\n\t\t\t * of the weight-raising period forward by the\n\t\t\t * time the queue has not received any\n\t\t\t * service. Otherwise, a relatively long\n\t\t\t * service delay is likely to cause the\n\t\t\t * weight-raising period of the queue to end,\n\t\t\t * because of the short duration of the\n\t\t\t * weight-raising period of a soft real-time\n\t\t\t * queue.  It is worth noting that this move\n\t\t\t * is not so dangerous for the other queues,\n\t\t\t * because soft real-time queues are not\n\t\t\t * greedy.\n\t\t\t *\n\t\t\t * To not add a further variable, we use the\n\t\t\t * overloaded field budget_timeout to\n\t\t\t * determine for how long the queue has not\n\t\t\t * received service, i.e., how much time has\n\t\t\t * elapsed since the queue expired. However,\n\t\t\t * this is a little imprecise, because\n\t\t\t * budget_timeout is set to jiffies if bfqq\n\t\t\t * not only expires, but also remains with no\n\t\t\t * request.\n\t\t\t */\n\t\t\tif (time_after(bfqq->budget_timeout,\n\t\t\t\t       bfqq->last_wr_start_finish))\n\t\t\t\tbfqq->last_wr_start_finish +=\n\t\t\t\t\tjiffies - bfqq->budget_timeout;\n\t\t\telse\n\t\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\n\t\tbfq_set_budget_timeout(bfqd, bfqq);\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t     \"set_in_service_queue, cur-budget = %d\",\n\t\t\t     bfqq->entity.budget);\n\t}\n\n\tbfqd->in_service_queue = bfqq;\n\tbfqd->in_serv_last_pos = 0;\n}\n\n/*\n * Get and set a new queue for service.\n */\nstatic struct bfq_queue *bfq_set_in_service_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfq_get_next_queue(bfqd);\n\n\t__bfq_set_in_service_queue(bfqd, bfqq);\n\treturn bfqq;\n}\n\nstatic void bfq_arm_slice_timer(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\tu32 sl;\n\n\tbfq_mark_bfqq_wait_request(bfqq);\n\n\t/*\n\t * We don't want to idle for seeks, but we do want to allow\n\t * fair distribution of slice time for a process doing back-to-back\n\t * seeks. So allow a little bit of time for him to submit a new rq.\n\t */\n\tsl = bfqd->bfq_slice_idle;\n\t/*\n\t * Unless the queue is being weight-raised or the scenario is\n\t * asymmetric, grant only minimum idle time if the queue\n\t * is seeky. A long idling is preserved for a weight-raised\n\t * queue, or, more in general, in an asymmetric scenario,\n\t * because a long idling is needed for guaranteeing to a queue\n\t * its reserved share of the throughput (in particular, it is\n\t * needed if the queue has a higher weight than some other\n\t * queue).\n\t */\n\tif (BFQQ_SEEKY(bfqq) && bfqq->wr_coeff == 1 &&\n\t    !bfq_asymmetric_scenario(bfqd, bfqq))\n\t\tsl = min_t(u64, sl, BFQ_MIN_TT);\n\telse if (bfqq->wr_coeff > 1)\n\t\tsl = max_t(u32, sl, 20ULL * NSEC_PER_MSEC);\n\n\tbfqd->last_idling_start = blk_time_get();\n\tbfqd->last_idling_start_jiffies = jiffies;\n\n\thrtimer_start(&bfqd->idle_slice_timer, ns_to_ktime(sl),\n\t\t      HRTIMER_MODE_REL);\n\tbfqg_stats_set_start_idle_time(bfqq_group(bfqq));\n}\n\n/*\n * In autotuning mode, max_budget is dynamically recomputed as the\n * amount of sectors transferred in timeout at the estimated peak\n * rate. This enables BFQ to utilize a full timeslice with a full\n * budget, even if the in-service queue is served at peak rate. And\n * this maximises throughput with sequential workloads.\n */\nstatic unsigned long bfq_calc_max_budget(struct bfq_data *bfqd)\n{\n\treturn (u64)bfqd->peak_rate * USEC_PER_MSEC *\n\t\tjiffies_to_msecs(bfqd->bfq_timeout)>>BFQ_RATE_SHIFT;\n}\n\n/*\n * Update parameters related to throughput and responsiveness, as a\n * function of the estimated peak rate. See comments on\n * bfq_calc_max_budget(), and on the ref_wr_duration array.\n */\nstatic void update_thr_responsiveness_params(struct bfq_data *bfqd)\n{\n\tif (bfqd->bfq_user_max_budget == 0) {\n\t\tbfqd->bfq_max_budget =\n\t\t\tbfq_calc_max_budget(bfqd);\n\t\tbfq_log(bfqd, \"new max_budget = %d\", bfqd->bfq_max_budget);\n\t}\n}\n\nstatic void bfq_reset_rate_computation(struct bfq_data *bfqd,\n\t\t\t\t       struct request *rq)\n{\n\tif (rq != NULL) { /* new rq dispatch now, reset accordingly */\n\t\tbfqd->last_dispatch = bfqd->first_dispatch = blk_time_get_ns();\n\t\tbfqd->peak_rate_samples = 1;\n\t\tbfqd->sequential_samples = 0;\n\t\tbfqd->tot_sectors_dispatched = bfqd->last_rq_max_size =\n\t\t\tblk_rq_sectors(rq);\n\t} else /* no new rq dispatched, just reset the number of samples */\n\t\tbfqd->peak_rate_samples = 0; /* full re-init on next disp. */\n\n\tbfq_log(bfqd,\n\t\t\"reset_rate_computation at end, sample %u/%u tot_sects %llu\",\n\t\tbfqd->peak_rate_samples, bfqd->sequential_samples,\n\t\tbfqd->tot_sectors_dispatched);\n}\n\nstatic void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)\n{\n\tu32 rate, weight, divisor;\n\n\t/*\n\t * For the convergence property to hold (see comments on\n\t * bfq_update_peak_rate()) and for the assessment to be\n\t * reliable, a minimum number of samples must be present, and\n\t * a minimum amount of time must have elapsed. If not so, do\n\t * not compute new rate. Just reset parameters, to get ready\n\t * for a new evaluation attempt.\n\t */\n\tif (bfqd->peak_rate_samples < BFQ_RATE_MIN_SAMPLES ||\n\t    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL)\n\t\tgoto reset_computation;\n\n\t/*\n\t * If a new request completion has occurred after last\n\t * dispatch, then, to approximate the rate at which requests\n\t * have been served by the device, it is more precise to\n\t * extend the observation interval to the last completion.\n\t */\n\tbfqd->delta_from_first =\n\t\tmax_t(u64, bfqd->delta_from_first,\n\t\t      bfqd->last_completion - bfqd->first_dispatch);\n\n\t/*\n\t * Rate computed in sects/usec, and not sects/nsec, for\n\t * precision issues.\n\t */\n\trate = div64_ul(bfqd->tot_sectors_dispatched<<BFQ_RATE_SHIFT,\n\t\t\tdiv_u64(bfqd->delta_from_first, NSEC_PER_USEC));\n\n\t/*\n\t * Peak rate not updated if:\n\t * - the percentage of sequential dispatches is below 3/4 of the\n\t *   total, and rate is below the current estimated peak rate\n\t * - rate is unreasonably high (> 20M sectors/sec)\n\t */\n\tif ((bfqd->sequential_samples < (3 * bfqd->peak_rate_samples)>>2 &&\n\t     rate <= bfqd->peak_rate) ||\n\t\trate > 20<<BFQ_RATE_SHIFT)\n\t\tgoto reset_computation;\n\n\t/*\n\t * We have to update the peak rate, at last! To this purpose,\n\t * we use a low-pass filter. We compute the smoothing constant\n\t * of the filter as a function of the 'weight' of the new\n\t * measured rate.\n\t *\n\t * As can be seen in next formulas, we define this weight as a\n\t * quantity proportional to how sequential the workload is,\n\t * and to how long the observation time interval is.\n\t *\n\t * The weight runs from 0 to 8. The maximum value of the\n\t * weight, 8, yields the minimum value for the smoothing\n\t * constant. At this minimum value for the smoothing constant,\n\t * the measured rate contributes for half of the next value of\n\t * the estimated peak rate.\n\t *\n\t * So, the first step is to compute the weight as a function\n\t * of how sequential the workload is. Note that the weight\n\t * cannot reach 9, because bfqd->sequential_samples cannot\n\t * become equal to bfqd->peak_rate_samples, which, in its\n\t * turn, holds true because bfqd->sequential_samples is not\n\t * incremented for the first sample.\n\t */\n\tweight = (9 * bfqd->sequential_samples) / bfqd->peak_rate_samples;\n\n\t/*\n\t * Second step: further refine the weight as a function of the\n\t * duration of the observation interval.\n\t */\n\tweight = min_t(u32, 8,\n\t\t       div_u64(weight * bfqd->delta_from_first,\n\t\t\t       BFQ_RATE_REF_INTERVAL));\n\n\t/*\n\t * Divisor ranging from 10, for minimum weight, to 2, for\n\t * maximum weight.\n\t */\n\tdivisor = 10 - weight;\n\n\t/*\n\t * Finally, update peak rate:\n\t *\n\t * peak_rate = peak_rate * (divisor-1) / divisor  +  rate / divisor\n\t */\n\tbfqd->peak_rate *= divisor-1;\n\tbfqd->peak_rate /= divisor;\n\trate /= divisor; /* smoothing constant alpha = 1/divisor */\n\n\tbfqd->peak_rate += rate;\n\n\t/*\n\t * For a very slow device, bfqd->peak_rate can reach 0 (see\n\t * the minimum representable values reported in the comments\n\t * on BFQ_RATE_SHIFT). Push to 1 if this happens, to avoid\n\t * divisions by zero where bfqd->peak_rate is used as a\n\t * divisor.\n\t */\n\tbfqd->peak_rate = max_t(u32, 1, bfqd->peak_rate);\n\n\tupdate_thr_responsiveness_params(bfqd);\n\nreset_computation:\n\tbfq_reset_rate_computation(bfqd, rq);\n}\n\n/*\n * Update the read/write peak rate (the main quantity used for\n * auto-tuning, see update_thr_responsiveness_params()).\n *\n * It is not trivial to estimate the peak rate (correctly): because of\n * the presence of sw and hw queues between the scheduler and the\n * device components that finally serve I/O requests, it is hard to\n * say exactly when a given dispatched request is served inside the\n * device, and for how long. As a consequence, it is hard to know\n * precisely at what rate a given set of requests is actually served\n * by the device.\n *\n * On the opposite end, the dispatch time of any request is trivially\n * available, and, from this piece of information, the \"dispatch rate\"\n * of requests can be immediately computed. So, the idea in the next\n * function is to use what is known, namely request dispatch times\n * (plus, when useful, request completion times), to estimate what is\n * unknown, namely in-device request service rate.\n *\n * The main issue is that, because of the above facts, the rate at\n * which a certain set of requests is dispatched over a certain time\n * interval can vary greatly with respect to the rate at which the\n * same requests are then served. But, since the size of any\n * intermediate queue is limited, and the service scheme is lossless\n * (no request is silently dropped), the following obvious convergence\n * property holds: the number of requests dispatched MUST become\n * closer and closer to the number of requests completed as the\n * observation interval grows. This is the key property used in\n * the next function to estimate the peak service rate as a function\n * of the observed dispatch rate. The function assumes to be invoked\n * on every request dispatch.\n */\nstatic void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)\n{\n\tu64 now_ns = blk_time_get_ns();\n\n\tif (bfqd->peak_rate_samples == 0) { /* first dispatch */\n\t\tbfq_log(bfqd, \"update_peak_rate: goto reset, samples %d\",\n\t\t\tbfqd->peak_rate_samples);\n\t\tbfq_reset_rate_computation(bfqd, rq);\n\t\tgoto update_last_values; /* will add one sample */\n\t}\n\n\t/*\n\t * Device idle for very long: the observation interval lasting\n\t * up to this dispatch cannot be a valid observation interval\n\t * for computing a new peak rate (similarly to the late-\n\t * completion event in bfq_completed_request()). Go to\n\t * update_rate_and_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - start a new observation interval with this dispatch\n\t */\n\tif (now_ns - bfqd->last_dispatch > 100*NSEC_PER_MSEC &&\n\t    bfqd->tot_rq_in_driver == 0)\n\t\tgoto update_rate_and_reset;\n\n\t/* Update sampling information */\n\tbfqd->peak_rate_samples++;\n\n\tif ((bfqd->tot_rq_in_driver > 0 ||\n\t\tnow_ns - bfqd->last_completion < BFQ_MIN_TT)\n\t    && !BFQ_RQ_SEEKY(bfqd, bfqd->last_position, rq))\n\t\tbfqd->sequential_samples++;\n\n\tbfqd->tot_sectors_dispatched += blk_rq_sectors(rq);\n\n\t/* Reset max observed rq size every 32 dispatches */\n\tif (likely(bfqd->peak_rate_samples % 32))\n\t\tbfqd->last_rq_max_size =\n\t\t\tmax_t(u32, blk_rq_sectors(rq), bfqd->last_rq_max_size);\n\telse\n\t\tbfqd->last_rq_max_size = blk_rq_sectors(rq);\n\n\tbfqd->delta_from_first = now_ns - bfqd->first_dispatch;\n\n\t/* Target observation interval not yet reached, go on sampling */\n\tif (bfqd->delta_from_first < BFQ_RATE_REF_INTERVAL)\n\t\tgoto update_last_values;\n\nupdate_rate_and_reset:\n\tbfq_update_rate_reset(bfqd, rq);\nupdate_last_values:\n\tbfqd->last_position = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\tif (RQ_BFQQ(rq) == bfqd->in_service_queue)\n\t\tbfqd->in_serv_last_pos = bfqd->last_position;\n\tbfqd->last_dispatch = now_ns;\n}\n\n/*\n * Remove request from internal lists.\n */\nstatic void bfq_dispatch_remove(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\n\t/*\n\t * For consistency, the next instruction should have been\n\t * executed after removing the request from the queue and\n\t * dispatching it.  We execute instead this instruction before\n\t * bfq_remove_request() (and hence introduce a temporary\n\t * inconsistency), for efficiency.  In fact, should this\n\t * dispatch occur for a non in-service bfqq, this anticipated\n\t * increment prevents two counters related to bfqq->dispatched\n\t * from risking to be, first, uselessly decremented, and then\n\t * incremented again when the (new) value of bfqq->dispatched\n\t * happens to be taken into account.\n\t */\n\tbfqq->dispatched++;\n\tbfq_update_peak_rate(q->elevator->elevator_data, rq);\n\n\tbfq_remove_request(q, rq);\n}\n\n/*\n * There is a case where idling does not have to be performed for\n * throughput concerns, but to preserve the throughput share of\n * the process associated with bfqq.\n *\n * To introduce this case, we can note that allowing the drive\n * to enqueue more than one request at a time, and hence\n * delegating de facto final scheduling decisions to the\n * drive's internal scheduler, entails loss of control on the\n * actual request service order. In particular, the critical\n * situation is when requests from different processes happen\n * to be present, at the same time, in the internal queue(s)\n * of the drive. In such a situation, the drive, by deciding\n * the service order of the internally-queued requests, does\n * determine also the actual throughput distribution among\n * these processes. But the drive typically has no notion or\n * concern about per-process throughput distribution, and\n * makes its decisions only on a per-request basis. Therefore,\n * the service distribution enforced by the drive's internal\n * scheduler is likely to coincide with the desired throughput\n * distribution only in a completely symmetric, or favorably\n * skewed scenario where:\n * (i-a) each of these processes must get the same throughput as\n *\t the others,\n * (i-b) in case (i-a) does not hold, it holds that the process\n *       associated with bfqq must receive a lower or equal\n *\t throughput than any of the other processes;\n * (ii)  the I/O of each process has the same properties, in\n *       terms of locality (sequential or random), direction\n *       (reads or writes), request sizes, greediness\n *       (from I/O-bound to sporadic), and so on;\n\n * In fact, in such a scenario, the drive tends to treat the requests\n * of each process in about the same way as the requests of the\n * others, and thus to provide each of these processes with about the\n * same throughput.  This is exactly the desired throughput\n * distribution if (i-a) holds, or, if (i-b) holds instead, this is an\n * even more convenient distribution for (the process associated with)\n * bfqq.\n *\n * In contrast, in any asymmetric or unfavorable scenario, device\n * idling (I/O-dispatch plugging) is certainly needed to guarantee\n * that bfqq receives its assigned fraction of the device throughput\n * (see [1] for details).\n *\n * The problem is that idling may significantly reduce throughput with\n * certain combinations of types of I/O and devices. An important\n * example is sync random I/O on flash storage with command\n * queueing. So, unless bfqq falls in cases where idling also boosts\n * throughput, it is important to check conditions (i-a), i(-b) and\n * (ii) accurately, so as to avoid idling when not strictly needed for\n * service guarantees.\n *\n * Unfortunately, it is extremely difficult to thoroughly check\n * condition (ii). And, in case there are active groups, it becomes\n * very difficult to check conditions (i-a) and (i-b) too.  In fact,\n * if there are active groups, then, for conditions (i-a) or (i-b) to\n * become false 'indirectly', it is enough that an active group\n * contains more active processes or sub-groups than some other active\n * group. More precisely, for conditions (i-a) or (i-b) to become\n * false because of such a group, it is not even necessary that the\n * group is (still) active: it is sufficient that, even if the group\n * has become inactive, some of its descendant processes still have\n * some request already dispatched but still waiting for\n * completion. In fact, requests have still to be guaranteed their\n * share of the throughput even after being dispatched. In this\n * respect, it is easy to show that, if a group frequently becomes\n * inactive while still having in-flight requests, and if, when this\n * happens, the group is not considered in the calculation of whether\n * the scenario is asymmetric, then the group may fail to be\n * guaranteed its fair share of the throughput (basically because\n * idling may not be performed for the descendant processes of the\n * group, but it had to be).  We address this issue with the following\n * bi-modal behavior, implemented in the function\n * bfq_asymmetric_scenario().\n *\n * If there are groups with requests waiting for completion\n * (as commented above, some of these groups may even be\n * already inactive), then the scenario is tagged as\n * asymmetric, conservatively, without checking any of the\n * conditions (i-a), (i-b) or (ii). So the device is idled for bfqq.\n * This behavior matches also the fact that groups are created\n * exactly if controlling I/O is a primary concern (to\n * preserve bandwidth and latency guarantees).\n *\n * On the opposite end, if there are no groups with requests waiting\n * for completion, then only conditions (i-a) and (i-b) are actually\n * controlled, i.e., provided that conditions (i-a) or (i-b) holds,\n * idling is not performed, regardless of whether condition (ii)\n * holds.  In other words, only if conditions (i-a) and (i-b) do not\n * hold, then idling is allowed, and the device tends to be prevented\n * from queueing many requests, possibly of several processes. Since\n * there are no groups with requests waiting for completion, then, to\n * control conditions (i-a) and (i-b) it is enough to check just\n * whether all the queues with requests waiting for completion also\n * have the same weight.\n *\n * Not checking condition (ii) evidently exposes bfqq to the\n * risk of getting less throughput than its fair share.\n * However, for queues with the same weight, a further\n * mechanism, preemption, mitigates or even eliminates this\n * problem. And it does so without consequences on overall\n * throughput. This mechanism and its benefits are explained\n * in the next three paragraphs.\n *\n * Even if a queue, say Q, is expired when it remains idle, Q\n * can still preempt the new in-service queue if the next\n * request of Q arrives soon (see the comments on\n * bfq_bfqq_update_budg_for_activation). If all queues and\n * groups have the same weight, this form of preemption,\n * combined with the hole-recovery heuristic described in the\n * comments on function bfq_bfqq_update_budg_for_activation,\n * are enough to preserve a correct bandwidth distribution in\n * the mid term, even without idling. In fact, even if not\n * idling allows the internal queues of the device to contain\n * many requests, and thus to reorder requests, we can rather\n * safely assume that the internal scheduler still preserves a\n * minimum of mid-term fairness.\n *\n * More precisely, this preemption-based, idleless approach\n * provides fairness in terms of IOPS, and not sectors per\n * second. This can be seen with a simple example. Suppose\n * that there are two queues with the same weight, but that\n * the first queue receives requests of 8 sectors, while the\n * second queue receives requests of 1024 sectors. In\n * addition, suppose that each of the two queues contains at\n * most one request at a time, which implies that each queue\n * always remains idle after it is served. Finally, after\n * remaining idle, each queue receives very quickly a new\n * request. It follows that the two queues are served\n * alternatively, preempting each other if needed. This\n * implies that, although both queues have the same weight,\n * the queue with large requests receives a service that is\n * 1024/8 times as high as the service received by the other\n * queue.\n *\n * The motivation for using preemption instead of idling (for\n * queues with the same weight) is that, by not idling,\n * service guarantees are preserved (completely or at least in\n * part) without minimally sacrificing throughput. And, if\n * there is no active group, then the primary expectation for\n * this device is probably a high throughput.\n *\n * We are now left only with explaining the two sub-conditions in the\n * additional compound condition that is checked below for deciding\n * whether the scenario is asymmetric. To explain the first\n * sub-condition, we need to add that the function\n * bfq_asymmetric_scenario checks the weights of only\n * non-weight-raised queues, for efficiency reasons (see comments on\n * bfq_weights_tree_add()). Then the fact that bfqq is weight-raised\n * is checked explicitly here. More precisely, the compound condition\n * below takes into account also the fact that, even if bfqq is being\n * weight-raised, the scenario is still symmetric if all queues with\n * requests waiting for completion happen to be\n * weight-raised. Actually, we should be even more precise here, and\n * differentiate between interactive weight raising and soft real-time\n * weight raising.\n *\n * The second sub-condition checked in the compound condition is\n * whether there is a fair amount of already in-flight I/O not\n * belonging to bfqq. If so, I/O dispatching is to be plugged, for the\n * following reason. The drive may decide to serve in-flight\n * non-bfqq's I/O requests before bfqq's ones, thereby delaying the\n * arrival of new I/O requests for bfqq (recall that bfqq is sync). If\n * I/O-dispatching is not plugged, then, while bfqq remains empty, a\n * basically uncontrolled amount of I/O from other queues may be\n * dispatched too, possibly causing the service of bfqq's I/O to be\n * delayed even longer in the drive. This problem gets more and more\n * serious as the speed and the queue depth of the drive grow,\n * because, as these two quantities grow, the probability to find no\n * queue busy but many requests in flight grows too. By contrast,\n * plugging I/O dispatching minimizes the delay induced by already\n * in-flight I/O, and enables bfqq to recover the bandwidth it may\n * lose because of this delay.\n *\n * As a side note, it is worth considering that the above\n * device-idling countermeasures may however fail in the following\n * unlucky scenario: if I/O-dispatch plugging is (correctly) disabled\n * in a time period during which all symmetry sub-conditions hold, and\n * therefore the device is allowed to enqueue many requests, but at\n * some later point in time some sub-condition stops to hold, then it\n * may become impossible to make requests be served in the desired\n * order until all the requests already queued in the device have been\n * served. The last sub-condition commented above somewhat mitigates\n * this problem for weight-raised queues.\n *\n * However, as an additional mitigation for this problem, we preserve\n * plugging for a special symmetric case that may suddenly turn into\n * asymmetric: the case where only bfqq is busy. In this case, not\n * expiring bfqq does not cause any harm to any other queues in terms\n * of service guarantees. In contrast, it avoids the following unlucky\n * sequence of events: (1) bfqq is expired, (2) a new queue with a\n * lower weight than bfqq becomes busy (or more queues), (3) the new\n * queue is served until a new request arrives for bfqq, (4) when bfqq\n * is finally served, there are so many requests of the new queue in\n * the drive that the pending requests for bfqq take a lot of time to\n * be served. In particular, event (2) may case even already\n * dispatched requests of bfqq to be delayed, inside the drive. So, to\n * avoid this series of events, the scenario is preventively declared\n * as asymmetric also if bfqq is the only busy queues\n */\nstatic bool idling_needed_for_service_guarantees(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tint tot_busy_queues = bfq_tot_busy_queues(bfqd);\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\treturn (bfqq->wr_coeff > 1 &&\n\t\t(bfqd->wr_busy_queues < tot_busy_queues ||\n\t\t bfqd->tot_rq_in_driver >= bfqq->dispatched + 4)) ||\n\t\tbfq_asymmetric_scenario(bfqd, bfqq) ||\n\t\ttot_busy_queues == 1;\n}\n\nstatic bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t      enum bfqq_expiration reason)\n{\n\t/*\n\t * If this bfqq is shared between multiple processes, check\n\t * to make sure that those processes are still issuing I/Os\n\t * within the mean seek distance. If not, it may be time to\n\t * break the queues apart again.\n\t */\n\tif (bfq_bfqq_coop(bfqq) && BFQQ_SEEKY(bfqq))\n\t\tbfq_mark_bfqq_split_coop(bfqq);\n\n\t/*\n\t * Consider queues with a higher finish virtual time than\n\t * bfqq. If idling_needed_for_service_guarantees(bfqq) returns\n\t * true, then bfqq's bandwidth would be violated if an\n\t * uncontrolled amount of I/O from these queues were\n\t * dispatched while bfqq is waiting for its new I/O to\n\t * arrive. This is exactly what may happen if this is a forced\n\t * expiration caused by a preemption attempt, and if bfqq is\n\t * not re-scheduled. To prevent this from happening, re-queue\n\t * bfqq if it needs I/O-dispatch plugging, even if it is\n\t * empty. By doing so, bfqq is granted to be served before the\n\t * above queues (provided that bfqq is of course eligible).\n\t */\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    !(reason == BFQQE_PREEMPTED &&\n\t      idling_needed_for_service_guarantees(bfqd, bfqq))) {\n\t\tif (bfqq->dispatched == 0)\n\t\t\t/*\n\t\t\t * Overloading budget_timeout field to store\n\t\t\t * the time at which the queue remains with no\n\t\t\t * backlog and no outstanding request; used by\n\t\t\t * the weight-raising mechanism.\n\t\t\t */\n\t\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_busy(bfqq, true);\n\t} else {\n\t\tbfq_requeue_bfqq(bfqd, bfqq, true);\n\t\t/*\n\t\t * Resort priority tree of potential close cooperators.\n\t\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t\t */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing &&\n\t\t\t     !RB_EMPTY_ROOT(&bfqq->sort_list)))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\t/*\n\t * All in-service entities must have been properly deactivated\n\t * or requeued before executing the next function, which\n\t * resets all in-service entities as no more in service. This\n\t * may cause bfqq to be freed. If this happens, the next\n\t * function returns true.\n\t */\n\treturn __bfq_bfqd_reset_in_service(bfqd);\n}\n\n/**\n * __bfq_bfqq_recalc_budget - try to adapt the budget to the @bfqq behavior.\n * @bfqd: device data.\n * @bfqq: queue to update.\n * @reason: reason for expiration.\n *\n * Handle the feedback on @bfqq budget at queue expiration.\n * See the body for detailed comments.\n */\nstatic void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,\n\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t     enum bfqq_expiration reason)\n{\n\tstruct request *next_rq;\n\tint budget, min_budget;\n\n\tmin_budget = bfq_min_budget(bfqd);\n\n\tif (bfqq->wr_coeff == 1)\n\t\tbudget = bfqq->max_budget;\n\telse /*\n\t      * Use a constant, low budget for weight-raised queues,\n\t      * to help achieve a low latency. Keep it slightly higher\n\t      * than the minimum possible budget, to cause a little\n\t      * bit fewer expirations.\n\t      */\n\t\tbudget = 2 * min_budget;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last budg %d, budg left %d\",\n\t\tbfqq->entity.budget, bfq_bfqq_budget_left(bfqq));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last max_budg %d, min budg %d\",\n\t\tbudget, bfq_min_budget(bfqd));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: sync %d, seeky %d\",\n\t\tbfq_bfqq_sync(bfqq), BFQQ_SEEKY(bfqd->in_service_queue));\n\n\tif (bfq_bfqq_sync(bfqq) && bfqq->wr_coeff == 1) {\n\t\tswitch (reason) {\n\t\t/*\n\t\t * Caveat: in all the following cases we trade latency\n\t\t * for throughput.\n\t\t */\n\t\tcase BFQQE_TOO_IDLE:\n\t\t\t/*\n\t\t\t * This is the only case where we may reduce\n\t\t\t * the budget: if there is no request of the\n\t\t\t * process still waiting for completion, then\n\t\t\t * we assume (tentatively) that the timer has\n\t\t\t * expired because the batch of requests of\n\t\t\t * the process could have been served with a\n\t\t\t * smaller budget.  Hence, betting that\n\t\t\t * process will behave in the same way when it\n\t\t\t * becomes backlogged again, we reduce its\n\t\t\t * next budget.  As long as we guess right,\n\t\t\t * this budget cut reduces the latency\n\t\t\t * experienced by the process.\n\t\t\t *\n\t\t\t * However, if there are still outstanding\n\t\t\t * requests, then the process may have not yet\n\t\t\t * issued its next request just because it is\n\t\t\t * still waiting for the completion of some of\n\t\t\t * the still outstanding ones.  So in this\n\t\t\t * subcase we do not reduce its budget, on the\n\t\t\t * contrary we increase it to possibly boost\n\t\t\t * the throughput, as discussed in the\n\t\t\t * comments to the BUDGET_TIMEOUT case.\n\t\t\t */\n\t\t\tif (bfqq->dispatched > 0) /* still outstanding reqs */\n\t\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\telse {\n\t\t\t\tif (budget > 5 * min_budget)\n\t\t\t\t\tbudget -= 4 * min_budget;\n\t\t\t\telse\n\t\t\t\t\tbudget = min_budget;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_TIMEOUT:\n\t\t\t/*\n\t\t\t * We double the budget here because it gives\n\t\t\t * the chance to boost the throughput if this\n\t\t\t * is not a seeky process (and has bumped into\n\t\t\t * this timeout because of, e.g., ZBR).\n\t\t\t */\n\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_EXHAUSTED:\n\t\t\t/*\n\t\t\t * The process still has backlog, and did not\n\t\t\t * let either the budget timeout or the disk\n\t\t\t * idling timeout expire. Hence it is not\n\t\t\t * seeky, has a short thinktime and may be\n\t\t\t * happy with a higher budget too. So\n\t\t\t * definitely increase the budget of this good\n\t\t\t * candidate to boost the disk throughput.\n\t\t\t */\n\t\t\tbudget = min(budget * 4, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_NO_MORE_REQUESTS:\n\t\t\t/*\n\t\t\t * For queues that expire for this reason, it\n\t\t\t * is particularly important to keep the\n\t\t\t * budget close to the actual service they\n\t\t\t * need. Doing so reduces the timestamp\n\t\t\t * misalignment problem described in the\n\t\t\t * comments in the body of\n\t\t\t * __bfq_activate_entity. In fact, suppose\n\t\t\t * that a queue systematically expires for\n\t\t\t * BFQQE_NO_MORE_REQUESTS and presents a\n\t\t\t * new request in time to enjoy timestamp\n\t\t\t * back-shifting. The larger the budget of the\n\t\t\t * queue is with respect to the service the\n\t\t\t * queue actually requests in each service\n\t\t\t * slot, the more times the queue can be\n\t\t\t * reactivated with the same virtual finish\n\t\t\t * time. It follows that, even if this finish\n\t\t\t * time is pushed to the system virtual time\n\t\t\t * to reduce the consequent timestamp\n\t\t\t * misalignment, the queue unjustly enjoys for\n\t\t\t * many re-activations a lower finish time\n\t\t\t * than all newly activated queues.\n\t\t\t *\n\t\t\t * The service needed by bfqq is measured\n\t\t\t * quite precisely by bfqq->entity.service.\n\t\t\t * Since bfqq does not enjoy device idling,\n\t\t\t * bfqq->entity.service is equal to the number\n\t\t\t * of sectors that the process associated with\n\t\t\t * bfqq requested to read/write before waiting\n\t\t\t * for request completions, or blocking for\n\t\t\t * other reasons.\n\t\t\t */\n\t\t\tbudget = max_t(int, bfqq->entity.service, min_budget);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else if (!bfq_bfqq_sync(bfqq)) {\n\t\t/*\n\t\t * Async queues get always the maximum possible\n\t\t * budget, as for them we do not care about latency\n\t\t * (in addition, their ability to dispatch is limited\n\t\t * by the charging factor).\n\t\t */\n\t\tbudget = bfqd->bfq_max_budget;\n\t}\n\n\tbfqq->max_budget = budget;\n\n\tif (bfqd->budgets_assigned >= bfq_stats_min_budgets &&\n\t    !bfqd->bfq_user_max_budget)\n\t\tbfqq->max_budget = min(bfqq->max_budget, bfqd->bfq_max_budget);\n\n\t/*\n\t * If there is still backlog, then assign a new budget, making\n\t * sure that it is large enough for the next request.  Since\n\t * the finish time of bfqq must be kept in sync with the\n\t * budget, be sure to call __bfq_bfqq_expire() *after* this\n\t * update.\n\t *\n\t * If there is no backlog, then no need to update the budget;\n\t * it will be updated on the arrival of a new request.\n\t */\n\tnext_rq = bfqq->next_rq;\n\tif (next_rq)\n\t\tbfqq->entity.budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t\t    bfq_serv_to_charge(next_rq, bfqq));\n\n\tbfq_log_bfqq(bfqd, bfqq, \"head sect: %u, new budget %d\",\n\t\t\tnext_rq ? blk_rq_sectors(next_rq) : 0,\n\t\t\tbfqq->entity.budget);\n}\n\n/*\n * Return true if the process associated with bfqq is \"slow\". The slow\n * flag is used, in addition to the budget timeout, to reduce the\n * amount of service provided to seeky processes, and thus reduce\n * their chances to lower the throughput. More details in the comments\n * on the function bfq_bfqq_expire().\n *\n * An important observation is in order: as discussed in the comments\n * on the function bfq_update_peak_rate(), with devices with internal\n * queues, it is hard if ever possible to know when and for how long\n * an I/O request is processed by the device (apart from the trivial\n * I/O pattern where a new request is dispatched only after the\n * previous one has been completed). This makes it hard to evaluate\n * the real rate at which the I/O requests of each bfq_queue are\n * served.  In fact, for an I/O scheduler like BFQ, serving a\n * bfq_queue means just dispatching its requests during its service\n * slot (i.e., until the budget of the queue is exhausted, or the\n * queue remains idle, or, finally, a timeout fires). But, during the\n * service slot of a bfq_queue, around 100 ms at most, the device may\n * be even still processing requests of bfq_queues served in previous\n * service slots. On the opposite end, the requests of the in-service\n * bfq_queue may be completed after the service slot of the queue\n * finishes.\n *\n * Anyway, unless more sophisticated solutions are used\n * (where possible), the sum of the sizes of the requests dispatched\n * during the service slot of a bfq_queue is probably the only\n * approximation available for the service received by the bfq_queue\n * during its service slot. And this sum is the quantity used in this\n * function to evaluate the I/O speed of a process.\n */\nstatic bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t\t bool compensate, unsigned long *delta_ms)\n{\n\tktime_t delta_ktime;\n\tu32 delta_usecs;\n\tbool slow = BFQQ_SEEKY(bfqq); /* if delta too short, use seekyness */\n\n\tif (!bfq_bfqq_sync(bfqq))\n\t\treturn false;\n\n\tif (compensate)\n\t\tdelta_ktime = bfqd->last_idling_start;\n\telse\n\t\tdelta_ktime = blk_time_get();\n\tdelta_ktime = ktime_sub(delta_ktime, bfqd->last_budget_start);\n\tdelta_usecs = ktime_to_us(delta_ktime);\n\n\t/* don't use too short time intervals */\n\tif (delta_usecs < 1000) {\n\t\tif (blk_queue_nonrot(bfqd->queue))\n\t\t\t /*\n\t\t\t  * give same worst-case guarantees as idling\n\t\t\t  * for seeky\n\t\t\t  */\n\t\t\t*delta_ms = BFQ_MIN_TT / NSEC_PER_MSEC;\n\t\telse /* charge at least one seek */\n\t\t\t*delta_ms = bfq_slice_idle / NSEC_PER_MSEC;\n\n\t\treturn slow;\n\t}\n\n\t*delta_ms = delta_usecs / USEC_PER_MSEC;\n\n\t/*\n\t * Use only long (> 20ms) intervals to filter out excessive\n\t * spikes in service rate estimation.\n\t */\n\tif (delta_usecs > 20000) {\n\t\t/*\n\t\t * Caveat for rotational devices: processes doing I/O\n\t\t * in the slower disk zones tend to be slow(er) even\n\t\t * if not seeky. In this respect, the estimated peak\n\t\t * rate is likely to be an average over the disk\n\t\t * surface. Accordingly, to not be too harsh with\n\t\t * unlucky processes, a process is deemed slow only if\n\t\t * its rate has been lower than half of the estimated\n\t\t * peak rate.\n\t\t */\n\t\tslow = bfqq->entity.service < bfqd->bfq_max_budget / 2;\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"bfq_bfqq_is_slow: slow %d\", slow);\n\n\treturn slow;\n}\n\n/*\n * To be deemed as soft real-time, an application must meet two\n * requirements. First, the application must not require an average\n * bandwidth higher than the approximate bandwidth required to playback or\n * record a compressed high-definition video.\n * The next function is invoked on the completion of the last request of a\n * batch, to compute the next-start time instant, soft_rt_next_start, such\n * that, if the next request of the application does not arrive before\n * soft_rt_next_start, then the above requirement on the bandwidth is met.\n *\n * The second requirement is that the request pattern of the application is\n * isochronous, i.e., that, after issuing a request or a batch of requests,\n * the application stops issuing new requests until all its pending requests\n * have been completed. After that, the application may issue a new batch,\n * and so on.\n * For this reason the next function is invoked to compute\n * soft_rt_next_start only for applications that meet this requirement,\n * whereas soft_rt_next_start is set to infinity for applications that do\n * not.\n *\n * Unfortunately, even a greedy (i.e., I/O-bound) application may\n * happen to meet, occasionally or systematically, both the above\n * bandwidth and isochrony requirements. This may happen at least in\n * the following circumstances. First, if the CPU load is high. The\n * application may stop issuing requests while the CPUs are busy\n * serving other processes, then restart, then stop again for a while,\n * and so on. The other circumstances are related to the storage\n * device: the storage device is highly loaded or reaches a low-enough\n * throughput with the I/O of the application (e.g., because the I/O\n * is random and/or the device is slow). In all these cases, the\n * I/O of the application may be simply slowed down enough to meet\n * the bandwidth and isochrony requirements. To reduce the probability\n * that greedy applications are deemed as soft real-time in these\n * corner cases, a further rule is used in the computation of\n * soft_rt_next_start: the return value of this function is forced to\n * be higher than the maximum between the following two quantities.\n *\n * (a) Current time plus: (1) the maximum time for which the arrival\n *     of a request is waited for when a sync queue becomes idle,\n *     namely bfqd->bfq_slice_idle, and (2) a few extra jiffies. We\n *     postpone for a moment the reason for adding a few extra\n *     jiffies; we get back to it after next item (b).  Lower-bounding\n *     the return value of this function with the current time plus\n *     bfqd->bfq_slice_idle tends to filter out greedy applications,\n *     because the latter issue their next request as soon as possible\n *     after the last one has been completed. In contrast, a soft\n *     real-time application spends some time processing data, after a\n *     batch of its requests has been completed.\n *\n * (b) Current value of bfqq->soft_rt_next_start. As pointed out\n *     above, greedy applications may happen to meet both the\n *     bandwidth and isochrony requirements under heavy CPU or\n *     storage-device load. In more detail, in these scenarios, these\n *     applications happen, only for limited time periods, to do I/O\n *     slowly enough to meet all the requirements described so far,\n *     including the filtering in above item (a). These slow-speed\n *     time intervals are usually interspersed between other time\n *     intervals during which these applications do I/O at a very high\n *     speed. Fortunately, exactly because of the high speed of the\n *     I/O in the high-speed intervals, the values returned by this\n *     function happen to be so high, near the end of any such\n *     high-speed interval, to be likely to fall *after* the end of\n *     the low-speed time interval that follows. These high values are\n *     stored in bfqq->soft_rt_next_start after each invocation of\n *     this function. As a consequence, if the last value of\n *     bfqq->soft_rt_next_start is constantly used to lower-bound the\n *     next value that this function may return, then, from the very\n *     beginning of a low-speed interval, bfqq->soft_rt_next_start is\n *     likely to be constantly kept so high that any I/O request\n *     issued during the low-speed interval is considered as arriving\n *     to soon for the application to be deemed as soft\n *     real-time. Then, in the high-speed interval that follows, the\n *     application will not be deemed as soft real-time, just because\n *     it will do I/O at a high speed. And so on.\n *\n * Getting back to the filtering in item (a), in the following two\n * cases this filtering might be easily passed by a greedy\n * application, if the reference quantity was just\n * bfqd->bfq_slice_idle:\n * 1) HZ is so low that the duration of a jiffy is comparable to or\n *    higher than bfqd->bfq_slice_idle. This happens, e.g., on slow\n *    devices with HZ=100. The time granularity may be so coarse\n *    that the approximation, in jiffies, of bfqd->bfq_slice_idle\n *    is rather lower than the exact value.\n * 2) jiffies, instead of increasing at a constant rate, may stop increasing\n *    for a while, then suddenly 'jump' by several units to recover the lost\n *    increments. This seems to happen, e.g., inside virtual machines.\n * To address this issue, in the filtering in (a) we do not use as a\n * reference time interval just bfqd->bfq_slice_idle, but\n * bfqd->bfq_slice_idle plus a few jiffies. In particular, we add the\n * minimum number of jiffies for which the filter seems to be quite\n * precise also in embedded systems and KVM/QEMU virtual machines.\n */\nstatic unsigned long bfq_bfqq_softrt_next_start(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn max3(bfqq->soft_rt_next_start,\n\t\t    bfqq->last_idle_bklogged +\n\t\t    HZ * bfqq->service_from_backlogged /\n\t\t    bfqd->bfq_wr_max_softrt_rate,\n\t\t    jiffies + nsecs_to_jiffies(bfqq->bfqd->bfq_slice_idle) + 4);\n}\n\n/**\n * bfq_bfqq_expire - expire a queue.\n * @bfqd: device owning the queue.\n * @bfqq: the queue to expire.\n * @compensate: if true, compensate for the time spent idling.\n * @reason: the reason causing the expiration.\n *\n * If the process associated with bfqq does slow I/O (e.g., because it\n * issues random requests), we charge bfqq with the time it has been\n * in service instead of the service it has received (see\n * bfq_bfqq_charge_time for details on how this goal is achieved). As\n * a consequence, bfqq will typically get higher timestamps upon\n * reactivation, and hence it will be rescheduled as if it had\n * received more service than what it has actually received. In the\n * end, bfqq receives less service in proportion to how slowly its\n * associated process consumes its budgets (and hence how seriously it\n * tends to lower the throughput). In addition, this time-charging\n * strategy guarantees time fairness among slow processes. In\n * contrast, if the process associated with bfqq is not slow, we\n * charge bfqq exactly with the service it has received.\n *\n * Charging time to the first type of queues and the exact service to\n * the other has the effect of using the WF2Q+ policy to schedule the\n * former on a timeslice basis, without violating service domain\n * guarantees among the latter.\n */\nvoid bfq_bfqq_expire(struct bfq_data *bfqd,\n\t\t     struct bfq_queue *bfqq,\n\t\t     bool compensate,\n\t\t     enum bfqq_expiration reason)\n{\n\tbool slow;\n\tunsigned long delta = 0;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * Check whether the process is slow (see bfq_bfqq_is_slow).\n\t */\n\tslow = bfq_bfqq_is_slow(bfqd, bfqq, compensate, &delta);\n\n\t/*\n\t * As above explained, charge slow (typically seeky) and\n\t * timed-out queues with the time and not the service\n\t * received, to favor sequential workloads.\n\t *\n\t * Processes doing I/O in the slower disk zones will tend to\n\t * be slow(er) even if not seeky. Therefore, since the\n\t * estimated peak rate is actually an average over the disk\n\t * surface, these processes may timeout just for bad luck. To\n\t * avoid punishing them, do not charge time to processes that\n\t * succeeded in consuming at least 2/3 of their budget. This\n\t * allows BFQ to preserve enough elasticity to still perform\n\t * bandwidth, and not time, distribution with little unlucky\n\t * or quasi-sequential processes.\n\t */\n\tif (bfqq->wr_coeff == 1 &&\n\t    (slow ||\n\t     (reason == BFQQE_BUDGET_TIMEOUT &&\n\t      bfq_bfqq_budget_left(bfqq) >=  entity->budget / 3)))\n\t\tbfq_bfqq_charge_time(bfqd, bfqq, delta);\n\n\tif (bfqd->low_latency && bfqq->wr_coeff == 1)\n\t\tbfqq->last_wr_start_finish = jiffies;\n\n\tif (bfqd->low_latency && bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\t/*\n\t\t * If we get here, and there are no outstanding\n\t\t * requests, then the request pattern is isochronous\n\t\t * (see the comments on the function\n\t\t * bfq_bfqq_softrt_next_start()). Therefore we can\n\t\t * compute soft_rt_next_start.\n\t\t *\n\t\t * If, instead, the queue still has outstanding\n\t\t * requests, then we have to wait for the completion\n\t\t * of all the outstanding requests to discover whether\n\t\t * the request pattern is actually isochronous.\n\t\t */\n\t\tif (bfqq->dispatched == 0)\n\t\t\tbfqq->soft_rt_next_start =\n\t\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\t\telse if (bfqq->dispatched > 0) {\n\t\t\t/*\n\t\t\t * Schedule an update of soft_rt_next_start to when\n\t\t\t * the task may be discovered to be isochronous.\n\t\t\t */\n\t\t\tbfq_mark_bfqq_softrt_update(bfqq);\n\t\t}\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\"expire (%d, slow %d, num_disp %d, short_ttime %d)\", reason,\n\t\tslow, bfqq->dispatched, bfq_bfqq_has_short_ttime(bfqq));\n\n\t/*\n\t * bfqq expired, so no total service time needs to be computed\n\t * any longer: reset state machine for measuring total service\n\t * times.\n\t */\n\tbfqd->rqs_injected = bfqd->wait_dispatch = false;\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * Increase, decrease or leave budget unchanged according to\n\t * reason.\n\t */\n\t__bfq_bfqq_recalc_budget(bfqd, bfqq, reason);\n\tif (__bfq_bfqq_expire(bfqd, bfqq, reason))\n\t\t/* bfqq is gone, no more actions on it */\n\t\treturn;\n\n\t/* mark bfqq as waiting a request only if a bic still points to it */\n\tif (!bfq_bfqq_busy(bfqq) &&\n\t    reason != BFQQE_BUDGET_TIMEOUT &&\n\t    reason != BFQQE_BUDGET_EXHAUSTED) {\n\t\tbfq_mark_bfqq_non_blocking_wait_rq(bfqq);\n\t\t/*\n\t\t * Not setting service to 0, because, if the next rq\n\t\t * arrives in time, the queue will go on receiving\n\t\t * service with this same budget (as if it never expired)\n\t\t */\n\t} else\n\t\tentity->service = 0;\n\n\t/*\n\t * Reset the received-service counter for every parent entity.\n\t * Differently from what happens with bfqq->entity.service,\n\t * the resetting of this counter never needs to be postponed\n\t * for parent entities. In fact, in case bfqq may have a\n\t * chance to go on being served using the last, partially\n\t * consumed budget, bfqq->entity.service needs to be kept,\n\t * because if bfqq then actually goes on being served using\n\t * the same budget, the last value of bfqq->entity.service is\n\t * needed to properly decrement bfqq->entity.budget by the\n\t * portion already consumed. In contrast, it is not necessary\n\t * to keep entity->service for parent entities too, because\n\t * the bubble up of the new value of bfqq->entity.budget will\n\t * make sure that the budgets of parent entities are correct,\n\t * even in case bfqq and thus parent entities go on receiving\n\t * service with the same budget.\n\t */\n\tentity = entity->parent;\n\tfor_each_entity(entity)\n\t\tentity->service = 0;\n}\n\n/*\n * Budget timeout is not implemented through a dedicated timer, but\n * just checked on request arrivals and completions, as well as on\n * idle timer expirations.\n */\nstatic bool bfq_bfqq_budget_timeout(struct bfq_queue *bfqq)\n{\n\treturn time_is_before_eq_jiffies(bfqq->budget_timeout);\n}\n\n/*\n * If we expire a queue that is actively waiting (i.e., with the\n * device idled) for the arrival of a new request, then we may incur\n * the timestamp misalignment problem described in the body of the\n * function __bfq_activate_entity. Hence we return true only if this\n * condition does not hold, or if the queue is slow enough to deserve\n * only to be kicked off for preserving a high throughput.\n */\nstatic bool bfq_may_expire_for_budg_timeout(struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\"may_budget_timeout: wait_request %d left %d timeout %d\",\n\t\tbfq_bfqq_wait_request(bfqq),\n\t\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3,\n\t\tbfq_bfqq_budget_timeout(bfqq));\n\n\treturn (!bfq_bfqq_wait_request(bfqq) ||\n\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3)\n\t\t&&\n\t\tbfq_bfqq_budget_timeout(bfqq);\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq)\n{\n\tbool rot_without_queueing =\n\t\t!blk_queue_nonrot(bfqd->queue) && !bfqd->hw_tag,\n\t\tbfqq_sequential_and_IO_bound,\n\t\tidling_boosts_thr;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tbfqq_sequential_and_IO_bound = !BFQQ_SEEKY(bfqq) &&\n\t\tbfq_bfqq_IO_bound(bfqq) && bfq_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * The next variable takes into account the cases where idling\n\t * boosts the throughput.\n\t *\n\t * The value of the variable is computed considering, first, that\n\t * idling is virtually always beneficial for the throughput if:\n\t * (a) the device is not NCQ-capable and rotational, or\n\t * (b) regardless of the presence of NCQ, the device is rotational and\n\t *     the request pattern for bfqq is I/O-bound and sequential, or\n\t * (c) regardless of whether it is rotational, the device is\n\t *     not NCQ-capable and the request pattern for bfqq is\n\t *     I/O-bound and sequential.\n\t *\n\t * Secondly, and in contrast to the above item (b), idling an\n\t * NCQ-capable flash-based device would not boost the\n\t * throughput even with sequential I/O; rather it would lower\n\t * the throughput in proportion to how fast the device\n\t * is. Accordingly, the next variable is true if any of the\n\t * above conditions (a), (b) or (c) is true, and, in\n\t * particular, happens to be false if bfqd is an NCQ-capable\n\t * flash-based device.\n\t */\n\tidling_boosts_thr = rot_without_queueing ||\n\t\t((!blk_queue_nonrot(bfqd->queue) || !bfqd->hw_tag) &&\n\t\t bfqq_sequential_and_IO_bound);\n\n\t/*\n\t * The return value of this function is equal to that of\n\t * idling_boosts_thr, unless a special case holds. In this\n\t * special case, described below, idling may cause problems to\n\t * weight-raised queues.\n\t *\n\t * When the request pool is saturated (e.g., in the presence\n\t * of write hogs), if the processes associated with\n\t * non-weight-raised queues ask for requests at a lower rate,\n\t * then processes associated with weight-raised queues have a\n\t * higher probability to get a request from the pool\n\t * immediately (or at least soon) when they need one. Thus\n\t * they have a higher probability to actually get a fraction\n\t * of the device throughput proportional to their high\n\t * weight. This is especially true with NCQ-capable drives,\n\t * which enqueue several requests in advance, and further\n\t * reorder internally-queued requests.\n\t *\n\t * For this reason, we force to false the return value if\n\t * there are weight-raised busy queues. In this case, and if\n\t * bfqq is not weight-raised, this guarantees that the device\n\t * is not idled for bfqq (if, instead, bfqq is weight-raised,\n\t * then idling will be guaranteed by another variable, see\n\t * below). Combined with the timestamping rules of BFQ (see\n\t * [1] for details), this behavior causes bfqq, and hence any\n\t * sync non-weight-raised queue, to get a lower number of\n\t * requests served, and thus to ask for a lower number of\n\t * requests from the request pool, before the busy\n\t * weight-raised queues get served again. This often mitigates\n\t * starvation problems in the presence of heavy write\n\t * workloads and NCQ, thereby guaranteeing a higher\n\t * application and system responsiveness in these hostile\n\t * scenarios.\n\t */\n\treturn idling_boosts_thr &&\n\t\tbfqd->wr_busy_queues == 0;\n}\n\n/*\n * For a queue that becomes empty, device idling is allowed only if\n * this function returns true for that queue. As a consequence, since\n * device idling plays a critical role for both throughput boosting\n * and service guarantees, the return value of this function plays a\n * critical role as well.\n *\n * In a nutshell, this function returns true only if idling is\n * beneficial for throughput or, even if detrimental for throughput,\n * idling is however necessary to preserve service guarantees (low\n * latency, desired throughput distribution, ...). In particular, on\n * NCQ-capable devices, this function tries to return false, so as to\n * help keep the drives' internal queues full, whenever this helps the\n * device boost the throughput without causing any service-guarantee\n * issue.\n *\n * Most of the issues taken into account to get the return value of\n * this function are not trivial. We discuss these issues in the two\n * functions providing the main pieces of information needed by this\n * function.\n */\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tbool idling_boosts_thr_with_no_issue, idling_needed_for_service_guar;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tif (unlikely(bfqd->strict_guarantees))\n\t\treturn true;\n\n\t/*\n\t * Idling is performed only if slice_idle > 0. In addition, we\n\t * do not idle if\n\t * (a) bfqq is async\n\t * (b) bfqq is in the idle io prio class: in this case we do\n\t * not idle because we want to minimize the bandwidth that\n\t * queues in this class can steal to higher-priority queues\n\t */\n\tif (bfqd->bfq_slice_idle == 0 || !bfq_bfqq_sync(bfqq) ||\n\t   bfq_class_idle(bfqq))\n\t\treturn false;\n\n\tidling_boosts_thr_with_no_issue =\n\t\tidling_boosts_thr_without_issues(bfqd, bfqq);\n\n\tidling_needed_for_service_guar =\n\t\tidling_needed_for_service_guarantees(bfqd, bfqq);\n\n\t/*\n\t * We have now the two components we need to compute the\n\t * return value of the function, which is true only if idling\n\t * either boosts the throughput (without issues), or is\n\t * necessary to preserve service guarantees.\n\t */\n\treturn idling_boosts_thr_with_no_issue ||\n\t\tidling_needed_for_service_guar;\n}\n\n/*\n * If the in-service queue is empty but the function bfq_better_to_idle\n * returns true, then:\n * 1) the queue must remain in service and cannot be expired, and\n * 2) the device must be idled to wait for the possible arrival of a new\n *    request for the queue.\n * See the comments on the function bfq_better_to_idle for the reasons\n * why performing device idling is the best choice to boost the throughput\n * and preserve service guarantees when bfq_better_to_idle itself\n * returns true.\n */\nstatic bool bfq_bfqq_must_idle(struct bfq_queue *bfqq)\n{\n\treturn RB_EMPTY_ROOT(&bfqq->sort_list) && bfq_better_to_idle(bfqq);\n}\n\n/*\n * This function chooses the queue from which to pick the next extra\n * I/O request to inject, if it finds a compatible queue. See the\n * comments on bfq_update_inject_limit() for details on the injection\n * mechanism, and for the definitions of the quantities mentioned\n * below.\n */\nstatic struct bfq_queue *\nbfq_choose_bfqq_for_injection(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *in_serv_bfqq = bfqd->in_service_queue;\n\tunsigned int limit = in_serv_bfqq->inject_limit;\n\tint i;\n\n\t/*\n\t * If\n\t * - bfqq is not weight-raised and therefore does not carry\n\t *   time-critical I/O,\n\t * or\n\t * - regardless of whether bfqq is weight-raised, bfqq has\n\t *   however a long think time, during which it can absorb the\n\t *   effect of an appropriate number of extra I/O requests\n\t *   from other queues (see bfq_update_inject_limit for\n\t *   details on the computation of this number);\n\t * then injection can be performed without restrictions.\n\t */\n\tbool in_serv_always_inject = in_serv_bfqq->wr_coeff == 1 ||\n\t\t!bfq_bfqq_has_short_ttime(in_serv_bfqq);\n\n\t/*\n\t * If\n\t * - the baseline total service time could not be sampled yet,\n\t *   so the inject limit happens to be still 0, and\n\t * - a lot of time has elapsed since the plugging of I/O\n\t *   dispatching started, so drive speed is being wasted\n\t *   significantly;\n\t * then temporarily raise inject limit to one request.\n\t */\n\tif (limit == 0 && in_serv_bfqq->last_serv_time_ns == 0 &&\n\t    bfq_bfqq_wait_request(in_serv_bfqq) &&\n\t    time_is_before_eq_jiffies(bfqd->last_idling_start_jiffies +\n\t\t\t\t      bfqd->bfq_slice_idle)\n\t\t)\n\t\tlimit = 1;\n\n\tif (bfqd->tot_rq_in_driver >= limit)\n\t\treturn NULL;\n\n\t/*\n\t * Linear search of the source queue for injection; but, with\n\t * a high probability, very few steps are needed to find a\n\t * candidate queue, i.e., a queue with enough budget left for\n\t * its next request. In fact:\n\t * - BFQ dynamically updates the budget of every queue so as\n\t *   to accommodate the expected backlog of the queue;\n\t * - if a queue gets all its requests dispatched as injected\n\t *   service, then the queue is removed from the active list\n\t *   (and re-added only if it gets new requests, but then it\n\t *   is assigned again enough budget for its new backlog).\n\t */\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tlist_for_each_entry(bfqq, &bfqd->active_list[i], bfqq_list)\n\t\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t\t(in_serv_always_inject || bfqq->wr_coeff > 1) &&\n\t\t\t\tbfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Allow for only one large in-flight request\n\t\t\t * on non-rotational devices, for the\n\t\t\t * following reason. On non-rotationl drives,\n\t\t\t * large requests take much longer than\n\t\t\t * smaller requests to be served. In addition,\n\t\t\t * the drive prefers to serve large requests\n\t\t\t * w.r.t. to small ones, if it can choose. So,\n\t\t\t * having more than one large requests queued\n\t\t\t * in the drive may easily make the next first\n\t\t\t * request of the in-service queue wait for so\n\t\t\t * long to break bfqq's service guarantees. On\n\t\t\t * the bright side, large requests let the\n\t\t\t * drive reach a very high throughput, even if\n\t\t\t * there is only one in-flight large request\n\t\t\t * at a time.\n\t\t\t */\n\t\t\tif (blk_queue_nonrot(bfqd->queue) &&\n\t\t\t    blk_rq_sectors(bfqq->next_rq) >=\n\t\t\t    BFQQ_SECT_THR_NONROT &&\n\t\t\t    bfqd->tot_rq_in_driver >= 1)\n\t\t\t\tcontinue;\n\t\t\telse {\n\t\t\t\tbfqd->rqs_injected = true;\n\t\t\t\treturn bfqq;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *\nbfq_find_active_bfqq_for_actuator(struct bfq_data *bfqd, int idx)\n{\n\tstruct bfq_queue *bfqq;\n\n\tif (bfqd->in_service_queue &&\n\t    bfqd->in_service_queue->actuator_idx == idx)\n\t\treturn bfqd->in_service_queue;\n\n\tlist_for_each_entry(bfqq, &bfqd->active_list[idx], bfqq_list) {\n\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\tbfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\treturn bfqq;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Perform a linear scan of each actuator, until an actuator is found\n * for which the following three conditions hold: the load of the\n * actuator is below the threshold (see comments on\n * actuator_load_threshold for details) and lower than that of the\n * next actuator (comments on this extra condition below), and there\n * is a queue that contains I/O for that actuator. On success, return\n * that queue.\n *\n * Performing a plain linear scan entails a prioritization among\n * actuators. The extra condition above breaks this prioritization and\n * tends to distribute injection uniformly across actuators.\n */\nstatic struct bfq_queue *\nbfq_find_bfqq_for_underused_actuator(struct bfq_data *bfqd)\n{\n\tint i;\n\n\tfor (i = 0 ; i < bfqd->num_actuators; i++) {\n\t\tif (bfqd->rq_in_driver[i] < bfqd->actuator_load_threshold &&\n\t\t    (i == bfqd->num_actuators - 1 ||\n\t\t     bfqd->rq_in_driver[i] < bfqd->rq_in_driver[i+1])) {\n\t\t\tstruct bfq_queue *bfqq =\n\t\t\t\tbfq_find_active_bfqq_for_actuator(bfqd, i);\n\n\t\t\tif (bfqq)\n\t\t\t\treturn bfqq;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n\n/*\n * Select a queue for service.  If we have a current queue in service,\n * check whether to continue servicing it, or retrieve and set a new one.\n */\nstatic struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *inject_bfqq;\n\tstruct request *next_rq;\n\tenum bfqq_expiration reason = BFQQE_BUDGET_TIMEOUT;\n\n\tbfqq = bfqd->in_service_queue;\n\tif (!bfqq)\n\t\tgoto new_queue;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: already in-service queue\");\n\n\t/*\n\t * Do not expire bfqq for budget timeout if bfqq may be about\n\t * to enjoy device idling. The reason why, in this case, we\n\t * prevent bfqq from expiring is the same as in the comments\n\t * on the case where bfq_bfqq_must_idle() returns true, in\n\t * bfq_completed_request().\n\t */\n\tif (bfq_may_expire_for_budg_timeout(bfqq) &&\n\t    !bfq_bfqq_must_idle(bfqq))\n\t\tgoto expire;\n\ncheck_queue:\n\t/*\n\t *  If some actuator is underutilized, but the in-service\n\t *  queue does not contain I/O for that actuator, then try to\n\t *  inject I/O for that actuator.\n\t */\n\tinject_bfqq = bfq_find_bfqq_for_underused_actuator(bfqd);\n\tif (inject_bfqq && inject_bfqq != bfqq)\n\t\treturn inject_bfqq;\n\n\t/*\n\t * This loop is rarely executed more than once. Even when it\n\t * happens, it is much more convenient to re-execute this loop\n\t * than to return NULL and trigger a new dispatch to get a\n\t * request served.\n\t */\n\tnext_rq = bfqq->next_rq;\n\t/*\n\t * If bfqq has requests queued and it has enough budget left to\n\t * serve them, keep the queue, otherwise expire it.\n\t */\n\tif (next_rq) {\n\t\tif (bfq_serv_to_charge(next_rq, bfqq) >\n\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Expire the queue for budget exhaustion,\n\t\t\t * which makes sure that the next budget is\n\t\t\t * enough to serve the next request, even if\n\t\t\t * it comes from the fifo expired path.\n\t\t\t */\n\t\t\treason = BFQQE_BUDGET_EXHAUSTED;\n\t\t\tgoto expire;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The idle timer may be pending because we may\n\t\t\t * not disable disk idling even when a new request\n\t\t\t * arrives.\n\t\t\t */\n\t\t\tif (bfq_bfqq_wait_request(bfqq)) {\n\t\t\t\t/*\n\t\t\t\t * If we get here: 1) at least a new request\n\t\t\t\t * has arrived but we have not disabled the\n\t\t\t\t * timer because the request was too small,\n\t\t\t\t * 2) then the block layer has unplugged\n\t\t\t\t * the device, causing the dispatch to be\n\t\t\t\t * invoked.\n\t\t\t\t *\n\t\t\t\t * Since the device is unplugged, now the\n\t\t\t\t * requests are probably large enough to\n\t\t\t\t * provide a reasonable throughput.\n\t\t\t\t * So we disable idling.\n\t\t\t\t */\n\t\t\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\t\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\t\t\t}\n\t\t\tgoto keep_queue;\n\t\t}\n\t}\n\n\t/*\n\t * No requests pending. However, if the in-service queue is idling\n\t * for a new request, or has requests waiting for a completion and\n\t * may idle after their completion, then keep it anyway.\n\t *\n\t * Yet, inject service from other queues if it boosts\n\t * throughput and is possible.\n\t */\n\tif (bfq_bfqq_wait_request(bfqq) ||\n\t    (bfqq->dispatched != 0 && bfq_better_to_idle(bfqq))) {\n\t\tunsigned int act_idx = bfqq->actuator_idx;\n\t\tstruct bfq_queue *async_bfqq = NULL;\n\t\tstruct bfq_queue *blocked_bfqq =\n\t\t\t!hlist_empty(&bfqq->woken_list) ?\n\t\t\tcontainer_of(bfqq->woken_list.first,\n\t\t\t\t     struct bfq_queue,\n\t\t\t\t     woken_list_node)\n\t\t\t: NULL;\n\n\t\tif (bfqq->bic && bfqq->bic->bfqq[0][act_idx] &&\n\t\t    bfq_bfqq_busy(bfqq->bic->bfqq[0][act_idx]) &&\n\t\t    bfqq->bic->bfqq[0][act_idx]->next_rq)\n\t\t\tasync_bfqq = bfqq->bic->bfqq[0][act_idx];\n\t\t/*\n\t\t * The next four mutually-exclusive ifs decide\n\t\t * whether to try injection, and choose the queue to\n\t\t * pick an I/O request from.\n\t\t *\n\t\t * The first if checks whether the process associated\n\t\t * with bfqq has also async I/O pending. If so, it\n\t\t * injects such I/O unconditionally. Injecting async\n\t\t * I/O from the same process can cause no harm to the\n\t\t * process. On the contrary, it can only increase\n\t\t * bandwidth and reduce latency for the process.\n\t\t *\n\t\t * The second if checks whether there happens to be a\n\t\t * non-empty waker queue for bfqq, i.e., a queue whose\n\t\t * I/O needs to be completed for bfqq to receive new\n\t\t * I/O. This happens, e.g., if bfqq is associated with\n\t\t * a process that does some sync. A sync generates\n\t\t * extra blocking I/O, which must be completed before\n\t\t * the process associated with bfqq can go on with its\n\t\t * I/O. If the I/O of the waker queue is not served,\n\t\t * then bfqq remains empty, and no I/O is dispatched,\n\t\t * until the idle timeout fires for bfqq. This is\n\t\t * likely to result in lower bandwidth and higher\n\t\t * latencies for bfqq, and in a severe loss of total\n\t\t * throughput. The best action to take is therefore to\n\t\t * serve the waker queue as soon as possible. So do it\n\t\t * (without relying on the third alternative below for\n\t\t * eventually serving waker_bfqq's I/O; see the last\n\t\t * paragraph for further details). This systematic\n\t\t * injection of I/O from the waker queue does not\n\t\t * cause any delay to bfqq's I/O. On the contrary,\n\t\t * next bfqq's I/O is brought forward dramatically,\n\t\t * for it is not blocked for milliseconds.\n\t\t *\n\t\t * The third if checks whether there is a queue woken\n\t\t * by bfqq, and currently with pending I/O. Such a\n\t\t * woken queue does not steal bandwidth from bfqq,\n\t\t * because it remains soon without I/O if bfqq is not\n\t\t * served. So there is virtually no risk of loss of\n\t\t * bandwidth for bfqq if this woken queue has I/O\n\t\t * dispatched while bfqq is waiting for new I/O.\n\t\t *\n\t\t * The fourth if checks whether bfqq is a queue for\n\t\t * which it is better to avoid injection. It is so if\n\t\t * bfqq delivers more throughput when served without\n\t\t * any further I/O from other queues in the middle, or\n\t\t * if the service times of bfqq's I/O requests both\n\t\t * count more than overall throughput, and may be\n\t\t * easily increased by injection (this happens if bfqq\n\t\t * has a short think time). If none of these\n\t\t * conditions holds, then a candidate queue for\n\t\t * injection is looked for through\n\t\t * bfq_choose_bfqq_for_injection(). Note that the\n\t\t * latter may return NULL (for example if the inject\n\t\t * limit for bfqq is currently 0).\n\t\t *\n\t\t * NOTE: motivation for the second alternative\n\t\t *\n\t\t * Thanks to the way the inject limit is updated in\n\t\t * bfq_update_has_short_ttime(), it is rather likely\n\t\t * that, if I/O is being plugged for bfqq and the\n\t\t * waker queue has pending I/O requests that are\n\t\t * blocking bfqq's I/O, then the fourth alternative\n\t\t * above lets the waker queue get served before the\n\t\t * I/O-plugging timeout fires. So one may deem the\n\t\t * second alternative superfluous. It is not, because\n\t\t * the fourth alternative may be way less effective in\n\t\t * case of a synchronization. For two main\n\t\t * reasons. First, throughput may be low because the\n\t\t * inject limit may be too low to guarantee the same\n\t\t * amount of injected I/O, from the waker queue or\n\t\t * other queues, that the second alternative\n\t\t * guarantees (the second alternative unconditionally\n\t\t * injects a pending I/O request of the waker queue\n\t\t * for each bfq_dispatch_request()). Second, with the\n\t\t * fourth alternative, the duration of the plugging,\n\t\t * i.e., the time before bfqq finally receives new I/O,\n\t\t * may not be minimized, because the waker queue may\n\t\t * happen to be served only after other queues.\n\t\t */\n\t\tif (async_bfqq &&\n\t\t    icq_to_bic(async_bfqq->next_rq->elv.icq) == bfqq->bic &&\n\t\t    bfq_serv_to_charge(async_bfqq->next_rq, async_bfqq) <=\n\t\t    bfq_bfqq_budget_left(async_bfqq))\n\t\t\tbfqq = async_bfqq;\n\t\telse if (bfqq->waker_bfqq &&\n\t\t\t   bfq_bfqq_busy(bfqq->waker_bfqq) &&\n\t\t\t   bfqq->waker_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(bfqq->waker_bfqq->next_rq,\n\t\t\t\t\t      bfqq->waker_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(bfqq->waker_bfqq)\n\t\t\t)\n\t\t\tbfqq = bfqq->waker_bfqq;\n\t\telse if (blocked_bfqq &&\n\t\t\t   bfq_bfqq_busy(blocked_bfqq) &&\n\t\t\t   blocked_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(blocked_bfqq->next_rq,\n\t\t\t\t\t      blocked_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(blocked_bfqq)\n\t\t\t)\n\t\t\tbfqq = blocked_bfqq;\n\t\telse if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t (bfqq->wr_coeff == 1 || bfqd->wr_busy_queues > 1 ||\n\t\t\t  !bfq_bfqq_has_short_ttime(bfqq)))\n\t\t\tbfqq = bfq_choose_bfqq_for_injection(bfqd);\n\t\telse\n\t\t\tbfqq = NULL;\n\n\t\tgoto keep_queue;\n\t}\n\n\treason = BFQQE_NO_MORE_REQUESTS;\nexpire:\n\tbfq_bfqq_expire(bfqd, bfqq, false, reason);\nnew_queue:\n\tbfqq = bfq_set_in_service_queue(bfqd);\n\tif (bfqq) {\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: checking new queue\");\n\t\tgoto check_queue;\n\t}\nkeep_queue:\n\tif (bfqq)\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: returned this queue\");\n\telse\n\t\tbfq_log(bfqd, \"select_queue: no queue returned\");\n\n\treturn bfqq;\n}\n\nstatic void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tif (bfqq->wr_coeff > 1) { /* queue is being weight-raised */\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t\"raising period dur %u/%u msec, old coeff %u, w %d(%d)\",\n\t\t\tjiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),\n\t\t\tjiffies_to_msecs(bfqq->wr_cur_max_time),\n\t\t\tbfqq->wr_coeff,\n\t\t\tbfqq->entity.weight, bfqq->entity.orig_weight);\n\n\t\tif (entity->prio_changed)\n\t\t\tbfq_log_bfqq(bfqd, bfqq, \"WARN: pending prio change\");\n\n\t\t/*\n\t\t * If the queue was activated in a burst, or too much\n\t\t * time has elapsed from the beginning of this\n\t\t * weight-raising period, then end weight raising.\n\t\t */\n\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\telse if (time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t\t\tbfqq->wr_cur_max_time)) {\n\t\t\tif (bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time ||\n\t\t\ttime_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t       bfq_wr_duration(bfqd))) {\n\t\t\t\t/*\n\t\t\t\t * Either in interactive weight\n\t\t\t\t * raising, or in soft_rt weight\n\t\t\t\t * raising with the\n\t\t\t\t * interactive-weight-raising period\n\t\t\t\t * elapsed (so no switch back to\n\t\t\t\t * interactive weight raising).\n\t\t\t\t */\n\t\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t\t} else { /*\n\t\t\t\t  * soft_rt finishing while still in\n\t\t\t\t  * interactive period, switch back to\n\t\t\t\t  * interactive weight raising\n\t\t\t\t  */\n\t\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t\t}\n\t\t}\n\t\tif (bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time &&\n\t\t    bfqq->service_from_wr > max_service_from_wr) {\n\t\t\t/* see comments on max_service_from_wr */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t}\n\t}\n\t/*\n\t * To improve latency (for this or other queues), immediately\n\t * update weight both if it must be raised and if it must be\n\t * lowered. Since, entity may be on some active tree here, and\n\t * might have a pending change of its ioprio class, invoke\n\t * next function with the last parameter unset (see the\n\t * comments on the function).\n\t */\n\tif ((entity->weight > entity->orig_weight) != (bfqq->wr_coeff > 1))\n\t\t__bfq_entity_update_weight_prio(bfq_entity_service_tree(entity),\n\t\t\t\t\t\tentity, false);\n}\n\n/*\n * Dispatch next request from bfqq.\n */\nstatic struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct request *rq = bfqq->next_rq;\n\tunsigned long service_to_charge;\n\n\tservice_to_charge = bfq_serv_to_charge(rq, bfqq);\n\n\tbfq_bfqq_served(bfqq, service_to_charge);\n\n\tif (bfqq == bfqd->in_service_queue && bfqd->wait_dispatch) {\n\t\tbfqd->wait_dispatch = false;\n\t\tbfqd->waited_rq = rq;\n\t}\n\n\tbfq_dispatch_remove(bfqd->queue, rq);\n\n\tif (bfqq != bfqd->in_service_queue)\n\t\treturn rq;\n\n\t/*\n\t * If weight raising has to terminate for bfqq, then next\n\t * function causes an immediate update of bfqq's weight,\n\t * without waiting for next activation. As a consequence, on\n\t * expiration, bfqq will be timestamped as if has never been\n\t * weight-raised during this service slot, even if it has\n\t * received part or even most of the service as a\n\t * weight-raised queue. This inflates bfqq's timestamps, which\n\t * is beneficial, as bfqq is then more willing to leave the\n\t * device immediately to possible other weight-raised queues.\n\t */\n\tbfq_update_wr_data(bfqd, bfqq);\n\n\t/*\n\t * Expire bfqq, pretending that its budget expired, if bfqq\n\t * belongs to CLASS_IDLE and other queues are waiting for\n\t * service.\n\t */\n\tif (bfq_tot_busy_queues(bfqd) > 1 && bfq_class_idle(bfqq))\n\t\tbfq_bfqq_expire(bfqd, bfqq, false, BFQQE_BUDGET_EXHAUSTED);\n\n\treturn rq;\n}\n\nstatic bool bfq_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\n\t/*\n\t * Avoiding lock: a race on bfqd->queued should cause at\n\t * most a call to dispatch for nothing\n\t */\n\treturn !list_empty_careful(&bfqd->dispatch) ||\n\t\tREAD_ONCE(bfqd->queued);\n}\n\nstatic struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq = NULL;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tif (!list_empty(&bfqd->dispatch)) {\n\t\trq = list_first_entry(&bfqd->dispatch, struct request,\n\t\t\t\t      queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (bfqq) {\n\t\t\t/*\n\t\t\t * Increment counters here, because this\n\t\t\t * dispatch does not follow the standard\n\t\t\t * dispatch flow (where counters are\n\t\t\t * incremented)\n\t\t\t */\n\t\t\tbfqq->dispatched++;\n\n\t\t\tgoto inc_in_driver_start_rq;\n\t\t}\n\n\t\t/*\n\t\t * We exploit the bfq_finish_requeue_request hook to\n\t\t * decrement tot_rq_in_driver, but\n\t\t * bfq_finish_requeue_request will not be invoked on\n\t\t * this request. So, to avoid unbalance, just start\n\t\t * this request, without incrementing tot_rq_in_driver. As\n\t\t * a negative consequence, tot_rq_in_driver is deceptively\n\t\t * lower than it should be while this request is in\n\t\t * service. This may cause bfq_schedule_dispatch to be\n\t\t * invoked uselessly.\n\t\t *\n\t\t * As for implementing an exact solution, the\n\t\t * bfq_finish_requeue_request hook, if defined, is\n\t\t * probably invoked also on this request. So, by\n\t\t * exploiting this hook, we could 1) increment\n\t\t * tot_rq_in_driver here, and 2) decrement it in\n\t\t * bfq_finish_requeue_request. Such a solution would\n\t\t * let the value of the counter be always accurate,\n\t\t * but it would entail using an extra interface\n\t\t * function. This cost seems higher than the benefit,\n\t\t * being the frequency of non-elevator-private\n\t\t * requests very low.\n\t\t */\n\t\tgoto start_rq;\n\t}\n\n\tbfq_log(bfqd, \"dispatch requests: %d busy queues\",\n\t\tbfq_tot_busy_queues(bfqd));\n\n\tif (bfq_tot_busy_queues(bfqd) == 0)\n\t\tgoto exit;\n\n\t/*\n\t * Force device to serve one request at a time if\n\t * strict_guarantees is true. Forcing this service scheme is\n\t * currently the ONLY way to guarantee that the request\n\t * service order enforced by the scheduler is respected by a\n\t * queueing device. Otherwise the device is free even to make\n\t * some unlucky request wait for as long as the device\n\t * wishes.\n\t *\n\t * Of course, serving one request at a time may cause loss of\n\t * throughput.\n\t */\n\tif (bfqd->strict_guarantees && bfqd->tot_rq_in_driver > 0)\n\t\tgoto exit;\n\n\tbfqq = bfq_select_queue(bfqd);\n\tif (!bfqq)\n\t\tgoto exit;\n\n\trq = bfq_dispatch_rq_from_bfqq(bfqd, bfqq);\n\n\tif (rq) {\ninc_in_driver_start_rq:\n\t\tbfqd->rq_in_driver[bfqq->actuator_idx]++;\n\t\tbfqd->tot_rq_in_driver++;\nstart_rq:\n\t\trq->rq_flags |= RQF_STARTED;\n\t}\nexit:\n\treturn rq;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t      struct request *rq,\n\t\t\t\t      struct bfq_queue *in_serv_queue,\n\t\t\t\t      bool idle_timer_disabled)\n{\n\tstruct bfq_queue *bfqq = rq ? RQ_BFQQ(rq) : NULL;\n\n\tif (!idle_timer_disabled && !bfqq)\n\t\treturn;\n\n\t/*\n\t * rq and bfqq are guaranteed to exist until this function\n\t * ends, for the following reasons. First, rq can be\n\t * dispatched to the device, and then can be completed and\n\t * freed, only after this function ends. Second, rq cannot be\n\t * merged (and thus freed because of a merge) any longer,\n\t * because it has already started. Thus rq cannot be freed\n\t * before this function ends, and, since rq has a reference to\n\t * bfqq, the same guarantee holds for bfqq too.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tif (idle_timer_disabled)\n\t\t/*\n\t\t * Since the idle timer has been disabled,\n\t\t * in_serv_queue contained some request when\n\t\t * __bfq_dispatch_request was invoked above, which\n\t\t * implies that rq was picked exactly from\n\t\t * in_serv_queue. Thus in_serv_queue == bfqq, and is\n\t\t * therefore guaranteed to exist because of the above\n\t\t * arguments.\n\t\t */\n\t\tbfqg_stats_update_idle_time(bfqq_group(in_serv_queue));\n\tif (bfqq) {\n\t\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\t\tbfqg_stats_update_avg_queue_size(bfqg);\n\t\tbfqg_stats_set_start_empty_time(bfqg);\n\t\tbfqg_stats_update_io_remove(bfqg, rq->cmd_flags);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     struct bfq_queue *in_serv_queue,\n\t\t\t\t\t     bool idle_timer_disabled) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq;\n\tstruct bfq_queue *in_serv_queue;\n\tbool waiting_rq, idle_timer_disabled = false;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tin_serv_queue = bfqd->in_service_queue;\n\twaiting_rq = in_serv_queue && bfq_bfqq_wait_request(in_serv_queue);\n\n\trq = __bfq_dispatch_request(hctx);\n\tif (in_serv_queue == bfqd->in_service_queue) {\n\t\tidle_timer_disabled =\n\t\t\twaiting_rq && !bfq_bfqq_wait_request(in_serv_queue);\n\t}\n\n\tspin_unlock_irq(&bfqd->lock);\n\tbfq_update_dispatch_stats(hctx->queue, rq,\n\t\t\tidle_timer_disabled ? in_serv_queue : NULL,\n\t\t\t\tidle_timer_disabled);\n\n\treturn rq;\n}\n\n/*\n * Task holds one reference to the queue, dropped when task exits.  Each rq\n * in-flight on this queue also holds a reference, dropped when rq is freed.\n *\n * Scheduler lock must be held here. Recall not to use bfqq after calling\n * this function on it.\n */\nvoid bfq_put_queue(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"put_queue: %p %d\", bfqq, bfqq->ref);\n\n\tbfqq->ref--;\n\tif (bfqq->ref)\n\t\treturn;\n\n\tif (!hlist_unhashed(&bfqq->burst_list_node)) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\t/*\n\t\t * Decrement also burst size after the removal, if the\n\t\t * process associated with bfqq is exiting, and thus\n\t\t * does not contribute to the burst any longer. This\n\t\t * decrement helps filter out false positives of large\n\t\t * bursts, when some short-lived process (often due to\n\t\t * the execution of commands by some service) happens\n\t\t * to start and exit while a complex application is\n\t\t * starting, and thus spawning several processes that\n\t\t * do I/O (and that *must not* be treated as a large\n\t\t * burst, see comments on bfq_handle_burst).\n\t\t *\n\t\t * In particular, the decrement is performed only if:\n\t\t * 1) bfqq is not a merged queue, because, if it is,\n\t\t * then this free of bfqq is not triggered by the exit\n\t\t * of the process bfqq is associated with, but exactly\n\t\t * by the fact that bfqq has just been merged.\n\t\t * 2) burst_size is greater than 0, to handle\n\t\t * unbalanced decrements. Unbalanced decrements may\n\t\t * happen in te following case: bfqq is inserted into\n\t\t * the current burst list--without incrementing\n\t\t * bust_size--because of a split, but the current\n\t\t * burst list is not the burst list bfqq belonged to\n\t\t * (see comments on the case of a split in\n\t\t * bfq_set_request).\n\t\t */\n\t\tif (bfqq->bic && bfqq->bfqd->burst_size > 0)\n\t\t\tbfqq->bfqd->burst_size--;\n\t}\n\n\t/*\n\t * bfqq does not exist any longer, so it cannot be woken by\n\t * any other queue, and cannot wake any other queue. Then bfqq\n\t * must be removed from the woken list of its possible waker\n\t * queue, and all queues in the woken list of bfqq must stop\n\t * having a waker queue. Strictly speaking, these updates\n\t * should be performed when bfqq remains with no I/O source\n\t * attached to it, which happens before bfqq gets freed. In\n\t * particular, this happens when the last process associated\n\t * with bfqq exits or gets associated with a different\n\t * queue. However, both events lead to bfqq being freed soon,\n\t * and dangling references would come out only after bfqq gets\n\t * freed. So these updates are done here, as a simple and safe\n\t * way to handle all cases.\n\t */\n\t/* remove bfqq from woken list */\n\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\thlist_del_init(&bfqq->woken_list_node);\n\n\t/* reset waker for all queues in woken list */\n\thlist_for_each_entry_safe(item, n, &bfqq->woken_list,\n\t\t\t\t  woken_list_node) {\n\t\titem->waker_bfqq = NULL;\n\t\thlist_del_init(&item->woken_list_node);\n\t}\n\n\tif (bfqq->bfqd->last_completed_rq_bfqq == bfqq)\n\t\tbfqq->bfqd->last_completed_rq_bfqq = NULL;\n\n\tWARN_ON_ONCE(!list_empty(&bfqq->fifo));\n\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&bfqq->sort_list));\n\tWARN_ON_ONCE(bfqq->dispatched);\n\n\tkmem_cache_free(bfq_pool, bfqq);\n\tbfqg_and_blkg_put(bfqg);\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq)\n{\n\tbfqq->stable_ref--;\n\tbfq_put_queue(bfqq);\n}\n\nvoid bfq_put_cooperator(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *__bfqq, *next;\n\n\t/*\n\t * If this queue was scheduled to merge with another queue, be\n\t * sure to drop the reference taken on that queue (and others in\n\t * the merge chain). See bfq_setup_merge and bfq_merge_bfqqs.\n\t */\n\t__bfqq = bfqq->new_bfqq;\n\twhile (__bfqq) {\n\t\tnext = __bfqq->new_bfqq;\n\t\tbfq_put_queue(__bfqq);\n\t\t__bfqq = next;\n\t}\n}\n\nstatic void bfq_exit_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tif (bfqq == bfqd->in_service_queue) {\n\t\t__bfq_bfqq_expire(bfqd, bfqq, BFQQE_BUDGET_TIMEOUT);\n\t\tbfq_schedule_dispatch(bfqd);\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"exit_bfqq: %p, %d\", bfqq, bfqq->ref);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n}\n\nstatic void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync,\n\t\t\t      unsigned int actuator_idx)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync, actuator_idx);\n\tstruct bfq_data *bfqd;\n\n\tif (bfqq)\n\t\tbfqd = bfqq->bfqd; /* NULL if scheduler already exited */\n\n\tif (bfqq && bfqd) {\n\t\tbic_set_bfqq(bic, NULL, is_sync, actuator_idx);\n\t\tbfq_exit_bfqq(bfqd, bfqq);\n\t}\n}\n\nstatic void _bfq_exit_icq(struct bfq_io_cq *bic, unsigned int num_actuators)\n{\n\tstruct bfq_iocq_bfqq_data *bfqq_data = bic->bfqq_data;\n\tunsigned int act_idx;\n\n\tfor (act_idx = 0; act_idx < num_actuators; act_idx++) {\n\t\tif (bfqq_data[act_idx].stable_merge_bfqq)\n\t\t\tbfq_put_stable_ref(bfqq_data[act_idx].stable_merge_bfqq);\n\n\t\tbfq_exit_icq_bfqq(bic, true, act_idx);\n\t\tbfq_exit_icq_bfqq(bic, false, act_idx);\n\t}\n}\n\nstatic void bfq_exit_icq(struct io_cq *icq)\n{\n\tstruct bfq_io_cq *bic = icq_to_bic(icq);\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tunsigned long flags;\n\n\t/*\n\t * If bfqd and thus bfqd->num_actuators is not available any\n\t * longer, then cycle over all possible per-actuator bfqqs in\n\t * next loop. We rely on bic being zeroed on creation, and\n\t * therefore on its unused per-actuator fields being NULL.\n\t *\n\t * bfqd is NULL if scheduler already exited, and in that case\n\t * this is the last time these queues are accessed.\n\t */\n\tif (bfqd) {\n\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\t_bfq_exit_icq(bic, bfqd->num_actuators);\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t} else {\n\t\t_bfq_exit_icq(bic, BFQ_MAX_ACTUATORS);\n\t}\n}\n\n/*\n * Update the entity prio values; note that the new values will not\n * be used until the next (re)activation.\n */\nstatic void\nbfq_set_next_ioprio_data(struct bfq_queue *bfqq, struct bfq_io_cq *bic)\n{\n\tstruct task_struct *tsk = current;\n\tint ioprio_class;\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\n\tif (!bfqd)\n\t\treturn;\n\n\tioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tswitch (ioprio_class) {\n\tdefault:\n\t\tpr_err(\"bdi %s: bfq: bad prio class %d\\n\",\n\t\t\tbdi_dev_name(bfqq->bfqd->queue->disk->bdi),\n\t\t\tioprio_class);\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_NONE:\n\t\t/*\n\t\t * No prio set, inherit CPU scheduling settings.\n\t\t */\n\t\tbfqq->new_ioprio = task_nice_ioprio(tsk);\n\t\tbfqq->new_ioprio_class = task_nice_ioclass(tsk);\n\t\tbreak;\n\tcase IOPRIO_CLASS_RT:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_RT;\n\t\tbreak;\n\tcase IOPRIO_CLASS_BE:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_BE;\n\t\tbreak;\n\tcase IOPRIO_CLASS_IDLE:\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_IDLE;\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t\tbreak;\n\t}\n\n\tif (bfqq->new_ioprio >= IOPRIO_NR_LEVELS) {\n\t\tpr_crit(\"bfq_set_next_ioprio_data: new_ioprio %d\\n\",\n\t\t\tbfqq->new_ioprio);\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t}\n\n\tbfqq->entity.new_weight = bfq_ioprio_to_weight(bfqq->new_ioprio);\n\tbfq_log_bfqq(bfqd, bfqq, \"new_ioprio %d new_weight %d\",\n\t\t     bfqq->new_ioprio, bfqq->entity.new_weight);\n\tbfqq->entity.prio_changed = 1;\n}\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn);\n\nstatic void bfq_check_ioprio_change(struct bfq_io_cq *bic, struct bio *bio)\n{\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tstruct bfq_queue *bfqq;\n\tint ioprio = bic->icq.ioc->ioprio;\n\n\t/*\n\t * This condition may trigger on a newly created bic, be sure to\n\t * drop the lock before returning.\n\t */\n\tif (unlikely(!bfqd) || likely(bic->ioprio == ioprio))\n\t\treturn;\n\n\tbic->ioprio = ioprio;\n\n\tbfqq = bic_to_bfqq(bic, false, bfq_actuator_index(bfqd, bio));\n\tif (bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\n\t\tbfqq = bfq_get_queue(bfqd, bio, false, bic, true);\n\t\tbic_set_bfqq(bic, bfqq, false, bfq_actuator_index(bfqd, bio));\n\t\tbfq_release_process_ref(bfqd, old_bfqq);\n\t}\n\n\tbfqq = bic_to_bfqq(bic, true, bfq_actuator_index(bfqd, bio));\n\tif (bfqq)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n}\n\nstatic void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic, pid_t pid, int is_sync,\n\t\t\t  unsigned int act_idx)\n{\n\tu64 now_ns = blk_time_get_ns();\n\n\tbfqq->actuator_idx = act_idx;\n\tRB_CLEAR_NODE(&bfqq->entity.rb_node);\n\tINIT_LIST_HEAD(&bfqq->fifo);\n\tINIT_HLIST_NODE(&bfqq->burst_list_node);\n\tINIT_HLIST_NODE(&bfqq->woken_list_node);\n\tINIT_HLIST_HEAD(&bfqq->woken_list);\n\n\tbfqq->ref = 0;\n\tbfqq->bfqd = bfqd;\n\n\tif (bic)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n\n\tif (is_sync) {\n\t\t/*\n\t\t * No need to mark as has_short_ttime if in\n\t\t * idle_class, because no device idling is performed\n\t\t * for queues in idle class\n\t\t */\n\t\tif (!bfq_class_idle(bfqq))\n\t\t\t/* tentatively mark as has_short_ttime */\n\t\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\t\tbfq_mark_bfqq_sync(bfqq);\n\t\tbfq_mark_bfqq_just_created(bfqq);\n\t} else\n\t\tbfq_clear_bfqq_sync(bfqq);\n\n\t/* set end request to minus infinity from now */\n\tbfqq->ttime.last_end_request = now_ns + 1;\n\n\tbfqq->creation_time = jiffies;\n\n\tbfqq->io_start_time = now_ns;\n\n\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\tbfqq->pid = pid;\n\n\t/* Tentative initial value to trade off between thr and lat */\n\tbfqq->max_budget = (2 * bfq_max_budget(bfqd)) / 3;\n\tbfqq->budget_timeout = bfq_smallest_from_now();\n\n\tbfqq->wr_coeff = 1;\n\tbfqq->last_wr_start_finish = jiffies;\n\tbfqq->wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\tbfqq->split_time = bfq_smallest_from_now();\n\n\t/*\n\t * To not forget the possibly high bandwidth consumed by a\n\t * process/queue in the recent past,\n\t * bfq_bfqq_softrt_next_start() returns a value at least equal\n\t * to the current value of bfqq->soft_rt_next_start (see\n\t * comments on bfq_bfqq_softrt_next_start).  Set\n\t * soft_rt_next_start to now, to mean that bfqq has consumed\n\t * no bandwidth so far.\n\t */\n\tbfqq->soft_rt_next_start = jiffies;\n\n\t/* first request is almost certainly seeky */\n\tbfqq->seek_history = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic struct bfq_queue **bfq_async_queue_prio(struct bfq_data *bfqd,\n\t\t\t\t\t       struct bfq_group *bfqg,\n\t\t\t\t\t       int ioprio_class, int ioprio, int act_idx)\n{\n\tswitch (ioprio_class) {\n\tcase IOPRIO_CLASS_RT:\n\t\treturn &bfqg->async_bfqq[0][ioprio][act_idx];\n\tcase IOPRIO_CLASS_NONE:\n\t\tioprio = IOPRIO_BE_NORM;\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_BE:\n\t\treturn &bfqg->async_bfqq[1][ioprio][act_idx];\n\tcase IOPRIO_CLASS_IDLE:\n\t\treturn &bfqg->async_idle_bfqq[act_idx];\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bfq_queue *\nbfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic,\n\t\t\t  struct bfq_queue *last_bfqq_created)\n{\n\tunsigned int a_idx = last_bfqq_created->actuator_idx;\n\tstruct bfq_queue *new_bfqq =\n\t\tbfq_setup_merge(bfqq, last_bfqq_created);\n\n\tif (!new_bfqq)\n\t\treturn bfqq;\n\n\tif (new_bfqq->bic)\n\t\tnew_bfqq->bic->bfqq_data[a_idx].stably_merged = true;\n\tbic->bfqq_data[a_idx].stably_merged = true;\n\n\t/*\n\t * Reusing merge functions. This implies that\n\t * bfqq->bic must be set too, for\n\t * bfq_merge_bfqqs to correctly save bfqq's\n\t * state before killing it.\n\t */\n\tbfqq->bic = bic;\n\treturn bfq_merge_bfqqs(bfqd, bic, bfqq);\n}\n\n/*\n * Many throughput-sensitive workloads are made of several parallel\n * I/O flows, with all flows generated by the same application, or\n * more generically by the same task (e.g., system boot). The most\n * counterproductive action with these workloads is plugging I/O\n * dispatch when one of the bfq_queues associated with these flows\n * remains temporarily empty.\n *\n * To avoid this plugging, BFQ has been using a burst-handling\n * mechanism for years now. This mechanism has proven effective for\n * throughput, and not detrimental for service guarantees. The\n * following function pushes this mechanism a little bit further,\n * basing on the following two facts.\n *\n * First, all the I/O flows of a the same application or task\n * contribute to the execution/completion of that common application\n * or task. So the performance figures that matter are total\n * throughput of the flows and task-wide I/O latency.  In particular,\n * these flows do not need to be protected from each other, in terms\n * of individual bandwidth or latency.\n *\n * Second, the above fact holds regardless of the number of flows.\n *\n * Putting these two facts together, this commits merges stably the\n * bfq_queues associated with these I/O flows, i.e., with the\n * processes that generate these IO/ flows, regardless of how many the\n * involved processes are.\n *\n * To decide whether a set of bfq_queues is actually associated with\n * the I/O flows of a common application or task, and to merge these\n * queues stably, this function operates as follows: given a bfq_queue,\n * say Q2, currently being created, and the last bfq_queue, say Q1,\n * created before Q2, Q2 is merged stably with Q1 if\n * - very little time has elapsed since when Q1 was created\n * - Q2 has the same ioprio as Q1\n * - Q2 belongs to the same group as Q1\n *\n * Merging bfq_queues also reduces scheduling overhead. A fio test\n * with ten random readers on /dev/nullb shows a throughput boost of\n * 40%, with a quadcore. Since BFQ's execution time amounts to ~50% of\n * the total per-request processing time, the above throughput boost\n * implies that BFQ's overhead is reduced by more than 50%.\n *\n * This new mechanism most certainly obsoletes the current\n * burst-handling heuristics. We keep those heuristics for the moment.\n */\nstatic struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,\n\t\t\t\t\t\t      struct bfq_queue *bfqq,\n\t\t\t\t\t\t      struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue **source_bfqq = bfqq->entity.parent ?\n\t\t&bfqq->entity.parent->last_bfqq_created :\n\t\t&bfqd->last_bfqq_created;\n\n\tstruct bfq_queue *last_bfqq_created = *source_bfqq;\n\n\t/*\n\t * If last_bfqq_created has not been set yet, then init it. If\n\t * it has been set already, but too long ago, then move it\n\t * forward to bfqq. Finally, move also if bfqq belongs to a\n\t * different group than last_bfqq_created, or if bfqq has a\n\t * different ioprio, ioprio_class or actuator_idx. If none of\n\t * these conditions holds true, then try an early stable merge\n\t * or schedule a delayed stable merge. As for the condition on\n\t * actuator_idx, the reason is that, if queues associated with\n\t * different actuators are merged, then control is lost on\n\t * each actuator. Therefore some actuator may be\n\t * underutilized, and throughput may decrease.\n\t *\n\t * A delayed merge is scheduled (instead of performing an\n\t * early merge), in case bfqq might soon prove to be more\n\t * throughput-beneficial if not merged. Currently this is\n\t * possible only if bfqd is rotational with no queueing. For\n\t * such a drive, not merging bfqq is better for throughput if\n\t * bfqq happens to contain sequential I/O. So, we wait a\n\t * little bit for enough I/O to flow through bfqq. After that,\n\t * if such an I/O is sequential, then the merge is\n\t * canceled. Otherwise the merge is finally performed.\n\t */\n\tif (!last_bfqq_created ||\n\t    time_before(last_bfqq_created->creation_time +\n\t\t\tmsecs_to_jiffies(bfq_activation_stable_merging),\n\t\t\tbfqq->creation_time) ||\n\t\tbfqq->entity.parent != last_bfqq_created->entity.parent ||\n\t\tbfqq->ioprio != last_bfqq_created->ioprio ||\n\t\tbfqq->ioprio_class != last_bfqq_created->ioprio_class ||\n\t\tbfqq->actuator_idx != last_bfqq_created->actuator_idx)\n\t\t*source_bfqq = bfqq;\n\telse if (time_after_eq(last_bfqq_created->creation_time +\n\t\t\t\t bfqd->bfq_burst_interval,\n\t\t\t\t bfqq->creation_time)) {\n\t\tif (likely(bfqd->nonrot_with_queueing))\n\t\t\t/*\n\t\t\t * With this type of drive, leaving\n\t\t\t * bfqq alone may provide no\n\t\t\t * throughput benefits compared with\n\t\t\t * merging bfqq. So merge bfqq now.\n\t\t\t */\n\t\t\tbfqq = bfq_do_early_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t\t bic,\n\t\t\t\t\t\t\t last_bfqq_created);\n\t\telse { /* schedule tentative stable merge */\n\t\t\t/*\n\t\t\t * get reference on last_bfqq_created,\n\t\t\t * to prevent it from being freed,\n\t\t\t * until we decide whether to merge\n\t\t\t */\n\t\t\tlast_bfqq_created->ref++;\n\t\t\t/*\n\t\t\t * need to keep track of stable refs, to\n\t\t\t * compute process refs correctly\n\t\t\t */\n\t\t\tlast_bfqq_created->stable_ref++;\n\t\t\t/*\n\t\t\t * Record the bfqq to merge to.\n\t\t\t */\n\t\t\tbic->bfqq_data[last_bfqq_created->actuator_idx].stable_merge_bfqq =\n\t\t\t\tlast_bfqq_created;\n\t\t}\n\t}\n\n\treturn bfqq;\n}\n\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn)\n{\n\tconst int ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\tconst int ioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tstruct bfq_queue **async_bfqq = NULL;\n\tstruct bfq_queue *bfqq;\n\tstruct bfq_group *bfqg;\n\n\tbfqg = bfq_bio_bfqg(bfqd, bio);\n\tif (!is_sync) {\n\t\tasync_bfqq = bfq_async_queue_prio(bfqd, bfqg, ioprio_class,\n\t\t\t\t\t\t  ioprio,\n\t\t\t\t\t\t  bfq_actuator_index(bfqd, bio));\n\t\tbfqq = *async_bfqq;\n\t\tif (bfqq)\n\t\t\tgoto out;\n\t}\n\n\tbfqq = kmem_cache_alloc_node(bfq_pool,\n\t\t\t\t     GFP_NOWAIT | __GFP_ZERO | __GFP_NOWARN,\n\t\t\t\t     bfqd->queue->node);\n\n\tif (bfqq) {\n\t\tbfq_init_bfqq(bfqd, bfqq, bic, current->pid,\n\t\t\t      is_sync, bfq_actuator_index(bfqd, bio));\n\t\tbfq_init_entity(&bfqq->entity, bfqg);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"allocated\");\n\t} else {\n\t\tbfqq = &bfqd->oom_bfqq;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"using oom bfqq\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Pin the queue now that it's allocated, scheduler exit will\n\t * prune it.\n\t */\n\tif (async_bfqq) {\n\t\tbfqq->ref++; /*\n\t\t\t      * Extra group reference, w.r.t. sync\n\t\t\t      * queue. This extra reference is removed\n\t\t\t      * only if bfqq->bfqg disappears, to\n\t\t\t      * guarantee that this queue is not freed\n\t\t\t      * until its group goes away.\n\t\t\t      */\n\t\tbfq_log_bfqq(bfqd, bfqq, \"get_queue, bfqq not in async: %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\t*async_bfqq = bfqq;\n\t}\n\nout:\n\tbfqq->ref++; /* get a process reference to this queue */\n\n\tif (bfqq != &bfqd->oom_bfqq && is_sync && !respawn)\n\t\tbfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);\n\treturn bfqq;\n}\n\nstatic void bfq_update_io_thinktime(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tstruct bfq_ttime *ttime = &bfqq->ttime;\n\tu64 elapsed;\n\n\t/*\n\t * We are really interested in how long it takes for the queue to\n\t * become busy when there is no outstanding IO for this queue. So\n\t * ignore cases when the bfq queue has already IO queued.\n\t */\n\tif (bfqq->dispatched || bfq_bfqq_busy(bfqq))\n\t\treturn;\n\telapsed = blk_time_get_ns() - bfqq->ttime.last_end_request;\n\telapsed = min_t(u64, elapsed, 2ULL * bfqd->bfq_slice_idle);\n\n\tttime->ttime_samples = (7*ttime->ttime_samples + 256) / 8;\n\tttime->ttime_total = div_u64(7*ttime->ttime_total + 256*elapsed,  8);\n\tttime->ttime_mean = div64_ul(ttime->ttime_total + 128,\n\t\t\t\t     ttime->ttime_samples);\n}\n\nstatic void\nbfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct request *rq)\n{\n\tbfqq->seek_history <<= 1;\n\tbfqq->seek_history |= BFQ_RQ_SEEKY(bfqd, bfqq->last_request_pos, rq);\n\n\tif (bfqq->wr_coeff > 1 &&\n\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t    BFQQ_TOTALLY_SEEKY(bfqq)) {\n\t\tif (time_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t   bfq_wr_duration(bfqd))) {\n\t\t\t/*\n\t\t\t * In soft_rt weight raising with the\n\t\t\t * interactive-weight-raising period\n\t\t\t * elapsed (so no switch back to\n\t\t\t * interactive weight raising).\n\t\t\t */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t} else { /*\n\t\t\t  * stopping soft_rt weight raising\n\t\t\t  * while still in interactive period,\n\t\t\t  * switch back to interactive weight\n\t\t\t  * raising\n\t\t\t  */\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n}\n\nstatic void bfq_update_has_short_ttime(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq,\n\t\t\t\t       struct bfq_io_cq *bic)\n{\n\tbool has_short_ttime = true, state_changed;\n\n\t/*\n\t * No need to update has_short_ttime if bfqq is async or in\n\t * idle io prio class, or if bfq_slice_idle is zero, because\n\t * no device idling is performed for bfqq in this case.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || bfq_class_idle(bfqq) ||\n\t    bfqd->bfq_slice_idle == 0)\n\t\treturn;\n\n\t/* Idle window just restored, statistics are meaningless. */\n\tif (time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     bfqd->bfq_wr_min_idle_time))\n\t\treturn;\n\n\t/* Think time is infinite if no process is linked to\n\t * bfqq. Otherwise check average think time to decide whether\n\t * to mark as has_short_ttime. To this goal, compare average\n\t * think time with half the I/O-plugging timeout.\n\t */\n\tif (atomic_read(&bic->icq.ioc->active_ref) == 0 ||\n\t    (bfq_sample_valid(bfqq->ttime.ttime_samples) &&\n\t     bfqq->ttime.ttime_mean > bfqd->bfq_slice_idle>>1))\n\t\thas_short_ttime = false;\n\n\tstate_changed = has_short_ttime != bfq_bfqq_has_short_ttime(bfqq);\n\n\tif (has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * Until the base value for the total service time gets\n\t * finally computed for bfqq, the inject limit does depend on\n\t * the think-time state (short|long). In particular, the limit\n\t * is 0 or 1 if the think time is deemed, respectively, as\n\t * short or long (details in the comments in\n\t * bfq_update_inject_limit()). Accordingly, the next\n\t * instructions reset the inject limit if the think-time state\n\t * has changed and the above base value is still to be\n\t * computed.\n\t *\n\t * However, the reset is performed only if more than 100 ms\n\t * have elapsed since the last update of the inject limit, or\n\t * (inclusive) if the change is from short to long think\n\t * time. The reason for this waiting is as follows.\n\t *\n\t * bfqq may have a long think time because of a\n\t * synchronization with some other queue, i.e., because the\n\t * I/O of some other queue may need to be completed for bfqq\n\t * to receive new I/O. Details in the comments on the choice\n\t * of the queue for injection in bfq_select_queue().\n\t *\n\t * As stressed in those comments, if such a synchronization is\n\t * actually in place, then, without injection on bfqq, the\n\t * blocking I/O cannot happen to served while bfqq is in\n\t * service. As a consequence, if bfqq is granted\n\t * I/O-dispatch-plugging, then bfqq remains empty, and no I/O\n\t * is dispatched, until the idle timeout fires. This is likely\n\t * to result in lower bandwidth and higher latencies for bfqq,\n\t * and in a severe loss of total throughput.\n\t *\n\t * On the opposite end, a non-zero inject limit may allow the\n\t * I/O that blocks bfqq to be executed soon, and therefore\n\t * bfqq to receive new I/O soon.\n\t *\n\t * But, if the blocking gets actually eliminated, then the\n\t * next think-time sample for bfqq may be very low. This in\n\t * turn may cause bfqq's think time to be deemed\n\t * short. Without the 100 ms barrier, this new state change\n\t * would cause the body of the next if to be executed\n\t * immediately. But this would set to 0 the inject\n\t * limit. Without injection, the blocking I/O would cause the\n\t * think time of bfqq to become long again, and therefore the\n\t * inject limit to be raised again, and so on. The only effect\n\t * of such a steady oscillation between the two think-time\n\t * states would be to prevent effective injection on bfqq.\n\t *\n\t * In contrast, if the inject limit is not reset during such a\n\t * long time interval as 100 ms, then the number of short\n\t * think time samples can grow significantly before the reset\n\t * is performed. As a consequence, the think time state can\n\t * become stable before the reset. Therefore there will be no\n\t * state change when the 100 ms elapse, and no reset of the\n\t * inject limit. The inject limit remains steadily equal to 1\n\t * both during and after the 100 ms. So injection can be\n\t * performed at all times, and throughput gets boosted.\n\t *\n\t * An inject limit equal to 1 is however in conflict, in\n\t * general, with the fact that the think time of bfqq is\n\t * short, because injection may be likely to delay bfqq's I/O\n\t * (as explained in the comments in\n\t * bfq_update_inject_limit()). But this does not happen in\n\t * this special case, because bfqq's low think time is due to\n\t * an effective handling of a synchronization, through\n\t * injection. In this special case, bfqq's I/O does not get\n\t * delayed by injection; on the contrary, bfqq's I/O is\n\t * brought forward, because it is not blocked for\n\t * milliseconds.\n\t *\n\t * In addition, serving the blocking I/O much sooner, and much\n\t * more frequently than once per I/O-plugging timeout, makes\n\t * it much quicker to detect a waker queue (the concept of\n\t * waker queue is defined in the comments in\n\t * bfq_add_request()). This makes it possible to start sooner\n\t * to boost throughput more effectively, by injecting the I/O\n\t * of the waker queue unconditionally on every\n\t * bfq_dispatch_request().\n\t *\n\t * One last, important benefit of not resetting the inject\n\t * limit before 100 ms is that, during this time interval, the\n\t * base value for the total service time is likely to get\n\t * finally computed for bfqq, freeing the inject limit from\n\t * its relation with the think time.\n\t */\n\tif (state_changed && bfqq->last_serv_time_ns == 0 &&\n\t    (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t      msecs_to_jiffies(100)) ||\n\t     !has_short_ttime))\n\t\tbfq_reset_inject_limit(bfqd, bfqq);\n}\n\n/*\n * Called when a new fs request (rq) is added to bfqq.  Check if there's\n * something we should do about it.\n */\nstatic void bfq_rq_enqueued(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending++;\n\n\tbfqq->last_request_pos = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\n\tif (bfqq == bfqd->in_service_queue && bfq_bfqq_wait_request(bfqq)) {\n\t\tbool small_req = bfqq->queued[rq_is_sync(rq)] == 1 &&\n\t\t\t\t blk_rq_sectors(rq) < 32;\n\t\tbool budget_timeout = bfq_bfqq_budget_timeout(bfqq);\n\n\t\t/*\n\t\t * There is just this request queued: if\n\t\t * - the request is small, and\n\t\t * - we are idling to boost throughput, and\n\t\t * - the queue is not to be expired,\n\t\t * then just exit.\n\t\t *\n\t\t * In this way, if the device is being idled to wait\n\t\t * for a new request from the in-service queue, we\n\t\t * avoid unplugging the device and committing the\n\t\t * device to serve just a small request. In contrast\n\t\t * we wait for the block layer to decide when to\n\t\t * unplug the device: hopefully, new requests will be\n\t\t * merged to this one quickly, then the device will be\n\t\t * unplugged and larger requests will be dispatched.\n\t\t */\n\t\tif (small_req && idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t    !budget_timeout)\n\t\t\treturn;\n\n\t\t/*\n\t\t * A large enough request arrived, or idling is being\n\t\t * performed to preserve service guarantees, or\n\t\t * finally the queue is to be expired: in all these\n\t\t * cases disk idling is to be stopped, so clear\n\t\t * wait_request flag and reset timer.\n\t\t */\n\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\n\t\t/*\n\t\t * The queue is not empty, because a new request just\n\t\t * arrived. Hence we can safely expire the queue, in\n\t\t * case of budget timeout, without risking that the\n\t\t * timestamps of the queue are not updated correctly.\n\t\t * See [1] for more details.\n\t\t */\n\t\tif (budget_timeout)\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t}\n}\n\nstatic void bfqq_request_allocated(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated++;\n}\n\nstatic void bfqq_request_freed(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated--;\n}\n\n/* returns true if it causes the idle timer to be disabled */\nstatic bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,\n\t\t\t\t\t\t RQ_BIC(rq));\n\tbool waiting, idle_timer_disabled = false;\n\n\tif (new_bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\t\t/*\n\t\t * Release the request's reference to the old bfqq\n\t\t * and make sure one is taken to the shared queue.\n\t\t */\n\t\tbfqq_request_allocated(new_bfqq);\n\t\tbfqq_request_freed(bfqq);\n\t\tnew_bfqq->ref++;\n\t\t/*\n\t\t * If the bic associated with the process\n\t\t * issuing this request still points to bfqq\n\t\t * (and thus has not been already redirected\n\t\t * to new_bfqq or even some other bfq_queue),\n\t\t * then complete the merge and redirect it to\n\t\t * new_bfqq.\n\t\t */\n\t\tif (bic_to_bfqq(RQ_BIC(rq), true,\n\t\t\t\tbfq_actuator_index(bfqd, rq->bio)) == bfqq) {\n\t\t\twhile (bfqq != new_bfqq)\n\t\t\t\tbfqq = bfq_merge_bfqqs(bfqd, RQ_BIC(rq), bfqq);\n\t\t}\n\n\t\tbfq_clear_bfqq_just_created(old_bfqq);\n\t\t/*\n\t\t * rq is about to be enqueued into new_bfqq,\n\t\t * release rq reference on bfqq\n\t\t */\n\t\tbfq_put_queue(old_bfqq);\n\t\trq->elv.priv[1] = new_bfqq;\n\t}\n\n\tbfq_update_io_thinktime(bfqd, bfqq);\n\tbfq_update_has_short_ttime(bfqd, bfqq, RQ_BIC(rq));\n\tbfq_update_io_seektime(bfqd, bfqq, rq);\n\n\twaiting = bfqq && bfq_bfqq_wait_request(bfqq);\n\tbfq_add_request(rq);\n\tidle_timer_disabled = waiting && !bfq_bfqq_wait_request(bfqq);\n\n\trq->fifo_time = blk_time_get_ns() + bfqd->bfq_fifo_expire[rq_is_sync(rq)];\n\tlist_add_tail(&rq->queuelist, &bfqq->fifo);\n\n\tbfq_rq_enqueued(bfqd, bfqq, rq);\n\n\treturn idle_timer_disabled;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t    struct bfq_queue *bfqq,\n\t\t\t\t    bool idle_timer_disabled,\n\t\t\t\t    blk_opf_t cmd_flags)\n{\n\tif (!bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq still exists, because it can disappear only after\n\t * either it is merged with another queue, or the process it\n\t * is associated with exits. But both actions must be taken by\n\t * the same process currently executing this flow of\n\t * instructions.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tbfqg_stats_update_io_add(bfqq_group(bfqq), bfqq, cmd_flags);\n\tif (idle_timer_disabled)\n\t\tbfqg_stats_update_idle_time(bfqq_group(bfqq));\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t\t   struct bfq_queue *bfqq,\n\t\t\t\t\t   bool idle_timer_disabled,\n\t\t\t\t\t   blk_opf_t cmd_flags) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct bfq_queue *bfq_init_rq(struct request *rq);\n\nstatic void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t       blk_insert_t flags)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_queue *bfqq;\n\tbool idle_timer_disabled = false;\n\tblk_opf_t cmd_flags;\n\tLIST_HEAD(free);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)\n\t\tbfqg_stats_update_legacy_io(q, rq);\n#endif\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bfq_init_rq(rq);\n\tif (blk_mq_sched_try_insert_merge(q, rq, &free)) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tblk_mq_free_requests(&free);\n\t\treturn;\n\t}\n\n\ttrace_block_rq_insert(rq);\n\n\tif (flags & BLK_MQ_INSERT_AT_HEAD) {\n\t\tlist_add(&rq->queuelist, &bfqd->dispatch);\n\t} else if (!bfqq) {\n\t\tlist_add_tail(&rq->queuelist, &bfqd->dispatch);\n\t} else {\n\t\tidle_timer_disabled = __bfq_insert_request(bfqd, rq);\n\t\t/*\n\t\t * Update bfqq, because, if a queue merge has occurred\n\t\t * in __bfq_insert_request, then rq has been\n\t\t * redirected into a new queue.\n\t\t */\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (rq_mergeable(rq)) {\n\t\t\telv_rqhash_add(q, rq);\n\t\t\tif (!q->last_merge)\n\t\t\t\tq->last_merge = rq;\n\t\t}\n\t}\n\n\t/*\n\t * Cache cmd_flags before releasing scheduler lock, because rq\n\t * may disappear afterwards (for example, because of a request\n\t * merge).\n\t */\n\tcmd_flags = rq->cmd_flags;\n\tspin_unlock_irq(&bfqd->lock);\n\n\tbfq_update_insert_stats(q, bfqq, idle_timer_disabled,\n\t\t\t\tcmd_flags);\n}\n\nstatic void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t\tstruct list_head *list,\n\t\t\t\tblk_insert_t flags)\n{\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tbfq_insert_request(hctx, rq, flags);\n\t}\n}\n\nstatic void bfq_update_hw_tag(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\tbfqd->max_rq_in_driver = max_t(int, bfqd->max_rq_in_driver,\n\t\t\t\t       bfqd->tot_rq_in_driver);\n\n\tif (bfqd->hw_tag == 1)\n\t\treturn;\n\n\t/*\n\t * This sample is valid if the number of outstanding requests\n\t * is large enough to allow a queueing behavior.  Note that the\n\t * sum is not exact, as it's not taking into account deactivated\n\t * requests.\n\t */\n\tif (bfqd->tot_rq_in_driver + bfqd->queued <= BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\t/*\n\t * If active queue hasn't enough requests and can idle, bfq might not\n\t * dispatch sufficient requests to hardware. Don't zero hw_tag in this\n\t * case\n\t */\n\tif (bfqq && bfq_bfqq_has_short_ttime(bfqq) &&\n\t    bfqq->dispatched + bfqq->queued[0] + bfqq->queued[1] <\n\t    BFQ_HW_QUEUE_THRESHOLD &&\n\t    bfqd->tot_rq_in_driver < BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\tif (bfqd->hw_tag_samples++ < BFQ_HW_QUEUE_SAMPLES)\n\t\treturn;\n\n\tbfqd->hw_tag = bfqd->max_rq_in_driver > BFQ_HW_QUEUE_THRESHOLD;\n\tbfqd->max_rq_in_driver = 0;\n\tbfqd->hw_tag_samples = 0;\n\n\tbfqd->nonrot_with_queueing =\n\t\tblk_queue_nonrot(bfqd->queue) && bfqd->hw_tag;\n}\n\nstatic void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)\n{\n\tu64 now_ns;\n\tu32 delta_us;\n\n\tbfq_update_hw_tag(bfqd);\n\n\tbfqd->rq_in_driver[bfqq->actuator_idx]--;\n\tbfqd->tot_rq_in_driver--;\n\tbfqq->dispatched--;\n\n\tif (!bfqq->dispatched && !bfq_bfqq_busy(bfqq)) {\n\t\t/*\n\t\t * Set budget_timeout (which we overload to store the\n\t\t * time at which the queue remains with no backlog and\n\t\t * no outstanding request; used by the weight-raising\n\t\t * mechanism).\n\t\t */\n\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_in_groups_with_pending_reqs(bfqq);\n\t\tbfq_weights_tree_remove(bfqq);\n\t}\n\n\tnow_ns = blk_time_get_ns();\n\n\tbfqq->ttime.last_end_request = now_ns;\n\n\t/*\n\t * Using us instead of ns, to get a reasonable precision in\n\t * computing rate in next check.\n\t */\n\tdelta_us = div_u64(now_ns - bfqd->last_completion, NSEC_PER_USEC);\n\n\t/*\n\t * If the request took rather long to complete, and, according\n\t * to the maximum request size recorded, this completion latency\n\t * implies that the request was certainly served at a very low\n\t * rate (less than 1M sectors/sec), then the whole observation\n\t * interval that lasts up to this time instant cannot be a\n\t * valid time interval for computing a new peak rate.  Invoke\n\t * bfq_update_rate_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - reset to zero samples, which will trigger a proper\n\t *   re-initialization of the observation interval on next\n\t *   dispatch\n\t */\n\tif (delta_us > BFQ_MIN_TT/NSEC_PER_USEC &&\n\t   (bfqd->last_rq_max_size<<BFQ_RATE_SHIFT)/delta_us <\n\t\t\t1UL<<(BFQ_RATE_SHIFT - 10))\n\t\tbfq_update_rate_reset(bfqd, NULL);\n\tbfqd->last_completion = now_ns;\n\t/*\n\t * Shared queues are likely to receive I/O at a high\n\t * rate. This may deceptively let them be considered as wakers\n\t * of other queues. But a false waker will unjustly steal\n\t * bandwidth to its supposedly woken queue. So considering\n\t * also shared queues in the waking mechanism may cause more\n\t * control troubles than throughput benefits. Then reset\n\t * last_completed_rq_bfqq if bfqq is a shared queue.\n\t */\n\tif (!bfq_bfqq_coop(bfqq))\n\t\tbfqd->last_completed_rq_bfqq = bfqq;\n\telse\n\t\tbfqd->last_completed_rq_bfqq = NULL;\n\n\t/*\n\t * If we are waiting to discover whether the request pattern\n\t * of the task associated with the queue is actually\n\t * isochronous, and both requisites for this condition to hold\n\t * are now satisfied, then compute soft_rt_next_start (see the\n\t * comments on the function bfq_bfqq_softrt_next_start()). We\n\t * do not compute soft_rt_next_start if bfqq is in interactive\n\t * weight raising (see the comments in bfq_bfqq_expire() for\n\t * an explanation). We schedule this delayed update when bfqq\n\t * expires, if it still has in-flight requests.\n\t */\n\tif (bfq_bfqq_softrt_update(bfqq) && bfqq->dispatched == 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq->wr_coeff != bfqd->bfq_wr_coeff)\n\t\tbfqq->soft_rt_next_start =\n\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\n\t/*\n\t * If this is the in-service queue, check if it needs to be expired,\n\t * or if we want to idle in case it has no pending requests.\n\t */\n\tif (bfqd->in_service_queue == bfqq) {\n\t\tif (bfq_bfqq_must_idle(bfqq)) {\n\t\t\tif (bfqq->dispatched == 0)\n\t\t\t\tbfq_arm_slice_timer(bfqd);\n\t\t\t/*\n\t\t\t * If we get here, we do not expire bfqq, even\n\t\t\t * if bfqq was in budget timeout or had no\n\t\t\t * more requests (as controlled in the next\n\t\t\t * conditional instructions). The reason for\n\t\t\t * not expiring bfqq is as follows.\n\t\t\t *\n\t\t\t * Here bfqq->dispatched > 0 holds, but\n\t\t\t * bfq_bfqq_must_idle() returned true. This\n\t\t\t * implies that, even if no request arrives\n\t\t\t * for bfqq before bfqq->dispatched reaches 0,\n\t\t\t * bfqq will, however, not be expired on the\n\t\t\t * completion event that causes bfqq->dispatch\n\t\t\t * to reach zero. In contrast, on this event,\n\t\t\t * bfqq will start enjoying device idling\n\t\t\t * (I/O-dispatch plugging).\n\t\t\t *\n\t\t\t * But, if we expired bfqq here, bfqq would\n\t\t\t * not have the chance to enjoy device idling\n\t\t\t * when bfqq->dispatched finally reaches\n\t\t\t * zero. This would expose bfqq to violation\n\t\t\t * of its reserved service guarantees.\n\t\t\t */\n\t\t\treturn;\n\t\t} else if (bfq_may_expire_for_budg_timeout(bfqq))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t\telse if (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t (bfqq->dispatched == 0 ||\n\t\t\t  !bfq_better_to_idle(bfqq)))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_NO_MORE_REQUESTS);\n\t}\n\n\tif (!bfqd->tot_rq_in_driver)\n\t\tbfq_schedule_dispatch(bfqd);\n}\n\n/*\n * The processes associated with bfqq may happen to generate their\n * cumulative I/O at a lower rate than the rate at which the device\n * could serve the same I/O. This is rather probable, e.g., if only\n * one process is associated with bfqq and the device is an SSD. It\n * results in bfqq becoming often empty while in service. In this\n * respect, if BFQ is allowed to switch to another queue when bfqq\n * remains empty, then the device goes on being fed with I/O requests,\n * and the throughput is not affected. In contrast, if BFQ is not\n * allowed to switch to another queue---because bfqq is sync and\n * I/O-dispatch needs to be plugged while bfqq is temporarily\n * empty---then, during the service of bfqq, there will be frequent\n * \"service holes\", i.e., time intervals during which bfqq gets empty\n * and the device can only consume the I/O already queued in its\n * hardware queues. During service holes, the device may even get to\n * remaining idle. In the end, during the service of bfqq, the device\n * is driven at a lower speed than the one it can reach with the kind\n * of I/O flowing through bfqq.\n *\n * To counter this loss of throughput, BFQ implements a \"request\n * injection mechanism\", which tries to fill the above service holes\n * with I/O requests taken from other queues. The hard part in this\n * mechanism is finding the right amount of I/O to inject, so as to\n * both boost throughput and not break bfqq's bandwidth and latency\n * guarantees. In this respect, the mechanism maintains a per-queue\n * inject limit, computed as below. While bfqq is empty, the injection\n * mechanism dispatches extra I/O requests only until the total number\n * of I/O requests in flight---i.e., already dispatched but not yet\n * completed---remains lower than this limit.\n *\n * A first definition comes in handy to introduce the algorithm by\n * which the inject limit is computed.  We define as first request for\n * bfqq, an I/O request for bfqq that arrives while bfqq is in\n * service, and causes bfqq to switch from empty to non-empty. The\n * algorithm updates the limit as a function of the effect of\n * injection on the service times of only the first requests of\n * bfqq. The reason for this restriction is that these are the\n * requests whose service time is affected most, because they are the\n * first to arrive after injection possibly occurred.\n *\n * To evaluate the effect of injection, the algorithm measures the\n * \"total service time\" of first requests. We define as total service\n * time of an I/O request, the time that elapses since when the\n * request is enqueued into bfqq, to when it is completed. This\n * quantity allows the whole effect of injection to be measured. It is\n * easy to see why. Suppose that some requests of other queues are\n * actually injected while bfqq is empty, and that a new request R\n * then arrives for bfqq. If the device does start to serve all or\n * part of the injected requests during the service hole, then,\n * because of this extra service, it may delay the next invocation of\n * the dispatch hook of BFQ. Then, even after R gets eventually\n * dispatched, the device may delay the actual service of R if it is\n * still busy serving the extra requests, or if it decides to serve,\n * before R, some extra request still present in its queues. As a\n * conclusion, the cumulative extra delay caused by injection can be\n * easily evaluated by just comparing the total service time of first\n * requests with and without injection.\n *\n * The limit-update algorithm works as follows. On the arrival of a\n * first request of bfqq, the algorithm measures the total time of the\n * request only if one of the three cases below holds, and, for each\n * case, it updates the limit as described below:\n *\n * (1) If there is no in-flight request. This gives a baseline for the\n *     total service time of the requests of bfqq. If the baseline has\n *     not been computed yet, then, after computing it, the limit is\n *     set to 1, to start boosting throughput, and to prepare the\n *     ground for the next case. If the baseline has already been\n *     computed, then it is updated, in case it results to be lower\n *     than the previous value.\n *\n * (2) If the limit is higher than 0 and there are in-flight\n *     requests. By comparing the total service time in this case with\n *     the above baseline, it is possible to know at which extent the\n *     current value of the limit is inflating the total service\n *     time. If the inflation is below a certain threshold, then bfqq\n *     is assumed to be suffering from no perceivable loss of its\n *     service guarantees, and the limit is even tentatively\n *     increased. If the inflation is above the threshold, then the\n *     limit is decreased. Due to the lack of any hysteresis, this\n *     logic makes the limit oscillate even in steady workload\n *     conditions. Yet we opted for it, because it is fast in reaching\n *     the best value for the limit, as a function of the current I/O\n *     workload. To reduce oscillations, this step is disabled for a\n *     short time interval after the limit happens to be decreased.\n *\n * (3) Periodically, after resetting the limit, to make sure that the\n *     limit eventually drops in case the workload changes. This is\n *     needed because, after the limit has gone safely up for a\n *     certain workload, it is impossible to guess whether the\n *     baseline total service time may have changed, without measuring\n *     it again without injection. A more effective version of this\n *     step might be to just sample the baseline, by interrupting\n *     injection only once, and then to reset/lower the limit only if\n *     the total service time with the current limit does happen to be\n *     too large.\n *\n * More details on each step are provided in the comments on the\n * pieces of code that implement these steps: the branch handling the\n * transition from empty to non empty in bfq_add_request(), the branch\n * handling injection in bfq_select_queue(), and the function\n * bfq_choose_bfqq_for_injection(). These comments also explain some\n * exceptions, made by the injection mechanism in some special cases.\n */\nstatic void bfq_update_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tu64 tot_time_ns = blk_time_get_ns() - bfqd->last_empty_occupied_ns;\n\tunsigned int old_limit = bfqq->inject_limit;\n\n\tif (bfqq->last_serv_time_ns > 0 && bfqd->rqs_injected) {\n\t\tu64 threshold = (bfqq->last_serv_time_ns * 3)>>1;\n\n\t\tif (tot_time_ns >= threshold && old_limit > 0) {\n\t\t\tbfqq->inject_limit--;\n\t\t\tbfqq->decrease_time_jif = jiffies;\n\t\t} else if (tot_time_ns < threshold &&\n\t\t\t   old_limit <= bfqd->max_rq_in_driver)\n\t\t\tbfqq->inject_limit++;\n\t}\n\n\t/*\n\t * Either we still have to compute the base value for the\n\t * total service time, and there seem to be the right\n\t * conditions to do it, or we can lower the last base value\n\t * computed.\n\t *\n\t * NOTE: (bfqd->tot_rq_in_driver == 1) means that there is no I/O\n\t * request in flight, because this function is in the code\n\t * path that handles the completion of a request of bfqq, and,\n\t * in particular, this function is executed before\n\t * bfqd->tot_rq_in_driver is decremented in such a code path.\n\t */\n\tif ((bfqq->last_serv_time_ns == 0 && bfqd->tot_rq_in_driver == 1) ||\n\t    tot_time_ns < bfqq->last_serv_time_ns) {\n\t\tif (bfqq->last_serv_time_ns == 0) {\n\t\t\t/*\n\t\t\t * Now we certainly have a base value: make sure we\n\t\t\t * start trying injection.\n\t\t\t */\n\t\t\tbfqq->inject_limit = max_t(unsigned int, 1, old_limit);\n\t\t}\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\t} else if (!bfqd->rqs_injected && bfqd->tot_rq_in_driver == 1)\n\t\t/*\n\t\t * No I/O injected and no request still in service in\n\t\t * the drive: these are the exact conditions for\n\t\t * computing the base value of the total service time\n\t\t * for bfqq. So let's update this value, because it is\n\t\t * rather variable. For example, it varies if the size\n\t\t * or the spatial locality of the I/O requests in bfqq\n\t\t * change.\n\t\t */\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\n\n\t/* update complete, not waiting for any request completion any longer */\n\tbfqd->waited_rq = NULL;\n\tbfqd->rqs_injected = false;\n}\n\n/*\n * Handle either a requeue or a finish for rq. The things to do are\n * the same in both cases: all references to rq are to be dropped. In\n * particular, rq is considered completed from the point of view of\n * the scheduler.\n */\nstatic void bfq_finish_requeue_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd;\n\tunsigned long flags;\n\n\t/*\n\t * rq either is not associated with any icq, or is an already\n\t * requeued request that has not (yet) been re-inserted into\n\t * a bfq_queue.\n\t */\n\tif (!rq->elv.icq || !bfqq)\n\t\treturn;\n\n\tbfqd = bfqq->bfqd;\n\n\tif (rq->rq_flags & RQF_STARTED)\n\t\tbfqg_stats_update_completion(bfqq_group(bfqq),\n\t\t\t\t\t     rq->start_time_ns,\n\t\t\t\t\t     rq->io_start_time_ns,\n\t\t\t\t\t     rq->cmd_flags);\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\tif (likely(rq->rq_flags & RQF_STARTED)) {\n\t\tif (rq == bfqd->waited_rq)\n\t\t\tbfq_update_inject_limit(bfqd, bfqq);\n\n\t\tbfq_completed_request(bfqq, bfqd);\n\t}\n\tbfqq_request_freed(bfqq);\n\tbfq_put_queue(bfqq);\n\tRQ_BIC(rq)->requests--;\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\n\t/*\n\t * Reset private fields. In case of a requeue, this allows\n\t * this function to correctly do nothing if it is spuriously\n\t * invoked again on this same request (see the check at the\n\t * beginning of the function). Probably, a better general\n\t * design would be to prevent blk-mq from invoking the requeue\n\t * or finish hooks of an elevator, for a request that is not\n\t * referred by that elevator.\n\t *\n\t * Resetting the following fields would break the\n\t * request-insertion logic if rq is re-inserted into a bfq\n\t * internal queue, without a re-preparation. Here we assume\n\t * that re-insertions of requeued requests, without\n\t * re-preparation, can happen only for pass_through or at_head\n\t * requests (which are not re-inserted into bfq internal\n\t * queues).\n\t */\n\trq->elv.priv[0] = NULL;\n\trq->elv.priv[1] = NULL;\n}\n\nstatic void bfq_finish_request(struct request *rq)\n{\n\tbfq_finish_requeue_request(rq);\n\n\tif (rq->elv.icq) {\n\t\tput_io_context(rq->elv.icq->ioc);\n\t\trq->elv.icq = NULL;\n\t}\n}\n\n/*\n * Removes the association between the current task and bfqq, assuming\n * that bic points to the bfq iocontext of the task.\n * Returns NULL if a new bfqq should be allocated, or the old bfqq if this\n * was the last process referring to that bfqq.\n */\nstatic struct bfq_queue *\nbfq_split_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"splitting queue\");\n\n\tif (bfqq_process_refs(bfqq) == 1 && !bfqq->new_bfqq) {\n\t\tbfqq->pid = current->pid;\n\t\tbfq_clear_bfqq_coop(bfqq);\n\t\tbfq_clear_bfqq_split_coop(bfqq);\n\t\treturn bfqq;\n\t}\n\n\tbic_set_bfqq(bic, NULL, true, bfqq->actuator_idx);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqq->bfqd, bfqq);\n\treturn NULL;\n}\n\nstatic struct bfq_queue *\n__bfq_get_bfqq_handle_split(struct bfq_data *bfqd, struct bfq_io_cq *bic,\n\t\t\t    struct bio *bio, bool split, bool is_sync,\n\t\t\t    bool *new_queue)\n{\n\tunsigned int act_idx = bfq_actuator_index(bfqd, bio);\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync, act_idx);\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[act_idx];\n\n\tif (likely(bfqq && bfqq != &bfqd->oom_bfqq))\n\t\treturn bfqq;\n\n\tif (new_queue)\n\t\t*new_queue = true;\n\n\tif (bfqq)\n\t\tbfq_put_queue(bfqq);\n\tbfqq = bfq_get_queue(bfqd, bio, is_sync, bic, split);\n\n\tbic_set_bfqq(bic, bfqq, is_sync, act_idx);\n\tif (split && is_sync) {\n\t\tif ((bfqq_data->was_in_burst_list && bfqd->large_burst) ||\n\t\t    bfqq_data->saved_in_large_burst)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\telse {\n\t\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t\t\tif (bfqq_data->was_in_burst_list)\n\t\t\t\t/*\n\t\t\t\t * If bfqq was in the current\n\t\t\t\t * burst list before being\n\t\t\t\t * merged, then we have to add\n\t\t\t\t * it back. And we do not need\n\t\t\t\t * to increase burst_size, as\n\t\t\t\t * we did not decrement\n\t\t\t\t * burst_size when we removed\n\t\t\t\t * bfqq from the burst list as\n\t\t\t\t * a consequence of a merge\n\t\t\t\t * (see comments in\n\t\t\t\t * bfq_put_queue). In this\n\t\t\t\t * respect, it would be rather\n\t\t\t\t * costly to know whether the\n\t\t\t\t * current burst list is still\n\t\t\t\t * the same burst list from\n\t\t\t\t * which bfqq was removed on\n\t\t\t\t * the merge. To avoid this\n\t\t\t\t * cost, if bfqq was in a\n\t\t\t\t * burst list, then we add\n\t\t\t\t * bfqq to the current burst\n\t\t\t\t * list without any further\n\t\t\t\t * check. This can cause\n\t\t\t\t * inappropriate insertions,\n\t\t\t\t * but rarely enough to not\n\t\t\t\t * harm the detection of large\n\t\t\t\t * bursts significantly.\n\t\t\t\t */\n\t\t\t\thlist_add_head(&bfqq->burst_list_node,\n\t\t\t\t\t       &bfqd->burst_list);\n\t\t}\n\t\tbfqq->split_time = jiffies;\n\t}\n\n\treturn bfqq;\n}\n\n/*\n * Only reset private fields. The actual request preparation will be\n * performed by bfq_init_rq, when rq is either inserted or merged. See\n * comments on bfq_init_rq for the reason behind this delayed\n * preparation.\n */\nstatic void bfq_prepare_request(struct request *rq)\n{\n\trq->elv.icq = ioc_find_get_icq(rq->q);\n\n\t/*\n\t * Regardless of whether we have an icq attached, we have to\n\t * clear the scheduler pointers, as they might point to\n\t * previously allocated bic/bfqq structs.\n\t */\n\trq->elv.priv[0] = rq->elv.priv[1] = NULL;\n}\n\nstatic struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\tstruct bfq_queue *waker_bfqq = bfqq->waker_bfqq;\n\n\tif (!waker_bfqq)\n\t\treturn NULL;\n\n\twhile (new_bfqq) {\n\t\tif (new_bfqq == waker_bfqq) {\n\t\t\t/*\n\t\t\t * If waker_bfqq is in the merge chain, and current\n\t\t\t * is the only procress.\n\t\t\t */\n\t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n\t\t\t\treturn NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t}\n\n\treturn waker_bfqq;\n}\n\nstatic struct bfq_queue *bfq_get_bfqq_handle_split(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_io_cq *bic,\n\t\t\t\t\t\t   struct bio *bio,\n\t\t\t\t\t\t   unsigned int idx,\n\t\t\t\t\t\t   bool is_sync)\n{\n\tstruct bfq_queue *waker_bfqq;\n\tstruct bfq_queue *bfqq;\n\tbool new_queue = false;\n\n\tbfqq = __bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync,\n\t\t\t\t\t   &new_queue);\n\tif (unlikely(new_queue))\n\t\treturn bfqq;\n\n\t/* If the queue was seeky for too long, break it apart. */\n\tif (!bfq_bfqq_coop(bfqq) || !bfq_bfqq_split_coop(bfqq) ||\n\t    bic->bfqq_data[idx].stably_merged)\n\t\treturn bfqq;\n\n\twaker_bfqq = bfq_waker_bfqq(bfqq);\n\n\t/* Update bic before losing reference to bfqq */\n\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\tbic->bfqq_data[idx].saved_in_large_burst = true;\n\n\tbfqq = bfq_split_bfqq(bic, bfqq);\n\tif (bfqq) {\n\t\tbfq_bfqq_resume_state(bfqq, bfqd, bic, true);\n\t\treturn bfqq;\n\t}\n\n\tbfqq = __bfq_get_bfqq_handle_split(bfqd, bic, bio, true, is_sync, NULL);\n\tif (unlikely(bfqq == &bfqd->oom_bfqq))\n\t\treturn bfqq;\n\n\tbfq_bfqq_resume_state(bfqq, bfqd, bic, false);\n\tbfqq->waker_bfqq = waker_bfqq;\n\tbfqq->tentative_waker_bfqq = NULL;\n\n\t/*\n\t * If the waker queue disappears, then new_bfqq->waker_bfqq must be\n\t * reset. So insert new_bfqq into the\n\t * woken_list of the waker. See\n\t * bfq_check_waker for details.\n\t */\n\tif (waker_bfqq)\n\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t       &bfqq->waker_bfqq->woken_list);\n\n\treturn bfqq;\n}\n\n/*\n * If needed, init rq, allocate bfq data structures associated with\n * rq, and increment reference counters in the destination bfq_queue\n * for rq. Return the destination bfq_queue for rq, or NULL is rq is\n * not associated with any bfq_queue.\n *\n * This function is invoked by the functions that perform rq insertion\n * or merging. One may have expected the above preparation operations\n * to be performed in bfq_prepare_request, and not delayed to when rq\n * is inserted or merged. The rationale behind this delayed\n * preparation is that, after the prepare_request hook is invoked for\n * rq, rq may still be transformed into a request with no icq, i.e., a\n * request not associated with any queue. No bfq hook is invoked to\n * signal this transformation. As a consequence, should these\n * preparation operations be performed when the prepare_request hook\n * is invoked, and should rq be transformed one moment later, bfq\n * would end up in an inconsistent state, because it would have\n * incremented some queue counters for an rq destined to\n * transformation, without any chance to correctly lower these\n * counters back. In contrast, no transformation can still happen for\n * rq after rq has been inserted or merged. So, it is safe to execute\n * these preparation operations when rq is finally inserted or merged.\n */\nstatic struct bfq_queue *bfq_init_rq(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio = rq->bio;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic;\n\tconst int is_sync = rq_is_sync(rq);\n\tstruct bfq_queue *bfqq;\n\tunsigned int a_idx = bfq_actuator_index(bfqd, bio);\n\n\tif (unlikely(!rq->elv.icq))\n\t\treturn NULL;\n\n\t/*\n\t * Assuming that RQ_BFQQ(rq) is set only if everything is set\n\t * for this rq. This holds true, because this function is\n\t * invoked only for insertion or merging, and, after such\n\t * events, a request cannot be manipulated any longer before\n\t * being removed from bfq.\n\t */\n\tif (RQ_BFQQ(rq))\n\t\treturn RQ_BFQQ(rq);\n\n\tbic = icq_to_bic(rq->elv.icq);\n\tbfq_check_ioprio_change(bic, bio);\n\tbfq_bic_update_cgroup(bic, bio);\n\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, a_idx, is_sync);\n\n\tbfqq_request_allocated(bfqq);\n\tbfqq->ref++;\n\tbic->requests++;\n\tbfq_log_bfqq(bfqd, bfqq, \"get_request %p: bfqq %p, %d\",\n\t\t     rq, bfqq, bfqq->ref);\n\n\trq->elv.priv[0] = bic;\n\trq->elv.priv[1] = bfqq;\n\n\t/*\n\t * If a bfq_queue has only one process reference, it is owned\n\t * by only this bic: we can then set bfqq->bic = bic. in\n\t * addition, if the queue has also just been split, we have to\n\t * resume its state.\n\t */\n\tif (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq &&\n\t    bfqq_process_refs(bfqq) == 1)\n\t\tbfqq->bic = bic;\n\n\t/*\n\t * Consider bfqq as possibly belonging to a burst of newly\n\t * created queues only if:\n\t * 1) A burst is actually happening (bfqd->burst_size > 0)\n\t * or\n\t * 2) There is no other active queue. In fact, if, in\n\t *    contrast, there are active queues not belonging to the\n\t *    possible burst bfqq may belong to, then there is no gain\n\t *    in considering bfqq as belonging to a burst, and\n\t *    therefore in not weight-raising bfqq. See comments on\n\t *    bfq_handle_burst().\n\t *\n\t * This filtering also helps eliminating false positives,\n\t * occurring when bfqq does not belong to an actual large\n\t * burst, but some background task (e.g., a service) happens\n\t * to trigger the creation of new queues very close to when\n\t * bfqq and its possible companion queues are created. See\n\t * comments on bfq_handle_burst() for further details also on\n\t * this issue.\n\t */\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     (bfqd->burst_size > 0 ||\n\t\t      bfq_tot_busy_queues(bfqd) == 0)))\n\t\tbfq_handle_burst(bfqd, bfqq);\n\n\treturn bfqq;\n}\n\nstatic void\nbfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\n\t/*\n\t * Considering that bfqq may be in race, we should firstly check\n\t * whether bfqq is in service before doing something on it. If\n\t * the bfqq in race is not in service, it has already been expired\n\t * through __bfq_bfqq_expire func and its wait_request flags has\n\t * been cleared in __bfq_bfqd_reset_in_service func.\n\t */\n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t/*\n\t\t * Also here the queue can be safely expired\n\t\t * for budget timeout without wasting\n\t\t * guarantees\n\t\t */\n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t/*\n\t\t * The queue may not be empty upon timer expiration,\n\t\t * because we may not disable the timer when the\n\t\t * first request of the in-service queue arrives\n\t\t * during disk idling.\n\t\t */\n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tbfq_schedule_dispatch(bfqd);\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n/*\n * Handler of the expiration of the timer running if the in-service queue\n * is idling inside its time slice.\n */\nstatic enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t/*\n\t * Theoretical race here: the in-service queue can be NULL or\n\t * different from the queue that was idling if a new request\n\t * arrives for the current queue and there is a full dispatch\n\t * cycle that changes the in-service queue.  This can hardly\n\t * happen, but in the worst case we just expire a queue too\n\t * early.\n\t */\n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __bfq_put_async_bfqq(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue **bfqq_ptr)\n{\n\tstruct bfq_queue *bfqq = *bfqq_ptr;\n\n\tbfq_log(bfqd, \"put_async_bfqq: %p\", bfqq);\n\tif (bfqq) {\n\t\tbfq_bfqq_move(bfqd, bfqq, bfqd->root_group);\n\n\t\tbfq_log_bfqq(bfqd, bfqq, \"put_async_bfqq: putting %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\tbfq_put_queue(bfqq);\n\t\t*bfqq_ptr = NULL;\n\t}\n}\n\n/*\n * Release all the bfqg references to its async queues.  If we are\n * deallocating the group these queues may still contain requests, so\n * we reparent them to the root cgroup (i.e., the only one that will\n * exist for sure until all the requests on a device are gone).\n */\nvoid bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)\n{\n\tint i, j, k;\n\n\tfor (k = 0; k < bfqd->num_actuators; k++) {\n\t\tfor (i = 0; i < 2; i++)\n\t\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_bfqq[i][j][k]);\n\n\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_idle_bfqq[k]);\n\t}\n}\n\n/*\n * See the comments on bfq_limit_depth for the purpose of\n * the depths set in the function. Return minimum shallow depth we'll use.\n */\nstatic void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)\n{\n\tunsigned int depth = 1U << bt->sb.shift;\n\n\tbfqd->full_depth_shift = bt->sb.shift;\n\t/*\n\t * In-word depths if no bfq_queue is being weight-raised:\n\t * leaving 25% of tags only for sync reads.\n\t *\n\t * In next formulas, right-shift the value\n\t * (1U<<bt->sb.shift), instead of computing directly\n\t * (1U<<(bt->sb.shift - something)), to be robust against\n\t * any possible value of bt->sb.shift, without having to\n\t * limit 'something'.\n\t */\n\t/* no more than 50% of tags for async I/O */\n\tbfqd->word_depths[0][0] = max(depth >> 1, 1U);\n\t/*\n\t * no more than 75% of tags for sync writes (25% extra tags\n\t * w.r.t. async I/O, to prevent async I/O from starving sync\n\t * writes)\n\t */\n\tbfqd->word_depths[0][1] = max((depth * 3) >> 2, 1U);\n\n\t/*\n\t * In-word depths in case some bfq_queue is being weight-\n\t * raised: leaving ~63% of tags for sync reads. This is the\n\t * highest percentage for which, in our tests, application\n\t * start-up times didn't suffer from any regression due to tag\n\t * shortage.\n\t */\n\t/* no more than ~18% of tags for async I/O */\n\tbfqd->word_depths[1][0] = max((depth * 3) >> 4, 1U);\n\t/* no more than ~37% of tags for sync writes (~20% extra tags) */\n\tbfqd->word_depths[1][1] = max((depth * 6) >> 4, 1U);\n}\n\nstatic void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\n\tbfq_update_depths(bfqd, &tags->bitmap_tags);\n\tsbitmap_queue_min_shallow_depth(&tags->bitmap_tags, 1);\n}\n\nstatic int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)\n{\n\tbfq_depth_updated(hctx);\n\treturn 0;\n}\n\nstatic void bfq_exit_queue(struct elevator_queue *e)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tstruct bfq_queue *bfqq, *n;\n\tunsigned int actuator;\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\tspin_lock_irq(&bfqd->lock);\n\tlist_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)\n\t\tbfq_deactivate_bfqq(bfqd, bfqq, false, false);\n\tspin_unlock_irq(&bfqd->lock);\n\n\tfor (actuator = 0; actuator < bfqd->num_actuators; actuator++)\n\t\tWARN_ON_ONCE(bfqd->rq_in_driver[actuator]);\n\tWARN_ON_ONCE(bfqd->tot_rq_in_driver);\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\t/* release oom-queue reference to root group */\n\tbfqg_and_blkg_put(bfqd->root_group);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_deactivate_policy(bfqd->queue->disk, &blkcg_policy_bfq);\n#else\n\tspin_lock_irq(&bfqd->lock);\n\tbfq_put_async_queues(bfqd, bfqd->root_group);\n\tkfree(bfqd->root_group);\n\tspin_unlock_irq(&bfqd->lock);\n#endif\n\n\tblk_stat_disable_accounting(bfqd->queue);\n\tclear_bit(ELEVATOR_FLAG_DISABLE_WBT, &e->flags);\n\twbt_enable_default(bfqd->queue->disk);\n\n\tkfree(bfqd);\n}\n\nstatic void bfq_init_root_group(struct bfq_group *root_group,\n\t\t\t\tstruct bfq_data *bfqd)\n{\n\tint i;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\troot_group->entity.parent = NULL;\n\troot_group->my_entity = NULL;\n\troot_group->bfqd = bfqd;\n#endif\n\troot_group->rq_pos_tree = RB_ROOT;\n\tfor (i = 0; i < BFQ_IOPRIO_CLASSES; i++)\n\t\troot_group->sched_data.service_tree[i] = BFQ_SERVICE_TREE_INIT;\n\troot_group->sched_data.bfq_class_idle_last_service = jiffies;\n}\n\nstatic int bfq_init_queue(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct bfq_data *bfqd;\n\tstruct elevator_queue *eq;\n\tunsigned int i;\n\tstruct blk_independent_access_ranges *ia_ranges = q->disk->ia_ranges;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn -ENOMEM;\n\n\tbfqd = kzalloc_node(sizeof(*bfqd), GFP_KERNEL, q->node);\n\tif (!bfqd) {\n\t\tkobject_put(&eq->kobj);\n\t\treturn -ENOMEM;\n\t}\n\teq->elevator_data = bfqd;\n\n\tspin_lock_irq(&q->queue_lock);\n\tq->elevator = eq;\n\tspin_unlock_irq(&q->queue_lock);\n\n\t/*\n\t * Our fallback bfqq if bfq_find_alloc_queue() runs into OOM issues.\n\t * Grab a permanent reference to it, so that the normal code flow\n\t * will not attempt to free it.\n\t * Set zero as actuator index: we will pretend that\n\t * all I/O requests are for the same actuator.\n\t */\n\tbfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0, 0);\n\tbfqd->oom_bfqq.ref++;\n\tbfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;\n\tbfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;\n\tbfqd->oom_bfqq.entity.new_weight =\n\t\tbfq_ioprio_to_weight(bfqd->oom_bfqq.new_ioprio);\n\n\t/* oom_bfqq does not participate to bursts */\n\tbfq_clear_bfqq_just_created(&bfqd->oom_bfqq);\n\n\t/*\n\t * Trigger weight initialization, according to ioprio, at the\n\t * oom_bfqq's first activation. The oom_bfqq's ioprio and ioprio\n\t * class won't be changed any more.\n\t */\n\tbfqd->oom_bfqq.entity.prio_changed = 1;\n\n\tbfqd->queue = q;\n\n\tbfqd->num_actuators = 1;\n\t/*\n\t * If the disk supports multiple actuators, copy independent\n\t * access ranges from the request queue structure.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tif (ia_ranges) {\n\t\t/*\n\t\t * Check if the disk ia_ranges size exceeds the current bfq\n\t\t * actuator limit.\n\t\t */\n\t\tif (ia_ranges->nr_ia_ranges > BFQ_MAX_ACTUATORS) {\n\t\t\tpr_crit(\"nr_ia_ranges higher than act limit: iars=%d, max=%d.\\n\",\n\t\t\t\tia_ranges->nr_ia_ranges, BFQ_MAX_ACTUATORS);\n\t\t\tpr_crit(\"Falling back to single actuator mode.\\n\");\n\t\t} else {\n\t\t\tbfqd->num_actuators = ia_ranges->nr_ia_ranges;\n\n\t\t\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\t\t\tbfqd->sector[i] = ia_ranges->ia_range[i].sector;\n\t\t\t\tbfqd->nr_sectors[i] =\n\t\t\t\t\tia_ranges->ia_range[i].nr_sectors;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Otherwise use single-actuator dev info */\n\tif (bfqd->num_actuators == 1) {\n\t\tbfqd->sector[0] = 0;\n\t\tbfqd->nr_sectors[0] = get_capacity(q->disk);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n\n\tINIT_LIST_HEAD(&bfqd->dispatch);\n\n\thrtimer_init(&bfqd->idle_slice_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL);\n\tbfqd->idle_slice_timer.function = bfq_idle_slice_timer;\n\n\tbfqd->queue_weights_tree = RB_ROOT_CACHED;\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tbfqd->num_groups_with_pending_reqs = 0;\n#endif\n\n\tINIT_LIST_HEAD(&bfqd->active_list[0]);\n\tINIT_LIST_HEAD(&bfqd->active_list[1]);\n\tINIT_LIST_HEAD(&bfqd->idle_list);\n\tINIT_HLIST_HEAD(&bfqd->burst_list);\n\n\tbfqd->hw_tag = -1;\n\tbfqd->nonrot_with_queueing = blk_queue_nonrot(bfqd->queue);\n\n\tbfqd->bfq_max_budget = bfq_default_max_budget;\n\n\tbfqd->bfq_fifo_expire[0] = bfq_fifo_expire[0];\n\tbfqd->bfq_fifo_expire[1] = bfq_fifo_expire[1];\n\tbfqd->bfq_back_max = bfq_back_max;\n\tbfqd->bfq_back_penalty = bfq_back_penalty;\n\tbfqd->bfq_slice_idle = bfq_slice_idle;\n\tbfqd->bfq_timeout = bfq_timeout;\n\n\tbfqd->bfq_large_burst_thresh = 8;\n\tbfqd->bfq_burst_interval = msecs_to_jiffies(180);\n\n\tbfqd->low_latency = true;\n\n\t/*\n\t * Trade-off between responsiveness and fairness.\n\t */\n\tbfqd->bfq_wr_coeff = 30;\n\tbfqd->bfq_wr_rt_max_time = msecs_to_jiffies(300);\n\tbfqd->bfq_wr_min_idle_time = msecs_to_jiffies(2000);\n\tbfqd->bfq_wr_min_inter_arr_async = msecs_to_jiffies(500);\n\tbfqd->bfq_wr_max_softrt_rate = 7000; /*\n\t\t\t\t\t      * Approximate rate required\n\t\t\t\t\t      * to playback or record a\n\t\t\t\t\t      * high-definition compressed\n\t\t\t\t\t      * video.\n\t\t\t\t\t      */\n\tbfqd->wr_busy_queues = 0;\n\n\t/*\n\t * Begin by assuming, optimistically, that the device peak\n\t * rate is equal to 2/3 of the highest reference rate.\n\t */\n\tbfqd->rate_dur_prod = ref_rate[blk_queue_nonrot(bfqd->queue)] *\n\t\tref_wr_duration[blk_queue_nonrot(bfqd->queue)];\n\tbfqd->peak_rate = ref_rate[blk_queue_nonrot(bfqd->queue)] * 2 / 3;\n\n\t/* see comments on the definition of next field inside bfq_data */\n\tbfqd->actuator_load_threshold = 4;\n\n\tspin_lock_init(&bfqd->lock);\n\n\t/*\n\t * The invocation of the next bfq_create_group_hierarchy\n\t * function is the head of a chain of function calls\n\t * (bfq_create_group_hierarchy->blkcg_activate_policy->\n\t * blk_mq_freeze_queue) that may lead to the invocation of the\n\t * has_work hook function. For this reason,\n\t * bfq_create_group_hierarchy is invoked only after all\n\t * scheduler data has been initialized, apart from the fields\n\t * that can be initialized only after invoking\n\t * bfq_create_group_hierarchy. This, in particular, enables\n\t * has_work to correctly return false. Of course, to avoid\n\t * other inconsistencies, the blk-mq stack must then refrain\n\t * from invoking further scheduler hooks before this init\n\t * function is finished.\n\t */\n\tbfqd->root_group = bfq_create_group_hierarchy(bfqd, q->node);\n\tif (!bfqd->root_group)\n\t\tgoto out_free;\n\tbfq_init_root_group(bfqd->root_group, bfqd);\n\tbfq_init_entity(&bfqd->oom_bfqq.entity, bfqd->root_group);\n\n\t/* We dispatch from request queue wide instead of hw queue */\n\tblk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);\n\n\tset_bit(ELEVATOR_FLAG_DISABLE_WBT, &eq->flags);\n\twbt_disable_default(q->disk);\n\tblk_stat_enable_accounting(q);\n\n\treturn 0;\n\nout_free:\n\tkfree(bfqd);\n\tkobject_put(&eq->kobj);\n\treturn -ENOMEM;\n}\n\nstatic void bfq_slab_kill(void)\n{\n\tkmem_cache_destroy(bfq_pool);\n}\n\nstatic int __init bfq_slab_setup(void)\n{\n\tbfq_pool = KMEM_CACHE(bfq_queue, 0);\n\tif (!bfq_pool)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic ssize_t bfq_var_show(unsigned int var, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", var);\n}\n\nstatic int bfq_var_store(unsigned long *var, const char *page)\n{\n\tunsigned long new_val;\n\tint ret = kstrtoul(page, 10, &new_val);\n\n\tif (ret)\n\t\treturn ret;\n\t*var = new_val;\n\treturn 0;\n}\n\n#define SHOW_FUNCTION(__FUNC, __VAR, __CONV)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t__data = jiffies_to_msecs(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t__data = div_u64(__data, NSEC_PER_MSEC);\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nSHOW_FUNCTION(bfq_fifo_expire_sync_show, bfqd->bfq_fifo_expire[1], 2);\nSHOW_FUNCTION(bfq_fifo_expire_async_show, bfqd->bfq_fifo_expire[0], 2);\nSHOW_FUNCTION(bfq_back_seek_max_show, bfqd->bfq_back_max, 0);\nSHOW_FUNCTION(bfq_back_seek_penalty_show, bfqd->bfq_back_penalty, 0);\nSHOW_FUNCTION(bfq_slice_idle_show, bfqd->bfq_slice_idle, 2);\nSHOW_FUNCTION(bfq_max_budget_show, bfqd->bfq_user_max_budget, 0);\nSHOW_FUNCTION(bfq_timeout_sync_show, bfqd->bfq_timeout, 1);\nSHOW_FUNCTION(bfq_strict_guarantees_show, bfqd->strict_guarantees, 0);\nSHOW_FUNCTION(bfq_low_latency_show, bfqd->low_latency, 0);\n#undef SHOW_FUNCTION\n\n#define USEC_SHOW_FUNCTION(__FUNC, __VAR)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\t__data = div_u64(__data, NSEC_PER_USEC);\t\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nUSEC_SHOW_FUNCTION(bfq_slice_idle_us_show, bfqd->bfq_slice_idle);\n#undef USEC_SHOW_FUNCTION\n\n#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX, __CONV)\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n__FUNC(struct elevator_queue *e, const char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t*(__PTR) = msecs_to_jiffies(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t*(__PTR) = (u64)__data * NSEC_PER_MSEC;\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t*(__PTR) = __data;\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nSTORE_FUNCTION(bfq_fifo_expire_sync_store, &bfqd->bfq_fifo_expire[1], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_fifo_expire_async_store, &bfqd->bfq_fifo_expire[0], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_back_seek_max_store, &bfqd->bfq_back_max, 0, INT_MAX, 0);\nSTORE_FUNCTION(bfq_back_seek_penalty_store, &bfqd->bfq_back_penalty, 1,\n\t\tINT_MAX, 0);\nSTORE_FUNCTION(bfq_slice_idle_store, &bfqd->bfq_slice_idle, 0, INT_MAX, 2);\n#undef STORE_FUNCTION\n\n#define USEC_STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\t*(__PTR) = (u64)__data * NSEC_PER_USEC;\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nUSEC_STORE_FUNCTION(bfq_slice_idle_us_store, &bfqd->bfq_slice_idle, 0,\n\t\t    UINT_MAX);\n#undef USEC_STORE_FUNCTION\n\nstatic ssize_t bfq_max_budget_store(struct elevator_queue *e,\n\t\t\t\t    const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\telse {\n\t\tif (__data > INT_MAX)\n\t\t\t__data = INT_MAX;\n\t\tbfqd->bfq_max_budget = __data;\n\t}\n\n\tbfqd->bfq_user_max_budget = __data;\n\n\treturn count;\n}\n\n/*\n * Leaving this name to preserve name compatibility with cfq\n * parameters, but this timeout is used for both sync and async.\n */\nstatic ssize_t bfq_timeout_sync_store(struct elevator_queue *e,\n\t\t\t\t      const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data < 1)\n\t\t__data = 1;\n\telse if (__data > INT_MAX)\n\t\t__data = INT_MAX;\n\n\tbfqd->bfq_timeout = msecs_to_jiffies(__data);\n\tif (bfqd->bfq_user_max_budget == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\n\treturn count;\n}\n\nstatic ssize_t bfq_strict_guarantees_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (!bfqd->strict_guarantees && __data == 1\n\t    && bfqd->bfq_slice_idle < 8 * NSEC_PER_MSEC)\n\t\tbfqd->bfq_slice_idle = 8 * NSEC_PER_MSEC;\n\n\tbfqd->strict_guarantees = __data;\n\n\treturn count;\n}\n\nstatic ssize_t bfq_low_latency_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (__data == 0 && bfqd->low_latency != 0)\n\t\tbfq_end_wr(bfqd);\n\tbfqd->low_latency = __data;\n\n\treturn count;\n}\n\n#define BFQ_ATTR(name) \\\n\t__ATTR(name, 0644, bfq_##name##_show, bfq_##name##_store)\n\nstatic struct elv_fs_entry bfq_attrs[] = {\n\tBFQ_ATTR(fifo_expire_sync),\n\tBFQ_ATTR(fifo_expire_async),\n\tBFQ_ATTR(back_seek_max),\n\tBFQ_ATTR(back_seek_penalty),\n\tBFQ_ATTR(slice_idle),\n\tBFQ_ATTR(slice_idle_us),\n\tBFQ_ATTR(max_budget),\n\tBFQ_ATTR(timeout_sync),\n\tBFQ_ATTR(strict_guarantees),\n\tBFQ_ATTR(low_latency),\n\t__ATTR_NULL\n};\n\nstatic struct elevator_type iosched_bfq_mq = {\n\t.ops = {\n\t\t.limit_depth\t\t= bfq_limit_depth,\n\t\t.prepare_request\t= bfq_prepare_request,\n\t\t.requeue_request        = bfq_finish_requeue_request,\n\t\t.finish_request\t\t= bfq_finish_request,\n\t\t.exit_icq\t\t= bfq_exit_icq,\n\t\t.insert_requests\t= bfq_insert_requests,\n\t\t.dispatch_request\t= bfq_dispatch_request,\n\t\t.next_request\t\t= elv_rb_latter_request,\n\t\t.former_request\t\t= elv_rb_former_request,\n\t\t.allow_merge\t\t= bfq_allow_bio_merge,\n\t\t.bio_merge\t\t= bfq_bio_merge,\n\t\t.request_merge\t\t= bfq_request_merge,\n\t\t.requests_merged\t= bfq_requests_merged,\n\t\t.request_merged\t\t= bfq_request_merged,\n\t\t.has_work\t\t= bfq_has_work,\n\t\t.depth_updated\t\t= bfq_depth_updated,\n\t\t.init_hctx\t\t= bfq_init_hctx,\n\t\t.init_sched\t\t= bfq_init_queue,\n\t\t.exit_sched\t\t= bfq_exit_queue,\n\t},\n\n\t.icq_size =\t\tsizeof(struct bfq_io_cq),\n\t.icq_align =\t\t__alignof__(struct bfq_io_cq),\n\t.elevator_attrs =\tbfq_attrs,\n\t.elevator_name =\t\"bfq\",\n\t.elevator_owner =\tTHIS_MODULE,\n};\nMODULE_ALIAS(\"bfq-iosched\");\n\nstatic int __init bfq_init(void)\n{\n\tint ret;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tret = blkcg_policy_register(&blkcg_policy_bfq);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = -ENOMEM;\n\tif (bfq_slab_setup())\n\t\tgoto err_pol_unreg;\n\n\t/*\n\t * Times to load large popular applications for the typical\n\t * systems installed on the reference devices (see the\n\t * comments before the definition of the next\n\t * array). Actually, we use slightly lower values, as the\n\t * estimated peak rate tends to be smaller than the actual\n\t * peak rate.  The reason for this last fact is that estimates\n\t * are computed over much shorter time intervals than the long\n\t * intervals typically used for benchmarking. Why? First, to\n\t * adapt more quickly to variations. Second, because an I/O\n\t * scheduler cannot rely on a peak-rate-evaluation workload to\n\t * be run for a long time.\n\t */\n\tref_wr_duration[0] = msecs_to_jiffies(7000); /* actually 8 sec */\n\tref_wr_duration[1] = msecs_to_jiffies(2500); /* actually 3 sec */\n\n\tret = elv_register(&iosched_bfq_mq);\n\tif (ret)\n\t\tgoto slab_kill;\n\n\treturn 0;\n\nslab_kill:\n\tbfq_slab_kill();\nerr_pol_unreg:\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\treturn ret;\n}\n\nstatic void __exit bfq_exit(void)\n{\n\telv_unregister(&iosched_bfq_mq);\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\tbfq_slab_kill();\n}\n\nmodule_init(bfq_init);\nmodule_exit(bfq_exit);\n\nMODULE_AUTHOR(\"Paolo Valente\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MQ Budget Fair Queueing I/O Scheduler\");\n"
                            }
                        },
                        {
                            "downstream_patch": "f587c1ac68956c4703857d650d9b1cd7bb2ac4d7",
                            "downstream_commit": "e43dfc4a9c15b92fed8d12d7b4a343824a8aac9b",
                            "commit_date": "2025-01-23 17:15:55 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file block/bfq-iosched.c\nHunk #1 succeeded at 6577 (offset -267 lines).",
                            "downstream_patch_content": "commit f587c1ac68956c4703857d650d9b1cd7bb2ac4d7\nAuthor: Yu Kuai <yukuai3@huawei.com>\nDate:   Wed Jan 8 16:41:48 2025 +0800\n\n    block, bfq: fix waker_bfqq UAF after bfq_split_bfqq()\n    \n    [ Upstream commit fcede1f0a043ccefe9bc6ad57f12718e42f63f1d ]\n    \n    Our syzkaller report a following UAF for v6.6:\n    \n    BUG: KASAN: slab-use-after-free in bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n    Read of size 8 at addr ffff8881b57147d8 by task fsstress/232726\n    \n    CPU: 2 PID: 232726 Comm: fsstress Not tainted 6.6.0-g3629d1885222 #39\n    Call Trace:\n     <TASK>\n     __dump_stack lib/dump_stack.c:88 [inline]\n     dump_stack_lvl+0x91/0xf0 lib/dump_stack.c:106\n     print_address_description.constprop.0+0x66/0x300 mm/kasan/report.c:364\n     print_report+0x3e/0x70 mm/kasan/report.c:475\n     kasan_report+0xb8/0xf0 mm/kasan/report.c:588\n     hlist_add_head include/linux/list.h:1023 [inline]\n     bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Allocated by task 232719:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     __kasan_slab_alloc+0x87/0x90 mm/kasan/common.c:328\n     kasan_slab_alloc include/linux/kasan.h:188 [inline]\n     slab_post_alloc_hook mm/slab.h:768 [inline]\n     slab_alloc_node mm/slub.c:3492 [inline]\n     kmem_cache_alloc_node+0x1b8/0x6f0 mm/slub.c:3537\n     bfq_get_queue+0x215/0x1f00 block/bfq-iosched.c:5869\n     bfq_get_bfqq_handle_split+0x167/0x5f0 block/bfq-iosched.c:6776\n     bfq_init_rq+0x13a4/0x17a0 block/bfq-iosched.c:6938\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh_nowait+0x15a/0x240 fs/ext4/super.c:217\n     ext4_read_bh_lock+0xac/0xd0 fs/ext4/super.c:242\n     ext4_bread_batch+0x268/0x500 fs/ext4/inode.c:958\n     __ext4_find_entry+0x448/0x10f0 fs/ext4/namei.c:1671\n     ext4_lookup_entry fs/ext4/namei.c:1774 [inline]\n     ext4_lookup.part.0+0x359/0x6f0 fs/ext4/namei.c:1842\n     ext4_lookup+0x72/0x90 fs/ext4/namei.c:1839\n     __lookup_slow+0x257/0x480 fs/namei.c:1696\n     lookup_slow fs/namei.c:1713 [inline]\n     walk_component+0x454/0x5c0 fs/namei.c:2004\n     link_path_walk.part.0+0x773/0xda0 fs/namei.c:2331\n     link_path_walk fs/namei.c:3826 [inline]\n     path_openat+0x1b9/0x520 fs/namei.c:3826\n     do_filp_open+0x1b7/0x400 fs/namei.c:3857\n     do_sys_openat2+0x5dc/0x6e0 fs/open.c:1428\n     do_sys_open fs/open.c:1443 [inline]\n     __do_sys_openat fs/open.c:1459 [inline]\n     __se_sys_openat fs/open.c:1454 [inline]\n     __x64_sys_openat+0x148/0x200 fs/open.c:1454\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    Freed by task 232726:\n     kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n     kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n     kasan_save_free_info+0x2b/0x50 mm/kasan/generic.c:522\n     ____kasan_slab_free mm/kasan/common.c:236 [inline]\n     __kasan_slab_free+0x12a/0x1b0 mm/kasan/common.c:244\n     kasan_slab_free include/linux/kasan.h:164 [inline]\n     slab_free_hook mm/slub.c:1827 [inline]\n     slab_free_freelist_hook mm/slub.c:1853 [inline]\n     slab_free mm/slub.c:3820 [inline]\n     kmem_cache_free+0x110/0x760 mm/slub.c:3842\n     bfq_put_queue+0x6a7/0xfb0 block/bfq-iosched.c:5428\n     bfq_forget_entity block/bfq-wf2q.c:634 [inline]\n     bfq_put_idle_entity+0x142/0x240 block/bfq-wf2q.c:645\n     bfq_forget_idle+0x189/0x1e0 block/bfq-wf2q.c:671\n     bfq_update_vtime block/bfq-wf2q.c:1280 [inline]\n     __bfq_lookup_next_entity block/bfq-wf2q.c:1374 [inline]\n     bfq_lookup_next_entity+0x350/0x480 block/bfq-wf2q.c:1433\n     bfq_update_next_in_service+0x1c0/0x4f0 block/bfq-wf2q.c:128\n     bfq_deactivate_entity+0x10a/0x240 block/bfq-wf2q.c:1188\n     bfq_deactivate_bfqq block/bfq-wf2q.c:1592 [inline]\n     bfq_del_bfqq_busy+0x2e8/0xad0 block/bfq-wf2q.c:1659\n     bfq_release_process_ref+0x1cc/0x220 block/bfq-iosched.c:3139\n     bfq_split_bfqq+0x481/0xdf0 block/bfq-iosched.c:6754\n     bfq_init_rq+0xf29/0x17a0 block/bfq-iosched.c:6934\n     bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n     bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n     blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n     blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n     __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n     __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n     submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n     submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n     __ext4_read_bh fs/ext4/super.c:205 [inline]\n     ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n     __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n     ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n     ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n     ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n     ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n     iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n     iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n     ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n     ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n     do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n     __do_sys_ioctl fs/ioctl.c:869 [inline]\n     __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n     do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n     do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n     entry_SYSCALL_64_after_hwframe+0x78/0xe2\n    \n    commit 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after\n    splitting\") fix the problem that if waker_bfqq is in the merge chain,\n    and current is the only procress, waker_bfqq can be freed from\n    bfq_split_bfqq(). However, the case that waker_bfqq is not in the merge\n    chain is missed, and if the procress reference of waker_bfqq is 0,\n    waker_bfqq can be freed as well.\n    \n    Fix the problem by checking procress reference if waker_bfqq is not in\n    the merge_chain.\n    \n    Fixes: 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after splitting\")\n    Signed-off-by: Hou Tao <houtao1@huawei.com>\n    Signed-off-by: Yu Kuai <yukuai3@huawei.com>\n    Reviewed-by: Jan Kara <jack@suse.cz>\n    Link: https://lore.kernel.org/r/20250108084148.1549973-1-yukuai1@huaweicloud.com\n    Signed-off-by: Jens Axboe <axboe@kernel.dk>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/block/bfq-iosched.c b/block/bfq-iosched.c\nindex c985c944fa65..d830ed169e65 100644\n--- a/block/bfq-iosched.c\n+++ b/block/bfq-iosched.c\n@@ -6577,16 +6577,24 @@ static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n \t\tif (new_bfqq == waker_bfqq) {\n \t\t\t/*\n \t\t\t * If waker_bfqq is in the merge chain, and current\n-\t\t\t * is the only procress.\n+\t\t\t * is the only process, waker_bfqq can be freed.\n \t\t\t */\n \t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n \t\t\t\treturn NULL;\n-\t\t\tbreak;\n+\n+\t\t\treturn waker_bfqq;\n \t\t}\n \n \t\tnew_bfqq = new_bfqq->new_bfqq;\n \t}\n \n+\t/*\n+\t * If waker_bfqq is not in the merge chain, and it's procress reference\n+\t * is 0, waker_bfqq can be freed.\n+\t */\n+\tif (bfqq_process_refs(waker_bfqq) == 0)\n+\t\treturn NULL;\n+\n \treturn waker_bfqq;\n }\n \n",
                            "downstream_file_content": {
                                "block/bfq-iosched.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Budget Fair Queueing (BFQ) I/O scheduler.\n *\n * Based on ideas and code from CFQ:\n * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>\n *\n * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it>\n *\t\t      Paolo Valente <paolo.valente@unimore.it>\n *\n * Copyright (C) 2010 Paolo Valente <paolo.valente@unimore.it>\n *                    Arianna Avanzini <avanzini@google.com>\n *\n * Copyright (C) 2017 Paolo Valente <paolo.valente@linaro.org>\n *\n * BFQ is a proportional-share I/O scheduler, with some extra\n * low-latency capabilities. BFQ also supports full hierarchical\n * scheduling through cgroups. Next paragraphs provide an introduction\n * on BFQ inner workings. Details on BFQ benefits, usage and\n * limitations can be found in Documentation/block/bfq-iosched.rst.\n *\n * BFQ is a proportional-share storage-I/O scheduling algorithm based\n * on the slice-by-slice service scheme of CFQ. But BFQ assigns\n * budgets, measured in number of sectors, to processes instead of\n * time slices. The device is not granted to the in-service process\n * for a given time slice, but until it has exhausted its assigned\n * budget. This change from the time to the service domain enables BFQ\n * to distribute the device throughput among processes as desired,\n * without any distortion due to throughput fluctuations, or to device\n * internal queueing. BFQ uses an ad hoc internal scheduler, called\n * B-WF2Q+, to schedule processes according to their budgets. More\n * precisely, BFQ schedules queues associated with processes. Each\n * process/queue is assigned a user-configurable weight, and B-WF2Q+\n * guarantees that each queue receives a fraction of the throughput\n * proportional to its weight. Thanks to the accurate policy of\n * B-WF2Q+, BFQ can afford to assign high budgets to I/O-bound\n * processes issuing sequential requests (to boost the throughput),\n * and yet guarantee a low latency to interactive and soft real-time\n * applications.\n *\n * In particular, to provide these low-latency guarantees, BFQ\n * explicitly privileges the I/O of two classes of time-sensitive\n * applications: interactive and soft real-time. In more detail, BFQ\n * behaves this way if the low_latency parameter is set (default\n * configuration). This feature enables BFQ to provide applications in\n * these classes with a very low latency.\n *\n * To implement this feature, BFQ constantly tries to detect whether\n * the I/O requests in a bfq_queue come from an interactive or a soft\n * real-time application. For brevity, in these cases, the queue is\n * said to be interactive or soft real-time. In both cases, BFQ\n * privileges the service of the queue, over that of non-interactive\n * and non-soft-real-time queues. This privileging is performed,\n * mainly, by raising the weight of the queue. So, for brevity, we\n * call just weight-raising periods the time periods during which a\n * queue is privileged, because deemed interactive or soft real-time.\n *\n * The detection of soft real-time queues/applications is described in\n * detail in the comments on the function\n * bfq_bfqq_softrt_next_start. On the other hand, the detection of an\n * interactive queue works as follows: a queue is deemed interactive\n * if it is constantly non empty only for a limited time interval,\n * after which it does become empty. The queue may be deemed\n * interactive again (for a limited time), if it restarts being\n * constantly non empty, provided that this happens only after the\n * queue has remained empty for a given minimum idle time.\n *\n * By default, BFQ computes automatically the above maximum time\n * interval, i.e., the time interval after which a constantly\n * non-empty queue stops being deemed interactive. Since a queue is\n * weight-raised while it is deemed interactive, this maximum time\n * interval happens to coincide with the (maximum) duration of the\n * weight-raising for interactive queues.\n *\n * Finally, BFQ also features additional heuristics for\n * preserving both a low latency and a high throughput on NCQ-capable,\n * rotational or flash-based devices, and to get the job done quickly\n * for applications consisting in many I/O-bound processes.\n *\n * NOTE: if the main or only goal, with a given device, is to achieve\n * the maximum-possible throughput at all times, then do switch off\n * all low-latency heuristics for that device, by setting low_latency\n * to 0.\n *\n * BFQ is described in [1], where also a reference to the initial,\n * more theoretical paper on BFQ can be found. The interested reader\n * can find in the latter paper full details on the main algorithm, as\n * well as formulas of the guarantees and formal proofs of all the\n * properties.  With respect to the version of BFQ presented in these\n * papers, this implementation adds a few more heuristics, such as the\n * ones that guarantee a low latency to interactive and soft real-time\n * applications, and a hierarchical extension based on H-WF2Q+.\n *\n * B-WF2Q+ is based on WF2Q+, which is described in [2], together with\n * H-WF2Q+, while the augmented tree used here to implement B-WF2Q+\n * with O(log N) complexity derives from the one introduced with EEVDF\n * in [3].\n *\n * [1] P. Valente, A. Avanzini, \"Evolution of the BFQ Storage I/O\n *     Scheduler\", Proceedings of the First Workshop on Mobile System\n *     Technologies (MST-2015), May 2015.\n *     http://algogroup.unimore.it/people/paolo/disk_sched/mst-2015.pdf\n *\n * [2] Jon C.R. Bennett and H. Zhang, \"Hierarchical Packet Fair Queueing\n *     Algorithms\", IEEE/ACM Transactions on Networking, 5(5):675-689,\n *     Oct 1997.\n *\n * http://www.cs.cmu.edu/~hzhang/papers/TON-97-Oct.ps.gz\n *\n * [3] I. Stoica and H. Abdel-Wahab, \"Earliest Eligible Virtual Deadline\n *     First: A Flexible and Accurate Mechanism for Proportional Share\n *     Resource Allocation\", technical report.\n *\n * http://www.cs.berkeley.edu/~istoica/papers/eevdf-tr-95.pdf\n */\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/cgroup.h>\n#include <linux/elevator.h>\n#include <linux/ktime.h>\n#include <linux/rbtree.h>\n#include <linux/ioprio.h>\n#include <linux/sbitmap.h>\n#include <linux/delay.h>\n#include <linux/backing-dev.h>\n\n#include <trace/events/block.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n#include \"blk-mq-sched.h\"\n#include \"bfq-iosched.h\"\n#include \"blk-wbt.h\"\n\n#define BFQ_BFQQ_FNS(name)\t\t\t\t\t\t\\\nvoid bfq_mark_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__set_bit(BFQQF_##name, &(bfqq)->flags);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nvoid bfq_clear_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__clear_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nint bfq_bfqq_##name(const struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\n\nBFQ_BFQQ_FNS(just_created);\nBFQ_BFQQ_FNS(busy);\nBFQ_BFQQ_FNS(wait_request);\nBFQ_BFQQ_FNS(non_blocking_wait_rq);\nBFQ_BFQQ_FNS(fifo_expire);\nBFQ_BFQQ_FNS(has_short_ttime);\nBFQ_BFQQ_FNS(sync);\nBFQ_BFQQ_FNS(IO_bound);\nBFQ_BFQQ_FNS(in_large_burst);\nBFQ_BFQQ_FNS(coop);\nBFQ_BFQQ_FNS(split_coop);\nBFQ_BFQQ_FNS(softrt_update);\n#undef BFQ_BFQQ_FNS\t\t\t\t\t\t\\\n\n/* Expiration time of async (0) and sync (1) requests, in ns. */\nstatic const u64 bfq_fifo_expire[2] = { NSEC_PER_SEC / 4, NSEC_PER_SEC / 8 };\n\n/* Maximum backwards seek (magic number lifted from CFQ), in KiB. */\nstatic const int bfq_back_max = 16 * 1024;\n\n/* Penalty of a backwards seek, in number of sectors. */\nstatic const int bfq_back_penalty = 2;\n\n/* Idling period duration, in ns. */\nstatic u64 bfq_slice_idle = NSEC_PER_SEC / 125;\n\n/* Minimum number of assigned budgets for which stats are safe to compute. */\nstatic const int bfq_stats_min_budgets = 194;\n\n/* Default maximum budget values, in sectors and number of requests. */\nstatic const int bfq_default_max_budget = 16 * 1024;\n\n/*\n * When a sync request is dispatched, the queue that contains that\n * request, and all the ancestor entities of that queue, are charged\n * with the number of sectors of the request. In contrast, if the\n * request is async, then the queue and its ancestor entities are\n * charged with the number of sectors of the request, multiplied by\n * the factor below. This throttles the bandwidth for async I/O,\n * w.r.t. to sync I/O, and it is done to counter the tendency of async\n * writes to steal I/O throughput to reads.\n *\n * The current value of this parameter is the result of a tuning with\n * several hardware and software configurations. We tried to find the\n * lowest value for which writes do not cause noticeable problems to\n * reads. In fact, the lower this parameter, the stabler I/O control,\n * in the following respect.  The lower this parameter is, the less\n * the bandwidth enjoyed by a group decreases\n * - when the group does writes, w.r.t. to when it does reads;\n * - when other groups do reads, w.r.t. to when they do writes.\n */\nstatic const int bfq_async_charge_factor = 3;\n\n/* Default timeout values, in jiffies, approximating CFQ defaults. */\nconst int bfq_timeout = HZ / 8;\n\n/*\n * Time limit for merging (see comments in bfq_setup_cooperator). Set\n * to the slowest value that, in our tests, proved to be effective in\n * removing false positives, while not causing true positives to miss\n * queue merging.\n *\n * As can be deduced from the low time limit below, queue merging, if\n * successful, happens at the very beginning of the I/O of the involved\n * cooperating processes, as a consequence of the arrival of the very\n * first requests from each cooperator.  After that, there is very\n * little chance to find cooperators.\n */\nstatic const unsigned long bfq_merge_time_limit = HZ/10;\n\nstatic struct kmem_cache *bfq_pool;\n\n/* Below this threshold (in ns), we consider thinktime immediate. */\n#define BFQ_MIN_TT\t\t(2 * NSEC_PER_MSEC)\n\n/* hw_tag detection: parallel requests threshold and min samples needed. */\n#define BFQ_HW_QUEUE_THRESHOLD\t3\n#define BFQ_HW_QUEUE_SAMPLES\t32\n\n#define BFQQ_SEEK_THR\t\t(sector_t)(8 * 100)\n#define BFQQ_SECT_THR_NONROT\t(sector_t)(2 * 32)\n#define BFQ_RQ_SEEKY(bfqd, last_pos, rq) \\\n\t(get_sdist(last_pos, rq) >\t\t\t\\\n\t BFQQ_SEEK_THR &&\t\t\t\t\\\n\t (!blk_queue_nonrot(bfqd->queue) ||\t\t\\\n\t  blk_rq_sectors(rq) < BFQQ_SECT_THR_NONROT))\n#define BFQQ_CLOSE_THR\t\t(sector_t)(8 * 1024)\n#define BFQQ_SEEKY(bfqq)\t(hweight32(bfqq->seek_history) > 19)\n/*\n * Sync random I/O is likely to be confused with soft real-time I/O,\n * because it is characterized by limited throughput and apparently\n * isochronous arrival pattern. To avoid false positives, queues\n * containing only random (seeky) I/O are prevented from being tagged\n * as soft real-time.\n */\n#define BFQQ_TOTALLY_SEEKY(bfqq)\t(bfqq->seek_history == -1)\n\n/* Min number of samples required to perform peak-rate update */\n#define BFQ_RATE_MIN_SAMPLES\t32\n/* Min observation time interval required to perform a peak-rate update (ns) */\n#define BFQ_RATE_MIN_INTERVAL\t(300*NSEC_PER_MSEC)\n/* Target observation time interval for a peak-rate update (ns) */\n#define BFQ_RATE_REF_INTERVAL\tNSEC_PER_SEC\n\n/*\n * Shift used for peak-rate fixed precision calculations.\n * With\n * - the current shift: 16 positions\n * - the current type used to store rate: u32\n * - the current unit of measure for rate: [sectors/usec], or, more precisely,\n *   [(sectors/usec) / 2^BFQ_RATE_SHIFT] to take into account the shift,\n * the range of rates that can be stored is\n * [1 / 2^BFQ_RATE_SHIFT, 2^(32 - BFQ_RATE_SHIFT)] sectors/usec =\n * [1 / 2^16, 2^16] sectors/usec = [15e-6, 65536] sectors/usec =\n * [15, 65G] sectors/sec\n * Which, assuming a sector size of 512B, corresponds to a range of\n * [7.5K, 33T] B/sec\n */\n#define BFQ_RATE_SHIFT\t\t16\n\n/*\n * When configured for computing the duration of the weight-raising\n * for interactive queues automatically (see the comments at the\n * beginning of this file), BFQ does it using the following formula:\n * duration = (ref_rate / r) * ref_wr_duration,\n * where r is the peak rate of the device, and ref_rate and\n * ref_wr_duration are two reference parameters.  In particular,\n * ref_rate is the peak rate of the reference storage device (see\n * below), and ref_wr_duration is about the maximum time needed, with\n * BFQ and while reading two files in parallel, to load typical large\n * applications on the reference device (see the comments on\n * max_service_from_wr below, for more details on how ref_wr_duration\n * is obtained).  In practice, the slower/faster the device at hand\n * is, the more/less it takes to load applications with respect to the\n * reference device.  Accordingly, the longer/shorter BFQ grants\n * weight raising to interactive applications.\n *\n * BFQ uses two different reference pairs (ref_rate, ref_wr_duration),\n * depending on whether the device is rotational or non-rotational.\n *\n * In the following definitions, ref_rate[0] and ref_wr_duration[0]\n * are the reference values for a rotational device, whereas\n * ref_rate[1] and ref_wr_duration[1] are the reference values for a\n * non-rotational device. The reference rates are not the actual peak\n * rates of the devices used as a reference, but slightly lower\n * values. The reason for using slightly lower values is that the\n * peak-rate estimator tends to yield slightly lower values than the\n * actual peak rate (it can yield the actual peak rate only if there\n * is only one process doing I/O, and the process does sequential\n * I/O).\n *\n * The reference peak rates are measured in sectors/usec, left-shifted\n * by BFQ_RATE_SHIFT.\n */\nstatic int ref_rate[2] = {14000, 33000};\n/*\n * To improve readability, a conversion function is used to initialize\n * the following array, which entails that the array can be\n * initialized only in a function.\n */\nstatic int ref_wr_duration[2];\n\n/*\n * BFQ uses the above-detailed, time-based weight-raising mechanism to\n * privilege interactive tasks. This mechanism is vulnerable to the\n * following false positives: I/O-bound applications that will go on\n * doing I/O for much longer than the duration of weight\n * raising. These applications have basically no benefit from being\n * weight-raised at the beginning of their I/O. On the opposite end,\n * while being weight-raised, these applications\n * a) unjustly steal throughput to applications that may actually need\n * low latency;\n * b) make BFQ uselessly perform device idling; device idling results\n * in loss of device throughput with most flash-based storage, and may\n * increase latencies when used purposelessly.\n *\n * BFQ tries to reduce these problems, by adopting the following\n * countermeasure. To introduce this countermeasure, we need first to\n * finish explaining how the duration of weight-raising for\n * interactive tasks is computed.\n *\n * For a bfq_queue deemed as interactive, the duration of weight\n * raising is dynamically adjusted, as a function of the estimated\n * peak rate of the device, so as to be equal to the time needed to\n * execute the 'largest' interactive task we benchmarked so far. By\n * largest task, we mean the task for which each involved process has\n * to do more I/O than for any of the other tasks we benchmarked. This\n * reference interactive task is the start-up of LibreOffice Writer,\n * and in this task each process/bfq_queue needs to have at most ~110K\n * sectors transferred.\n *\n * This last piece of information enables BFQ to reduce the actual\n * duration of weight-raising for at least one class of I/O-bound\n * applications: those doing sequential or quasi-sequential I/O. An\n * example is file copy. In fact, once started, the main I/O-bound\n * processes of these applications usually consume the above 110K\n * sectors in much less time than the processes of an application that\n * is starting, because these I/O-bound processes will greedily devote\n * almost all their CPU cycles only to their target,\n * throughput-friendly I/O operations. This is even more true if BFQ\n * happens to be underestimating the device peak rate, and thus\n * overestimating the duration of weight raising. But, according to\n * our measurements, once transferred 110K sectors, these processes\n * have no right to be weight-raised any longer.\n *\n * Basing on the last consideration, BFQ ends weight-raising for a\n * bfq_queue if the latter happens to have received an amount of\n * service at least equal to the following constant. The constant is\n * set to slightly more than 110K, to have a minimum safety margin.\n *\n * This early ending of weight-raising reduces the amount of time\n * during which interactive false positives cause the two problems\n * described at the beginning of these comments.\n */\nstatic const unsigned long max_service_from_wr = 120000;\n\n/*\n * Maximum time between the creation of two queues, for stable merge\n * to be activated (in ms)\n */\nstatic const unsigned long bfq_activation_stable_merging = 600;\n/*\n * Minimum time to be waited before evaluating delayed stable merge (in ms)\n */\nstatic const unsigned long bfq_late_stable_merging = 600;\n\n#define RQ_BIC(rq)\t\ticq_to_bic((rq)->elv.priv[0])\n#define RQ_BFQQ(rq)\t\t((rq)->elv.priv[1])\n\nstruct bfq_queue *bic_to_bfqq(struct bfq_io_cq *bic, bool is_sync)\n{\n\treturn bic->bfqq[is_sync];\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq);\n\nvoid bic_set_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq, bool is_sync)\n{\n\tstruct bfq_queue *old_bfqq = bic->bfqq[is_sync];\n\n\t/* Clear bic pointer if bfqq is detached from this bic */\n\tif (old_bfqq && old_bfqq->bic == bic)\n\t\told_bfqq->bic = NULL;\n\n\t/*\n\t * If bfqq != NULL, then a non-stable queue merge between\n\t * bic->bfqq and bfqq is happening here. This causes troubles\n\t * in the following case: bic->bfqq has also been scheduled\n\t * for a possible stable merge with bic->stable_merge_bfqq,\n\t * and bic->stable_merge_bfqq == bfqq happens to\n\t * hold. Troubles occur because bfqq may then undergo a split,\n\t * thereby becoming eligible for a stable merge. Yet, if\n\t * bic->stable_merge_bfqq points exactly to bfqq, then bfqq\n\t * would be stably merged with itself. To avoid this anomaly,\n\t * we cancel the stable merge if\n\t * bic->stable_merge_bfqq == bfqq.\n\t */\n\tbic->bfqq[is_sync] = bfqq;\n\n\tif (bfqq && bic->stable_merge_bfqq == bfqq) {\n\t\t/*\n\t\t * Actually, these same instructions are executed also\n\t\t * in bfq_setup_cooperator, in case of abort or actual\n\t\t * execution of a stable merge. We could avoid\n\t\t * repeating these instructions there too, but if we\n\t\t * did so, we would nest even more complexity in this\n\t\t * function.\n\t\t */\n\t\tbfq_put_stable_ref(bic->stable_merge_bfqq);\n\n\t\tbic->stable_merge_bfqq = NULL;\n\t}\n}\n\nstruct bfq_data *bic_to_bfqd(struct bfq_io_cq *bic)\n{\n\treturn bic->icq.q->elevator->elevator_data;\n}\n\n/**\n * icq_to_bic - convert iocontext queue structure to bfq_io_cq.\n * @icq: the iocontext queue.\n */\nstatic struct bfq_io_cq *icq_to_bic(struct io_cq *icq)\n{\n\t/* bic->icq is the first member, %NULL will convert to %NULL */\n\treturn container_of(icq, struct bfq_io_cq, icq);\n}\n\n/**\n * bfq_bic_lookup - search into @ioc a bic associated to @bfqd.\n * @bfqd: the lookup key.\n * @ioc: the io_context of the process doing I/O.\n * @q: the request queue.\n */\nstatic struct bfq_io_cq *bfq_bic_lookup(struct bfq_data *bfqd,\n\t\t\t\t\tstruct io_context *ioc,\n\t\t\t\t\tstruct request_queue *q)\n{\n\tif (ioc) {\n\t\tunsigned long flags;\n\t\tstruct bfq_io_cq *icq;\n\n\t\tspin_lock_irqsave(&q->queue_lock, flags);\n\t\ticq = icq_to_bic(ioc_lookup_icq(ioc, q));\n\t\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\n\t\treturn icq;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * Scheduler run of queue, if there are requests pending and no one in the\n * driver that will restart queueing.\n */\nvoid bfq_schedule_dispatch(struct bfq_data *bfqd)\n{\n\tlockdep_assert_held(&bfqd->lock);\n\n\tif (bfqd->queued != 0) {\n\t\tbfq_log(bfqd, \"schedule dispatch\");\n\t\tblk_mq_run_hw_queues(bfqd->queue, true);\n\t}\n}\n\n#define bfq_class_idle(bfqq)\t((bfqq)->ioprio_class == IOPRIO_CLASS_IDLE)\n\n#define bfq_sample_valid(samples)\t((samples) > 80)\n\n/*\n * Lifted from AS - choose which of rq1 and rq2 that is best served now.\n * We choose the request that is closer to the head right now.  Distance\n * behind the head is penalized and only allowed to a certain extent.\n */\nstatic struct request *bfq_choose_req(struct bfq_data *bfqd,\n\t\t\t\t      struct request *rq1,\n\t\t\t\t      struct request *rq2,\n\t\t\t\t      sector_t last)\n{\n\tsector_t s1, s2, d1 = 0, d2 = 0;\n\tunsigned long back_max;\n#define BFQ_RQ1_WRAP\t0x01 /* request 1 wraps */\n#define BFQ_RQ2_WRAP\t0x02 /* request 2 wraps */\n\tunsigned int wrap = 0; /* bit mask: requests behind the disk head? */\n\n\tif (!rq1 || rq1 == rq2)\n\t\treturn rq2;\n\tif (!rq2)\n\t\treturn rq1;\n\n\tif (rq_is_sync(rq1) && !rq_is_sync(rq2))\n\t\treturn rq1;\n\telse if (rq_is_sync(rq2) && !rq_is_sync(rq1))\n\t\treturn rq2;\n\tif ((rq1->cmd_flags & REQ_META) && !(rq2->cmd_flags & REQ_META))\n\t\treturn rq1;\n\telse if ((rq2->cmd_flags & REQ_META) && !(rq1->cmd_flags & REQ_META))\n\t\treturn rq2;\n\n\ts1 = blk_rq_pos(rq1);\n\ts2 = blk_rq_pos(rq2);\n\n\t/*\n\t * By definition, 1KiB is 2 sectors.\n\t */\n\tback_max = bfqd->bfq_back_max * 2;\n\n\t/*\n\t * Strict one way elevator _except_ in the case where we allow\n\t * short backward seeks which are biased as twice the cost of a\n\t * similar forward seek.\n\t */\n\tif (s1 >= last)\n\t\td1 = s1 - last;\n\telse if (s1 + back_max >= last)\n\t\td1 = (last - s1) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ1_WRAP;\n\n\tif (s2 >= last)\n\t\td2 = s2 - last;\n\telse if (s2 + back_max >= last)\n\t\td2 = (last - s2) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ2_WRAP;\n\n\t/* Found required data */\n\n\t/*\n\t * By doing switch() on the bit mask \"wrap\" we avoid having to\n\t * check two variables for all permutations: --> faster!\n\t */\n\tswitch (wrap) {\n\tcase 0: /* common case for CFQ: rq1 and rq2 not wrapped */\n\t\tif (d1 < d2)\n\t\t\treturn rq1;\n\t\telse if (d2 < d1)\n\t\t\treturn rq2;\n\n\t\tif (s1 >= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\n\tcase BFQ_RQ2_WRAP:\n\t\treturn rq1;\n\tcase BFQ_RQ1_WRAP:\n\t\treturn rq2;\n\tcase BFQ_RQ1_WRAP|BFQ_RQ2_WRAP: /* both rqs wrapped */\n\tdefault:\n\t\t/*\n\t\t * Since both rqs are wrapped,\n\t\t * start with the one that's further behind head\n\t\t * (--> only *one* back seek required),\n\t\t * since back seek takes more time than forward.\n\t\t */\n\t\tif (s1 <= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\t}\n}\n\n/*\n * Async I/O can easily starve sync I/O (both sync reads and sync\n * writes), by consuming all tags. Similarly, storms of sync writes,\n * such as those that sync(2) may trigger, can starve sync reads.\n * Limit depths of async I/O and sync writes so as to counter both\n * problems.\n */\nstatic void bfq_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)\n{\n\tstruct bfq_data *bfqd = data->q->elevator->elevator_data;\n\n\tif (op_is_sync(op) && !op_is_write(op))\n\t\treturn;\n\n\tdata->shallow_depth =\n\t\tbfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(op)];\n\n\tbfq_log(bfqd, \"[%s] wr_busy %d sync %d depth %u\",\n\t\t\t__func__, bfqd->wr_busy_queues, op_is_sync(op),\n\t\t\tdata->shallow_depth);\n}\n\nstatic struct bfq_queue *\nbfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,\n\t\t     sector_t sector, struct rb_node **ret_parent,\n\t\t     struct rb_node ***rb_link)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tparent = NULL;\n\tp = &root->rb_node;\n\twhile (*p) {\n\t\tstruct rb_node **n;\n\n\t\tparent = *p;\n\t\tbfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\n\t\t/*\n\t\t * Sort strictly based on sector. Smallest to the left,\n\t\t * largest to the right.\n\t\t */\n\t\tif (sector > blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_right;\n\t\telse if (sector < blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_left;\n\t\telse\n\t\t\tbreak;\n\t\tp = n;\n\t\tbfqq = NULL;\n\t}\n\n\t*ret_parent = parent;\n\tif (rb_link)\n\t\t*rb_link = p;\n\n\tbfq_log(bfqd, \"rq_pos_tree_lookup %llu: returning %d\",\n\t\t(unsigned long long)sector,\n\t\tbfqq ? bfqq->pid : 0);\n\n\treturn bfqq;\n}\n\nstatic bool bfq_too_late_for_merging(struct bfq_queue *bfqq)\n{\n\treturn bfqq->service_from_backlogged > 0 &&\n\t\ttime_is_before_jiffies(bfqq->first_IO_time +\n\t\t\t\t       bfq_merge_time_limit);\n}\n\n/*\n * The following function is not marked as __cold because it is\n * actually cold, but for the same performance goal described in the\n * comments on the likely() at the beginning of\n * bfq_setup_cooperator(). Unexpectedly, to reach an even lower\n * execution time for the case where this function is not invoked, we\n * had to add an unlikely() in each involved if().\n */\nvoid __cold\nbfq_pos_tree_add_move(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *__bfqq;\n\n\tif (bfqq->pos_root) {\n\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\tbfqq->pos_root = NULL;\n\t}\n\n\t/* oom_bfqq does not participate in queue merging */\n\tif (bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq cannot be merged any longer (see comments in\n\t * bfq_setup_cooperator): no point in adding bfqq into the\n\t * position tree.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn;\n\n\tif (bfq_class_idle(bfqq))\n\t\treturn;\n\tif (!bfqq->next_rq)\n\t\treturn;\n\n\tbfqq->pos_root = &bfq_bfqq_to_bfqg(bfqq)->rq_pos_tree;\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, bfqq->pos_root,\n\t\t\tblk_rq_pos(bfqq->next_rq), &parent, &p);\n\tif (!__bfqq) {\n\t\trb_link_node(&bfqq->pos_node, parent, p);\n\t\trb_insert_color(&bfqq->pos_node, bfqq->pos_root);\n\t} else\n\t\tbfqq->pos_root = NULL;\n}\n\n/*\n * The following function returns false either if every active queue\n * must receive the same share of the throughput (symmetric scenario),\n * or, as a special case, if bfqq must receive a share of the\n * throughput lower than or equal to the share that every other active\n * queue must receive.  If bfqq does sync I/O, then these are the only\n * two cases where bfqq happens to be guaranteed its share of the\n * throughput even if I/O dispatching is not plugged when bfqq remains\n * temporarily empty (for more details, see the comments in the\n * function bfq_better_to_idle()). For this reason, the return value\n * of this function is used to check whether I/O-dispatch plugging can\n * be avoided.\n *\n * The above first case (symmetric scenario) occurs when:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) all active groups at the same level in the groups tree have the same\n *    weight,\n * 4) all active groups at the same level in the groups tree have the same\n *    number of children.\n *\n * Unfortunately, keeping the necessary state for evaluating exactly\n * the last two symmetry sub-conditions above would be quite complex\n * and time consuming. Therefore this function evaluates, instead,\n * only the following stronger three sub-conditions, for which it is\n * much easier to maintain the needed state:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) there are no active groups.\n * In particular, the last condition is always true if hierarchical\n * support or the cgroups interface are not enabled, thus no state\n * needs to be maintained in this case.\n */\nstatic bool bfq_asymmetric_scenario(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tbool smallest_weight = bfqq &&\n\t\tbfqq->weight_counter &&\n\t\tbfqq->weight_counter ==\n\t\tcontainer_of(\n\t\t\trb_first_cached(&bfqd->queue_weights_tree),\n\t\t\tstruct bfq_weight_counter,\n\t\t\tweights_node);\n\n\t/*\n\t * For queue weights to differ, queue_weights_tree must contain\n\t * at least two nodes.\n\t */\n\tbool varied_queue_weights = !smallest_weight &&\n\t\t!RB_EMPTY_ROOT(&bfqd->queue_weights_tree.rb_root) &&\n\t\t(bfqd->queue_weights_tree.rb_root.rb_node->rb_left ||\n\t\t bfqd->queue_weights_tree.rb_root.rb_node->rb_right);\n\n\tbool multiple_classes_busy =\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[1]) ||\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[2]) ||\n\t\t(bfqd->busy_queues[1] && bfqd->busy_queues[2]);\n\n\treturn varied_queue_weights || multiple_classes_busy\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\t       || bfqd->num_groups_with_pending_reqs > 0\n#endif\n\t\t;\n}\n\n/*\n * If the weight-counter tree passed as input contains no counter for\n * the weight of the input queue, then add that counter; otherwise just\n * increment the existing counter.\n *\n * Note that weight-counter trees contain few nodes in mostly symmetric\n * scenarios. For example, if all queues have the same weight, then the\n * weight-counter tree for the queues may contain at most one node.\n * This holds even if low_latency is on, because weight-raised queues\n * are not inserted in the tree.\n * In most scenarios, the rate at which nodes are created/destroyed\n * should be low too.\n */\nvoid bfq_weights_tree_add(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct rb_root_cached *root)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tbool leftmost = true;\n\n\t/*\n\t * Do not insert if the queue is already associated with a\n\t * counter, which happens if:\n\t *   1) a request arrival has caused the queue to become both\n\t *      non-weight-raised, and hence change its weight, and\n\t *      backlogged; in this respect, each of the two events\n\t *      causes an invocation of this function,\n\t *   2) this is the invocation of this function caused by the\n\t *      second event. This second invocation is actually useless,\n\t *      and we handle this fact by exiting immediately. More\n\t *      efficient or clearer solutions might possibly be adopted.\n\t */\n\tif (bfqq->weight_counter)\n\t\treturn;\n\n\twhile (*new) {\n\t\tstruct bfq_weight_counter *__counter = container_of(*new,\n\t\t\t\t\t\tstruct bfq_weight_counter,\n\t\t\t\t\t\tweights_node);\n\t\tparent = *new;\n\n\t\tif (entity->weight == __counter->weight) {\n\t\t\tbfqq->weight_counter = __counter;\n\t\t\tgoto inc_counter;\n\t\t}\n\t\tif (entity->weight < __counter->weight)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\tbfqq->weight_counter = kzalloc(sizeof(struct bfq_weight_counter),\n\t\t\t\t       GFP_ATOMIC);\n\n\t/*\n\t * In the unlucky event of an allocation failure, we just\n\t * exit. This will cause the weight of queue to not be\n\t * considered in bfq_asymmetric_scenario, which, in its turn,\n\t * causes the scenario to be deemed wrongly symmetric in case\n\t * bfqq's weight would have been the only weight making the\n\t * scenario asymmetric.  On the bright side, no unbalance will\n\t * however occur when bfqq becomes inactive again (the\n\t * invocation of this function is triggered by an activation\n\t * of queue).  In fact, bfq_weights_tree_remove does nothing\n\t * if !bfqq->weight_counter.\n\t */\n\tif (unlikely(!bfqq->weight_counter))\n\t\treturn;\n\n\tbfqq->weight_counter->weight = entity->weight;\n\trb_link_node(&bfqq->weight_counter->weights_node, parent, new);\n\trb_insert_color_cached(&bfqq->weight_counter->weights_node, root,\n\t\t\t\tleftmost);\n\ninc_counter:\n\tbfqq->weight_counter->num_active++;\n\tbfqq->ref++;\n}\n\n/*\n * Decrement the weight counter associated with the queue, and, if the\n * counter reaches 0, remove the counter from the tree.\n * See the comments to the function bfq_weights_tree_add() for considerations\n * about overhead.\n */\nvoid __bfq_weights_tree_remove(struct bfq_data *bfqd,\n\t\t\t       struct bfq_queue *bfqq,\n\t\t\t       struct rb_root_cached *root)\n{\n\tif (!bfqq->weight_counter)\n\t\treturn;\n\n\tbfqq->weight_counter->num_active--;\n\tif (bfqq->weight_counter->num_active > 0)\n\t\tgoto reset_entity_pointer;\n\n\trb_erase_cached(&bfqq->weight_counter->weights_node, root);\n\tkfree(bfqq->weight_counter);\n\nreset_entity_pointer:\n\tbfqq->weight_counter = NULL;\n\tbfq_put_queue(bfqq);\n}\n\n/*\n * Invoke __bfq_weights_tree_remove on bfqq and decrement the number\n * of active groups for each queue's inactive parent entity.\n */\nvoid bfq_weights_tree_remove(struct bfq_data *bfqd,\n\t\t\t     struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = bfqq->entity.parent;\n\n\tfor_each_entity(entity) {\n\t\tstruct bfq_sched_data *sd = entity->my_sched_data;\n\n\t\tif (sd->next_in_service || sd->in_service_entity) {\n\t\t\t/*\n\t\t\t * entity is still active, because either\n\t\t\t * next_in_service or in_service_entity is not\n\t\t\t * NULL (see the comments on the definition of\n\t\t\t * next_in_service for details on why\n\t\t\t * in_service_entity must be checked too).\n\t\t\t *\n\t\t\t * As a consequence, its parent entities are\n\t\t\t * active as well, and thus this loop must\n\t\t\t * stop here.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * The decrement of num_groups_with_pending_reqs is\n\t\t * not performed immediately upon the deactivation of\n\t\t * entity, but it is delayed to when it also happens\n\t\t * that the first leaf descendant bfqq of entity gets\n\t\t * all its pending requests completed. The following\n\t\t * instructions perform this delayed decrement, if\n\t\t * needed. See the comments on\n\t\t * num_groups_with_pending_reqs for details.\n\t\t */\n\t\tif (entity->in_groups_with_pending_reqs) {\n\t\t\tentity->in_groups_with_pending_reqs = false;\n\t\t\tbfqd->num_groups_with_pending_reqs--;\n\t\t}\n\t}\n\n\t/*\n\t * Next function is invoked last, because it causes bfqq to be\n\t * freed if the following holds: bfqq is not in service and\n\t * has no dispatched request. DO NOT use bfqq after the next\n\t * function invocation.\n\t */\n\t__bfq_weights_tree_remove(bfqd, bfqq,\n\t\t\t\t  &bfqd->queue_weights_tree);\n}\n\n/*\n * Return expired entry, or NULL to just start from scratch in rbtree.\n */\nstatic struct request *bfq_check_fifo(struct bfq_queue *bfqq,\n\t\t\t\t      struct request *last)\n{\n\tstruct request *rq;\n\n\tif (bfq_bfqq_fifo_expire(bfqq))\n\t\treturn NULL;\n\n\tbfq_mark_bfqq_fifo_expire(bfqq);\n\n\trq = rq_entry_fifo(bfqq->fifo.next);\n\n\tif (rq == last || ktime_get_ns() < rq->fifo_time)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"check_fifo: returned %p\", rq);\n\treturn rq;\n}\n\nstatic struct request *bfq_find_next_rq(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\tstruct request *last)\n{\n\tstruct rb_node *rbnext = rb_next(&last->rb_node);\n\tstruct rb_node *rbprev = rb_prev(&last->rb_node);\n\tstruct request *next, *prev = NULL;\n\n\t/* Follow expired path, else get first next available. */\n\tnext = bfq_check_fifo(bfqq, last);\n\tif (next)\n\t\treturn next;\n\n\tif (rbprev)\n\t\tprev = rb_entry_rq(rbprev);\n\n\tif (rbnext)\n\t\tnext = rb_entry_rq(rbnext);\n\telse {\n\t\trbnext = rb_first(&bfqq->sort_list);\n\t\tif (rbnext && rbnext != &last->rb_node)\n\t\t\tnext = rb_entry_rq(rbnext);\n\t}\n\n\treturn bfq_choose_req(bfqd, next, prev, blk_rq_pos(last));\n}\n\n/* see the definition of bfq_async_charge_factor for details */\nstatic unsigned long bfq_serv_to_charge(struct request *rq,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\tif (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||\n\t    bfq_asymmetric_scenario(bfqq->bfqd, bfqq))\n\t\treturn blk_rq_sectors(rq);\n\n\treturn blk_rq_sectors(rq) * bfq_async_charge_factor;\n}\n\n/**\n * bfq_updated_next_req - update the queue after a new next_rq selection.\n * @bfqd: the device data the queue belongs to.\n * @bfqq: the queue to update.\n *\n * If the first request of a queue changes we make sure that the queue\n * has enough budget to serve at least its first request (if the\n * request has grown).  We do this because if the queue has not enough\n * budget for its first request, it has to go through two dispatch\n * rounds to actually get it dispatched.\n */\nstatic void bfq_updated_next_req(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct request *next_rq = bfqq->next_rq;\n\tunsigned long new_budget;\n\n\tif (!next_rq)\n\t\treturn;\n\n\tif (bfqq == bfqd->in_service_queue)\n\t\t/*\n\t\t * In order not to break guarantees, budgets cannot be\n\t\t * changed after an entity has been selected.\n\t\t */\n\t\treturn;\n\n\tnew_budget = max_t(unsigned long,\n\t\t\t   max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t bfq_serv_to_charge(next_rq, bfqq)),\n\t\t\t   entity->service);\n\tif (entity->budget != new_budget) {\n\t\tentity->budget = new_budget;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"updated next rq: new budget %lu\",\n\t\t\t\t\t new_budget);\n\t\tbfq_requeue_bfqq(bfqd, bfqq, false);\n\t}\n}\n\nstatic unsigned int bfq_wr_duration(struct bfq_data *bfqd)\n{\n\tu64 dur;\n\n\tif (bfqd->bfq_wr_max_time > 0)\n\t\treturn bfqd->bfq_wr_max_time;\n\n\tdur = bfqd->rate_dur_prod;\n\tdo_div(dur, bfqd->peak_rate);\n\n\t/*\n\t * Limit duration between 3 and 25 seconds. The upper limit\n\t * has been conservatively set after the following worst case:\n\t * on a QEMU/KVM virtual machine\n\t * - running in a slow PC\n\t * - with a virtual disk stacked on a slow low-end 5400rpm HDD\n\t * - serving a heavy I/O workload, such as the sequential reading\n\t *   of several files\n\t * mplayer took 23 seconds to start, if constantly weight-raised.\n\t *\n\t * As for higher values than that accommodating the above bad\n\t * scenario, tests show that higher values would often yield\n\t * the opposite of the desired result, i.e., would worsen\n\t * responsiveness by allowing non-interactive applications to\n\t * preserve weight raising for too long.\n\t *\n\t * On the other end, lower values than 3 seconds make it\n\t * difficult for most interactive tasks to complete their jobs\n\t * before weight-raising finishes.\n\t */\n\treturn clamp_val(dur, msecs_to_jiffies(3000), msecs_to_jiffies(25000));\n}\n\n/* switch back from soft real-time to interactive weight raising */\nstatic void switch_back_to_interactive_wr(struct bfq_queue *bfqq,\n\t\t\t\t\t  struct bfq_data *bfqd)\n{\n\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\tbfqq->last_wr_start_finish = bfqq->wr_start_at_switch_to_srt;\n}\n\nstatic void\nbfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,\n\t\t      struct bfq_io_cq *bic, bool bfq_already_existing)\n{\n\tunsigned int old_wr_coeff = 1;\n\tbool busy = bfq_already_existing && bfq_bfqq_busy(bfqq);\n\n\tif (bic->saved_has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\tif (bic->saved_IO_bound)\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\tbfqq->last_serv_time_ns = bic->saved_last_serv_time_ns;\n\tbfqq->inject_limit = bic->saved_inject_limit;\n\tbfqq->decrease_time_jif = bic->saved_decrease_time_jif;\n\n\tbfqq->entity.new_weight = bic->saved_weight;\n\tbfqq->ttime = bic->saved_ttime;\n\tbfqq->io_start_time = bic->saved_io_start_time;\n\tbfqq->tot_idle_time = bic->saved_tot_idle_time;\n\t/*\n\t * Restore weight coefficient only if low_latency is on\n\t */\n\tif (bfqd->low_latency) {\n\t\told_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq->wr_coeff = bic->saved_wr_coeff;\n\t}\n\tbfqq->service_from_wr = bic->saved_service_from_wr;\n\tbfqq->wr_start_at_switch_to_srt = bic->saved_wr_start_at_switch_to_srt;\n\tbfqq->last_wr_start_finish = bic->saved_last_wr_start_finish;\n\tbfqq->wr_cur_max_time = bic->saved_wr_cur_max_time;\n\n\tif (bfqq->wr_coeff > 1 && (bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t   bfqq->wr_cur_max_time))) {\n\t\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t    time_is_after_eq_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t     bfq_wr_duration(bfqd))) {\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t} else {\n\t\t\tbfqq->wr_coeff = 1;\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t     \"resume state: switching off wr\");\n\t\t}\n\t}\n\n\t/* make sure weight will be updated, however we got here */\n\tbfqq->entity.prio_changed = 1;\n\n\tif (likely(!busy))\n\t\treturn;\n\n\tif (old_wr_coeff == 1 && bfqq->wr_coeff > 1)\n\t\tbfqd->wr_busy_queues++;\n\telse if (old_wr_coeff > 1 && bfqq->wr_coeff == 1)\n\t\tbfqd->wr_busy_queues--;\n}\n\nstatic int bfqq_process_refs(struct bfq_queue *bfqq)\n{\n\treturn bfqq->ref - bfqq->allocated - bfqq->entity.on_st_or_in_serv -\n\t\t(bfqq->weight_counter != NULL) - bfqq->stable_ref;\n}\n\n/* Empty burst list and add just bfqq (see comments on bfq_handle_burst) */\nstatic void bfq_reset_burst_list(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(item, n, &bfqd->burst_list, burst_list_node)\n\t\thlist_del_init(&item->burst_list_node);\n\n\t/*\n\t * Start the creation of a new burst list only if there is no\n\t * active queue. See comments on the conditional invocation of\n\t * bfq_handle_burst().\n\t */\n\tif (bfq_tot_busy_queues(bfqd) == 0) {\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n\t\tbfqd->burst_size = 1;\n\t} else\n\t\tbfqd->burst_size = 0;\n\n\tbfqd->burst_parent_entity = bfqq->entity.parent;\n}\n\n/* Add bfqq to the list of queues in current burst (see bfq_handle_burst) */\nstatic void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/* Increment burst size to take into account also bfqq */\n\tbfqd->burst_size++;\n\n\tif (bfqd->burst_size == bfqd->bfq_large_burst_thresh) {\n\t\tstruct bfq_queue *pos, *bfqq_item;\n\t\tstruct hlist_node *n;\n\n\t\t/*\n\t\t * Enough queues have been activated shortly after each\n\t\t * other to consider this burst as large.\n\t\t */\n\t\tbfqd->large_burst = true;\n\n\t\t/*\n\t\t * We can now mark all queues in the burst list as\n\t\t * belonging to a large burst.\n\t\t */\n\t\thlist_for_each_entry(bfqq_item, &bfqd->burst_list,\n\t\t\t\t     burst_list_node)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq_item);\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\n\t\t/*\n\t\t * From now on, and until the current burst finishes, any\n\t\t * new queue being activated shortly after the last queue\n\t\t * was inserted in the burst can be immediately marked as\n\t\t * belonging to a large burst. So the burst list is not\n\t\t * needed any more. Remove it.\n\t\t */\n\t\thlist_for_each_entry_safe(pos, n, &bfqd->burst_list,\n\t\t\t\t\t  burst_list_node)\n\t\t\thlist_del_init(&pos->burst_list_node);\n\t} else /*\n\t\t* Burst not yet large: add bfqq to the burst list. Do\n\t\t* not increment the ref counter for bfqq, because bfqq\n\t\t* is removed from the burst list before freeing bfqq\n\t\t* in put_queue.\n\t\t*/\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n}\n\n/*\n * If many queues belonging to the same group happen to be created\n * shortly after each other, then the processes associated with these\n * queues have typically a common goal. In particular, bursts of queue\n * creations are usually caused by services or applications that spawn\n * many parallel threads/processes. Examples are systemd during boot,\n * or git grep. To help these processes get their job done as soon as\n * possible, it is usually better to not grant either weight-raising\n * or device idling to their queues, unless these queues must be\n * protected from the I/O flowing through other active queues.\n *\n * In this comment we describe, firstly, the reasons why this fact\n * holds, and, secondly, the next function, which implements the main\n * steps needed to properly mark these queues so that they can then be\n * treated in a different way.\n *\n * The above services or applications benefit mostly from a high\n * throughput: the quicker the requests of the activated queues are\n * cumulatively served, the sooner the target job of these queues gets\n * completed. As a consequence, weight-raising any of these queues,\n * which also implies idling the device for it, is almost always\n * counterproductive, unless there are other active queues to isolate\n * these new queues from. If there no other active queues, then\n * weight-raising these new queues just lowers throughput in most\n * cases.\n *\n * On the other hand, a burst of queue creations may be caused also by\n * the start of an application that does not consist of a lot of\n * parallel I/O-bound threads. In fact, with a complex application,\n * several short processes may need to be executed to start-up the\n * application. In this respect, to start an application as quickly as\n * possible, the best thing to do is in any case to privilege the I/O\n * related to the application with respect to all other\n * I/O. Therefore, the best strategy to start as quickly as possible\n * an application that causes a burst of queue creations is to\n * weight-raise all the queues created during the burst. This is the\n * exact opposite of the best strategy for the other type of bursts.\n *\n * In the end, to take the best action for each of the two cases, the\n * two types of bursts need to be distinguished. Fortunately, this\n * seems relatively easy, by looking at the sizes of the bursts. In\n * particular, we found a threshold such that only bursts with a\n * larger size than that threshold are apparently caused by\n * services or commands such as systemd or git grep. For brevity,\n * hereafter we call just 'large' these bursts. BFQ *does not*\n * weight-raise queues whose creation occurs in a large burst. In\n * addition, for each of these queues BFQ performs or does not perform\n * idling depending on which choice boosts the throughput more. The\n * exact choice depends on the device and request pattern at\n * hand.\n *\n * Unfortunately, false positives may occur while an interactive task\n * is starting (e.g., an application is being started). The\n * consequence is that the queues associated with the task do not\n * enjoy weight raising as expected. Fortunately these false positives\n * are very rare. They typically occur if some service happens to\n * start doing I/O exactly when the interactive task starts.\n *\n * Turning back to the next function, it is invoked only if there are\n * no active queues (apart from active queues that would belong to the\n * same, possible burst bfqq would belong to), and it implements all\n * the steps needed to detect the occurrence of a large burst and to\n * properly mark all the queues belonging to it (so that they can then\n * be treated in a different way). This goal is achieved by\n * maintaining a \"burst list\" that holds, temporarily, the queues that\n * belong to the burst in progress. The list is then used to mark\n * these queues as belonging to a large burst if the burst does become\n * large. The main steps are the following.\n *\n * . when the very first queue is created, the queue is inserted into the\n *   list (as it could be the first queue in a possible burst)\n *\n * . if the current burst has not yet become large, and a queue Q that does\n *   not yet belong to the burst is activated shortly after the last time\n *   at which a new queue entered the burst list, then the function appends\n *   Q to the burst list\n *\n * . if, as a consequence of the previous step, the burst size reaches\n *   the large-burst threshold, then\n *\n *     . all the queues in the burst list are marked as belonging to a\n *       large burst\n *\n *     . the burst list is deleted; in fact, the burst list already served\n *       its purpose (keeping temporarily track of the queues in a burst,\n *       so as to be able to mark them as belonging to a large burst in the\n *       previous sub-step), and now is not needed any more\n *\n *     . the device enters a large-burst mode\n *\n * . if a queue Q that does not belong to the burst is created while\n *   the device is in large-burst mode and shortly after the last time\n *   at which a queue either entered the burst list or was marked as\n *   belonging to the current large burst, then Q is immediately marked\n *   as belonging to a large burst.\n *\n * . if a queue Q that does not belong to the burst is created a while\n *   later, i.e., not shortly after, than the last time at which a queue\n *   either entered the burst list or was marked as belonging to the\n *   current large burst, then the current burst is deemed as finished and:\n *\n *        . the large-burst mode is reset if set\n *\n *        . the burst list is emptied\n *\n *        . Q is inserted in the burst list, as Q may be the first queue\n *          in a possible new burst (then the burst list contains just Q\n *          after this step).\n */\nstatic void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq is already in the burst list or is part of a large\n\t * burst, or finally has just been split, then there is\n\t * nothing else to do.\n\t */\n\tif (!hlist_unhashed(&bfqq->burst_list_node) ||\n\t    bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     msecs_to_jiffies(10)))\n\t\treturn;\n\n\t/*\n\t * If bfqq's creation happens late enough, or bfqq belongs to\n\t * a different group than the burst group, then the current\n\t * burst is finished, and related data structures must be\n\t * reset.\n\t *\n\t * In this respect, consider the special case where bfqq is\n\t * the very first queue created after BFQ is selected for this\n\t * device. In this case, last_ins_in_burst and\n\t * burst_parent_entity are not yet significant when we get\n\t * here. But it is easy to verify that, whether or not the\n\t * following condition is true, bfqq will end up being\n\t * inserted into the burst list. In particular the list will\n\t * happen to contain only bfqq. And this is exactly what has\n\t * to happen, as bfqq may be the first queue of the first\n\t * burst.\n\t */\n\tif (time_is_before_jiffies(bfqd->last_ins_in_burst +\n\t    bfqd->bfq_burst_interval) ||\n\t    bfqq->entity.parent != bfqd->burst_parent_entity) {\n\t\tbfqd->large_burst = false;\n\t\tbfq_reset_burst_list(bfqd, bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then bfqq is being activated shortly after the\n\t * last queue. So, if the current burst is also large, we can mark\n\t * bfqq as belonging to this large burst immediately.\n\t */\n\tif (bfqd->large_burst) {\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then a large-burst state has not yet been\n\t * reached, but bfqq is being activated shortly after the last\n\t * queue. Then we add bfqq to the burst.\n\t */\n\tbfq_add_to_burst(bfqd, bfqq);\nend:\n\t/*\n\t * At this point, bfqq either has been added to the current\n\t * burst or has caused the current burst to terminate and a\n\t * possible new burst to start. In particular, in the second\n\t * case, bfqq has become the first queue in the possible new\n\t * burst.  In both cases last_ins_in_burst needs to be moved\n\t * forward.\n\t */\n\tbfqd->last_ins_in_burst = jiffies;\n}\n\nstatic int bfq_bfqq_budget_left(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\treturn entity->budget - entity->service;\n}\n\n/*\n * If enough samples have been computed, return the current max budget\n * stored in bfqd, which is dynamically updated according to the\n * estimated disk peak rate; otherwise return the default max budget\n */\nstatic int bfq_max_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget;\n\telse\n\t\treturn bfqd->bfq_max_budget;\n}\n\n/*\n * Return min budget, which is a fraction of the current or default\n * max budget (trying with 1/32)\n */\nstatic int bfq_min_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget / 32;\n\telse\n\t\treturn bfqd->bfq_max_budget / 32;\n}\n\n/*\n * The next function, invoked after the input queue bfqq switches from\n * idle to busy, updates the budget of bfqq. The function also tells\n * whether the in-service queue should be expired, by returning\n * true. The purpose of expiring the in-service queue is to give bfqq\n * the chance to possibly preempt the in-service queue, and the reason\n * for preempting the in-service queue is to achieve one of the two\n * goals below.\n *\n * 1. Guarantee to bfqq its reserved bandwidth even if bfqq has\n * expired because it has remained idle. In particular, bfqq may have\n * expired for one of the following two reasons:\n *\n * - BFQQE_NO_MORE_REQUESTS bfqq did not enjoy any device idling\n *   and did not make it to issue a new request before its last\n *   request was served;\n *\n * - BFQQE_TOO_IDLE bfqq did enjoy device idling, but did not issue\n *   a new request before the expiration of the idling-time.\n *\n * Even if bfqq has expired for one of the above reasons, the process\n * associated with the queue may be however issuing requests greedily,\n * and thus be sensitive to the bandwidth it receives (bfqq may have\n * remained idle for other reasons: CPU high load, bfqq not enjoying\n * idling, I/O throttling somewhere in the path from the process to\n * the I/O scheduler, ...). But if, after every expiration for one of\n * the above two reasons, bfqq has to wait for the service of at least\n * one full budget of another queue before being served again, then\n * bfqq is likely to get a much lower bandwidth or resource time than\n * its reserved ones. To address this issue, two countermeasures need\n * to be taken.\n *\n * First, the budget and the timestamps of bfqq need to be updated in\n * a special way on bfqq reactivation: they need to be updated as if\n * bfqq did not remain idle and did not expire. In fact, if they are\n * computed as if bfqq expired and remained idle until reactivation,\n * then the process associated with bfqq is treated as if, instead of\n * being greedy, it stopped issuing requests when bfqq remained idle,\n * and restarts issuing requests only on this reactivation. In other\n * words, the scheduler does not help the process recover the \"service\n * hole\" between bfqq expiration and reactivation. As a consequence,\n * the process receives a lower bandwidth than its reserved one. In\n * contrast, to recover this hole, the budget must be updated as if\n * bfqq was not expired at all before this reactivation, i.e., it must\n * be set to the value of the remaining budget when bfqq was\n * expired. Along the same line, timestamps need to be assigned the\n * value they had the last time bfqq was selected for service, i.e.,\n * before last expiration. Thus timestamps need to be back-shifted\n * with respect to their normal computation (see [1] for more details\n * on this tricky aspect).\n *\n * Secondly, to allow the process to recover the hole, the in-service\n * queue must be expired too, to give bfqq the chance to preempt it\n * immediately. In fact, if bfqq has to wait for a full budget of the\n * in-service queue to be completed, then it may become impossible to\n * let the process recover the hole, even if the back-shifted\n * timestamps of bfqq are lower than those of the in-service queue. If\n * this happens for most or all of the holes, then the process may not\n * receive its reserved bandwidth. In this respect, it is worth noting\n * that, being the service of outstanding requests unpreemptible, a\n * little fraction of the holes may however be unrecoverable, thereby\n * causing a little loss of bandwidth.\n *\n * The last important point is detecting whether bfqq does need this\n * bandwidth recovery. In this respect, the next function deems the\n * process associated with bfqq greedy, and thus allows it to recover\n * the hole, if: 1) the process is waiting for the arrival of a new\n * request (which implies that bfqq expired for one of the above two\n * reasons), and 2) such a request has arrived soon. The first\n * condition is controlled through the flag non_blocking_wait_rq,\n * while the second through the flag arrived_in_time. If both\n * conditions hold, then the function computes the budget in the\n * above-described special way, and signals that the in-service queue\n * should be expired. Timestamp back-shifting is done later in\n * __bfq_activate_entity.\n *\n * 2. Reduce latency. Even if timestamps are not backshifted to let\n * the process associated with bfqq recover a service hole, bfqq may\n * however happen to have, after being (re)activated, a lower finish\n * timestamp than the in-service queue.\t That is, the next budget of\n * bfqq may have to be completed before the one of the in-service\n * queue. If this is the case, then preempting the in-service queue\n * allows this goal to be achieved, apart from the unpreemptible,\n * outstanding requests mentioned above.\n *\n * Unfortunately, regardless of which of the above two goals one wants\n * to achieve, service trees need first to be updated to know whether\n * the in-service queue must be preempted. To have service trees\n * correctly updated, the in-service queue must be expired and\n * rescheduled, and bfqq must be scheduled too. This is one of the\n * most costly operations (in future versions, the scheduling\n * mechanism may be re-designed in such a way to make it possible to\n * know whether preemption is needed without needing to update service\n * trees). In addition, queue preemptions almost always cause random\n * I/O, which may in turn cause loss of throughput. Finally, there may\n * even be no in-service queue when the next function is invoked (so,\n * no queue to compare timestamps with). Because of these facts, the\n * next function adopts the following simple scheme to avoid costly\n * operations, too frequent preemptions and too many dependencies on\n * the state of the scheduler: it requests the expiration of the\n * in-service queue (unconditionally) only for queues that need to\n * recover a hole. Then it delegates to other parts of the code the\n * responsibility of handling the above case 2.\n */\nstatic bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\t\tbool arrived_in_time)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * In the next compound condition, we check also whether there\n\t * is some budget left, because otherwise there is no point in\n\t * trying to go on serving bfqq with this same budget: bfqq\n\t * would be expired immediately after being selected for\n\t * service. This would only cause useless overhead.\n\t */\n\tif (bfq_bfqq_non_blocking_wait_rq(bfqq) && arrived_in_time &&\n\t    bfq_bfqq_budget_left(bfqq) > 0) {\n\t\t/*\n\t\t * We do not clear the flag non_blocking_wait_rq here, as\n\t\t * the latter is used in bfq_activate_bfqq to signal\n\t\t * that timestamps need to be back-shifted (and is\n\t\t * cleared right after).\n\t\t */\n\n\t\t/*\n\t\t * In next assignment we rely on that either\n\t\t * entity->service or entity->budget are not updated\n\t\t * on expiration if bfqq is empty (see\n\t\t * __bfq_bfqq_recalc_budget). Thus both quantities\n\t\t * remain unchanged after such an expiration, and the\n\t\t * following statement therefore assigns to\n\t\t * entity->budget the remaining budget on such an\n\t\t * expiration.\n\t\t */\n\t\tentity->budget = min_t(unsigned long,\n\t\t\t\t       bfq_bfqq_budget_left(bfqq),\n\t\t\t\t       bfqq->max_budget);\n\n\t\t/*\n\t\t * At this point, we have used entity->service to get\n\t\t * the budget left (needed for updating\n\t\t * entity->budget). Thus we finally can, and have to,\n\t\t * reset entity->service. The latter must be reset\n\t\t * because bfqq would otherwise be charged again for\n\t\t * the service it has received during its previous\n\t\t * service slot(s).\n\t\t */\n\t\tentity->service = 0;\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * We can finally complete expiration, by setting service to 0.\n\t */\n\tentity->service = 0;\n\tentity->budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t       bfq_serv_to_charge(bfqq->next_rq, bfqq));\n\tbfq_clear_bfqq_non_blocking_wait_rq(bfqq);\n\treturn false;\n}\n\n/*\n * Return the farthest past time instant according to jiffies\n * macros.\n */\nstatic unsigned long bfq_smallest_from_now(void)\n{\n\treturn jiffies - MAX_JIFFY_OFFSET;\n}\n\nstatic void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     unsigned int old_wr_coeff,\n\t\t\t\t\t     bool wr_or_deserves_wr,\n\t\t\t\t\t     bool interactive,\n\t\t\t\t\t     bool in_burst,\n\t\t\t\t\t     bool soft_rt)\n{\n\tif (old_wr_coeff == 1 && wr_or_deserves_wr) {\n\t\t/* start a weight-raising period */\n\t\tif (interactive) {\n\t\t\tbfqq->service_from_wr = 0;\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else {\n\t\t\t/*\n\t\t\t * No interactive weight raising in progress\n\t\t\t * here: assign minus infinity to\n\t\t\t * wr_start_at_switch_to_srt, to make sure\n\t\t\t * that, at the end of the soft-real-time\n\t\t\t * weight raising periods that is starting\n\t\t\t * now, no interactive weight-raising period\n\t\t\t * may be wrongly considered as still in\n\t\t\t * progress (and thus actually started by\n\t\t\t * mistake).\n\t\t\t */\n\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\tbfq_smallest_from_now();\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t}\n\n\t\t/*\n\t\t * If needed, further reduce budget to make sure it is\n\t\t * close to bfqq's backlog, so as to reduce the\n\t\t * scheduling-error component due to a too large\n\t\t * budget. Do not care about throughput consequences,\n\t\t * but only about latency. Finally, do not assign a\n\t\t * too small budget either, to avoid increasing\n\t\t * latency by causing too frequent expirations.\n\t\t */\n\t\tbfqq->entity.budget = min_t(unsigned long,\n\t\t\t\t\t    bfqq->entity.budget,\n\t\t\t\t\t    2 * bfq_min_budget(bfqd));\n\t} else if (old_wr_coeff > 1) {\n\t\tif (interactive) { /* update wr coeff and duration */\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else if (in_burst)\n\t\t\tbfqq->wr_coeff = 1;\n\t\telse if (soft_rt) {\n\t\t\t/*\n\t\t\t * The application is now or still meeting the\n\t\t\t * requirements for being deemed soft rt.  We\n\t\t\t * can then correctly and safely (re)charge\n\t\t\t * the weight-raising duration for the\n\t\t\t * application with the weight-raising\n\t\t\t * duration for soft rt applications.\n\t\t\t *\n\t\t\t * In particular, doing this recharge now, i.e.,\n\t\t\t * before the weight-raising period for the\n\t\t\t * application finishes, reduces the probability\n\t\t\t * of the following negative scenario:\n\t\t\t * 1) the weight of a soft rt application is\n\t\t\t *    raised at startup (as for any newly\n\t\t\t *    created application),\n\t\t\t * 2) since the application is not interactive,\n\t\t\t *    at a certain time weight-raising is\n\t\t\t *    stopped for the application,\n\t\t\t * 3) at that time the application happens to\n\t\t\t *    still have pending requests, and hence\n\t\t\t *    is destined to not have a chance to be\n\t\t\t *    deemed soft rt before these requests are\n\t\t\t *    completed (see the comments to the\n\t\t\t *    function bfq_bfqq_softrt_next_start()\n\t\t\t *    for details on soft rt detection),\n\t\t\t * 4) these pending requests experience a high\n\t\t\t *    latency because the application is not\n\t\t\t *    weight-raised while they are pending.\n\t\t\t */\n\t\t\tif (bfqq->wr_cur_max_time !=\n\t\t\t\tbfqd->bfq_wr_rt_max_time) {\n\t\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\t\tbfqq->last_wr_start_finish;\n\n\t\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\t}\n\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\t}\n}\n\nstatic bool bfq_bfqq_idle_for_long_time(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn bfqq->dispatched == 0 &&\n\t\ttime_is_before_jiffies(\n\t\t\tbfqq->budget_timeout +\n\t\t\tbfqd->bfq_wr_min_idle_time);\n}\n\n\n/*\n * Return true if bfqq is in a higher priority class, or has a higher\n * weight than the in-service queue.\n */\nstatic bool bfq_bfqq_higher_class_or_weight(struct bfq_queue *bfqq,\n\t\t\t\t\t    struct bfq_queue *in_serv_bfqq)\n{\n\tint bfqq_weight, in_serv_weight;\n\n\tif (bfqq->ioprio_class < in_serv_bfqq->ioprio_class)\n\t\treturn true;\n\n\tif (in_serv_bfqq->entity.parent == bfqq->entity.parent) {\n\t\tbfqq_weight = bfqq->entity.weight;\n\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t} else {\n\t\tif (bfqq->entity.parent)\n\t\t\tbfqq_weight = bfqq->entity.parent->weight;\n\t\telse\n\t\t\tbfqq_weight = bfqq->entity.weight;\n\t\tif (in_serv_bfqq->entity.parent)\n\t\t\tin_serv_weight = in_serv_bfqq->entity.parent->weight;\n\t\telse\n\t\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t}\n\n\treturn bfqq_weight > in_serv_weight;\n}\n\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq);\n\nstatic void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     int old_wr_coeff,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     bool *interactive)\n{\n\tbool soft_rt, in_burst,\twr_or_deserves_wr,\n\t\tbfqq_wants_to_preempt,\n\t\tidle_for_long_time = bfq_bfqq_idle_for_long_time(bfqd, bfqq),\n\t\t/*\n\t\t * See the comments on\n\t\t * bfq_bfqq_update_budg_for_activation for\n\t\t * details on the usage of the next variable.\n\t\t */\n\t\tarrived_in_time =  ktime_get_ns() <=\n\t\t\tbfqq->ttime.last_end_request +\n\t\t\tbfqd->bfq_slice_idle * 3;\n\n\n\t/*\n\t * bfqq deserves to be weight-raised if:\n\t * - it is sync,\n\t * - it does not belong to a large burst,\n\t * - it has been idle for enough time or is soft real-time,\n\t * - is linked to a bfq_io_cq (it is not shared in any sense),\n\t * - has a default weight (otherwise we assume the user wanted\n\t *   to control its weight explicitly)\n\t */\n\tin_burst = bfq_bfqq_in_large_burst(bfqq);\n\tsoft_rt = bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t\t!BFQQ_TOTALLY_SEEKY(bfqq) &&\n\t\t!in_burst &&\n\t\ttime_is_before_jiffies(bfqq->soft_rt_next_start) &&\n\t\tbfqq->dispatched == 0 &&\n\t\tbfqq->entity.new_weight == 40;\n\t*interactive = !in_burst && idle_for_long_time &&\n\t\tbfqq->entity.new_weight == 40;\n\t/*\n\t * Merged bfq_queues are kept out of weight-raising\n\t * (low-latency) mechanisms. The reason is that these queues\n\t * are usually created for non-interactive and\n\t * non-soft-real-time tasks. Yet this is not the case for\n\t * stably-merged queues. These queues are merged just because\n\t * they are created shortly after each other. So they may\n\t * easily serve the I/O of an interactive or soft-real time\n\t * application, if the application happens to spawn multiple\n\t * processes. So let also stably-merged queued enjoy weight\n\t * raising.\n\t */\n\twr_or_deserves_wr = bfqd->low_latency &&\n\t\t(bfqq->wr_coeff > 1 ||\n\t\t (bfq_bfqq_sync(bfqq) &&\n\t\t  (bfqq->bic || RQ_BIC(rq)->stably_merged) &&\n\t\t   (*interactive || soft_rt)));\n\n\t/*\n\t * Using the last flag, update budget and check whether bfqq\n\t * may want to preempt the in-service queue.\n\t */\n\tbfqq_wants_to_preempt =\n\t\tbfq_bfqq_update_budg_for_activation(bfqd, bfqq,\n\t\t\t\t\t\t    arrived_in_time);\n\n\t/*\n\t * If bfqq happened to be activated in a burst, but has been\n\t * idle for much more than an interactive queue, then we\n\t * assume that, in the overall I/O initiated in the burst, the\n\t * I/O associated with bfqq is finished. So bfqq does not need\n\t * to be treated as a queue belonging to a burst\n\t * anymore. Accordingly, we reset bfqq's in_large_burst flag\n\t * if set, and remove bfqq from the burst list if it's\n\t * there. We do not decrement burst_size, because the fact\n\t * that bfqq does not need to belong to the burst list any\n\t * more does not invalidate the fact that bfqq was created in\n\t * a burst.\n\t */\n\tif (likely(!bfq_bfqq_just_created(bfqq)) &&\n\t    idle_for_long_time &&\n\t    time_is_before_jiffies(\n\t\t    bfqq->budget_timeout +\n\t\t    msecs_to_jiffies(10000))) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t}\n\n\tbfq_clear_bfqq_just_created(bfqq);\n\n\tif (bfqd->low_latency) {\n\t\tif (unlikely(time_is_after_jiffies(bfqq->split_time)))\n\t\t\t/* wraparound */\n\t\t\tbfqq->split_time =\n\t\t\t\tjiffies - bfqd->bfq_wr_min_idle_time - 1;\n\n\t\tif (time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t   bfqd->bfq_wr_min_idle_time)) {\n\t\t\tbfq_update_bfqq_wr_on_rq_arrival(bfqd, bfqq,\n\t\t\t\t\t\t\t old_wr_coeff,\n\t\t\t\t\t\t\t wr_or_deserves_wr,\n\t\t\t\t\t\t\t *interactive,\n\t\t\t\t\t\t\t in_burst,\n\t\t\t\t\t\t\t soft_rt);\n\n\t\t\tif (old_wr_coeff != bfqq->wr_coeff)\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n\n\tbfqq->last_idle_bklogged = jiffies;\n\tbfqq->service_from_backlogged = 0;\n\tbfq_clear_bfqq_softrt_update(bfqq);\n\n\tbfq_add_bfqq_busy(bfqd, bfqq);\n\n\t/*\n\t * Expire in-service queue if preemption may be needed for\n\t * guarantees or throughput. As for guarantees, we care\n\t * explicitly about two cases. The first is that bfqq has to\n\t * recover a service hole, as explained in the comments on\n\t * bfq_bfqq_update_budg_for_activation(), i.e., that\n\t * bfqq_wants_to_preempt is true. However, if bfqq does not\n\t * carry time-critical I/O, then bfqq's bandwidth is less\n\t * important than that of queues that carry time-critical I/O.\n\t * So, as a further constraint, we consider this case only if\n\t * bfqq is at least as weight-raised, i.e., at least as time\n\t * critical, as the in-service queue.\n\t *\n\t * The second case is that bfqq is in a higher priority class,\n\t * or has a higher weight than the in-service queue. If this\n\t * condition does not hold, we don't care because, even if\n\t * bfqq does not start to be served immediately, the resulting\n\t * delay for bfqq's I/O is however lower or much lower than\n\t * the ideal completion time to be guaranteed to bfqq's I/O.\n\t *\n\t * In both cases, preemption is needed only if, according to\n\t * the timestamps of both bfqq and of the in-service queue,\n\t * bfqq actually is the next queue to serve. So, to reduce\n\t * useless preemptions, the return value of\n\t * next_queue_may_preempt() is considered in the next compound\n\t * condition too. Yet next_queue_may_preempt() just checks a\n\t * simple, necessary condition for bfqq to be the next queue\n\t * to serve. In fact, to evaluate a sufficient condition, the\n\t * timestamps of the in-service queue would need to be\n\t * updated, and this operation is quite costly (see the\n\t * comments on bfq_bfqq_update_budg_for_activation()).\n\t *\n\t * As for throughput, we ask bfq_better_to_idle() whether we\n\t * still need to plug I/O dispatching. If bfq_better_to_idle()\n\t * says no, then plugging is not needed any longer, either to\n\t * boost throughput or to perserve service guarantees. Then\n\t * the best option is to stop plugging I/O, as not doing so\n\t * would certainly lower throughput. We may end up in this\n\t * case if: (1) upon a dispatch attempt, we detected that it\n\t * was better to plug I/O dispatch, and to wait for a new\n\t * request to arrive for the currently in-service queue, but\n\t * (2) this switch of bfqq to busy changes the scenario.\n\t */\n\tif (bfqd->in_service_queue &&\n\t    ((bfqq_wants_to_preempt &&\n\t      bfqq->wr_coeff >= bfqd->in_service_queue->wr_coeff) ||\n\t     bfq_bfqq_higher_class_or_weight(bfqq, bfqd->in_service_queue) ||\n\t     !bfq_better_to_idle(bfqd->in_service_queue)) &&\n\t    next_queue_may_preempt(bfqd))\n\t\tbfq_bfqq_expire(bfqd, bfqd->in_service_queue,\n\t\t\t\tfalse, BFQQE_PREEMPTED);\n}\n\nstatic void bfq_reset_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\t/* invalidate baseline total service time */\n\tbfqq->last_serv_time_ns = 0;\n\n\t/*\n\t * Reset pointer in case we are waiting for\n\t * some request completion.\n\t */\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * If bfqq has a short think time, then start by setting the\n\t * inject limit to 0 prudentially, because the service time of\n\t * an injected I/O request may be higher than the think time\n\t * of bfqq, and therefore, if one request was injected when\n\t * bfqq remains empty, this injected request might delay the\n\t * service of the next I/O request for bfqq significantly. In\n\t * case bfqq can actually tolerate some injection, then the\n\t * adaptive update will however raise the limit soon. This\n\t * lucky circumstance holds exactly because bfqq has a short\n\t * think time, and thus, after remaining empty, is likely to\n\t * get new I/O enqueued---and then completed---before being\n\t * expired. This is the very pattern that gives the\n\t * limit-update algorithm the chance to measure the effect of\n\t * injection on request service times, and then to update the\n\t * limit accordingly.\n\t *\n\t * However, in the following special case, the inject limit is\n\t * left to 1 even if the think time is short: bfqq's I/O is\n\t * synchronized with that of some other queue, i.e., bfqq may\n\t * receive new I/O only after the I/O of the other queue is\n\t * completed. Keeping the inject limit to 1 allows the\n\t * blocking I/O to be served while bfqq is in service. And\n\t * this is very convenient both for bfqq and for overall\n\t * throughput, as explained in detail in the comments in\n\t * bfq_update_has_short_ttime().\n\t *\n\t * On the opposite end, if bfqq has a long think time, then\n\t * start directly by 1, because:\n\t * a) on the bright side, keeping at most one request in\n\t * service in the drive is unlikely to cause any harm to the\n\t * latency of bfqq's requests, as the service time of a single\n\t * request is likely to be lower than the think time of bfqq;\n\t * b) on the downside, after becoming empty, bfqq is likely to\n\t * expire before getting its next request. With this request\n\t * arrival pattern, it is very hard to sample total service\n\t * times and update the inject limit accordingly (see comments\n\t * on bfq_update_inject_limit()). So the limit is likely to be\n\t * never, or at least seldom, updated.  As a consequence, by\n\t * setting the limit to 1, we avoid that no injection ever\n\t * occurs with bfqq. On the downside, this proactive step\n\t * further reduces chances to actually compute the baseline\n\t * total service time. Thus it reduces chances to execute the\n\t * limit-update algorithm and possibly raise the limit to more\n\t * than 1.\n\t */\n\tif (bfq_bfqq_has_short_ttime(bfqq))\n\t\tbfqq->inject_limit = 0;\n\telse\n\t\tbfqq->inject_limit = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)\n{\n\tu64 tot_io_time = now_ns - bfqq->io_start_time;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) && bfqq->dispatched == 0)\n\t\tbfqq->tot_idle_time +=\n\t\t\tnow_ns - bfqq->ttime.last_end_request;\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq)))\n\t\treturn;\n\n\t/*\n\t * Must be busy for at least about 80% of the time to be\n\t * considered I/O bound.\n\t */\n\tif (bfqq->tot_idle_time * 5 > tot_io_time)\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * Keep an observation window of at most 200 ms in the past\n\t * from now.\n\t */\n\tif (tot_io_time > 200 * NSEC_PER_MSEC) {\n\t\tbfqq->io_start_time = now_ns - (tot_io_time>>1);\n\t\tbfqq->tot_idle_time >>= 1;\n\t}\n}\n\n/*\n * Detect whether bfqq's I/O seems synchronized with that of some\n * other queue, i.e., whether bfqq, after remaining empty, happens to\n * receive new I/O only right after some I/O request of the other\n * queue has been completed. We call waker queue the other queue, and\n * we assume, for simplicity, that bfqq may have at most one waker\n * queue.\n *\n * A remarkable throughput boost can be reached by unconditionally\n * injecting the I/O of the waker queue, every time a new\n * bfq_dispatch_request happens to be invoked while I/O is being\n * plugged for bfqq.  In addition to boosting throughput, this\n * unblocks bfqq's I/O, thereby improving bandwidth and latency for\n * bfqq. Note that these same results may be achieved with the general\n * injection mechanism, but less effectively. For details on this\n * aspect, see the comments on the choice of the queue for injection\n * in bfq_select_queue().\n *\n * Turning back to the detection of a waker queue, a queue Q is deemed\n * as a waker queue for bfqq if, for three consecutive times, bfqq\n * happens to become non empty right after a request of Q has been\n * completed. In this respect, even if bfqq is empty, we do not check\n * for a waker if it still has some in-flight I/O. In fact, in this\n * case bfqq is actually still being served by the drive, and may\n * receive new I/O on the completion of some of the in-flight\n * requests. In particular, on the first time, Q is tentatively set as\n * a candidate waker queue, while on the third consecutive time that Q\n * is detected, the field waker_bfqq is set to Q, to confirm that Q is\n * a waker queue for bfqq. These detection steps are performed only if\n * bfqq has a long think time, so as to make it more likely that\n * bfqq's I/O is actually being blocked by a synchronization. This\n * last filter, plus the above three-times requirement, make false\n * positives less likely.\n *\n * NOTE\n *\n * The sooner a waker queue is detected, the sooner throughput can be\n * boosted by injecting I/O from the waker queue. Fortunately,\n * detection is likely to be actually fast, for the following\n * reasons. While blocked by synchronization, bfqq has a long think\n * time. This implies that bfqq's inject limit is at least equal to 1\n * (see the comments in bfq_update_inject_limit()). So, thanks to\n * injection, the waker queue is likely to be served during the very\n * first I/O-plugging time interval for bfqq. This triggers the first\n * step of the detection mechanism. Thanks again to injection, the\n * candidate waker queue is then likely to be confirmed no later than\n * during the next I/O-plugging interval for bfqq.\n *\n * ISSUE\n *\n * On queue merging all waker information is lost.\n */\nstatic void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    u64 now_ns)\n{\n\tif (!bfqd->last_completed_rq_bfqq ||\n\t    bfqd->last_completed_rq_bfqq == bfqq ||\n\t    bfq_bfqq_has_short_ttime(bfqq) ||\n\t    now_ns - bfqd->last_completion >= 4 * NSEC_PER_MSEC)\n\t\treturn;\n\n\tif (bfqd->last_completed_rq_bfqq !=\n\t    bfqq->tentative_waker_bfqq) {\n\t\t/*\n\t\t * First synchronization detected with a\n\t\t * candidate waker queue, or with a different\n\t\t * candidate waker queue from the current one.\n\t\t */\n\t\tbfqq->tentative_waker_bfqq =\n\t\t\tbfqd->last_completed_rq_bfqq;\n\t\tbfqq->num_waker_detections = 1;\n\t} else /* Same tentative waker queue detected again */\n\t\tbfqq->num_waker_detections++;\n\n\tif (bfqq->num_waker_detections == 3) {\n\t\tbfqq->waker_bfqq = bfqd->last_completed_rq_bfqq;\n\t\tbfqq->tentative_waker_bfqq = NULL;\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * bfqq->waker_bfqq must be reset. To\n\t\t * this goal, we maintain in each\n\t\t * waker queue a list, woken_list, of\n\t\t * all the queues that reference the\n\t\t * waker queue through their\n\t\t * waker_bfqq pointer. When the waker\n\t\t * queue exits, the waker_bfqq pointer\n\t\t * of all the queues in the woken_list\n\t\t * is reset.\n\t\t *\n\t\t * In addition, if bfqq is already in\n\t\t * the woken_list of a waker queue,\n\t\t * then, before being inserted into\n\t\t * the woken_list of a new waker\n\t\t * queue, bfqq must be removed from\n\t\t * the woken_list of the old waker\n\t\t * queue.\n\t\t */\n\t\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\t\thlist_del_init(&bfqq->woken_list_node);\n\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t       &bfqd->last_completed_rq_bfqq->woken_list);\n\t}\n}\n\nstatic void bfq_add_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct request *next_rq, *prev;\n\tunsigned int old_wr_coeff = bfqq->wr_coeff;\n\tbool interactive = false;\n\tu64 now_ns = ktime_get_ns();\n\n\tbfq_log_bfqq(bfqd, bfqq, \"add_request %d\", rq_is_sync(rq));\n\tbfqq->queued[rq_is_sync(rq)]++;\n\tbfqd->queued++;\n\n\tif (bfq_bfqq_sync(bfqq) && RQ_BIC(rq)->requests <= 1) {\n\t\tbfq_check_waker(bfqd, bfqq, now_ns);\n\n\t\t/*\n\t\t * Periodically reset inject limit, to make sure that\n\t\t * the latter eventually drops in case workload\n\t\t * changes, see step (3) in the comments on\n\t\t * bfq_update_inject_limit().\n\t\t */\n\t\tif (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t     msecs_to_jiffies(1000)))\n\t\t\tbfq_reset_inject_limit(bfqd, bfqq);\n\n\t\t/*\n\t\t * The following conditions must hold to setup a new\n\t\t * sampling of total service time, and then a new\n\t\t * update of the inject limit:\n\t\t * - bfqq is in service, because the total service\n\t\t *   time is evaluated only for the I/O requests of\n\t\t *   the queues in service;\n\t\t * - this is the right occasion to compute or to\n\t\t *   lower the baseline total service time, because\n\t\t *   there are actually no requests in the drive,\n\t\t *   or\n\t\t *   the baseline total service time is available, and\n\t\t *   this is the right occasion to compute the other\n\t\t *   quantity needed to update the inject limit, i.e.,\n\t\t *   the total service time caused by the amount of\n\t\t *   injection allowed by the current value of the\n\t\t *   limit. It is the right occasion because injection\n\t\t *   has actually been performed during the service\n\t\t *   hole, and there are still in-flight requests,\n\t\t *   which are very likely to be exactly the injected\n\t\t *   requests, or part of them;\n\t\t * - the minimum interval for sampling the total\n\t\t *   service time and updating the inject limit has\n\t\t *   elapsed.\n\t\t */\n\t\tif (bfqq == bfqd->in_service_queue &&\n\t\t    (bfqd->rq_in_driver == 0 ||\n\t\t     (bfqq->last_serv_time_ns > 0 &&\n\t\t      bfqd->rqs_injected && bfqd->rq_in_driver > 0)) &&\n\t\t    time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t      msecs_to_jiffies(10))) {\n\t\t\tbfqd->last_empty_occupied_ns = ktime_get_ns();\n\t\t\t/*\n\t\t\t * Start the state machine for measuring the\n\t\t\t * total service time of rq: setting\n\t\t\t * wait_dispatch will cause bfqd->waited_rq to\n\t\t\t * be set when rq will be dispatched.\n\t\t\t */\n\t\t\tbfqd->wait_dispatch = true;\n\t\t\t/*\n\t\t\t * If there is no I/O in service in the drive,\n\t\t\t * then possible injection occurred before the\n\t\t\t * arrival of rq will not affect the total\n\t\t\t * service time of rq. So the injection limit\n\t\t\t * must not be updated as a function of such\n\t\t\t * total service time, unless new injection\n\t\t\t * occurs before rq is completed. To have the\n\t\t\t * injection limit updated only in the latter\n\t\t\t * case, reset rqs_injected here (rqs_injected\n\t\t\t * will be set in case injection is performed\n\t\t\t * on bfqq before rq is completed).\n\t\t\t */\n\t\t\tif (bfqd->rq_in_driver == 0)\n\t\t\t\tbfqd->rqs_injected = false;\n\t\t}\n\t}\n\n\tif (bfq_bfqq_sync(bfqq))\n\t\tbfq_update_io_intensity(bfqq, now_ns);\n\n\telv_rb_add(&bfqq->sort_list, rq);\n\n\t/*\n\t * Check if this request is a better next-serve candidate.\n\t */\n\tprev = bfqq->next_rq;\n\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, rq, bfqd->last_position);\n\tbfqq->next_rq = next_rq;\n\n\t/*\n\t * Adjust priority tree position, if next_rq changes.\n\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing && prev != bfqq->next_rq))\n\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\n\tif (!bfq_bfqq_busy(bfqq)) /* switching to busy ... */\n\t\tbfq_bfqq_handle_idle_busy_switch(bfqd, bfqq, old_wr_coeff,\n\t\t\t\t\t\t rq, &interactive);\n\telse {\n\t\tif (bfqd->low_latency && old_wr_coeff == 1 && !rq_is_sync(rq) &&\n\t\t    time_is_before_jiffies(\n\t\t\t\tbfqq->last_wr_start_finish +\n\t\t\t\tbfqd->bfq_wr_min_inter_arr_async)) {\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\n\t\t\tbfqd->wr_busy_queues++;\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t\tif (prev != bfqq->next_rq)\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\t/*\n\t * Assign jiffies to last_wr_start_finish in the following\n\t * cases:\n\t *\n\t * . if bfqq is not going to be weight-raised, because, for\n\t *   non weight-raised queues, last_wr_start_finish stores the\n\t *   arrival time of the last request; as of now, this piece\n\t *   of information is used only for deciding whether to\n\t *   weight-raise async queues\n\t *\n\t * . if bfqq is not weight-raised, because, if bfqq is now\n\t *   switching to weight-raised, then last_wr_start_finish\n\t *   stores the time when weight-raising starts\n\t *\n\t * . if bfqq is interactive, because, regardless of whether\n\t *   bfqq is currently weight-raised, the weight-raising\n\t *   period must start or restart (this case is considered\n\t *   separately because it is not detected by the above\n\t *   conditions, if bfqq is already weight-raised)\n\t *\n\t * last_wr_start_finish has to be updated also if bfqq is soft\n\t * real-time, because the weight-raising period is constantly\n\t * restarted on idle-to-busy transitions for these queues, but\n\t * this is already done in bfq_bfqq_handle_idle_busy_switch if\n\t * needed.\n\t */\n\tif (bfqd->low_latency &&\n\t\t(old_wr_coeff == 1 || bfqq->wr_coeff == 1 || interactive))\n\t\tbfqq->last_wr_start_finish = jiffies;\n}\n\nstatic struct request *bfq_find_rq_fmerge(struct bfq_data *bfqd,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq;\n\n\n\tif (bfqq)\n\t\treturn elv_rb_find(&bfqq->sort_list, bio_end_sector(bio));\n\n\treturn NULL;\n}\n\nstatic sector_t get_sdist(sector_t last_pos, struct request *rq)\n{\n\tif (last_pos)\n\t\treturn abs(blk_rq_pos(rq) - last_pos);\n\n\treturn 0;\n}\n\n#if 0 /* Still not clear if we can do without next two functions */\nstatic void bfq_activate_request(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\n\tbfqd->rq_in_driver++;\n}\n\nstatic void bfq_deactivate_request(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\n\tbfqd->rq_in_driver--;\n}\n#endif\n\nstatic void bfq_remove_request(struct request_queue *q,\n\t\t\t       struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tconst int sync = rq_is_sync(rq);\n\n\tif (bfqq->next_rq == rq) {\n\t\tbfqq->next_rq = bfq_find_next_rq(bfqd, bfqq, rq);\n\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\tif (rq->queuelist.prev != &rq->queuelist)\n\t\tlist_del_init(&rq->queuelist);\n\tbfqq->queued[sync]--;\n\tbfqd->queued--;\n\telv_rb_del(&bfqq->sort_list, rq);\n\n\telv_rqhash_del(q, rq);\n\tif (q->last_merge == rq)\n\t\tq->last_merge = NULL;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\tbfqq->next_rq = NULL;\n\n\t\tif (bfq_bfqq_busy(bfqq) && bfqq != bfqd->in_service_queue) {\n\t\t\tbfq_del_bfqq_busy(bfqd, bfqq, false);\n\t\t\t/*\n\t\t\t * bfqq emptied. In normal operation, when\n\t\t\t * bfqq is empty, bfqq->entity.service and\n\t\t\t * bfqq->entity.budget must contain,\n\t\t\t * respectively, the service received and the\n\t\t\t * budget used last time bfqq emptied. These\n\t\t\t * facts do not hold in this case, as at least\n\t\t\t * this last removal occurred while bfqq is\n\t\t\t * not in service. To avoid inconsistencies,\n\t\t\t * reset both bfqq->entity.service and\n\t\t\t * bfqq->entity.budget, if bfqq has still a\n\t\t\t * process that may issue I/O requests to it.\n\t\t\t */\n\t\t\tbfqq->entity.budget = bfqq->entity.service = 0;\n\t\t}\n\n\t\t/*\n\t\t * Remove queue from request-position tree as it is empty.\n\t\t */\n\t\tif (bfqq->pos_root) {\n\t\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\t\tbfqq->pos_root = NULL;\n\t\t}\n\t} else {\n\t\t/* see comments on bfq_pos_tree_add_move() for the unlikely() */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending--;\n\n}\n\nstatic bool bfq_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *free = NULL;\n\t/*\n\t * bfq_bic_lookup grabs the queue_lock: invoke it now and\n\t * store its return value for later use, to avoid nesting\n\t * queue_lock inside the bfqd->lock. We assume that the bic\n\t * returned by bfq_bic_lookup does not go away before\n\t * bfqd->lock is taken.\n\t */\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(bfqd, current->io_context, q);\n\tbool ret;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tif (bic) {\n\t\t/*\n\t\t * Make sure cgroup info is uptodate for current process before\n\t\t * considering the merge.\n\t\t */\n\t\tbfq_bic_update_cgroup(bic, bio);\n\n\t\tbfqd->bio_bfqq = bic_to_bfqq(bic, op_is_sync(bio->bi_opf));\n\t} else {\n\t\tbfqd->bio_bfqq = NULL;\n\t}\n\tbfqd->bio_bic = bic;\n\n\tret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);\n\n\tspin_unlock_irq(&bfqd->lock);\n\tif (free)\n\t\tblk_mq_free_request(free);\n\n\treturn ret;\n}\n\nstatic int bfq_request_merge(struct request_queue *q, struct request **req,\n\t\t\t     struct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *__rq;\n\n\t__rq = bfq_find_rq_fmerge(bfqd, bio, q);\n\tif (__rq && elv_bio_merge_ok(__rq, bio)) {\n\t\t*req = __rq;\n\n\t\tif (blk_discard_mergable(__rq))\n\t\t\treturn ELEVATOR_DISCARD_MERGE;\n\t\treturn ELEVATOR_FRONT_MERGE;\n\t}\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\nstatic void bfq_request_merged(struct request_queue *q, struct request *req,\n\t\t\t       enum elv_merge type)\n{\n\tif (type == ELEVATOR_FRONT_MERGE &&\n\t    rb_prev(&req->rb_node) &&\n\t    blk_rq_pos(req) <\n\t    blk_rq_pos(container_of(rb_prev(&req->rb_node),\n\t\t\t\t    struct request, rb_node))) {\n\t\tstruct bfq_queue *bfqq = RQ_BFQQ(req);\n\t\tstruct bfq_data *bfqd;\n\t\tstruct request *prev, *next_rq;\n\n\t\tif (!bfqq)\n\t\t\treturn;\n\n\t\tbfqd = bfqq->bfqd;\n\n\t\t/* Reposition request in its sort_list */\n\t\telv_rb_del(&bfqq->sort_list, req);\n\t\telv_rb_add(&bfqq->sort_list, req);\n\n\t\t/* Choose next request to be served for bfqq */\n\t\tprev = bfqq->next_rq;\n\t\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, req,\n\t\t\t\t\t bfqd->last_position);\n\t\tbfqq->next_rq = next_rq;\n\t\t/*\n\t\t * If next_rq changes, update both the queue's budget to\n\t\t * fit the new request and the queue's position in its\n\t\t * rq_pos_tree.\n\t\t */\n\t\tif (prev != bfqq->next_rq) {\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t\t\t/*\n\t\t\t * See comments on bfq_pos_tree_add_move() for\n\t\t\t * the unlikely().\n\t\t\t */\n\t\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t\t}\n\t}\n}\n\n/*\n * This function is called to notify the scheduler that the requests\n * rq and 'next' have been merged, with 'next' going away.  BFQ\n * exploits this hook to address the following issue: if 'next' has a\n * fifo_time lower that rq, then the fifo_time of rq must be set to\n * the value of 'next', to not forget the greater age of 'next'.\n *\n * NOTE: in this function we assume that rq is in a bfq_queue, basing\n * on that rq is picked from the hash table q->elevator->hash, which,\n * in its turn, is filled only with I/O requests present in\n * bfq_queues, while BFQ is in use for the request queue q. In fact,\n * the function that fills this hash table (elv_rqhash_add) is called\n * only by bfq_insert_request.\n */\nstatic void bfq_requests_merged(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*next_bfqq = RQ_BFQQ(next);\n\n\tif (!bfqq)\n\t\tgoto remove;\n\n\t/*\n\t * If next and rq belong to the same bfq_queue and next is older\n\t * than rq, then reposition rq in the fifo (by substituting next\n\t * with rq). Otherwise, if next and rq belong to different\n\t * bfq_queues, never reposition rq: in fact, we would have to\n\t * reposition it with respect to next's position in its own fifo,\n\t * which would most certainly be too expensive with respect to\n\t * the benefits.\n\t */\n\tif (bfqq == next_bfqq &&\n\t    !list_empty(&rq->queuelist) && !list_empty(&next->queuelist) &&\n\t    next->fifo_time < rq->fifo_time) {\n\t\tlist_del_init(&rq->queuelist);\n\t\tlist_replace_init(&next->queuelist, &rq->queuelist);\n\t\trq->fifo_time = next->fifo_time;\n\t}\n\n\tif (bfqq->next_rq == next)\n\t\tbfqq->next_rq = rq;\n\n\tbfqg_stats_update_io_merged(bfqq_group(bfqq), next->cmd_flags);\nremove:\n\t/* Merged request may be in the IO scheduler. Remove it. */\n\tif (!RB_EMPTY_NODE(&next->rb_node)) {\n\t\tbfq_remove_request(next->q, next);\n\t\tif (next_bfqq)\n\t\t\tbfqg_stats_update_io_remove(bfqq_group(next_bfqq),\n\t\t\t\t\t\t    next->cmd_flags);\n\t}\n}\n\n/* Must be called with bfqq != NULL */\nstatic void bfq_bfqq_end_wr(struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq has been enjoying interactive weight-raising, then\n\t * reset soft_rt_next_start. We do it for the following\n\t * reason. bfqq may have been conveying the I/O needed to load\n\t * a soft real-time application. Such an application actually\n\t * exhibits a soft real-time I/O pattern after it finishes\n\t * loading, and finally starts doing its job. But, if bfqq has\n\t * been receiving a lot of bandwidth so far (likely to happen\n\t * on a fast device), then soft_rt_next_start now contains a\n\t * high value that. So, without this reset, bfqq would be\n\t * prevented from being possibly considered as soft_rt for a\n\t * very long time.\n\t */\n\n\tif (bfqq->wr_cur_max_time !=\n\t    bfqq->bfqd->bfq_wr_rt_max_time)\n\t\tbfqq->soft_rt_next_start = jiffies;\n\n\tif (bfq_bfqq_busy(bfqq))\n\t\tbfqq->bfqd->wr_busy_queues--;\n\tbfqq->wr_coeff = 1;\n\tbfqq->wr_cur_max_time = 0;\n\tbfqq->last_wr_start_finish = jiffies;\n\t/*\n\t * Trigger a weight change on the next invocation of\n\t * __bfq_entity_update_weight_prio.\n\t */\n\tbfqq->entity.prio_changed = 1;\n}\n\nvoid bfq_end_wr_async_queues(struct bfq_data *bfqd,\n\t\t\t     struct bfq_group *bfqg)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 2; i++)\n\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\tif (bfqg->async_bfqq[i][j])\n\t\t\t\tbfq_bfqq_end_wr(bfqg->async_bfqq[i][j]);\n\tif (bfqg->async_idle_bfqq)\n\t\tbfq_bfqq_end_wr(bfqg->async_idle_bfqq);\n}\n\nstatic void bfq_end_wr(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tlist_for_each_entry(bfqq, &bfqd->active_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tlist_for_each_entry(bfqq, &bfqd->idle_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tbfq_end_wr_async(bfqd);\n\n\tspin_unlock_irq(&bfqd->lock);\n}\n\nstatic sector_t bfq_io_struct_pos(void *io_struct, bool request)\n{\n\tif (request)\n\t\treturn blk_rq_pos(io_struct);\n\telse\n\t\treturn ((struct bio *)io_struct)->bi_iter.bi_sector;\n}\n\nstatic int bfq_rq_close_to_sector(void *io_struct, bool request,\n\t\t\t\t  sector_t sector)\n{\n\treturn abs(bfq_io_struct_pos(io_struct, request) - sector) <=\n\t       BFQQ_CLOSE_THR;\n}\n\nstatic struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_queue *bfqq,\n\t\t\t\t\t sector_t sector)\n{\n\tstruct rb_root *root = &bfq_bfqq_to_bfqg(bfqq)->rq_pos_tree;\n\tstruct rb_node *parent, *node;\n\tstruct bfq_queue *__bfqq;\n\n\tif (RB_EMPTY_ROOT(root))\n\t\treturn NULL;\n\n\t/*\n\t * First, if we find a request starting at the end of the last\n\t * request, choose it.\n\t */\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, root, sector, &parent, NULL);\n\tif (__bfqq)\n\t\treturn __bfqq;\n\n\t/*\n\t * If the exact sector wasn't found, the parent of the NULL leaf\n\t * will contain the closest sector (rq_pos_tree sorted by\n\t * next_request position).\n\t */\n\t__bfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\tif (blk_rq_pos(__bfqq->next_rq) < sector)\n\t\tnode = rb_next(&__bfqq->pos_node);\n\telse\n\t\tnode = rb_prev(&__bfqq->pos_node);\n\tif (!node)\n\t\treturn NULL;\n\n\t__bfqq = rb_entry(node, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_find_close_cooperator(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_queue *cur_bfqq,\n\t\t\t\t\t\t   sector_t sector)\n{\n\tstruct bfq_queue *bfqq;\n\n\t/*\n\t * We shall notice if some of the queues are cooperating,\n\t * e.g., working closely on the same area of the device. In\n\t * that case, we can group them together and: 1) don't waste\n\t * time idling, and 2) serve the union of their requests in\n\t * the best possible order for throughput.\n\t */\n\tbfqq = bfqq_find_close(bfqd, cur_bfqq, sector);\n\tif (!bfqq || bfqq == cur_bfqq)\n\t\treturn NULL;\n\n\treturn bfqq;\n}\n\nstatic struct bfq_queue *\nbfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)\n{\n\tint process_refs, new_process_refs;\n\tstruct bfq_queue *__bfqq;\n\n\t/*\n\t * If there are no process references on the new_bfqq, then it is\n\t * unsafe to follow the ->new_bfqq chain as other bfqq's in the chain\n\t * may have dropped their last reference (not just their last process\n\t * reference).\n\t */\n\tif (!bfqq_process_refs(new_bfqq))\n\t\treturn NULL;\n\n\t/* Avoid a circular list and skip interim queue merges. */\n\twhile ((__bfqq = new_bfqq->new_bfqq)) {\n\t\tif (__bfqq == bfqq)\n\t\t\treturn NULL;\n\t\tnew_bfqq = __bfqq;\n\t}\n\n\tprocess_refs = bfqq_process_refs(bfqq);\n\tnew_process_refs = bfqq_process_refs(new_bfqq);\n\t/*\n\t * If the process for the bfqq has gone away, there is no\n\t * sense in merging the queues.\n\t */\n\tif (process_refs == 0 || new_process_refs == 0)\n\t\treturn NULL;\n\n\t/*\n\t * Make sure merged queues belong to the same parent. Parents could\n\t * have changed since the time we decided the two queues are suitable\n\t * for merging.\n\t */\n\tif (new_bfqq->entity.parent != bfqq->entity.parent)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"scheduling merge with queue %d\",\n\t\tnew_bfqq->pid);\n\n\t/*\n\t * Merging is just a redirection: the requests of the process\n\t * owning one of the two queues are redirected to the other queue.\n\t * The latter queue, in its turn, is set as shared if this is the\n\t * first time that the requests of some process are redirected to\n\t * it.\n\t *\n\t * We redirect bfqq to new_bfqq and not the opposite, because\n\t * we are in the context of the process owning bfqq, thus we\n\t * have the io_cq of this process. So we can immediately\n\t * configure this io_cq to redirect the requests of the\n\t * process to new_bfqq. In contrast, the io_cq of new_bfqq is\n\t * not available any more (new_bfqq->bic == NULL).\n\t *\n\t * Anyway, even in case new_bfqq coincides with the in-service\n\t * queue, redirecting requests the in-service queue is the\n\t * best option, as we feed the in-service queue with new\n\t * requests close to the last request served and, by doing so,\n\t * are likely to increase the throughput.\n\t */\n\tbfqq->new_bfqq = new_bfqq;\n\t/*\n\t * The above assignment schedules the following redirections:\n\t * each time some I/O for bfqq arrives, the process that\n\t * generated that I/O is disassociated from bfqq and\n\t * associated with new_bfqq. Here we increases new_bfqq->ref\n\t * in advance, adding the number of processes that are\n\t * expected to be associated with new_bfqq as they happen to\n\t * issue I/O.\n\t */\n\tnew_bfqq->ref += process_refs;\n\treturn new_bfqq;\n}\n\nstatic bool bfq_may_be_close_cooperator(struct bfq_queue *bfqq,\n\t\t\t\t\tstruct bfq_queue *new_bfqq)\n{\n\tif (bfq_too_late_for_merging(new_bfqq))\n\t\treturn false;\n\n\tif (bfq_class_idle(bfqq) || bfq_class_idle(new_bfqq) ||\n\t    (bfqq->ioprio_class != new_bfqq->ioprio_class))\n\t\treturn false;\n\n\t/*\n\t * If either of the queues has already been detected as seeky,\n\t * then merging it with the other queue is unlikely to lead to\n\t * sequential I/O.\n\t */\n\tif (BFQQ_SEEKY(bfqq) || BFQQ_SEEKY(new_bfqq))\n\t\treturn false;\n\n\t/*\n\t * Interleaved I/O is known to be done by (some) applications\n\t * only for reads, so it does not make sense to merge async\n\t * queues.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || !bfq_bfqq_sync(new_bfqq))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq);\n\n/*\n * Attempt to schedule a merge of bfqq with the currently in-service\n * queue or with a close queue among the scheduled queues.  Return\n * NULL if no merge was scheduled, a pointer to the shared bfq_queue\n * structure otherwise.\n *\n * The OOM queue is not allowed to participate to cooperation: in fact, since\n * the requests temporarily redirected to the OOM queue could be redirected\n * again to dedicated queues at any time, the state needed to correctly\n * handle merging with the OOM queue would be quite complex and expensive\n * to maintain. Besides, in such a critical condition as an out of memory,\n * the benefits of queue merging may be little relevant, or even negligible.\n *\n * WARNING: queue merging may impair fairness among non-weight raised\n * queues, for at least two reasons: 1) the original weight of a\n * merged queue may change during the merged state, 2) even being the\n * weight the same, a merged queue may be bloated with many more\n * requests than the ones produced by its originally-associated\n * process.\n */\nstatic struct bfq_queue *\nbfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t     void *io_struct, bool request, struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue *in_service_bfqq, *new_bfqq;\n\n\t/* if a merge has already been setup, then proceed with that first */\n\tnew_bfqq = bfqq->new_bfqq;\n\tif (new_bfqq) {\n\t\twhile (new_bfqq->new_bfqq)\n\t\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t\treturn new_bfqq;\n\t}\n\n\t/*\n\t * Check delayed stable merge for rotational or non-queueing\n\t * devs. For this branch to be executed, bfqq must not be\n\t * currently merged with some other queue (i.e., bfqq->bic\n\t * must be non null). If we considered also merged queues,\n\t * then we should also check whether bfqq has already been\n\t * merged with bic->stable_merge_bfqq. But this would be\n\t * costly and complicated.\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing)) {\n\t\t/*\n\t\t * Make sure also that bfqq is sync, because\n\t\t * bic->stable_merge_bfqq may point to some queue (for\n\t\t * stable merging) also if bic is associated with a\n\t\t * sync queue, but this bfqq is async\n\t\t */\n\t\tif (bfq_bfqq_sync(bfqq) && bic->stable_merge_bfqq &&\n\t\t    !bfq_bfqq_just_created(bfqq) &&\n\t\t    time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t  msecs_to_jiffies(bfq_late_stable_merging)) &&\n\t\t    time_is_before_jiffies(bfqq->creation_time +\n\t\t\t\t\t   msecs_to_jiffies(bfq_late_stable_merging))) {\n\t\t\tstruct bfq_queue *stable_merge_bfqq =\n\t\t\t\tbic->stable_merge_bfqq;\n\t\t\tint proc_ref = min(bfqq_process_refs(bfqq),\n\t\t\t\t\t   bfqq_process_refs(stable_merge_bfqq));\n\n\t\t\t/* deschedule stable merge, because done or aborted here */\n\t\t\tbfq_put_stable_ref(stable_merge_bfqq);\n\n\t\t\tbic->stable_merge_bfqq = NULL;\n\n\t\t\tif (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t    proc_ref > 0) {\n\t\t\t\t/* next function will take at least one ref */\n\t\t\t\tstruct bfq_queue *new_bfqq =\n\t\t\t\t\tbfq_setup_merge(bfqq, stable_merge_bfqq);\n\n\t\t\t\tif (new_bfqq) {\n\t\t\t\t\tbic->stably_merged = true;\n\t\t\t\t\tif (new_bfqq->bic)\n\t\t\t\t\t\tnew_bfqq->bic->stably_merged =\n\t\t\t\t\t\t\t\t\ttrue;\n\t\t\t\t}\n\t\t\t\treturn new_bfqq;\n\t\t\t} else\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\t/*\n\t * Do not perform queue merging if the device is non\n\t * rotational and performs internal queueing. In fact, such a\n\t * device reaches a high speed through internal parallelism\n\t * and pipelining. This means that, to reach a high\n\t * throughput, it must have many requests enqueued at the same\n\t * time. But, in this configuration, the internal scheduling\n\t * algorithm of the device does exactly the job of queue\n\t * merging: it reorders requests so as to obtain as much as\n\t * possible a sequential I/O pattern. As a consequence, with\n\t * the workload generated by processes doing interleaved I/O,\n\t * the throughput reached by the device is likely to be the\n\t * same, with and without queue merging.\n\t *\n\t * Disabling merging also provides a remarkable benefit in\n\t * terms of throughput. Merging tends to make many workloads\n\t * artificially more uneven, because of shared queues\n\t * remaining non empty for incomparably more time than\n\t * non-merged queues. This may accentuate workload\n\t * asymmetries. For example, if one of the queues in a set of\n\t * merged queues has a higher weight than a normal queue, then\n\t * the shared queue may inherit such a high weight and, by\n\t * staying almost always active, may force BFQ to perform I/O\n\t * plugging most of the time. This evidently makes it harder\n\t * for BFQ to let the device reach a high throughput.\n\t *\n\t * Finally, the likely() macro below is not used because one\n\t * of the two branches is more likely than the other, but to\n\t * have the code path after the following if() executed as\n\t * fast as possible for the case of a non rotational device\n\t * with queueing. We want it because this is the fastest kind\n\t * of device. On the opposite end, the likely() may lengthen\n\t * the execution time of BFQ for the case of slower devices\n\t * (rotational or at least without queueing). But in this case\n\t * the execution time of BFQ matters very little, if not at\n\t * all.\n\t */\n\tif (likely(bfqd->nonrot_with_queueing))\n\t\treturn NULL;\n\n\t/*\n\t * Prevent bfqq from being merged if it has been created too\n\t * long ago. The idea is that true cooperating processes, and\n\t * thus their associated bfq_queues, are supposed to be\n\t * created shortly after each other. This is the case, e.g.,\n\t * for KVM/QEMU and dump I/O threads. Basing on this\n\t * assumption, the following filtering greatly reduces the\n\t * probability that two non-cooperating processes, which just\n\t * happen to do close I/O for some short time interval, have\n\t * their queues merged by mistake.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn NULL;\n\n\tif (!io_struct || unlikely(bfqq == &bfqd->oom_bfqq))\n\t\treturn NULL;\n\n\t/* If there is only one backlogged queue, don't search. */\n\tif (bfq_tot_busy_queues(bfqd) == 1)\n\t\treturn NULL;\n\n\tin_service_bfqq = bfqd->in_service_queue;\n\n\tif (in_service_bfqq && in_service_bfqq != bfqq &&\n\t    likely(in_service_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_rq_close_to_sector(io_struct, request,\n\t\t\t\t   bfqd->in_serv_last_pos) &&\n\t    bfqq->entity.parent == in_service_bfqq->entity.parent &&\n\t    bfq_may_be_close_cooperator(bfqq, in_service_bfqq)) {\n\t\tnew_bfqq = bfq_setup_merge(bfqq, in_service_bfqq);\n\t\tif (new_bfqq)\n\t\t\treturn new_bfqq;\n\t}\n\t/*\n\t * Check whether there is a cooperator among currently scheduled\n\t * queues. The only thing we need is that the bio/request is not\n\t * NULL, as we need it to establish whether a cooperator exists.\n\t */\n\tnew_bfqq = bfq_find_close_cooperator(bfqd, bfqq,\n\t\t\tbfq_io_struct_pos(io_struct, request));\n\n\tif (new_bfqq && likely(new_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_may_be_close_cooperator(bfqq, new_bfqq))\n\t\treturn bfq_setup_merge(bfqq, new_bfqq);\n\n\treturn NULL;\n}\n\nstatic void bfq_bfqq_save_state(struct bfq_queue *bfqq)\n{\n\tstruct bfq_io_cq *bic = bfqq->bic;\n\n\t/*\n\t * If !bfqq->bic, the queue is already shared or its requests\n\t * have already been redirected to a shared queue; both idle window\n\t * and weight raising state have already been saved. Do nothing.\n\t */\n\tif (!bic)\n\t\treturn;\n\n\tbic->saved_last_serv_time_ns = bfqq->last_serv_time_ns;\n\tbic->saved_inject_limit = bfqq->inject_limit;\n\tbic->saved_decrease_time_jif = bfqq->decrease_time_jif;\n\n\tbic->saved_weight = bfqq->entity.orig_weight;\n\tbic->saved_ttime = bfqq->ttime;\n\tbic->saved_has_short_ttime = bfq_bfqq_has_short_ttime(bfqq);\n\tbic->saved_IO_bound = bfq_bfqq_IO_bound(bfqq);\n\tbic->saved_io_start_time = bfqq->io_start_time;\n\tbic->saved_tot_idle_time = bfqq->tot_idle_time;\n\tbic->saved_in_large_burst = bfq_bfqq_in_large_burst(bfqq);\n\tbic->was_in_burst_list = !hlist_unhashed(&bfqq->burst_list_node);\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t     bfqq->bfqd->low_latency)) {\n\t\t/*\n\t\t * bfqq being merged right after being created: bfqq\n\t\t * would have deserved interactive weight raising, but\n\t\t * did not make it to be set in a weight-raised state,\n\t\t * because of this early merge.\tStore directly the\n\t\t * weight-raising state that would have been assigned\n\t\t * to bfqq, so that to avoid that bfqq unjustly fails\n\t\t * to enjoy weight raising if split soon.\n\t\t */\n\t\tbic->saved_wr_coeff = bfqq->bfqd->bfq_wr_coeff;\n\t\tbic->saved_wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\t\tbic->saved_wr_cur_max_time = bfq_wr_duration(bfqq->bfqd);\n\t\tbic->saved_last_wr_start_finish = jiffies;\n\t} else {\n\t\tbic->saved_wr_coeff = bfqq->wr_coeff;\n\t\tbic->saved_wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tbic->saved_service_from_wr = bfqq->service_from_wr;\n\t\tbic->saved_last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tbic->saved_wr_cur_max_time = bfqq->wr_cur_max_time;\n\t}\n}\n\n\nstatic void\nbfq_reassign_last_bfqq(struct bfq_queue *cur_bfqq, struct bfq_queue *new_bfqq)\n{\n\tif (cur_bfqq->entity.parent &&\n\t    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->entity.parent->last_bfqq_created = new_bfqq;\n\telse if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->bfqd->last_bfqq_created = new_bfqq;\n}\n\nvoid bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * To prevent bfqq's service guarantees from being violated,\n\t * bfqq may be left busy, i.e., queued for service, even if\n\t * empty (see comments in __bfq_bfqq_expire() for\n\t * details). But, if no process will send requests to bfqq any\n\t * longer, then there is no point in keeping bfqq queued for\n\t * service. In addition, keeping bfqq queued for service, but\n\t * with no process ref any longer, may have caused bfqq to be\n\t * freed when dequeued from service. But this is assumed to\n\t * never happen.\n\t */\n\tif (bfq_bfqq_busy(bfqq) && RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq != bfqd->in_service_queue)\n\t\tbfq_del_bfqq_busy(bfqd, bfqq, false);\n\n\tbfq_reassign_last_bfqq(bfqq, NULL);\n\n\tbfq_put_queue(bfqq);\n}\n\nstatic struct bfq_queue *bfq_merge_bfqqs(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_io_cq *bic,\n\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"merging with queue %lu\",\n\t\t(unsigned long)new_bfqq->pid);\n\t/* Save weight raising and idle window of the merged queues */\n\tbfq_bfqq_save_state(bfqq);\n\tbfq_bfqq_save_state(new_bfqq);\n\tif (bfq_bfqq_IO_bound(bfqq))\n\t\tbfq_mark_bfqq_IO_bound(new_bfqq);\n\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * The processes associated with bfqq are cooperators of the\n\t * processes associated with new_bfqq. So, if bfqq has a\n\t * waker, then assume that all these processes will be happy\n\t * to let bfqq's waker freely inject I/O when they have no\n\t * I/O.\n\t */\n\tif (bfqq->waker_bfqq && !new_bfqq->waker_bfqq &&\n\t    bfqq->waker_bfqq != new_bfqq) {\n\t\tnew_bfqq->waker_bfqq = bfqq->waker_bfqq;\n\t\tnew_bfqq->tentative_waker_bfqq = NULL;\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * new_bfqq->waker_bfqq must be reset. So insert\n\t\t * new_bfqq into the woken_list of the waker. See\n\t\t * bfq_check_waker for details.\n\t\t */\n\t\thlist_add_head(&new_bfqq->woken_list_node,\n\t\t\t       &new_bfqq->waker_bfqq->woken_list);\n\n\t}\n\n\t/*\n\t * If bfqq is weight-raised, then let new_bfqq inherit\n\t * weight-raising. To reduce false positives, neglect the case\n\t * where bfqq has just been created, but has not yet made it\n\t * to be weight-raised (which may happen because EQM may merge\n\t * bfqq even before bfq_add_request is executed for the first\n\t * time for bfqq). Handling this case would however be very\n\t * easy, thanks to the flag just_created.\n\t */\n\tif (new_bfqq->wr_coeff == 1 && bfqq->wr_coeff > 1) {\n\t\tnew_bfqq->wr_coeff = bfqq->wr_coeff;\n\t\tnew_bfqq->wr_cur_max_time = bfqq->wr_cur_max_time;\n\t\tnew_bfqq->last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tnew_bfqq->wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tif (bfq_bfqq_busy(new_bfqq))\n\t\t\tbfqd->wr_busy_queues++;\n\t\tnew_bfqq->entity.prio_changed = 1;\n\t}\n\n\tif (bfqq->wr_coeff > 1) { /* bfqq has given its wr to new_bfqq */\n\t\tbfqq->wr_coeff = 1;\n\t\tbfqq->entity.prio_changed = 1;\n\t\tif (bfq_bfqq_busy(bfqq))\n\t\t\tbfqd->wr_busy_queues--;\n\t}\n\n\tbfq_log_bfqq(bfqd, new_bfqq, \"merge_bfqqs: wr_busy %d\",\n\t\t     bfqd->wr_busy_queues);\n\n\t/*\n\t * Merge queues (that is, let bic redirect its requests to new_bfqq)\n\t */\n\tbic_set_bfqq(bic, new_bfqq, true);\n\tbfq_mark_bfqq_coop(new_bfqq);\n\t/*\n\t * new_bfqq now belongs to at least two bics (it is a shared queue):\n\t * set new_bfqq->bic to NULL. bfqq either:\n\t * - does not belong to any bic any more, and hence bfqq->bic must\n\t *   be set to NULL, or\n\t * - is a queue whose owning bics have already been redirected to a\n\t *   different queue, hence the queue is destined to not belong to\n\t *   any bic soon and bfqq->bic is already NULL (therefore the next\n\t *   assignment causes no harm).\n\t */\n\tnew_bfqq->bic = NULL;\n\t/*\n\t * If the queue is shared, the pid is the pid of one of the associated\n\t * processes. Which pid depends on the exact sequence of merge events\n\t * the queue underwent. So printing such a pid is useless and confusing\n\t * because it reports a random pid between those of the associated\n\t * processes.\n\t * We mark such a queue with a pid -1, and then print SHARED instead of\n\t * a pid in logging messages.\n\t */\n\tnew_bfqq->pid = -1;\n\tbfqq->bic = NULL;\n\n\tbfq_reassign_last_bfqq(bfqq, new_bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n\n\treturn new_bfqq;\n}\n\nstatic bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tbool is_sync = op_is_sync(bio->bi_opf);\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq, *new_bfqq;\n\n\t/*\n\t * Disallow merge of a sync bio into an async request.\n\t */\n\tif (is_sync && !rq_is_sync(rq))\n\t\treturn false;\n\n\t/*\n\t * Lookup the bfqq that this bio will be queued with. Allow\n\t * merge only if rq is queued there.\n\t */\n\tif (!bfqq)\n\t\treturn false;\n\n\t/*\n\t * We take advantage of this function to perform an early merge\n\t * of the queues of possible cooperating processes.\n\t */\n\tnew_bfqq = bfq_setup_cooperator(bfqd, bfqq, bio, false, bfqd->bio_bic);\n\tif (new_bfqq) {\n\t\t/*\n\t\t * bic still points to bfqq, then it has not yet been\n\t\t * redirected to some other bfq_queue, and a queue\n\t\t * merge between bfqq and new_bfqq can be safely\n\t\t * fulfilled, i.e., bic can be redirected to new_bfqq\n\t\t * and bfqq can be put.\n\t\t */\n\t\twhile (bfqq != new_bfqq)\n\t\t\tbfqq = bfq_merge_bfqqs(bfqd, bfqd->bio_bic, bfqq);\n\n\t\t/*\n\t\t * Change also bqfd->bio_bfqq, as\n\t\t * bfqd->bio_bic now points to new_bfqq, and\n\t\t * this function may be invoked again (and then may\n\t\t * use again bqfd->bio_bfqq).\n\t\t */\n\t\tbfqd->bio_bfqq = bfqq;\n\t}\n\n\treturn bfqq == RQ_BFQQ(rq);\n}\n\n/*\n * Set the maximum time for the in-service queue to consume its\n * budget. This prevents seeky processes from lowering the throughput.\n * In practice, a time-slice service scheme is used with seeky\n * processes.\n */\nstatic void bfq_set_budget_timeout(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tunsigned int timeout_coeff;\n\n\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time)\n\t\ttimeout_coeff = 1;\n\telse\n\t\ttimeout_coeff = bfqq->entity.weight / bfqq->entity.orig_weight;\n\n\tbfqd->last_budget_start = ktime_get();\n\n\tbfqq->budget_timeout = jiffies +\n\t\tbfqd->bfq_timeout * timeout_coeff;\n}\n\nstatic void __bfq_set_in_service_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq)\n{\n\tif (bfqq) {\n\t\tbfq_clear_bfqq_fifo_expire(bfqq);\n\n\t\tbfqd->budgets_assigned = (bfqd->budgets_assigned * 7 + 256) / 8;\n\n\t\tif (time_is_before_jiffies(bfqq->last_wr_start_finish) &&\n\t\t    bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    time_is_before_jiffies(bfqq->budget_timeout)) {\n\t\t\t/*\n\t\t\t * For soft real-time queues, move the start\n\t\t\t * of the weight-raising period forward by the\n\t\t\t * time the queue has not received any\n\t\t\t * service. Otherwise, a relatively long\n\t\t\t * service delay is likely to cause the\n\t\t\t * weight-raising period of the queue to end,\n\t\t\t * because of the short duration of the\n\t\t\t * weight-raising period of a soft real-time\n\t\t\t * queue.  It is worth noting that this move\n\t\t\t * is not so dangerous for the other queues,\n\t\t\t * because soft real-time queues are not\n\t\t\t * greedy.\n\t\t\t *\n\t\t\t * To not add a further variable, we use the\n\t\t\t * overloaded field budget_timeout to\n\t\t\t * determine for how long the queue has not\n\t\t\t * received service, i.e., how much time has\n\t\t\t * elapsed since the queue expired. However,\n\t\t\t * this is a little imprecise, because\n\t\t\t * budget_timeout is set to jiffies if bfqq\n\t\t\t * not only expires, but also remains with no\n\t\t\t * request.\n\t\t\t */\n\t\t\tif (time_after(bfqq->budget_timeout,\n\t\t\t\t       bfqq->last_wr_start_finish))\n\t\t\t\tbfqq->last_wr_start_finish +=\n\t\t\t\t\tjiffies - bfqq->budget_timeout;\n\t\t\telse\n\t\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\n\t\tbfq_set_budget_timeout(bfqd, bfqq);\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t     \"set_in_service_queue, cur-budget = %d\",\n\t\t\t     bfqq->entity.budget);\n\t}\n\n\tbfqd->in_service_queue = bfqq;\n\tbfqd->in_serv_last_pos = 0;\n}\n\n/*\n * Get and set a new queue for service.\n */\nstatic struct bfq_queue *bfq_set_in_service_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfq_get_next_queue(bfqd);\n\n\t__bfq_set_in_service_queue(bfqd, bfqq);\n\treturn bfqq;\n}\n\nstatic void bfq_arm_slice_timer(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\tu32 sl;\n\n\tbfq_mark_bfqq_wait_request(bfqq);\n\n\t/*\n\t * We don't want to idle for seeks, but we do want to allow\n\t * fair distribution of slice time for a process doing back-to-back\n\t * seeks. So allow a little bit of time for him to submit a new rq.\n\t */\n\tsl = bfqd->bfq_slice_idle;\n\t/*\n\t * Unless the queue is being weight-raised or the scenario is\n\t * asymmetric, grant only minimum idle time if the queue\n\t * is seeky. A long idling is preserved for a weight-raised\n\t * queue, or, more in general, in an asymmetric scenario,\n\t * because a long idling is needed for guaranteeing to a queue\n\t * its reserved share of the throughput (in particular, it is\n\t * needed if the queue has a higher weight than some other\n\t * queue).\n\t */\n\tif (BFQQ_SEEKY(bfqq) && bfqq->wr_coeff == 1 &&\n\t    !bfq_asymmetric_scenario(bfqd, bfqq))\n\t\tsl = min_t(u64, sl, BFQ_MIN_TT);\n\telse if (bfqq->wr_coeff > 1)\n\t\tsl = max_t(u32, sl, 20ULL * NSEC_PER_MSEC);\n\n\tbfqd->last_idling_start = ktime_get();\n\tbfqd->last_idling_start_jiffies = jiffies;\n\n\thrtimer_start(&bfqd->idle_slice_timer, ns_to_ktime(sl),\n\t\t      HRTIMER_MODE_REL);\n\tbfqg_stats_set_start_idle_time(bfqq_group(bfqq));\n}\n\n/*\n * In autotuning mode, max_budget is dynamically recomputed as the\n * amount of sectors transferred in timeout at the estimated peak\n * rate. This enables BFQ to utilize a full timeslice with a full\n * budget, even if the in-service queue is served at peak rate. And\n * this maximises throughput with sequential workloads.\n */\nstatic unsigned long bfq_calc_max_budget(struct bfq_data *bfqd)\n{\n\treturn (u64)bfqd->peak_rate * USEC_PER_MSEC *\n\t\tjiffies_to_msecs(bfqd->bfq_timeout)>>BFQ_RATE_SHIFT;\n}\n\n/*\n * Update parameters related to throughput and responsiveness, as a\n * function of the estimated peak rate. See comments on\n * bfq_calc_max_budget(), and on the ref_wr_duration array.\n */\nstatic void update_thr_responsiveness_params(struct bfq_data *bfqd)\n{\n\tif (bfqd->bfq_user_max_budget == 0) {\n\t\tbfqd->bfq_max_budget =\n\t\t\tbfq_calc_max_budget(bfqd);\n\t\tbfq_log(bfqd, \"new max_budget = %d\", bfqd->bfq_max_budget);\n\t}\n}\n\nstatic void bfq_reset_rate_computation(struct bfq_data *bfqd,\n\t\t\t\t       struct request *rq)\n{\n\tif (rq != NULL) { /* new rq dispatch now, reset accordingly */\n\t\tbfqd->last_dispatch = bfqd->first_dispatch = ktime_get_ns();\n\t\tbfqd->peak_rate_samples = 1;\n\t\tbfqd->sequential_samples = 0;\n\t\tbfqd->tot_sectors_dispatched = bfqd->last_rq_max_size =\n\t\t\tblk_rq_sectors(rq);\n\t} else /* no new rq dispatched, just reset the number of samples */\n\t\tbfqd->peak_rate_samples = 0; /* full re-init on next disp. */\n\n\tbfq_log(bfqd,\n\t\t\"reset_rate_computation at end, sample %u/%u tot_sects %llu\",\n\t\tbfqd->peak_rate_samples, bfqd->sequential_samples,\n\t\tbfqd->tot_sectors_dispatched);\n}\n\nstatic void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)\n{\n\tu32 rate, weight, divisor;\n\n\t/*\n\t * For the convergence property to hold (see comments on\n\t * bfq_update_peak_rate()) and for the assessment to be\n\t * reliable, a minimum number of samples must be present, and\n\t * a minimum amount of time must have elapsed. If not so, do\n\t * not compute new rate. Just reset parameters, to get ready\n\t * for a new evaluation attempt.\n\t */\n\tif (bfqd->peak_rate_samples < BFQ_RATE_MIN_SAMPLES ||\n\t    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL)\n\t\tgoto reset_computation;\n\n\t/*\n\t * If a new request completion has occurred after last\n\t * dispatch, then, to approximate the rate at which requests\n\t * have been served by the device, it is more precise to\n\t * extend the observation interval to the last completion.\n\t */\n\tbfqd->delta_from_first =\n\t\tmax_t(u64, bfqd->delta_from_first,\n\t\t      bfqd->last_completion - bfqd->first_dispatch);\n\n\t/*\n\t * Rate computed in sects/usec, and not sects/nsec, for\n\t * precision issues.\n\t */\n\trate = div64_ul(bfqd->tot_sectors_dispatched<<BFQ_RATE_SHIFT,\n\t\t\tdiv_u64(bfqd->delta_from_first, NSEC_PER_USEC));\n\n\t/*\n\t * Peak rate not updated if:\n\t * - the percentage of sequential dispatches is below 3/4 of the\n\t *   total, and rate is below the current estimated peak rate\n\t * - rate is unreasonably high (> 20M sectors/sec)\n\t */\n\tif ((bfqd->sequential_samples < (3 * bfqd->peak_rate_samples)>>2 &&\n\t     rate <= bfqd->peak_rate) ||\n\t\trate > 20<<BFQ_RATE_SHIFT)\n\t\tgoto reset_computation;\n\n\t/*\n\t * We have to update the peak rate, at last! To this purpose,\n\t * we use a low-pass filter. We compute the smoothing constant\n\t * of the filter as a function of the 'weight' of the new\n\t * measured rate.\n\t *\n\t * As can be seen in next formulas, we define this weight as a\n\t * quantity proportional to how sequential the workload is,\n\t * and to how long the observation time interval is.\n\t *\n\t * The weight runs from 0 to 8. The maximum value of the\n\t * weight, 8, yields the minimum value for the smoothing\n\t * constant. At this minimum value for the smoothing constant,\n\t * the measured rate contributes for half of the next value of\n\t * the estimated peak rate.\n\t *\n\t * So, the first step is to compute the weight as a function\n\t * of how sequential the workload is. Note that the weight\n\t * cannot reach 9, because bfqd->sequential_samples cannot\n\t * become equal to bfqd->peak_rate_samples, which, in its\n\t * turn, holds true because bfqd->sequential_samples is not\n\t * incremented for the first sample.\n\t */\n\tweight = (9 * bfqd->sequential_samples) / bfqd->peak_rate_samples;\n\n\t/*\n\t * Second step: further refine the weight as a function of the\n\t * duration of the observation interval.\n\t */\n\tweight = min_t(u32, 8,\n\t\t       div_u64(weight * bfqd->delta_from_first,\n\t\t\t       BFQ_RATE_REF_INTERVAL));\n\n\t/*\n\t * Divisor ranging from 10, for minimum weight, to 2, for\n\t * maximum weight.\n\t */\n\tdivisor = 10 - weight;\n\n\t/*\n\t * Finally, update peak rate:\n\t *\n\t * peak_rate = peak_rate * (divisor-1) / divisor  +  rate / divisor\n\t */\n\tbfqd->peak_rate *= divisor-1;\n\tbfqd->peak_rate /= divisor;\n\trate /= divisor; /* smoothing constant alpha = 1/divisor */\n\n\tbfqd->peak_rate += rate;\n\n\t/*\n\t * For a very slow device, bfqd->peak_rate can reach 0 (see\n\t * the minimum representable values reported in the comments\n\t * on BFQ_RATE_SHIFT). Push to 1 if this happens, to avoid\n\t * divisions by zero where bfqd->peak_rate is used as a\n\t * divisor.\n\t */\n\tbfqd->peak_rate = max_t(u32, 1, bfqd->peak_rate);\n\n\tupdate_thr_responsiveness_params(bfqd);\n\nreset_computation:\n\tbfq_reset_rate_computation(bfqd, rq);\n}\n\n/*\n * Update the read/write peak rate (the main quantity used for\n * auto-tuning, see update_thr_responsiveness_params()).\n *\n * It is not trivial to estimate the peak rate (correctly): because of\n * the presence of sw and hw queues between the scheduler and the\n * device components that finally serve I/O requests, it is hard to\n * say exactly when a given dispatched request is served inside the\n * device, and for how long. As a consequence, it is hard to know\n * precisely at what rate a given set of requests is actually served\n * by the device.\n *\n * On the opposite end, the dispatch time of any request is trivially\n * available, and, from this piece of information, the \"dispatch rate\"\n * of requests can be immediately computed. So, the idea in the next\n * function is to use what is known, namely request dispatch times\n * (plus, when useful, request completion times), to estimate what is\n * unknown, namely in-device request service rate.\n *\n * The main issue is that, because of the above facts, the rate at\n * which a certain set of requests is dispatched over a certain time\n * interval can vary greatly with respect to the rate at which the\n * same requests are then served. But, since the size of any\n * intermediate queue is limited, and the service scheme is lossless\n * (no request is silently dropped), the following obvious convergence\n * property holds: the number of requests dispatched MUST become\n * closer and closer to the number of requests completed as the\n * observation interval grows. This is the key property used in\n * the next function to estimate the peak service rate as a function\n * of the observed dispatch rate. The function assumes to be invoked\n * on every request dispatch.\n */\nstatic void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tif (bfqd->peak_rate_samples == 0) { /* first dispatch */\n\t\tbfq_log(bfqd, \"update_peak_rate: goto reset, samples %d\",\n\t\t\tbfqd->peak_rate_samples);\n\t\tbfq_reset_rate_computation(bfqd, rq);\n\t\tgoto update_last_values; /* will add one sample */\n\t}\n\n\t/*\n\t * Device idle for very long: the observation interval lasting\n\t * up to this dispatch cannot be a valid observation interval\n\t * for computing a new peak rate (similarly to the late-\n\t * completion event in bfq_completed_request()). Go to\n\t * update_rate_and_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - start a new observation interval with this dispatch\n\t */\n\tif (now_ns - bfqd->last_dispatch > 100*NSEC_PER_MSEC &&\n\t    bfqd->rq_in_driver == 0)\n\t\tgoto update_rate_and_reset;\n\n\t/* Update sampling information */\n\tbfqd->peak_rate_samples++;\n\n\tif ((bfqd->rq_in_driver > 0 ||\n\t\tnow_ns - bfqd->last_completion < BFQ_MIN_TT)\n\t    && !BFQ_RQ_SEEKY(bfqd, bfqd->last_position, rq))\n\t\tbfqd->sequential_samples++;\n\n\tbfqd->tot_sectors_dispatched += blk_rq_sectors(rq);\n\n\t/* Reset max observed rq size every 32 dispatches */\n\tif (likely(bfqd->peak_rate_samples % 32))\n\t\tbfqd->last_rq_max_size =\n\t\t\tmax_t(u32, blk_rq_sectors(rq), bfqd->last_rq_max_size);\n\telse\n\t\tbfqd->last_rq_max_size = blk_rq_sectors(rq);\n\n\tbfqd->delta_from_first = now_ns - bfqd->first_dispatch;\n\n\t/* Target observation interval not yet reached, go on sampling */\n\tif (bfqd->delta_from_first < BFQ_RATE_REF_INTERVAL)\n\t\tgoto update_last_values;\n\nupdate_rate_and_reset:\n\tbfq_update_rate_reset(bfqd, rq);\nupdate_last_values:\n\tbfqd->last_position = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\tif (RQ_BFQQ(rq) == bfqd->in_service_queue)\n\t\tbfqd->in_serv_last_pos = bfqd->last_position;\n\tbfqd->last_dispatch = now_ns;\n}\n\n/*\n * Remove request from internal lists.\n */\nstatic void bfq_dispatch_remove(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\n\t/*\n\t * For consistency, the next instruction should have been\n\t * executed after removing the request from the queue and\n\t * dispatching it.  We execute instead this instruction before\n\t * bfq_remove_request() (and hence introduce a temporary\n\t * inconsistency), for efficiency.  In fact, should this\n\t * dispatch occur for a non in-service bfqq, this anticipated\n\t * increment prevents two counters related to bfqq->dispatched\n\t * from risking to be, first, uselessly decremented, and then\n\t * incremented again when the (new) value of bfqq->dispatched\n\t * happens to be taken into account.\n\t */\n\tbfqq->dispatched++;\n\tbfq_update_peak_rate(q->elevator->elevator_data, rq);\n\n\tbfq_remove_request(q, rq);\n}\n\n/*\n * There is a case where idling does not have to be performed for\n * throughput concerns, but to preserve the throughput share of\n * the process associated with bfqq.\n *\n * To introduce this case, we can note that allowing the drive\n * to enqueue more than one request at a time, and hence\n * delegating de facto final scheduling decisions to the\n * drive's internal scheduler, entails loss of control on the\n * actual request service order. In particular, the critical\n * situation is when requests from different processes happen\n * to be present, at the same time, in the internal queue(s)\n * of the drive. In such a situation, the drive, by deciding\n * the service order of the internally-queued requests, does\n * determine also the actual throughput distribution among\n * these processes. But the drive typically has no notion or\n * concern about per-process throughput distribution, and\n * makes its decisions only on a per-request basis. Therefore,\n * the service distribution enforced by the drive's internal\n * scheduler is likely to coincide with the desired throughput\n * distribution only in a completely symmetric, or favorably\n * skewed scenario where:\n * (i-a) each of these processes must get the same throughput as\n *\t the others,\n * (i-b) in case (i-a) does not hold, it holds that the process\n *       associated with bfqq must receive a lower or equal\n *\t throughput than any of the other processes;\n * (ii)  the I/O of each process has the same properties, in\n *       terms of locality (sequential or random), direction\n *       (reads or writes), request sizes, greediness\n *       (from I/O-bound to sporadic), and so on;\n\n * In fact, in such a scenario, the drive tends to treat the requests\n * of each process in about the same way as the requests of the\n * others, and thus to provide each of these processes with about the\n * same throughput.  This is exactly the desired throughput\n * distribution if (i-a) holds, or, if (i-b) holds instead, this is an\n * even more convenient distribution for (the process associated with)\n * bfqq.\n *\n * In contrast, in any asymmetric or unfavorable scenario, device\n * idling (I/O-dispatch plugging) is certainly needed to guarantee\n * that bfqq receives its assigned fraction of the device throughput\n * (see [1] for details).\n *\n * The problem is that idling may significantly reduce throughput with\n * certain combinations of types of I/O and devices. An important\n * example is sync random I/O on flash storage with command\n * queueing. So, unless bfqq falls in cases where idling also boosts\n * throughput, it is important to check conditions (i-a), i(-b) and\n * (ii) accurately, so as to avoid idling when not strictly needed for\n * service guarantees.\n *\n * Unfortunately, it is extremely difficult to thoroughly check\n * condition (ii). And, in case there are active groups, it becomes\n * very difficult to check conditions (i-a) and (i-b) too.  In fact,\n * if there are active groups, then, for conditions (i-a) or (i-b) to\n * become false 'indirectly', it is enough that an active group\n * contains more active processes or sub-groups than some other active\n * group. More precisely, for conditions (i-a) or (i-b) to become\n * false because of such a group, it is not even necessary that the\n * group is (still) active: it is sufficient that, even if the group\n * has become inactive, some of its descendant processes still have\n * some request already dispatched but still waiting for\n * completion. In fact, requests have still to be guaranteed their\n * share of the throughput even after being dispatched. In this\n * respect, it is easy to show that, if a group frequently becomes\n * inactive while still having in-flight requests, and if, when this\n * happens, the group is not considered in the calculation of whether\n * the scenario is asymmetric, then the group may fail to be\n * guaranteed its fair share of the throughput (basically because\n * idling may not be performed for the descendant processes of the\n * group, but it had to be).  We address this issue with the following\n * bi-modal behavior, implemented in the function\n * bfq_asymmetric_scenario().\n *\n * If there are groups with requests waiting for completion\n * (as commented above, some of these groups may even be\n * already inactive), then the scenario is tagged as\n * asymmetric, conservatively, without checking any of the\n * conditions (i-a), (i-b) or (ii). So the device is idled for bfqq.\n * This behavior matches also the fact that groups are created\n * exactly if controlling I/O is a primary concern (to\n * preserve bandwidth and latency guarantees).\n *\n * On the opposite end, if there are no groups with requests waiting\n * for completion, then only conditions (i-a) and (i-b) are actually\n * controlled, i.e., provided that conditions (i-a) or (i-b) holds,\n * idling is not performed, regardless of whether condition (ii)\n * holds.  In other words, only if conditions (i-a) and (i-b) do not\n * hold, then idling is allowed, and the device tends to be prevented\n * from queueing many requests, possibly of several processes. Since\n * there are no groups with requests waiting for completion, then, to\n * control conditions (i-a) and (i-b) it is enough to check just\n * whether all the queues with requests waiting for completion also\n * have the same weight.\n *\n * Not checking condition (ii) evidently exposes bfqq to the\n * risk of getting less throughput than its fair share.\n * However, for queues with the same weight, a further\n * mechanism, preemption, mitigates or even eliminates this\n * problem. And it does so without consequences on overall\n * throughput. This mechanism and its benefits are explained\n * in the next three paragraphs.\n *\n * Even if a queue, say Q, is expired when it remains idle, Q\n * can still preempt the new in-service queue if the next\n * request of Q arrives soon (see the comments on\n * bfq_bfqq_update_budg_for_activation). If all queues and\n * groups have the same weight, this form of preemption,\n * combined with the hole-recovery heuristic described in the\n * comments on function bfq_bfqq_update_budg_for_activation,\n * are enough to preserve a correct bandwidth distribution in\n * the mid term, even without idling. In fact, even if not\n * idling allows the internal queues of the device to contain\n * many requests, and thus to reorder requests, we can rather\n * safely assume that the internal scheduler still preserves a\n * minimum of mid-term fairness.\n *\n * More precisely, this preemption-based, idleless approach\n * provides fairness in terms of IOPS, and not sectors per\n * second. This can be seen with a simple example. Suppose\n * that there are two queues with the same weight, but that\n * the first queue receives requests of 8 sectors, while the\n * second queue receives requests of 1024 sectors. In\n * addition, suppose that each of the two queues contains at\n * most one request at a time, which implies that each queue\n * always remains idle after it is served. Finally, after\n * remaining idle, each queue receives very quickly a new\n * request. It follows that the two queues are served\n * alternatively, preempting each other if needed. This\n * implies that, although both queues have the same weight,\n * the queue with large requests receives a service that is\n * 1024/8 times as high as the service received by the other\n * queue.\n *\n * The motivation for using preemption instead of idling (for\n * queues with the same weight) is that, by not idling,\n * service guarantees are preserved (completely or at least in\n * part) without minimally sacrificing throughput. And, if\n * there is no active group, then the primary expectation for\n * this device is probably a high throughput.\n *\n * We are now left only with explaining the two sub-conditions in the\n * additional compound condition that is checked below for deciding\n * whether the scenario is asymmetric. To explain the first\n * sub-condition, we need to add that the function\n * bfq_asymmetric_scenario checks the weights of only\n * non-weight-raised queues, for efficiency reasons (see comments on\n * bfq_weights_tree_add()). Then the fact that bfqq is weight-raised\n * is checked explicitly here. More precisely, the compound condition\n * below takes into account also the fact that, even if bfqq is being\n * weight-raised, the scenario is still symmetric if all queues with\n * requests waiting for completion happen to be\n * weight-raised. Actually, we should be even more precise here, and\n * differentiate between interactive weight raising and soft real-time\n * weight raising.\n *\n * The second sub-condition checked in the compound condition is\n * whether there is a fair amount of already in-flight I/O not\n * belonging to bfqq. If so, I/O dispatching is to be plugged, for the\n * following reason. The drive may decide to serve in-flight\n * non-bfqq's I/O requests before bfqq's ones, thereby delaying the\n * arrival of new I/O requests for bfqq (recall that bfqq is sync). If\n * I/O-dispatching is not plugged, then, while bfqq remains empty, a\n * basically uncontrolled amount of I/O from other queues may be\n * dispatched too, possibly causing the service of bfqq's I/O to be\n * delayed even longer in the drive. This problem gets more and more\n * serious as the speed and the queue depth of the drive grow,\n * because, as these two quantities grow, the probability to find no\n * queue busy but many requests in flight grows too. By contrast,\n * plugging I/O dispatching minimizes the delay induced by already\n * in-flight I/O, and enables bfqq to recover the bandwidth it may\n * lose because of this delay.\n *\n * As a side note, it is worth considering that the above\n * device-idling countermeasures may however fail in the following\n * unlucky scenario: if I/O-dispatch plugging is (correctly) disabled\n * in a time period during which all symmetry sub-conditions hold, and\n * therefore the device is allowed to enqueue many requests, but at\n * some later point in time some sub-condition stops to hold, then it\n * may become impossible to make requests be served in the desired\n * order until all the requests already queued in the device have been\n * served. The last sub-condition commented above somewhat mitigates\n * this problem for weight-raised queues.\n *\n * However, as an additional mitigation for this problem, we preserve\n * plugging for a special symmetric case that may suddenly turn into\n * asymmetric: the case where only bfqq is busy. In this case, not\n * expiring bfqq does not cause any harm to any other queues in terms\n * of service guarantees. In contrast, it avoids the following unlucky\n * sequence of events: (1) bfqq is expired, (2) a new queue with a\n * lower weight than bfqq becomes busy (or more queues), (3) the new\n * queue is served until a new request arrives for bfqq, (4) when bfqq\n * is finally served, there are so many requests of the new queue in\n * the drive that the pending requests for bfqq take a lot of time to\n * be served. In particular, event (2) may case even already\n * dispatched requests of bfqq to be delayed, inside the drive. So, to\n * avoid this series of events, the scenario is preventively declared\n * as asymmetric also if bfqq is the only busy queues\n */\nstatic bool idling_needed_for_service_guarantees(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tint tot_busy_queues = bfq_tot_busy_queues(bfqd);\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\treturn (bfqq->wr_coeff > 1 &&\n\t\t(bfqd->wr_busy_queues <\n\t\t tot_busy_queues ||\n\t\t bfqd->rq_in_driver >=\n\t\t bfqq->dispatched + 4)) ||\n\t\tbfq_asymmetric_scenario(bfqd, bfqq) ||\n\t\ttot_busy_queues == 1;\n}\n\nstatic bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t      enum bfqq_expiration reason)\n{\n\t/*\n\t * If this bfqq is shared between multiple processes, check\n\t * to make sure that those processes are still issuing I/Os\n\t * within the mean seek distance. If not, it may be time to\n\t * break the queues apart again.\n\t */\n\tif (bfq_bfqq_coop(bfqq) && BFQQ_SEEKY(bfqq))\n\t\tbfq_mark_bfqq_split_coop(bfqq);\n\n\t/*\n\t * Consider queues with a higher finish virtual time than\n\t * bfqq. If idling_needed_for_service_guarantees(bfqq) returns\n\t * true, then bfqq's bandwidth would be violated if an\n\t * uncontrolled amount of I/O from these queues were\n\t * dispatched while bfqq is waiting for its new I/O to\n\t * arrive. This is exactly what may happen if this is a forced\n\t * expiration caused by a preemption attempt, and if bfqq is\n\t * not re-scheduled. To prevent this from happening, re-queue\n\t * bfqq if it needs I/O-dispatch plugging, even if it is\n\t * empty. By doing so, bfqq is granted to be served before the\n\t * above queues (provided that bfqq is of course eligible).\n\t */\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    !(reason == BFQQE_PREEMPTED &&\n\t      idling_needed_for_service_guarantees(bfqd, bfqq))) {\n\t\tif (bfqq->dispatched == 0)\n\t\t\t/*\n\t\t\t * Overloading budget_timeout field to store\n\t\t\t * the time at which the queue remains with no\n\t\t\t * backlog and no outstanding request; used by\n\t\t\t * the weight-raising mechanism.\n\t\t\t */\n\t\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_busy(bfqd, bfqq, true);\n\t} else {\n\t\tbfq_requeue_bfqq(bfqd, bfqq, true);\n\t\t/*\n\t\t * Resort priority tree of potential close cooperators.\n\t\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t\t */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing &&\n\t\t\t     !RB_EMPTY_ROOT(&bfqq->sort_list)))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\t/*\n\t * All in-service entities must have been properly deactivated\n\t * or requeued before executing the next function, which\n\t * resets all in-service entities as no more in service. This\n\t * may cause bfqq to be freed. If this happens, the next\n\t * function returns true.\n\t */\n\treturn __bfq_bfqd_reset_in_service(bfqd);\n}\n\n/**\n * __bfq_bfqq_recalc_budget - try to adapt the budget to the @bfqq behavior.\n * @bfqd: device data.\n * @bfqq: queue to update.\n * @reason: reason for expiration.\n *\n * Handle the feedback on @bfqq budget at queue expiration.\n * See the body for detailed comments.\n */\nstatic void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,\n\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t     enum bfqq_expiration reason)\n{\n\tstruct request *next_rq;\n\tint budget, min_budget;\n\n\tmin_budget = bfq_min_budget(bfqd);\n\n\tif (bfqq->wr_coeff == 1)\n\t\tbudget = bfqq->max_budget;\n\telse /*\n\t      * Use a constant, low budget for weight-raised queues,\n\t      * to help achieve a low latency. Keep it slightly higher\n\t      * than the minimum possible budget, to cause a little\n\t      * bit fewer expirations.\n\t      */\n\t\tbudget = 2 * min_budget;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last budg %d, budg left %d\",\n\t\tbfqq->entity.budget, bfq_bfqq_budget_left(bfqq));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last max_budg %d, min budg %d\",\n\t\tbudget, bfq_min_budget(bfqd));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: sync %d, seeky %d\",\n\t\tbfq_bfqq_sync(bfqq), BFQQ_SEEKY(bfqd->in_service_queue));\n\n\tif (bfq_bfqq_sync(bfqq) && bfqq->wr_coeff == 1) {\n\t\tswitch (reason) {\n\t\t/*\n\t\t * Caveat: in all the following cases we trade latency\n\t\t * for throughput.\n\t\t */\n\t\tcase BFQQE_TOO_IDLE:\n\t\t\t/*\n\t\t\t * This is the only case where we may reduce\n\t\t\t * the budget: if there is no request of the\n\t\t\t * process still waiting for completion, then\n\t\t\t * we assume (tentatively) that the timer has\n\t\t\t * expired because the batch of requests of\n\t\t\t * the process could have been served with a\n\t\t\t * smaller budget.  Hence, betting that\n\t\t\t * process will behave in the same way when it\n\t\t\t * becomes backlogged again, we reduce its\n\t\t\t * next budget.  As long as we guess right,\n\t\t\t * this budget cut reduces the latency\n\t\t\t * experienced by the process.\n\t\t\t *\n\t\t\t * However, if there are still outstanding\n\t\t\t * requests, then the process may have not yet\n\t\t\t * issued its next request just because it is\n\t\t\t * still waiting for the completion of some of\n\t\t\t * the still outstanding ones.  So in this\n\t\t\t * subcase we do not reduce its budget, on the\n\t\t\t * contrary we increase it to possibly boost\n\t\t\t * the throughput, as discussed in the\n\t\t\t * comments to the BUDGET_TIMEOUT case.\n\t\t\t */\n\t\t\tif (bfqq->dispatched > 0) /* still outstanding reqs */\n\t\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\telse {\n\t\t\t\tif (budget > 5 * min_budget)\n\t\t\t\t\tbudget -= 4 * min_budget;\n\t\t\t\telse\n\t\t\t\t\tbudget = min_budget;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_TIMEOUT:\n\t\t\t/*\n\t\t\t * We double the budget here because it gives\n\t\t\t * the chance to boost the throughput if this\n\t\t\t * is not a seeky process (and has bumped into\n\t\t\t * this timeout because of, e.g., ZBR).\n\t\t\t */\n\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_EXHAUSTED:\n\t\t\t/*\n\t\t\t * The process still has backlog, and did not\n\t\t\t * let either the budget timeout or the disk\n\t\t\t * idling timeout expire. Hence it is not\n\t\t\t * seeky, has a short thinktime and may be\n\t\t\t * happy with a higher budget too. So\n\t\t\t * definitely increase the budget of this good\n\t\t\t * candidate to boost the disk throughput.\n\t\t\t */\n\t\t\tbudget = min(budget * 4, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_NO_MORE_REQUESTS:\n\t\t\t/*\n\t\t\t * For queues that expire for this reason, it\n\t\t\t * is particularly important to keep the\n\t\t\t * budget close to the actual service they\n\t\t\t * need. Doing so reduces the timestamp\n\t\t\t * misalignment problem described in the\n\t\t\t * comments in the body of\n\t\t\t * __bfq_activate_entity. In fact, suppose\n\t\t\t * that a queue systematically expires for\n\t\t\t * BFQQE_NO_MORE_REQUESTS and presents a\n\t\t\t * new request in time to enjoy timestamp\n\t\t\t * back-shifting. The larger the budget of the\n\t\t\t * queue is with respect to the service the\n\t\t\t * queue actually requests in each service\n\t\t\t * slot, the more times the queue can be\n\t\t\t * reactivated with the same virtual finish\n\t\t\t * time. It follows that, even if this finish\n\t\t\t * time is pushed to the system virtual time\n\t\t\t * to reduce the consequent timestamp\n\t\t\t * misalignment, the queue unjustly enjoys for\n\t\t\t * many re-activations a lower finish time\n\t\t\t * than all newly activated queues.\n\t\t\t *\n\t\t\t * The service needed by bfqq is measured\n\t\t\t * quite precisely by bfqq->entity.service.\n\t\t\t * Since bfqq does not enjoy device idling,\n\t\t\t * bfqq->entity.service is equal to the number\n\t\t\t * of sectors that the process associated with\n\t\t\t * bfqq requested to read/write before waiting\n\t\t\t * for request completions, or blocking for\n\t\t\t * other reasons.\n\t\t\t */\n\t\t\tbudget = max_t(int, bfqq->entity.service, min_budget);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else if (!bfq_bfqq_sync(bfqq)) {\n\t\t/*\n\t\t * Async queues get always the maximum possible\n\t\t * budget, as for them we do not care about latency\n\t\t * (in addition, their ability to dispatch is limited\n\t\t * by the charging factor).\n\t\t */\n\t\tbudget = bfqd->bfq_max_budget;\n\t}\n\n\tbfqq->max_budget = budget;\n\n\tif (bfqd->budgets_assigned >= bfq_stats_min_budgets &&\n\t    !bfqd->bfq_user_max_budget)\n\t\tbfqq->max_budget = min(bfqq->max_budget, bfqd->bfq_max_budget);\n\n\t/*\n\t * If there is still backlog, then assign a new budget, making\n\t * sure that it is large enough for the next request.  Since\n\t * the finish time of bfqq must be kept in sync with the\n\t * budget, be sure to call __bfq_bfqq_expire() *after* this\n\t * update.\n\t *\n\t * If there is no backlog, then no need to update the budget;\n\t * it will be updated on the arrival of a new request.\n\t */\n\tnext_rq = bfqq->next_rq;\n\tif (next_rq)\n\t\tbfqq->entity.budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t\t    bfq_serv_to_charge(next_rq, bfqq));\n\n\tbfq_log_bfqq(bfqd, bfqq, \"head sect: %u, new budget %d\",\n\t\t\tnext_rq ? blk_rq_sectors(next_rq) : 0,\n\t\t\tbfqq->entity.budget);\n}\n\n/*\n * Return true if the process associated with bfqq is \"slow\". The slow\n * flag is used, in addition to the budget timeout, to reduce the\n * amount of service provided to seeky processes, and thus reduce\n * their chances to lower the throughput. More details in the comments\n * on the function bfq_bfqq_expire().\n *\n * An important observation is in order: as discussed in the comments\n * on the function bfq_update_peak_rate(), with devices with internal\n * queues, it is hard if ever possible to know when and for how long\n * an I/O request is processed by the device (apart from the trivial\n * I/O pattern where a new request is dispatched only after the\n * previous one has been completed). This makes it hard to evaluate\n * the real rate at which the I/O requests of each bfq_queue are\n * served.  In fact, for an I/O scheduler like BFQ, serving a\n * bfq_queue means just dispatching its requests during its service\n * slot (i.e., until the budget of the queue is exhausted, or the\n * queue remains idle, or, finally, a timeout fires). But, during the\n * service slot of a bfq_queue, around 100 ms at most, the device may\n * be even still processing requests of bfq_queues served in previous\n * service slots. On the opposite end, the requests of the in-service\n * bfq_queue may be completed after the service slot of the queue\n * finishes.\n *\n * Anyway, unless more sophisticated solutions are used\n * (where possible), the sum of the sizes of the requests dispatched\n * during the service slot of a bfq_queue is probably the only\n * approximation available for the service received by the bfq_queue\n * during its service slot. And this sum is the quantity used in this\n * function to evaluate the I/O speed of a process.\n */\nstatic bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t\t bool compensate, enum bfqq_expiration reason,\n\t\t\t\t unsigned long *delta_ms)\n{\n\tktime_t delta_ktime;\n\tu32 delta_usecs;\n\tbool slow = BFQQ_SEEKY(bfqq); /* if delta too short, use seekyness */\n\n\tif (!bfq_bfqq_sync(bfqq))\n\t\treturn false;\n\n\tif (compensate)\n\t\tdelta_ktime = bfqd->last_idling_start;\n\telse\n\t\tdelta_ktime = ktime_get();\n\tdelta_ktime = ktime_sub(delta_ktime, bfqd->last_budget_start);\n\tdelta_usecs = ktime_to_us(delta_ktime);\n\n\t/* don't use too short time intervals */\n\tif (delta_usecs < 1000) {\n\t\tif (blk_queue_nonrot(bfqd->queue))\n\t\t\t /*\n\t\t\t  * give same worst-case guarantees as idling\n\t\t\t  * for seeky\n\t\t\t  */\n\t\t\t*delta_ms = BFQ_MIN_TT / NSEC_PER_MSEC;\n\t\telse /* charge at least one seek */\n\t\t\t*delta_ms = bfq_slice_idle / NSEC_PER_MSEC;\n\n\t\treturn slow;\n\t}\n\n\t*delta_ms = delta_usecs / USEC_PER_MSEC;\n\n\t/*\n\t * Use only long (> 20ms) intervals to filter out excessive\n\t * spikes in service rate estimation.\n\t */\n\tif (delta_usecs > 20000) {\n\t\t/*\n\t\t * Caveat for rotational devices: processes doing I/O\n\t\t * in the slower disk zones tend to be slow(er) even\n\t\t * if not seeky. In this respect, the estimated peak\n\t\t * rate is likely to be an average over the disk\n\t\t * surface. Accordingly, to not be too harsh with\n\t\t * unlucky processes, a process is deemed slow only if\n\t\t * its rate has been lower than half of the estimated\n\t\t * peak rate.\n\t\t */\n\t\tslow = bfqq->entity.service < bfqd->bfq_max_budget / 2;\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"bfq_bfqq_is_slow: slow %d\", slow);\n\n\treturn slow;\n}\n\n/*\n * To be deemed as soft real-time, an application must meet two\n * requirements. First, the application must not require an average\n * bandwidth higher than the approximate bandwidth required to playback or\n * record a compressed high-definition video.\n * The next function is invoked on the completion of the last request of a\n * batch, to compute the next-start time instant, soft_rt_next_start, such\n * that, if the next request of the application does not arrive before\n * soft_rt_next_start, then the above requirement on the bandwidth is met.\n *\n * The second requirement is that the request pattern of the application is\n * isochronous, i.e., that, after issuing a request or a batch of requests,\n * the application stops issuing new requests until all its pending requests\n * have been completed. After that, the application may issue a new batch,\n * and so on.\n * For this reason the next function is invoked to compute\n * soft_rt_next_start only for applications that meet this requirement,\n * whereas soft_rt_next_start is set to infinity for applications that do\n * not.\n *\n * Unfortunately, even a greedy (i.e., I/O-bound) application may\n * happen to meet, occasionally or systematically, both the above\n * bandwidth and isochrony requirements. This may happen at least in\n * the following circumstances. First, if the CPU load is high. The\n * application may stop issuing requests while the CPUs are busy\n * serving other processes, then restart, then stop again for a while,\n * and so on. The other circumstances are related to the storage\n * device: the storage device is highly loaded or reaches a low-enough\n * throughput with the I/O of the application (e.g., because the I/O\n * is random and/or the device is slow). In all these cases, the\n * I/O of the application may be simply slowed down enough to meet\n * the bandwidth and isochrony requirements. To reduce the probability\n * that greedy applications are deemed as soft real-time in these\n * corner cases, a further rule is used in the computation of\n * soft_rt_next_start: the return value of this function is forced to\n * be higher than the maximum between the following two quantities.\n *\n * (a) Current time plus: (1) the maximum time for which the arrival\n *     of a request is waited for when a sync queue becomes idle,\n *     namely bfqd->bfq_slice_idle, and (2) a few extra jiffies. We\n *     postpone for a moment the reason for adding a few extra\n *     jiffies; we get back to it after next item (b).  Lower-bounding\n *     the return value of this function with the current time plus\n *     bfqd->bfq_slice_idle tends to filter out greedy applications,\n *     because the latter issue their next request as soon as possible\n *     after the last one has been completed. In contrast, a soft\n *     real-time application spends some time processing data, after a\n *     batch of its requests has been completed.\n *\n * (b) Current value of bfqq->soft_rt_next_start. As pointed out\n *     above, greedy applications may happen to meet both the\n *     bandwidth and isochrony requirements under heavy CPU or\n *     storage-device load. In more detail, in these scenarios, these\n *     applications happen, only for limited time periods, to do I/O\n *     slowly enough to meet all the requirements described so far,\n *     including the filtering in above item (a). These slow-speed\n *     time intervals are usually interspersed between other time\n *     intervals during which these applications do I/O at a very high\n *     speed. Fortunately, exactly because of the high speed of the\n *     I/O in the high-speed intervals, the values returned by this\n *     function happen to be so high, near the end of any such\n *     high-speed interval, to be likely to fall *after* the end of\n *     the low-speed time interval that follows. These high values are\n *     stored in bfqq->soft_rt_next_start after each invocation of\n *     this function. As a consequence, if the last value of\n *     bfqq->soft_rt_next_start is constantly used to lower-bound the\n *     next value that this function may return, then, from the very\n *     beginning of a low-speed interval, bfqq->soft_rt_next_start is\n *     likely to be constantly kept so high that any I/O request\n *     issued during the low-speed interval is considered as arriving\n *     to soon for the application to be deemed as soft\n *     real-time. Then, in the high-speed interval that follows, the\n *     application will not be deemed as soft real-time, just because\n *     it will do I/O at a high speed. And so on.\n *\n * Getting back to the filtering in item (a), in the following two\n * cases this filtering might be easily passed by a greedy\n * application, if the reference quantity was just\n * bfqd->bfq_slice_idle:\n * 1) HZ is so low that the duration of a jiffy is comparable to or\n *    higher than bfqd->bfq_slice_idle. This happens, e.g., on slow\n *    devices with HZ=100. The time granularity may be so coarse\n *    that the approximation, in jiffies, of bfqd->bfq_slice_idle\n *    is rather lower than the exact value.\n * 2) jiffies, instead of increasing at a constant rate, may stop increasing\n *    for a while, then suddenly 'jump' by several units to recover the lost\n *    increments. This seems to happen, e.g., inside virtual machines.\n * To address this issue, in the filtering in (a) we do not use as a\n * reference time interval just bfqd->bfq_slice_idle, but\n * bfqd->bfq_slice_idle plus a few jiffies. In particular, we add the\n * minimum number of jiffies for which the filter seems to be quite\n * precise also in embedded systems and KVM/QEMU virtual machines.\n */\nstatic unsigned long bfq_bfqq_softrt_next_start(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn max3(bfqq->soft_rt_next_start,\n\t\t    bfqq->last_idle_bklogged +\n\t\t    HZ * bfqq->service_from_backlogged /\n\t\t    bfqd->bfq_wr_max_softrt_rate,\n\t\t    jiffies + nsecs_to_jiffies(bfqq->bfqd->bfq_slice_idle) + 4);\n}\n\n/**\n * bfq_bfqq_expire - expire a queue.\n * @bfqd: device owning the queue.\n * @bfqq: the queue to expire.\n * @compensate: if true, compensate for the time spent idling.\n * @reason: the reason causing the expiration.\n *\n * If the process associated with bfqq does slow I/O (e.g., because it\n * issues random requests), we charge bfqq with the time it has been\n * in service instead of the service it has received (see\n * bfq_bfqq_charge_time for details on how this goal is achieved). As\n * a consequence, bfqq will typically get higher timestamps upon\n * reactivation, and hence it will be rescheduled as if it had\n * received more service than what it has actually received. In the\n * end, bfqq receives less service in proportion to how slowly its\n * associated process consumes its budgets (and hence how seriously it\n * tends to lower the throughput). In addition, this time-charging\n * strategy guarantees time fairness among slow processes. In\n * contrast, if the process associated with bfqq is not slow, we\n * charge bfqq exactly with the service it has received.\n *\n * Charging time to the first type of queues and the exact service to\n * the other has the effect of using the WF2Q+ policy to schedule the\n * former on a timeslice basis, without violating service domain\n * guarantees among the latter.\n */\nvoid bfq_bfqq_expire(struct bfq_data *bfqd,\n\t\t     struct bfq_queue *bfqq,\n\t\t     bool compensate,\n\t\t     enum bfqq_expiration reason)\n{\n\tbool slow;\n\tunsigned long delta = 0;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * Check whether the process is slow (see bfq_bfqq_is_slow).\n\t */\n\tslow = bfq_bfqq_is_slow(bfqd, bfqq, compensate, reason, &delta);\n\n\t/*\n\t * As above explained, charge slow (typically seeky) and\n\t * timed-out queues with the time and not the service\n\t * received, to favor sequential workloads.\n\t *\n\t * Processes doing I/O in the slower disk zones will tend to\n\t * be slow(er) even if not seeky. Therefore, since the\n\t * estimated peak rate is actually an average over the disk\n\t * surface, these processes may timeout just for bad luck. To\n\t * avoid punishing them, do not charge time to processes that\n\t * succeeded in consuming at least 2/3 of their budget. This\n\t * allows BFQ to preserve enough elasticity to still perform\n\t * bandwidth, and not time, distribution with little unlucky\n\t * or quasi-sequential processes.\n\t */\n\tif (bfqq->wr_coeff == 1 &&\n\t    (slow ||\n\t     (reason == BFQQE_BUDGET_TIMEOUT &&\n\t      bfq_bfqq_budget_left(bfqq) >=  entity->budget / 3)))\n\t\tbfq_bfqq_charge_time(bfqd, bfqq, delta);\n\n\tif (bfqd->low_latency && bfqq->wr_coeff == 1)\n\t\tbfqq->last_wr_start_finish = jiffies;\n\n\tif (bfqd->low_latency && bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\t/*\n\t\t * If we get here, and there are no outstanding\n\t\t * requests, then the request pattern is isochronous\n\t\t * (see the comments on the function\n\t\t * bfq_bfqq_softrt_next_start()). Therefore we can\n\t\t * compute soft_rt_next_start.\n\t\t *\n\t\t * If, instead, the queue still has outstanding\n\t\t * requests, then we have to wait for the completion\n\t\t * of all the outstanding requests to discover whether\n\t\t * the request pattern is actually isochronous.\n\t\t */\n\t\tif (bfqq->dispatched == 0)\n\t\t\tbfqq->soft_rt_next_start =\n\t\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\t\telse if (bfqq->dispatched > 0) {\n\t\t\t/*\n\t\t\t * Schedule an update of soft_rt_next_start to when\n\t\t\t * the task may be discovered to be isochronous.\n\t\t\t */\n\t\t\tbfq_mark_bfqq_softrt_update(bfqq);\n\t\t}\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\"expire (%d, slow %d, num_disp %d, short_ttime %d)\", reason,\n\t\tslow, bfqq->dispatched, bfq_bfqq_has_short_ttime(bfqq));\n\n\t/*\n\t * bfqq expired, so no total service time needs to be computed\n\t * any longer: reset state machine for measuring total service\n\t * times.\n\t */\n\tbfqd->rqs_injected = bfqd->wait_dispatch = false;\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * Increase, decrease or leave budget unchanged according to\n\t * reason.\n\t */\n\t__bfq_bfqq_recalc_budget(bfqd, bfqq, reason);\n\tif (__bfq_bfqq_expire(bfqd, bfqq, reason))\n\t\t/* bfqq is gone, no more actions on it */\n\t\treturn;\n\n\t/* mark bfqq as waiting a request only if a bic still points to it */\n\tif (!bfq_bfqq_busy(bfqq) &&\n\t    reason != BFQQE_BUDGET_TIMEOUT &&\n\t    reason != BFQQE_BUDGET_EXHAUSTED) {\n\t\tbfq_mark_bfqq_non_blocking_wait_rq(bfqq);\n\t\t/*\n\t\t * Not setting service to 0, because, if the next rq\n\t\t * arrives in time, the queue will go on receiving\n\t\t * service with this same budget (as if it never expired)\n\t\t */\n\t} else\n\t\tentity->service = 0;\n\n\t/*\n\t * Reset the received-service counter for every parent entity.\n\t * Differently from what happens with bfqq->entity.service,\n\t * the resetting of this counter never needs to be postponed\n\t * for parent entities. In fact, in case bfqq may have a\n\t * chance to go on being served using the last, partially\n\t * consumed budget, bfqq->entity.service needs to be kept,\n\t * because if bfqq then actually goes on being served using\n\t * the same budget, the last value of bfqq->entity.service is\n\t * needed to properly decrement bfqq->entity.budget by the\n\t * portion already consumed. In contrast, it is not necessary\n\t * to keep entity->service for parent entities too, because\n\t * the bubble up of the new value of bfqq->entity.budget will\n\t * make sure that the budgets of parent entities are correct,\n\t * even in case bfqq and thus parent entities go on receiving\n\t * service with the same budget.\n\t */\n\tentity = entity->parent;\n\tfor_each_entity(entity)\n\t\tentity->service = 0;\n}\n\n/*\n * Budget timeout is not implemented through a dedicated timer, but\n * just checked on request arrivals and completions, as well as on\n * idle timer expirations.\n */\nstatic bool bfq_bfqq_budget_timeout(struct bfq_queue *bfqq)\n{\n\treturn time_is_before_eq_jiffies(bfqq->budget_timeout);\n}\n\n/*\n * If we expire a queue that is actively waiting (i.e., with the\n * device idled) for the arrival of a new request, then we may incur\n * the timestamp misalignment problem described in the body of the\n * function __bfq_activate_entity. Hence we return true only if this\n * condition does not hold, or if the queue is slow enough to deserve\n * only to be kicked off for preserving a high throughput.\n */\nstatic bool bfq_may_expire_for_budg_timeout(struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\"may_budget_timeout: wait_request %d left %d timeout %d\",\n\t\tbfq_bfqq_wait_request(bfqq),\n\t\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3,\n\t\tbfq_bfqq_budget_timeout(bfqq));\n\n\treturn (!bfq_bfqq_wait_request(bfqq) ||\n\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3)\n\t\t&&\n\t\tbfq_bfqq_budget_timeout(bfqq);\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq)\n{\n\tbool rot_without_queueing =\n\t\t!blk_queue_nonrot(bfqd->queue) && !bfqd->hw_tag,\n\t\tbfqq_sequential_and_IO_bound,\n\t\tidling_boosts_thr;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tbfqq_sequential_and_IO_bound = !BFQQ_SEEKY(bfqq) &&\n\t\tbfq_bfqq_IO_bound(bfqq) && bfq_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * The next variable takes into account the cases where idling\n\t * boosts the throughput.\n\t *\n\t * The value of the variable is computed considering, first, that\n\t * idling is virtually always beneficial for the throughput if:\n\t * (a) the device is not NCQ-capable and rotational, or\n\t * (b) regardless of the presence of NCQ, the device is rotational and\n\t *     the request pattern for bfqq is I/O-bound and sequential, or\n\t * (c) regardless of whether it is rotational, the device is\n\t *     not NCQ-capable and the request pattern for bfqq is\n\t *     I/O-bound and sequential.\n\t *\n\t * Secondly, and in contrast to the above item (b), idling an\n\t * NCQ-capable flash-based device would not boost the\n\t * throughput even with sequential I/O; rather it would lower\n\t * the throughput in proportion to how fast the device\n\t * is. Accordingly, the next variable is true if any of the\n\t * above conditions (a), (b) or (c) is true, and, in\n\t * particular, happens to be false if bfqd is an NCQ-capable\n\t * flash-based device.\n\t */\n\tidling_boosts_thr = rot_without_queueing ||\n\t\t((!blk_queue_nonrot(bfqd->queue) || !bfqd->hw_tag) &&\n\t\t bfqq_sequential_and_IO_bound);\n\n\t/*\n\t * The return value of this function is equal to that of\n\t * idling_boosts_thr, unless a special case holds. In this\n\t * special case, described below, idling may cause problems to\n\t * weight-raised queues.\n\t *\n\t * When the request pool is saturated (e.g., in the presence\n\t * of write hogs), if the processes associated with\n\t * non-weight-raised queues ask for requests at a lower rate,\n\t * then processes associated with weight-raised queues have a\n\t * higher probability to get a request from the pool\n\t * immediately (or at least soon) when they need one. Thus\n\t * they have a higher probability to actually get a fraction\n\t * of the device throughput proportional to their high\n\t * weight. This is especially true with NCQ-capable drives,\n\t * which enqueue several requests in advance, and further\n\t * reorder internally-queued requests.\n\t *\n\t * For this reason, we force to false the return value if\n\t * there are weight-raised busy queues. In this case, and if\n\t * bfqq is not weight-raised, this guarantees that the device\n\t * is not idled for bfqq (if, instead, bfqq is weight-raised,\n\t * then idling will be guaranteed by another variable, see\n\t * below). Combined with the timestamping rules of BFQ (see\n\t * [1] for details), this behavior causes bfqq, and hence any\n\t * sync non-weight-raised queue, to get a lower number of\n\t * requests served, and thus to ask for a lower number of\n\t * requests from the request pool, before the busy\n\t * weight-raised queues get served again. This often mitigates\n\t * starvation problems in the presence of heavy write\n\t * workloads and NCQ, thereby guaranteeing a higher\n\t * application and system responsiveness in these hostile\n\t * scenarios.\n\t */\n\treturn idling_boosts_thr &&\n\t\tbfqd->wr_busy_queues == 0;\n}\n\n/*\n * For a queue that becomes empty, device idling is allowed only if\n * this function returns true for that queue. As a consequence, since\n * device idling plays a critical role for both throughput boosting\n * and service guarantees, the return value of this function plays a\n * critical role as well.\n *\n * In a nutshell, this function returns true only if idling is\n * beneficial for throughput or, even if detrimental for throughput,\n * idling is however necessary to preserve service guarantees (low\n * latency, desired throughput distribution, ...). In particular, on\n * NCQ-capable devices, this function tries to return false, so as to\n * help keep the drives' internal queues full, whenever this helps the\n * device boost the throughput without causing any service-guarantee\n * issue.\n *\n * Most of the issues taken into account to get the return value of\n * this function are not trivial. We discuss these issues in the two\n * functions providing the main pieces of information needed by this\n * function.\n */\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tbool idling_boosts_thr_with_no_issue, idling_needed_for_service_guar;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tif (unlikely(bfqd->strict_guarantees))\n\t\treturn true;\n\n\t/*\n\t * Idling is performed only if slice_idle > 0. In addition, we\n\t * do not idle if\n\t * (a) bfqq is async\n\t * (b) bfqq is in the idle io prio class: in this case we do\n\t * not idle because we want to minimize the bandwidth that\n\t * queues in this class can steal to higher-priority queues\n\t */\n\tif (bfqd->bfq_slice_idle == 0 || !bfq_bfqq_sync(bfqq) ||\n\t   bfq_class_idle(bfqq))\n\t\treturn false;\n\n\tidling_boosts_thr_with_no_issue =\n\t\tidling_boosts_thr_without_issues(bfqd, bfqq);\n\n\tidling_needed_for_service_guar =\n\t\tidling_needed_for_service_guarantees(bfqd, bfqq);\n\n\t/*\n\t * We have now the two components we need to compute the\n\t * return value of the function, which is true only if idling\n\t * either boosts the throughput (without issues), or is\n\t * necessary to preserve service guarantees.\n\t */\n\treturn idling_boosts_thr_with_no_issue ||\n\t\tidling_needed_for_service_guar;\n}\n\n/*\n * If the in-service queue is empty but the function bfq_better_to_idle\n * returns true, then:\n * 1) the queue must remain in service and cannot be expired, and\n * 2) the device must be idled to wait for the possible arrival of a new\n *    request for the queue.\n * See the comments on the function bfq_better_to_idle for the reasons\n * why performing device idling is the best choice to boost the throughput\n * and preserve service guarantees when bfq_better_to_idle itself\n * returns true.\n */\nstatic bool bfq_bfqq_must_idle(struct bfq_queue *bfqq)\n{\n\treturn RB_EMPTY_ROOT(&bfqq->sort_list) && bfq_better_to_idle(bfqq);\n}\n\n/*\n * This function chooses the queue from which to pick the next extra\n * I/O request to inject, if it finds a compatible queue. See the\n * comments on bfq_update_inject_limit() for details on the injection\n * mechanism, and for the definitions of the quantities mentioned\n * below.\n */\nstatic struct bfq_queue *\nbfq_choose_bfqq_for_injection(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *in_serv_bfqq = bfqd->in_service_queue;\n\tunsigned int limit = in_serv_bfqq->inject_limit;\n\t/*\n\t * If\n\t * - bfqq is not weight-raised and therefore does not carry\n\t *   time-critical I/O,\n\t * or\n\t * - regardless of whether bfqq is weight-raised, bfqq has\n\t *   however a long think time, during which it can absorb the\n\t *   effect of an appropriate number of extra I/O requests\n\t *   from other queues (see bfq_update_inject_limit for\n\t *   details on the computation of this number);\n\t * then injection can be performed without restrictions.\n\t */\n\tbool in_serv_always_inject = in_serv_bfqq->wr_coeff == 1 ||\n\t\t!bfq_bfqq_has_short_ttime(in_serv_bfqq);\n\n\t/*\n\t * If\n\t * - the baseline total service time could not be sampled yet,\n\t *   so the inject limit happens to be still 0, and\n\t * - a lot of time has elapsed since the plugging of I/O\n\t *   dispatching started, so drive speed is being wasted\n\t *   significantly;\n\t * then temporarily raise inject limit to one request.\n\t */\n\tif (limit == 0 && in_serv_bfqq->last_serv_time_ns == 0 &&\n\t    bfq_bfqq_wait_request(in_serv_bfqq) &&\n\t    time_is_before_eq_jiffies(bfqd->last_idling_start_jiffies +\n\t\t\t\t      bfqd->bfq_slice_idle)\n\t\t)\n\t\tlimit = 1;\n\n\tif (bfqd->rq_in_driver >= limit)\n\t\treturn NULL;\n\n\t/*\n\t * Linear search of the source queue for injection; but, with\n\t * a high probability, very few steps are needed to find a\n\t * candidate queue, i.e., a queue with enough budget left for\n\t * its next request. In fact:\n\t * - BFQ dynamically updates the budget of every queue so as\n\t *   to accommodate the expected backlog of the queue;\n\t * - if a queue gets all its requests dispatched as injected\n\t *   service, then the queue is removed from the active list\n\t *   (and re-added only if it gets new requests, but then it\n\t *   is assigned again enough budget for its new backlog).\n\t */\n\tlist_for_each_entry(bfqq, &bfqd->active_list, bfqq_list)\n\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t    (in_serv_always_inject || bfqq->wr_coeff > 1) &&\n\t\t    bfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t    bfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Allow for only one large in-flight request\n\t\t\t * on non-rotational devices, for the\n\t\t\t * following reason. On non-rotationl drives,\n\t\t\t * large requests take much longer than\n\t\t\t * smaller requests to be served. In addition,\n\t\t\t * the drive prefers to serve large requests\n\t\t\t * w.r.t. to small ones, if it can choose. So,\n\t\t\t * having more than one large requests queued\n\t\t\t * in the drive may easily make the next first\n\t\t\t * request of the in-service queue wait for so\n\t\t\t * long to break bfqq's service guarantees. On\n\t\t\t * the bright side, large requests let the\n\t\t\t * drive reach a very high throughput, even if\n\t\t\t * there is only one in-flight large request\n\t\t\t * at a time.\n\t\t\t */\n\t\t\tif (blk_queue_nonrot(bfqd->queue) &&\n\t\t\t    blk_rq_sectors(bfqq->next_rq) >=\n\t\t\t    BFQQ_SECT_THR_NONROT)\n\t\t\t\tlimit = min_t(unsigned int, 1, limit);\n\t\t\telse\n\t\t\t\tlimit = in_serv_bfqq->inject_limit;\n\n\t\t\tif (bfqd->rq_in_driver < limit) {\n\t\t\t\tbfqd->rqs_injected = true;\n\t\t\t\treturn bfqq;\n\t\t\t}\n\t\t}\n\n\treturn NULL;\n}\n\n/*\n * Select a queue for service.  If we have a current queue in service,\n * check whether to continue servicing it, or retrieve and set a new one.\n */\nstatic struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\tstruct request *next_rq;\n\tenum bfqq_expiration reason = BFQQE_BUDGET_TIMEOUT;\n\n\tbfqq = bfqd->in_service_queue;\n\tif (!bfqq)\n\t\tgoto new_queue;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: already in-service queue\");\n\n\t/*\n\t * Do not expire bfqq for budget timeout if bfqq may be about\n\t * to enjoy device idling. The reason why, in this case, we\n\t * prevent bfqq from expiring is the same as in the comments\n\t * on the case where bfq_bfqq_must_idle() returns true, in\n\t * bfq_completed_request().\n\t */\n\tif (bfq_may_expire_for_budg_timeout(bfqq) &&\n\t    !bfq_bfqq_must_idle(bfqq))\n\t\tgoto expire;\n\ncheck_queue:\n\t/*\n\t * This loop is rarely executed more than once. Even when it\n\t * happens, it is much more convenient to re-execute this loop\n\t * than to return NULL and trigger a new dispatch to get a\n\t * request served.\n\t */\n\tnext_rq = bfqq->next_rq;\n\t/*\n\t * If bfqq has requests queued and it has enough budget left to\n\t * serve them, keep the queue, otherwise expire it.\n\t */\n\tif (next_rq) {\n\t\tif (bfq_serv_to_charge(next_rq, bfqq) >\n\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Expire the queue for budget exhaustion,\n\t\t\t * which makes sure that the next budget is\n\t\t\t * enough to serve the next request, even if\n\t\t\t * it comes from the fifo expired path.\n\t\t\t */\n\t\t\treason = BFQQE_BUDGET_EXHAUSTED;\n\t\t\tgoto expire;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The idle timer may be pending because we may\n\t\t\t * not disable disk idling even when a new request\n\t\t\t * arrives.\n\t\t\t */\n\t\t\tif (bfq_bfqq_wait_request(bfqq)) {\n\t\t\t\t/*\n\t\t\t\t * If we get here: 1) at least a new request\n\t\t\t\t * has arrived but we have not disabled the\n\t\t\t\t * timer because the request was too small,\n\t\t\t\t * 2) then the block layer has unplugged\n\t\t\t\t * the device, causing the dispatch to be\n\t\t\t\t * invoked.\n\t\t\t\t *\n\t\t\t\t * Since the device is unplugged, now the\n\t\t\t\t * requests are probably large enough to\n\t\t\t\t * provide a reasonable throughput.\n\t\t\t\t * So we disable idling.\n\t\t\t\t */\n\t\t\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\t\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\t\t\t}\n\t\t\tgoto keep_queue;\n\t\t}\n\t}\n\n\t/*\n\t * No requests pending. However, if the in-service queue is idling\n\t * for a new request, or has requests waiting for a completion and\n\t * may idle after their completion, then keep it anyway.\n\t *\n\t * Yet, inject service from other queues if it boosts\n\t * throughput and is possible.\n\t */\n\tif (bfq_bfqq_wait_request(bfqq) ||\n\t    (bfqq->dispatched != 0 && bfq_better_to_idle(bfqq))) {\n\t\tstruct bfq_queue *async_bfqq =\n\t\t\tbfqq->bic && bfqq->bic->bfqq[0] &&\n\t\t\tbfq_bfqq_busy(bfqq->bic->bfqq[0]) &&\n\t\t\tbfqq->bic->bfqq[0]->next_rq ?\n\t\t\tbfqq->bic->bfqq[0] : NULL;\n\t\tstruct bfq_queue *blocked_bfqq =\n\t\t\t!hlist_empty(&bfqq->woken_list) ?\n\t\t\tcontainer_of(bfqq->woken_list.first,\n\t\t\t\t     struct bfq_queue,\n\t\t\t\t     woken_list_node)\n\t\t\t: NULL;\n\n\t\t/*\n\t\t * The next four mutually-exclusive ifs decide\n\t\t * whether to try injection, and choose the queue to\n\t\t * pick an I/O request from.\n\t\t *\n\t\t * The first if checks whether the process associated\n\t\t * with bfqq has also async I/O pending. If so, it\n\t\t * injects such I/O unconditionally. Injecting async\n\t\t * I/O from the same process can cause no harm to the\n\t\t * process. On the contrary, it can only increase\n\t\t * bandwidth and reduce latency for the process.\n\t\t *\n\t\t * The second if checks whether there happens to be a\n\t\t * non-empty waker queue for bfqq, i.e., a queue whose\n\t\t * I/O needs to be completed for bfqq to receive new\n\t\t * I/O. This happens, e.g., if bfqq is associated with\n\t\t * a process that does some sync. A sync generates\n\t\t * extra blocking I/O, which must be completed before\n\t\t * the process associated with bfqq can go on with its\n\t\t * I/O. If the I/O of the waker queue is not served,\n\t\t * then bfqq remains empty, and no I/O is dispatched,\n\t\t * until the idle timeout fires for bfqq. This is\n\t\t * likely to result in lower bandwidth and higher\n\t\t * latencies for bfqq, and in a severe loss of total\n\t\t * throughput. The best action to take is therefore to\n\t\t * serve the waker queue as soon as possible. So do it\n\t\t * (without relying on the third alternative below for\n\t\t * eventually serving waker_bfqq's I/O; see the last\n\t\t * paragraph for further details). This systematic\n\t\t * injection of I/O from the waker queue does not\n\t\t * cause any delay to bfqq's I/O. On the contrary,\n\t\t * next bfqq's I/O is brought forward dramatically,\n\t\t * for it is not blocked for milliseconds.\n\t\t *\n\t\t * The third if checks whether there is a queue woken\n\t\t * by bfqq, and currently with pending I/O. Such a\n\t\t * woken queue does not steal bandwidth from bfqq,\n\t\t * because it remains soon without I/O if bfqq is not\n\t\t * served. So there is virtually no risk of loss of\n\t\t * bandwidth for bfqq if this woken queue has I/O\n\t\t * dispatched while bfqq is waiting for new I/O.\n\t\t *\n\t\t * The fourth if checks whether bfqq is a queue for\n\t\t * which it is better to avoid injection. It is so if\n\t\t * bfqq delivers more throughput when served without\n\t\t * any further I/O from other queues in the middle, or\n\t\t * if the service times of bfqq's I/O requests both\n\t\t * count more than overall throughput, and may be\n\t\t * easily increased by injection (this happens if bfqq\n\t\t * has a short think time). If none of these\n\t\t * conditions holds, then a candidate queue for\n\t\t * injection is looked for through\n\t\t * bfq_choose_bfqq_for_injection(). Note that the\n\t\t * latter may return NULL (for example if the inject\n\t\t * limit for bfqq is currently 0).\n\t\t *\n\t\t * NOTE: motivation for the second alternative\n\t\t *\n\t\t * Thanks to the way the inject limit is updated in\n\t\t * bfq_update_has_short_ttime(), it is rather likely\n\t\t * that, if I/O is being plugged for bfqq and the\n\t\t * waker queue has pending I/O requests that are\n\t\t * blocking bfqq's I/O, then the fourth alternative\n\t\t * above lets the waker queue get served before the\n\t\t * I/O-plugging timeout fires. So one may deem the\n\t\t * second alternative superfluous. It is not, because\n\t\t * the fourth alternative may be way less effective in\n\t\t * case of a synchronization. For two main\n\t\t * reasons. First, throughput may be low because the\n\t\t * inject limit may be too low to guarantee the same\n\t\t * amount of injected I/O, from the waker queue or\n\t\t * other queues, that the second alternative\n\t\t * guarantees (the second alternative unconditionally\n\t\t * injects a pending I/O request of the waker queue\n\t\t * for each bfq_dispatch_request()). Second, with the\n\t\t * fourth alternative, the duration of the plugging,\n\t\t * i.e., the time before bfqq finally receives new I/O,\n\t\t * may not be minimized, because the waker queue may\n\t\t * happen to be served only after other queues.\n\t\t */\n\t\tif (async_bfqq &&\n\t\t    icq_to_bic(async_bfqq->next_rq->elv.icq) == bfqq->bic &&\n\t\t    bfq_serv_to_charge(async_bfqq->next_rq, async_bfqq) <=\n\t\t    bfq_bfqq_budget_left(async_bfqq))\n\t\t\tbfqq = bfqq->bic->bfqq[0];\n\t\telse if (bfqq->waker_bfqq &&\n\t\t\t   bfq_bfqq_busy(bfqq->waker_bfqq) &&\n\t\t\t   bfqq->waker_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(bfqq->waker_bfqq->next_rq,\n\t\t\t\t\t      bfqq->waker_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(bfqq->waker_bfqq)\n\t\t\t)\n\t\t\tbfqq = bfqq->waker_bfqq;\n\t\telse if (blocked_bfqq &&\n\t\t\t   bfq_bfqq_busy(blocked_bfqq) &&\n\t\t\t   blocked_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(blocked_bfqq->next_rq,\n\t\t\t\t\t      blocked_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(blocked_bfqq)\n\t\t\t)\n\t\t\tbfqq = blocked_bfqq;\n\t\telse if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t (bfqq->wr_coeff == 1 || bfqd->wr_busy_queues > 1 ||\n\t\t\t  !bfq_bfqq_has_short_ttime(bfqq)))\n\t\t\tbfqq = bfq_choose_bfqq_for_injection(bfqd);\n\t\telse\n\t\t\tbfqq = NULL;\n\n\t\tgoto keep_queue;\n\t}\n\n\treason = BFQQE_NO_MORE_REQUESTS;\nexpire:\n\tbfq_bfqq_expire(bfqd, bfqq, false, reason);\nnew_queue:\n\tbfqq = bfq_set_in_service_queue(bfqd);\n\tif (bfqq) {\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: checking new queue\");\n\t\tgoto check_queue;\n\t}\nkeep_queue:\n\tif (bfqq)\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: returned this queue\");\n\telse\n\t\tbfq_log(bfqd, \"select_queue: no queue returned\");\n\n\treturn bfqq;\n}\n\nstatic void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tif (bfqq->wr_coeff > 1) { /* queue is being weight-raised */\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t\"raising period dur %u/%u msec, old coeff %u, w %d(%d)\",\n\t\t\tjiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),\n\t\t\tjiffies_to_msecs(bfqq->wr_cur_max_time),\n\t\t\tbfqq->wr_coeff,\n\t\t\tbfqq->entity.weight, bfqq->entity.orig_weight);\n\n\t\tif (entity->prio_changed)\n\t\t\tbfq_log_bfqq(bfqd, bfqq, \"WARN: pending prio change\");\n\n\t\t/*\n\t\t * If the queue was activated in a burst, or too much\n\t\t * time has elapsed from the beginning of this\n\t\t * weight-raising period, then end weight raising.\n\t\t */\n\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\telse if (time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t\t\tbfqq->wr_cur_max_time)) {\n\t\t\tif (bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time ||\n\t\t\ttime_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t       bfq_wr_duration(bfqd))) {\n\t\t\t\t/*\n\t\t\t\t * Either in interactive weight\n\t\t\t\t * raising, or in soft_rt weight\n\t\t\t\t * raising with the\n\t\t\t\t * interactive-weight-raising period\n\t\t\t\t * elapsed (so no switch back to\n\t\t\t\t * interactive weight raising).\n\t\t\t\t */\n\t\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t\t} else { /*\n\t\t\t\t  * soft_rt finishing while still in\n\t\t\t\t  * interactive period, switch back to\n\t\t\t\t  * interactive weight raising\n\t\t\t\t  */\n\t\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t\t}\n\t\t}\n\t\tif (bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time &&\n\t\t    bfqq->service_from_wr > max_service_from_wr) {\n\t\t\t/* see comments on max_service_from_wr */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t}\n\t}\n\t/*\n\t * To improve latency (for this or other queues), immediately\n\t * update weight both if it must be raised and if it must be\n\t * lowered. Since, entity may be on some active tree here, and\n\t * might have a pending change of its ioprio class, invoke\n\t * next function with the last parameter unset (see the\n\t * comments on the function).\n\t */\n\tif ((entity->weight > entity->orig_weight) != (bfqq->wr_coeff > 1))\n\t\t__bfq_entity_update_weight_prio(bfq_entity_service_tree(entity),\n\t\t\t\t\t\tentity, false);\n}\n\n/*\n * Dispatch next request from bfqq.\n */\nstatic struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct request *rq = bfqq->next_rq;\n\tunsigned long service_to_charge;\n\n\tservice_to_charge = bfq_serv_to_charge(rq, bfqq);\n\n\tbfq_bfqq_served(bfqq, service_to_charge);\n\n\tif (bfqq == bfqd->in_service_queue && bfqd->wait_dispatch) {\n\t\tbfqd->wait_dispatch = false;\n\t\tbfqd->waited_rq = rq;\n\t}\n\n\tbfq_dispatch_remove(bfqd->queue, rq);\n\n\tif (bfqq != bfqd->in_service_queue)\n\t\tgoto return_rq;\n\n\t/*\n\t * If weight raising has to terminate for bfqq, then next\n\t * function causes an immediate update of bfqq's weight,\n\t * without waiting for next activation. As a consequence, on\n\t * expiration, bfqq will be timestamped as if has never been\n\t * weight-raised during this service slot, even if it has\n\t * received part or even most of the service as a\n\t * weight-raised queue. This inflates bfqq's timestamps, which\n\t * is beneficial, as bfqq is then more willing to leave the\n\t * device immediately to possible other weight-raised queues.\n\t */\n\tbfq_update_wr_data(bfqd, bfqq);\n\n\t/*\n\t * Expire bfqq, pretending that its budget expired, if bfqq\n\t * belongs to CLASS_IDLE and other queues are waiting for\n\t * service.\n\t */\n\tif (!(bfq_tot_busy_queues(bfqd) > 1 && bfq_class_idle(bfqq)))\n\t\tgoto return_rq;\n\n\tbfq_bfqq_expire(bfqd, bfqq, false, BFQQE_BUDGET_EXHAUSTED);\n\nreturn_rq:\n\treturn rq;\n}\n\nstatic bool bfq_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\n\t/*\n\t * Avoiding lock: a race on bfqd->busy_queues should cause at\n\t * most a call to dispatch for nothing\n\t */\n\treturn !list_empty_careful(&bfqd->dispatch) ||\n\t\tbfq_tot_busy_queues(bfqd) > 0;\n}\n\nstatic struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq = NULL;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tif (!list_empty(&bfqd->dispatch)) {\n\t\trq = list_first_entry(&bfqd->dispatch, struct request,\n\t\t\t\t      queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (bfqq) {\n\t\t\t/*\n\t\t\t * Increment counters here, because this\n\t\t\t * dispatch does not follow the standard\n\t\t\t * dispatch flow (where counters are\n\t\t\t * incremented)\n\t\t\t */\n\t\t\tbfqq->dispatched++;\n\n\t\t\tgoto inc_in_driver_start_rq;\n\t\t}\n\n\t\t/*\n\t\t * We exploit the bfq_finish_requeue_request hook to\n\t\t * decrement rq_in_driver, but\n\t\t * bfq_finish_requeue_request will not be invoked on\n\t\t * this request. So, to avoid unbalance, just start\n\t\t * this request, without incrementing rq_in_driver. As\n\t\t * a negative consequence, rq_in_driver is deceptively\n\t\t * lower than it should be while this request is in\n\t\t * service. This may cause bfq_schedule_dispatch to be\n\t\t * invoked uselessly.\n\t\t *\n\t\t * As for implementing an exact solution, the\n\t\t * bfq_finish_requeue_request hook, if defined, is\n\t\t * probably invoked also on this request. So, by\n\t\t * exploiting this hook, we could 1) increment\n\t\t * rq_in_driver here, and 2) decrement it in\n\t\t * bfq_finish_requeue_request. Such a solution would\n\t\t * let the value of the counter be always accurate,\n\t\t * but it would entail using an extra interface\n\t\t * function. This cost seems higher than the benefit,\n\t\t * being the frequency of non-elevator-private\n\t\t * requests very low.\n\t\t */\n\t\tgoto start_rq;\n\t}\n\n\tbfq_log(bfqd, \"dispatch requests: %d busy queues\",\n\t\tbfq_tot_busy_queues(bfqd));\n\n\tif (bfq_tot_busy_queues(bfqd) == 0)\n\t\tgoto exit;\n\n\t/*\n\t * Force device to serve one request at a time if\n\t * strict_guarantees is true. Forcing this service scheme is\n\t * currently the ONLY way to guarantee that the request\n\t * service order enforced by the scheduler is respected by a\n\t * queueing device. Otherwise the device is free even to make\n\t * some unlucky request wait for as long as the device\n\t * wishes.\n\t *\n\t * Of course, serving one request at a time may cause loss of\n\t * throughput.\n\t */\n\tif (bfqd->strict_guarantees && bfqd->rq_in_driver > 0)\n\t\tgoto exit;\n\n\tbfqq = bfq_select_queue(bfqd);\n\tif (!bfqq)\n\t\tgoto exit;\n\n\trq = bfq_dispatch_rq_from_bfqq(bfqd, bfqq);\n\n\tif (rq) {\ninc_in_driver_start_rq:\n\t\tbfqd->rq_in_driver++;\nstart_rq:\n\t\trq->rq_flags |= RQF_STARTED;\n\t}\nexit:\n\treturn rq;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t      struct request *rq,\n\t\t\t\t      struct bfq_queue *in_serv_queue,\n\t\t\t\t      bool idle_timer_disabled)\n{\n\tstruct bfq_queue *bfqq = rq ? RQ_BFQQ(rq) : NULL;\n\n\tif (!idle_timer_disabled && !bfqq)\n\t\treturn;\n\n\t/*\n\t * rq and bfqq are guaranteed to exist until this function\n\t * ends, for the following reasons. First, rq can be\n\t * dispatched to the device, and then can be completed and\n\t * freed, only after this function ends. Second, rq cannot be\n\t * merged (and thus freed because of a merge) any longer,\n\t * because it has already started. Thus rq cannot be freed\n\t * before this function ends, and, since rq has a reference to\n\t * bfqq, the same guarantee holds for bfqq too.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tif (idle_timer_disabled)\n\t\t/*\n\t\t * Since the idle timer has been disabled,\n\t\t * in_serv_queue contained some request when\n\t\t * __bfq_dispatch_request was invoked above, which\n\t\t * implies that rq was picked exactly from\n\t\t * in_serv_queue. Thus in_serv_queue == bfqq, and is\n\t\t * therefore guaranteed to exist because of the above\n\t\t * arguments.\n\t\t */\n\t\tbfqg_stats_update_idle_time(bfqq_group(in_serv_queue));\n\tif (bfqq) {\n\t\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\t\tbfqg_stats_update_avg_queue_size(bfqg);\n\t\tbfqg_stats_set_start_empty_time(bfqg);\n\t\tbfqg_stats_update_io_remove(bfqg, rq->cmd_flags);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     struct bfq_queue *in_serv_queue,\n\t\t\t\t\t     bool idle_timer_disabled) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq;\n\tstruct bfq_queue *in_serv_queue;\n\tbool waiting_rq, idle_timer_disabled = false;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tin_serv_queue = bfqd->in_service_queue;\n\twaiting_rq = in_serv_queue && bfq_bfqq_wait_request(in_serv_queue);\n\n\trq = __bfq_dispatch_request(hctx);\n\tif (in_serv_queue == bfqd->in_service_queue) {\n\t\tidle_timer_disabled =\n\t\t\twaiting_rq && !bfq_bfqq_wait_request(in_serv_queue);\n\t}\n\n\tspin_unlock_irq(&bfqd->lock);\n\tbfq_update_dispatch_stats(hctx->queue, rq,\n\t\t\tidle_timer_disabled ? in_serv_queue : NULL,\n\t\t\t\tidle_timer_disabled);\n\n\treturn rq;\n}\n\n/*\n * Task holds one reference to the queue, dropped when task exits.  Each rq\n * in-flight on this queue also holds a reference, dropped when rq is freed.\n *\n * Scheduler lock must be held here. Recall not to use bfqq after calling\n * this function on it.\n */\nvoid bfq_put_queue(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\tif (bfqq->bfqd)\n\t\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"put_queue: %p %d\",\n\t\t\t     bfqq, bfqq->ref);\n\n\tbfqq->ref--;\n\tif (bfqq->ref)\n\t\treturn;\n\n\tif (!hlist_unhashed(&bfqq->burst_list_node)) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\t/*\n\t\t * Decrement also burst size after the removal, if the\n\t\t * process associated with bfqq is exiting, and thus\n\t\t * does not contribute to the burst any longer. This\n\t\t * decrement helps filter out false positives of large\n\t\t * bursts, when some short-lived process (often due to\n\t\t * the execution of commands by some service) happens\n\t\t * to start and exit while a complex application is\n\t\t * starting, and thus spawning several processes that\n\t\t * do I/O (and that *must not* be treated as a large\n\t\t * burst, see comments on bfq_handle_burst).\n\t\t *\n\t\t * In particular, the decrement is performed only if:\n\t\t * 1) bfqq is not a merged queue, because, if it is,\n\t\t * then this free of bfqq is not triggered by the exit\n\t\t * of the process bfqq is associated with, but exactly\n\t\t * by the fact that bfqq has just been merged.\n\t\t * 2) burst_size is greater than 0, to handle\n\t\t * unbalanced decrements. Unbalanced decrements may\n\t\t * happen in te following case: bfqq is inserted into\n\t\t * the current burst list--without incrementing\n\t\t * bust_size--because of a split, but the current\n\t\t * burst list is not the burst list bfqq belonged to\n\t\t * (see comments on the case of a split in\n\t\t * bfq_set_request).\n\t\t */\n\t\tif (bfqq->bic && bfqq->bfqd->burst_size > 0)\n\t\t\tbfqq->bfqd->burst_size--;\n\t}\n\n\t/*\n\t * bfqq does not exist any longer, so it cannot be woken by\n\t * any other queue, and cannot wake any other queue. Then bfqq\n\t * must be removed from the woken list of its possible waker\n\t * queue, and all queues in the woken list of bfqq must stop\n\t * having a waker queue. Strictly speaking, these updates\n\t * should be performed when bfqq remains with no I/O source\n\t * attached to it, which happens before bfqq gets freed. In\n\t * particular, this happens when the last process associated\n\t * with bfqq exits or gets associated with a different\n\t * queue. However, both events lead to bfqq being freed soon,\n\t * and dangling references would come out only after bfqq gets\n\t * freed. So these updates are done here, as a simple and safe\n\t * way to handle all cases.\n\t */\n\t/* remove bfqq from woken list */\n\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\thlist_del_init(&bfqq->woken_list_node);\n\n\t/* reset waker for all queues in woken list */\n\thlist_for_each_entry_safe(item, n, &bfqq->woken_list,\n\t\t\t\t  woken_list_node) {\n\t\titem->waker_bfqq = NULL;\n\t\thlist_del_init(&item->woken_list_node);\n\t}\n\n\tif (bfqq->bfqd && bfqq->bfqd->last_completed_rq_bfqq == bfqq)\n\t\tbfqq->bfqd->last_completed_rq_bfqq = NULL;\n\n\tkmem_cache_free(bfq_pool, bfqq);\n\tbfqg_and_blkg_put(bfqg);\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq)\n{\n\tbfqq->stable_ref--;\n\tbfq_put_queue(bfqq);\n}\n\nvoid bfq_put_cooperator(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *__bfqq, *next;\n\n\t/*\n\t * If this queue was scheduled to merge with another queue, be\n\t * sure to drop the reference taken on that queue (and others in\n\t * the merge chain). See bfq_setup_merge and bfq_merge_bfqqs.\n\t */\n\t__bfqq = bfqq->new_bfqq;\n\twhile (__bfqq) {\n\t\tif (__bfqq == bfqq)\n\t\t\tbreak;\n\t\tnext = __bfqq->new_bfqq;\n\t\tbfq_put_queue(__bfqq);\n\t\t__bfqq = next;\n\t}\n}\n\nstatic void bfq_exit_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tif (bfqq == bfqd->in_service_queue) {\n\t\t__bfq_bfqq_expire(bfqd, bfqq, BFQQE_BUDGET_TIMEOUT);\n\t\tbfq_schedule_dispatch(bfqd);\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"exit_bfqq: %p, %d\", bfqq, bfqq->ref);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n}\n\nstatic void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync);\n\tstruct bfq_data *bfqd;\n\n\tif (bfqq)\n\t\tbfqd = bfqq->bfqd; /* NULL if scheduler already exited */\n\n\tif (bfqq && bfqd) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\tbic_set_bfqq(bic, NULL, is_sync);\n\t\tbfq_exit_bfqq(bfqd, bfqq);\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t}\n}\n\nstatic void bfq_exit_icq(struct io_cq *icq)\n{\n\tstruct bfq_io_cq *bic = icq_to_bic(icq);\n\n\tif (bic->stable_merge_bfqq) {\n\t\tstruct bfq_data *bfqd = bic->stable_merge_bfqq->bfqd;\n\n\t\t/*\n\t\t * bfqd is NULL if scheduler already exited, and in\n\t\t * that case this is the last time bfqq is accessed.\n\t\t */\n\t\tif (bfqd) {\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\t\tbfq_put_stable_ref(bic->stable_merge_bfqq);\n\t\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\t} else {\n\t\t\tbfq_put_stable_ref(bic->stable_merge_bfqq);\n\t\t}\n\t}\n\n\tbfq_exit_icq_bfqq(bic, true);\n\tbfq_exit_icq_bfqq(bic, false);\n}\n\n/*\n * Update the entity prio values; note that the new values will not\n * be used until the next (re)activation.\n */\nstatic void\nbfq_set_next_ioprio_data(struct bfq_queue *bfqq, struct bfq_io_cq *bic)\n{\n\tstruct task_struct *tsk = current;\n\tint ioprio_class;\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\n\tif (!bfqd)\n\t\treturn;\n\n\tioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tswitch (ioprio_class) {\n\tdefault:\n\t\tpr_err(\"bdi %s: bfq: bad prio class %d\\n\",\n\t\t\tbdi_dev_name(bfqq->bfqd->queue->disk->bdi),\n\t\t\tioprio_class);\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_NONE:\n\t\t/*\n\t\t * No prio set, inherit CPU scheduling settings.\n\t\t */\n\t\tbfqq->new_ioprio = task_nice_ioprio(tsk);\n\t\tbfqq->new_ioprio_class = task_nice_ioclass(tsk);\n\t\tbreak;\n\tcase IOPRIO_CLASS_RT:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_DATA(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_RT;\n\t\tbreak;\n\tcase IOPRIO_CLASS_BE:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_DATA(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_BE;\n\t\tbreak;\n\tcase IOPRIO_CLASS_IDLE:\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_IDLE;\n\t\tbfqq->new_ioprio = 7;\n\t\tbreak;\n\t}\n\n\tif (bfqq->new_ioprio >= IOPRIO_NR_LEVELS) {\n\t\tpr_crit(\"bfq_set_next_ioprio_data: new_ioprio %d\\n\",\n\t\t\tbfqq->new_ioprio);\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t}\n\n\tbfqq->entity.new_weight = bfq_ioprio_to_weight(bfqq->new_ioprio);\n\tbfq_log_bfqq(bfqd, bfqq, \"new_ioprio %d new_weight %d\",\n\t\t     bfqq->new_ioprio, bfqq->entity.new_weight);\n\tbfqq->entity.prio_changed = 1;\n}\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn);\n\nstatic void bfq_check_ioprio_change(struct bfq_io_cq *bic, struct bio *bio)\n{\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tstruct bfq_queue *bfqq;\n\tint ioprio = bic->icq.ioc->ioprio;\n\n\t/*\n\t * This condition may trigger on a newly created bic, be sure to\n\t * drop the lock before returning.\n\t */\n\tif (unlikely(!bfqd) || likely(bic->ioprio == ioprio))\n\t\treturn;\n\n\tbic->ioprio = ioprio;\n\n\tbfqq = bic_to_bfqq(bic, false);\n\tif (bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\n\t\tbfqq = bfq_get_queue(bfqd, bio, false, bic, true);\n\t\tbic_set_bfqq(bic, bfqq, false);\n\t\tbfq_release_process_ref(bfqd, old_bfqq);\n\t}\n\n\tbfqq = bic_to_bfqq(bic, true);\n\tif (bfqq)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n}\n\nstatic void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic, pid_t pid, int is_sync)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tRB_CLEAR_NODE(&bfqq->entity.rb_node);\n\tINIT_LIST_HEAD(&bfqq->fifo);\n\tINIT_HLIST_NODE(&bfqq->burst_list_node);\n\tINIT_HLIST_NODE(&bfqq->woken_list_node);\n\tINIT_HLIST_HEAD(&bfqq->woken_list);\n\n\tbfqq->ref = 0;\n\tbfqq->bfqd = bfqd;\n\n\tif (bic)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n\n\tif (is_sync) {\n\t\t/*\n\t\t * No need to mark as has_short_ttime if in\n\t\t * idle_class, because no device idling is performed\n\t\t * for queues in idle class\n\t\t */\n\t\tif (!bfq_class_idle(bfqq))\n\t\t\t/* tentatively mark as has_short_ttime */\n\t\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\t\tbfq_mark_bfqq_sync(bfqq);\n\t\tbfq_mark_bfqq_just_created(bfqq);\n\t} else\n\t\tbfq_clear_bfqq_sync(bfqq);\n\n\t/* set end request to minus infinity from now */\n\tbfqq->ttime.last_end_request = now_ns + 1;\n\n\tbfqq->creation_time = jiffies;\n\n\tbfqq->io_start_time = now_ns;\n\n\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\tbfqq->pid = pid;\n\n\t/* Tentative initial value to trade off between thr and lat */\n\tbfqq->max_budget = (2 * bfq_max_budget(bfqd)) / 3;\n\tbfqq->budget_timeout = bfq_smallest_from_now();\n\n\tbfqq->wr_coeff = 1;\n\tbfqq->last_wr_start_finish = jiffies;\n\tbfqq->wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\tbfqq->split_time = bfq_smallest_from_now();\n\n\t/*\n\t * To not forget the possibly high bandwidth consumed by a\n\t * process/queue in the recent past,\n\t * bfq_bfqq_softrt_next_start() returns a value at least equal\n\t * to the current value of bfqq->soft_rt_next_start (see\n\t * comments on bfq_bfqq_softrt_next_start).  Set\n\t * soft_rt_next_start to now, to mean that bfqq has consumed\n\t * no bandwidth so far.\n\t */\n\tbfqq->soft_rt_next_start = jiffies;\n\n\t/* first request is almost certainly seeky */\n\tbfqq->seek_history = 1;\n}\n\nstatic struct bfq_queue **bfq_async_queue_prio(struct bfq_data *bfqd,\n\t\t\t\t\t       struct bfq_group *bfqg,\n\t\t\t\t\t       int ioprio_class, int ioprio)\n{\n\tswitch (ioprio_class) {\n\tcase IOPRIO_CLASS_RT:\n\t\treturn &bfqg->async_bfqq[0][ioprio];\n\tcase IOPRIO_CLASS_NONE:\n\t\tioprio = IOPRIO_BE_NORM;\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_BE:\n\t\treturn &bfqg->async_bfqq[1][ioprio];\n\tcase IOPRIO_CLASS_IDLE:\n\t\treturn &bfqg->async_idle_bfqq;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bfq_queue *\nbfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic,\n\t\t\t  struct bfq_queue *last_bfqq_created)\n{\n\tstruct bfq_queue *new_bfqq =\n\t\tbfq_setup_merge(bfqq, last_bfqq_created);\n\n\tif (!new_bfqq)\n\t\treturn bfqq;\n\n\tif (new_bfqq->bic)\n\t\tnew_bfqq->bic->stably_merged = true;\n\tbic->stably_merged = true;\n\n\t/*\n\t * Reusing merge functions. This implies that\n\t * bfqq->bic must be set too, for\n\t * bfq_merge_bfqqs to correctly save bfqq's\n\t * state before killing it.\n\t */\n\tbfqq->bic = bic;\n\treturn bfq_merge_bfqqs(bfqd, bic, bfqq);\n}\n\n/*\n * Many throughput-sensitive workloads are made of several parallel\n * I/O flows, with all flows generated by the same application, or\n * more generically by the same task (e.g., system boot). The most\n * counterproductive action with these workloads is plugging I/O\n * dispatch when one of the bfq_queues associated with these flows\n * remains temporarily empty.\n *\n * To avoid this plugging, BFQ has been using a burst-handling\n * mechanism for years now. This mechanism has proven effective for\n * throughput, and not detrimental for service guarantees. The\n * following function pushes this mechanism a little bit further,\n * basing on the following two facts.\n *\n * First, all the I/O flows of a the same application or task\n * contribute to the execution/completion of that common application\n * or task. So the performance figures that matter are total\n * throughput of the flows and task-wide I/O latency.  In particular,\n * these flows do not need to be protected from each other, in terms\n * of individual bandwidth or latency.\n *\n * Second, the above fact holds regardless of the number of flows.\n *\n * Putting these two facts together, this commits merges stably the\n * bfq_queues associated with these I/O flows, i.e., with the\n * processes that generate these IO/ flows, regardless of how many the\n * involved processes are.\n *\n * To decide whether a set of bfq_queues is actually associated with\n * the I/O flows of a common application or task, and to merge these\n * queues stably, this function operates as follows: given a bfq_queue,\n * say Q2, currently being created, and the last bfq_queue, say Q1,\n * created before Q2, Q2 is merged stably with Q1 if\n * - very little time has elapsed since when Q1 was created\n * - Q2 has the same ioprio as Q1\n * - Q2 belongs to the same group as Q1\n *\n * Merging bfq_queues also reduces scheduling overhead. A fio test\n * with ten random readers on /dev/nullb shows a throughput boost of\n * 40%, with a quadcore. Since BFQ's execution time amounts to ~50% of\n * the total per-request processing time, the above throughput boost\n * implies that BFQ's overhead is reduced by more than 50%.\n *\n * This new mechanism most certainly obsoletes the current\n * burst-handling heuristics. We keep those heuristics for the moment.\n */\nstatic struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,\n\t\t\t\t\t\t      struct bfq_queue *bfqq,\n\t\t\t\t\t\t      struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue **source_bfqq = bfqq->entity.parent ?\n\t\t&bfqq->entity.parent->last_bfqq_created :\n\t\t&bfqd->last_bfqq_created;\n\n\tstruct bfq_queue *last_bfqq_created = *source_bfqq;\n\n\t/*\n\t * If last_bfqq_created has not been set yet, then init it. If\n\t * it has been set already, but too long ago, then move it\n\t * forward to bfqq. Finally, move also if bfqq belongs to a\n\t * different group than last_bfqq_created, or if bfqq has a\n\t * different ioprio or ioprio_class. If none of these\n\t * conditions holds true, then try an early stable merge or\n\t * schedule a delayed stable merge.\n\t *\n\t * A delayed merge is scheduled (instead of performing an\n\t * early merge), in case bfqq might soon prove to be more\n\t * throughput-beneficial if not merged. Currently this is\n\t * possible only if bfqd is rotational with no queueing. For\n\t * such a drive, not merging bfqq is better for throughput if\n\t * bfqq happens to contain sequential I/O. So, we wait a\n\t * little bit for enough I/O to flow through bfqq. After that,\n\t * if such an I/O is sequential, then the merge is\n\t * canceled. Otherwise the merge is finally performed.\n\t */\n\tif (!last_bfqq_created ||\n\t    time_before(last_bfqq_created->creation_time +\n\t\t\tmsecs_to_jiffies(bfq_activation_stable_merging),\n\t\t\tbfqq->creation_time) ||\n\t\tbfqq->entity.parent != last_bfqq_created->entity.parent ||\n\t\tbfqq->ioprio != last_bfqq_created->ioprio ||\n\t\tbfqq->ioprio_class != last_bfqq_created->ioprio_class)\n\t\t*source_bfqq = bfqq;\n\telse if (time_after_eq(last_bfqq_created->creation_time +\n\t\t\t\t bfqd->bfq_burst_interval,\n\t\t\t\t bfqq->creation_time)) {\n\t\tif (likely(bfqd->nonrot_with_queueing))\n\t\t\t/*\n\t\t\t * With this type of drive, leaving\n\t\t\t * bfqq alone may provide no\n\t\t\t * throughput benefits compared with\n\t\t\t * merging bfqq. So merge bfqq now.\n\t\t\t */\n\t\t\tbfqq = bfq_do_early_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t\t bic,\n\t\t\t\t\t\t\t last_bfqq_created);\n\t\telse { /* schedule tentative stable merge */\n\t\t\t/*\n\t\t\t * get reference on last_bfqq_created,\n\t\t\t * to prevent it from being freed,\n\t\t\t * until we decide whether to merge\n\t\t\t */\n\t\t\tlast_bfqq_created->ref++;\n\t\t\t/*\n\t\t\t * need to keep track of stable refs, to\n\t\t\t * compute process refs correctly\n\t\t\t */\n\t\t\tlast_bfqq_created->stable_ref++;\n\t\t\t/*\n\t\t\t * Record the bfqq to merge to.\n\t\t\t */\n\t\t\tbic->stable_merge_bfqq = last_bfqq_created;\n\t\t}\n\t}\n\n\treturn bfqq;\n}\n\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn)\n{\n\tconst int ioprio = IOPRIO_PRIO_DATA(bic->ioprio);\n\tconst int ioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tstruct bfq_queue **async_bfqq = NULL;\n\tstruct bfq_queue *bfqq;\n\tstruct bfq_group *bfqg;\n\n\tbfqg = bfq_bio_bfqg(bfqd, bio);\n\tif (!is_sync) {\n\t\tasync_bfqq = bfq_async_queue_prio(bfqd, bfqg, ioprio_class,\n\t\t\t\t\t\t  ioprio);\n\t\tbfqq = *async_bfqq;\n\t\tif (bfqq)\n\t\t\tgoto out;\n\t}\n\n\tbfqq = kmem_cache_alloc_node(bfq_pool,\n\t\t\t\t     GFP_NOWAIT | __GFP_ZERO | __GFP_NOWARN,\n\t\t\t\t     bfqd->queue->node);\n\n\tif (bfqq) {\n\t\tbfq_init_bfqq(bfqd, bfqq, bic, current->pid,\n\t\t\t      is_sync);\n\t\tbfq_init_entity(&bfqq->entity, bfqg);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"allocated\");\n\t} else {\n\t\tbfqq = &bfqd->oom_bfqq;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"using oom bfqq\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Pin the queue now that it's allocated, scheduler exit will\n\t * prune it.\n\t */\n\tif (async_bfqq) {\n\t\tbfqq->ref++; /*\n\t\t\t      * Extra group reference, w.r.t. sync\n\t\t\t      * queue. This extra reference is removed\n\t\t\t      * only if bfqq->bfqg disappears, to\n\t\t\t      * guarantee that this queue is not freed\n\t\t\t      * until its group goes away.\n\t\t\t      */\n\t\tbfq_log_bfqq(bfqd, bfqq, \"get_queue, bfqq not in async: %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\t*async_bfqq = bfqq;\n\t}\n\nout:\n\tbfqq->ref++; /* get a process reference to this queue */\n\n\tif (bfqq != &bfqd->oom_bfqq && is_sync && !respawn)\n\t\tbfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);\n\treturn bfqq;\n}\n\nstatic void bfq_update_io_thinktime(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tstruct bfq_ttime *ttime = &bfqq->ttime;\n\tu64 elapsed;\n\n\t/*\n\t * We are really interested in how long it takes for the queue to\n\t * become busy when there is no outstanding IO for this queue. So\n\t * ignore cases when the bfq queue has already IO queued.\n\t */\n\tif (bfqq->dispatched || bfq_bfqq_busy(bfqq))\n\t\treturn;\n\telapsed = ktime_get_ns() - bfqq->ttime.last_end_request;\n\telapsed = min_t(u64, elapsed, 2ULL * bfqd->bfq_slice_idle);\n\n\tttime->ttime_samples = (7*ttime->ttime_samples + 256) / 8;\n\tttime->ttime_total = div_u64(7*ttime->ttime_total + 256*elapsed,  8);\n\tttime->ttime_mean = div64_ul(ttime->ttime_total + 128,\n\t\t\t\t     ttime->ttime_samples);\n}\n\nstatic void\nbfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct request *rq)\n{\n\tbfqq->seek_history <<= 1;\n\tbfqq->seek_history |= BFQ_RQ_SEEKY(bfqd, bfqq->last_request_pos, rq);\n\n\tif (bfqq->wr_coeff > 1 &&\n\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t    BFQQ_TOTALLY_SEEKY(bfqq)) {\n\t\tif (time_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t   bfq_wr_duration(bfqd))) {\n\t\t\t/*\n\t\t\t * In soft_rt weight raising with the\n\t\t\t * interactive-weight-raising period\n\t\t\t * elapsed (so no switch back to\n\t\t\t * interactive weight raising).\n\t\t\t */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t} else { /*\n\t\t\t  * stopping soft_rt weight raising\n\t\t\t  * while still in interactive period,\n\t\t\t  * switch back to interactive weight\n\t\t\t  * raising\n\t\t\t  */\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n}\n\nstatic void bfq_update_has_short_ttime(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq,\n\t\t\t\t       struct bfq_io_cq *bic)\n{\n\tbool has_short_ttime = true, state_changed;\n\n\t/*\n\t * No need to update has_short_ttime if bfqq is async or in\n\t * idle io prio class, or if bfq_slice_idle is zero, because\n\t * no device idling is performed for bfqq in this case.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || bfq_class_idle(bfqq) ||\n\t    bfqd->bfq_slice_idle == 0)\n\t\treturn;\n\n\t/* Idle window just restored, statistics are meaningless. */\n\tif (time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     bfqd->bfq_wr_min_idle_time))\n\t\treturn;\n\n\t/* Think time is infinite if no process is linked to\n\t * bfqq. Otherwise check average think time to decide whether\n\t * to mark as has_short_ttime. To this goal, compare average\n\t * think time with half the I/O-plugging timeout.\n\t */\n\tif (atomic_read(&bic->icq.ioc->active_ref) == 0 ||\n\t    (bfq_sample_valid(bfqq->ttime.ttime_samples) &&\n\t     bfqq->ttime.ttime_mean > bfqd->bfq_slice_idle>>1))\n\t\thas_short_ttime = false;\n\n\tstate_changed = has_short_ttime != bfq_bfqq_has_short_ttime(bfqq);\n\n\tif (has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * Until the base value for the total service time gets\n\t * finally computed for bfqq, the inject limit does depend on\n\t * the think-time state (short|long). In particular, the limit\n\t * is 0 or 1 if the think time is deemed, respectively, as\n\t * short or long (details in the comments in\n\t * bfq_update_inject_limit()). Accordingly, the next\n\t * instructions reset the inject limit if the think-time state\n\t * has changed and the above base value is still to be\n\t * computed.\n\t *\n\t * However, the reset is performed only if more than 100 ms\n\t * have elapsed since the last update of the inject limit, or\n\t * (inclusive) if the change is from short to long think\n\t * time. The reason for this waiting is as follows.\n\t *\n\t * bfqq may have a long think time because of a\n\t * synchronization with some other queue, i.e., because the\n\t * I/O of some other queue may need to be completed for bfqq\n\t * to receive new I/O. Details in the comments on the choice\n\t * of the queue for injection in bfq_select_queue().\n\t *\n\t * As stressed in those comments, if such a synchronization is\n\t * actually in place, then, without injection on bfqq, the\n\t * blocking I/O cannot happen to served while bfqq is in\n\t * service. As a consequence, if bfqq is granted\n\t * I/O-dispatch-plugging, then bfqq remains empty, and no I/O\n\t * is dispatched, until the idle timeout fires. This is likely\n\t * to result in lower bandwidth and higher latencies for bfqq,\n\t * and in a severe loss of total throughput.\n\t *\n\t * On the opposite end, a non-zero inject limit may allow the\n\t * I/O that blocks bfqq to be executed soon, and therefore\n\t * bfqq to receive new I/O soon.\n\t *\n\t * But, if the blocking gets actually eliminated, then the\n\t * next think-time sample for bfqq may be very low. This in\n\t * turn may cause bfqq's think time to be deemed\n\t * short. Without the 100 ms barrier, this new state change\n\t * would cause the body of the next if to be executed\n\t * immediately. But this would set to 0 the inject\n\t * limit. Without injection, the blocking I/O would cause the\n\t * think time of bfqq to become long again, and therefore the\n\t * inject limit to be raised again, and so on. The only effect\n\t * of such a steady oscillation between the two think-time\n\t * states would be to prevent effective injection on bfqq.\n\t *\n\t * In contrast, if the inject limit is not reset during such a\n\t * long time interval as 100 ms, then the number of short\n\t * think time samples can grow significantly before the reset\n\t * is performed. As a consequence, the think time state can\n\t * become stable before the reset. Therefore there will be no\n\t * state change when the 100 ms elapse, and no reset of the\n\t * inject limit. The inject limit remains steadily equal to 1\n\t * both during and after the 100 ms. So injection can be\n\t * performed at all times, and throughput gets boosted.\n\t *\n\t * An inject limit equal to 1 is however in conflict, in\n\t * general, with the fact that the think time of bfqq is\n\t * short, because injection may be likely to delay bfqq's I/O\n\t * (as explained in the comments in\n\t * bfq_update_inject_limit()). But this does not happen in\n\t * this special case, because bfqq's low think time is due to\n\t * an effective handling of a synchronization, through\n\t * injection. In this special case, bfqq's I/O does not get\n\t * delayed by injection; on the contrary, bfqq's I/O is\n\t * brought forward, because it is not blocked for\n\t * milliseconds.\n\t *\n\t * In addition, serving the blocking I/O much sooner, and much\n\t * more frequently than once per I/O-plugging timeout, makes\n\t * it much quicker to detect a waker queue (the concept of\n\t * waker queue is defined in the comments in\n\t * bfq_add_request()). This makes it possible to start sooner\n\t * to boost throughput more effectively, by injecting the I/O\n\t * of the waker queue unconditionally on every\n\t * bfq_dispatch_request().\n\t *\n\t * One last, important benefit of not resetting the inject\n\t * limit before 100 ms is that, during this time interval, the\n\t * base value for the total service time is likely to get\n\t * finally computed for bfqq, freeing the inject limit from\n\t * its relation with the think time.\n\t */\n\tif (state_changed && bfqq->last_serv_time_ns == 0 &&\n\t    (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t      msecs_to_jiffies(100)) ||\n\t     !has_short_ttime))\n\t\tbfq_reset_inject_limit(bfqd, bfqq);\n}\n\n/*\n * Called when a new fs request (rq) is added to bfqq.  Check if there's\n * something we should do about it.\n */\nstatic void bfq_rq_enqueued(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending++;\n\n\tbfqq->last_request_pos = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\n\tif (bfqq == bfqd->in_service_queue && bfq_bfqq_wait_request(bfqq)) {\n\t\tbool small_req = bfqq->queued[rq_is_sync(rq)] == 1 &&\n\t\t\t\t blk_rq_sectors(rq) < 32;\n\t\tbool budget_timeout = bfq_bfqq_budget_timeout(bfqq);\n\n\t\t/*\n\t\t * There is just this request queued: if\n\t\t * - the request is small, and\n\t\t * - we are idling to boost throughput, and\n\t\t * - the queue is not to be expired,\n\t\t * then just exit.\n\t\t *\n\t\t * In this way, if the device is being idled to wait\n\t\t * for a new request from the in-service queue, we\n\t\t * avoid unplugging the device and committing the\n\t\t * device to serve just a small request. In contrast\n\t\t * we wait for the block layer to decide when to\n\t\t * unplug the device: hopefully, new requests will be\n\t\t * merged to this one quickly, then the device will be\n\t\t * unplugged and larger requests will be dispatched.\n\t\t */\n\t\tif (small_req && idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t    !budget_timeout)\n\t\t\treturn;\n\n\t\t/*\n\t\t * A large enough request arrived, or idling is being\n\t\t * performed to preserve service guarantees, or\n\t\t * finally the queue is to be expired: in all these\n\t\t * cases disk idling is to be stopped, so clear\n\t\t * wait_request flag and reset timer.\n\t\t */\n\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\n\t\t/*\n\t\t * The queue is not empty, because a new request just\n\t\t * arrived. Hence we can safely expire the queue, in\n\t\t * case of budget timeout, without risking that the\n\t\t * timestamps of the queue are not updated correctly.\n\t\t * See [1] for more details.\n\t\t */\n\t\tif (budget_timeout)\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t}\n}\n\n/* returns true if it causes the idle timer to be disabled */\nstatic bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,\n\t\t\t\t\t\t RQ_BIC(rq));\n\tbool waiting, idle_timer_disabled = false;\n\n\tif (new_bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\t\t/*\n\t\t * Release the request's reference to the old bfqq\n\t\t * and make sure one is taken to the shared queue.\n\t\t */\n\t\tnew_bfqq->allocated++;\n\t\tbfqq->allocated--;\n\t\tnew_bfqq->ref++;\n\t\t/*\n\t\t * If the bic associated with the process\n\t\t * issuing this request still points to bfqq\n\t\t * (and thus has not been already redirected\n\t\t * to new_bfqq or even some other bfq_queue),\n\t\t * then complete the merge and redirect it to\n\t\t * new_bfqq.\n\t\t */\n\t\tif (bic_to_bfqq(RQ_BIC(rq), 1) == bfqq) {\n\t\t\twhile (bfqq != new_bfqq)\n\t\t\t\tbfqq = bfq_merge_bfqqs(bfqd, RQ_BIC(rq), bfqq);\n\t\t}\n\n\t\tbfq_clear_bfqq_just_created(old_bfqq);\n\t\t/*\n\t\t * rq is about to be enqueued into new_bfqq,\n\t\t * release rq reference on bfqq\n\t\t */\n\t\tbfq_put_queue(old_bfqq);\n\t\trq->elv.priv[1] = new_bfqq;\n\t}\n\n\tbfq_update_io_thinktime(bfqd, bfqq);\n\tbfq_update_has_short_ttime(bfqd, bfqq, RQ_BIC(rq));\n\tbfq_update_io_seektime(bfqd, bfqq, rq);\n\n\twaiting = bfqq && bfq_bfqq_wait_request(bfqq);\n\tbfq_add_request(rq);\n\tidle_timer_disabled = waiting && !bfq_bfqq_wait_request(bfqq);\n\n\trq->fifo_time = ktime_get_ns() + bfqd->bfq_fifo_expire[rq_is_sync(rq)];\n\tlist_add_tail(&rq->queuelist, &bfqq->fifo);\n\n\tbfq_rq_enqueued(bfqd, bfqq, rq);\n\n\treturn idle_timer_disabled;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t    struct bfq_queue *bfqq,\n\t\t\t\t    bool idle_timer_disabled,\n\t\t\t\t    unsigned int cmd_flags)\n{\n\tif (!bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq still exists, because it can disappear only after\n\t * either it is merged with another queue, or the process it\n\t * is associated with exits. But both actions must be taken by\n\t * the same process currently executing this flow of\n\t * instructions.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tbfqg_stats_update_io_add(bfqq_group(bfqq), bfqq, cmd_flags);\n\tif (idle_timer_disabled)\n\t\tbfqg_stats_update_idle_time(bfqq_group(bfqq));\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t\t   struct bfq_queue *bfqq,\n\t\t\t\t\t   bool idle_timer_disabled,\n\t\t\t\t\t   unsigned int cmd_flags) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct bfq_queue *bfq_init_rq(struct request *rq);\n\nstatic void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t       bool at_head)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_queue *bfqq;\n\tbool idle_timer_disabled = false;\n\tunsigned int cmd_flags;\n\tLIST_HEAD(free);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)\n\t\tbfqg_stats_update_legacy_io(q, rq);\n#endif\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bfq_init_rq(rq);\n\tif (blk_mq_sched_try_insert_merge(q, rq, &free)) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tblk_mq_free_requests(&free);\n\t\treturn;\n\t}\n\n\ttrace_block_rq_insert(rq);\n\n\tif (!bfqq || at_head) {\n\t\tif (at_head)\n\t\t\tlist_add(&rq->queuelist, &bfqd->dispatch);\n\t\telse\n\t\t\tlist_add_tail(&rq->queuelist, &bfqd->dispatch);\n\t} else {\n\t\tidle_timer_disabled = __bfq_insert_request(bfqd, rq);\n\t\t/*\n\t\t * Update bfqq, because, if a queue merge has occurred\n\t\t * in __bfq_insert_request, then rq has been\n\t\t * redirected into a new queue.\n\t\t */\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (rq_mergeable(rq)) {\n\t\t\telv_rqhash_add(q, rq);\n\t\t\tif (!q->last_merge)\n\t\t\t\tq->last_merge = rq;\n\t\t}\n\t}\n\n\t/*\n\t * Cache cmd_flags before releasing scheduler lock, because rq\n\t * may disappear afterwards (for example, because of a request\n\t * merge).\n\t */\n\tcmd_flags = rq->cmd_flags;\n\tspin_unlock_irq(&bfqd->lock);\n\n\tbfq_update_insert_stats(q, bfqq, idle_timer_disabled,\n\t\t\t\tcmd_flags);\n}\n\nstatic void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t\tstruct list_head *list, bool at_head)\n{\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tbfq_insert_request(hctx, rq, at_head);\n\t}\n}\n\nstatic void bfq_update_hw_tag(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\tbfqd->max_rq_in_driver = max_t(int, bfqd->max_rq_in_driver,\n\t\t\t\t       bfqd->rq_in_driver);\n\n\tif (bfqd->hw_tag == 1)\n\t\treturn;\n\n\t/*\n\t * This sample is valid if the number of outstanding requests\n\t * is large enough to allow a queueing behavior.  Note that the\n\t * sum is not exact, as it's not taking into account deactivated\n\t * requests.\n\t */\n\tif (bfqd->rq_in_driver + bfqd->queued <= BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\t/*\n\t * If active queue hasn't enough requests and can idle, bfq might not\n\t * dispatch sufficient requests to hardware. Don't zero hw_tag in this\n\t * case\n\t */\n\tif (bfqq && bfq_bfqq_has_short_ttime(bfqq) &&\n\t    bfqq->dispatched + bfqq->queued[0] + bfqq->queued[1] <\n\t    BFQ_HW_QUEUE_THRESHOLD &&\n\t    bfqd->rq_in_driver < BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\tif (bfqd->hw_tag_samples++ < BFQ_HW_QUEUE_SAMPLES)\n\t\treturn;\n\n\tbfqd->hw_tag = bfqd->max_rq_in_driver > BFQ_HW_QUEUE_THRESHOLD;\n\tbfqd->max_rq_in_driver = 0;\n\tbfqd->hw_tag_samples = 0;\n\n\tbfqd->nonrot_with_queueing =\n\t\tblk_queue_nonrot(bfqd->queue) && bfqd->hw_tag;\n}\n\nstatic void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)\n{\n\tu64 now_ns;\n\tu32 delta_us;\n\n\tbfq_update_hw_tag(bfqd);\n\n\tbfqd->rq_in_driver--;\n\tbfqq->dispatched--;\n\n\tif (!bfqq->dispatched && !bfq_bfqq_busy(bfqq)) {\n\t\t/*\n\t\t * Set budget_timeout (which we overload to store the\n\t\t * time at which the queue remains with no backlog and\n\t\t * no outstanding request; used by the weight-raising\n\t\t * mechanism).\n\t\t */\n\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_weights_tree_remove(bfqd, bfqq);\n\t}\n\n\tnow_ns = ktime_get_ns();\n\n\tbfqq->ttime.last_end_request = now_ns;\n\n\t/*\n\t * Using us instead of ns, to get a reasonable precision in\n\t * computing rate in next check.\n\t */\n\tdelta_us = div_u64(now_ns - bfqd->last_completion, NSEC_PER_USEC);\n\n\t/*\n\t * If the request took rather long to complete, and, according\n\t * to the maximum request size recorded, this completion latency\n\t * implies that the request was certainly served at a very low\n\t * rate (less than 1M sectors/sec), then the whole observation\n\t * interval that lasts up to this time instant cannot be a\n\t * valid time interval for computing a new peak rate.  Invoke\n\t * bfq_update_rate_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - reset to zero samples, which will trigger a proper\n\t *   re-initialization of the observation interval on next\n\t *   dispatch\n\t */\n\tif (delta_us > BFQ_MIN_TT/NSEC_PER_USEC &&\n\t   (bfqd->last_rq_max_size<<BFQ_RATE_SHIFT)/delta_us <\n\t\t\t1UL<<(BFQ_RATE_SHIFT - 10))\n\t\tbfq_update_rate_reset(bfqd, NULL);\n\tbfqd->last_completion = now_ns;\n\t/*\n\t * Shared queues are likely to receive I/O at a high\n\t * rate. This may deceptively let them be considered as wakers\n\t * of other queues. But a false waker will unjustly steal\n\t * bandwidth to its supposedly woken queue. So considering\n\t * also shared queues in the waking mechanism may cause more\n\t * control troubles than throughput benefits. Then reset\n\t * last_completed_rq_bfqq if bfqq is a shared queue.\n\t */\n\tif (!bfq_bfqq_coop(bfqq))\n\t\tbfqd->last_completed_rq_bfqq = bfqq;\n\telse\n\t\tbfqd->last_completed_rq_bfqq = NULL;\n\n\t/*\n\t * If we are waiting to discover whether the request pattern\n\t * of the task associated with the queue is actually\n\t * isochronous, and both requisites for this condition to hold\n\t * are now satisfied, then compute soft_rt_next_start (see the\n\t * comments on the function bfq_bfqq_softrt_next_start()). We\n\t * do not compute soft_rt_next_start if bfqq is in interactive\n\t * weight raising (see the comments in bfq_bfqq_expire() for\n\t * an explanation). We schedule this delayed update when bfqq\n\t * expires, if it still has in-flight requests.\n\t */\n\tif (bfq_bfqq_softrt_update(bfqq) && bfqq->dispatched == 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq->wr_coeff != bfqd->bfq_wr_coeff)\n\t\tbfqq->soft_rt_next_start =\n\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\n\t/*\n\t * If this is the in-service queue, check if it needs to be expired,\n\t * or if we want to idle in case it has no pending requests.\n\t */\n\tif (bfqd->in_service_queue == bfqq) {\n\t\tif (bfq_bfqq_must_idle(bfqq)) {\n\t\t\tif (bfqq->dispatched == 0)\n\t\t\t\tbfq_arm_slice_timer(bfqd);\n\t\t\t/*\n\t\t\t * If we get here, we do not expire bfqq, even\n\t\t\t * if bfqq was in budget timeout or had no\n\t\t\t * more requests (as controlled in the next\n\t\t\t * conditional instructions). The reason for\n\t\t\t * not expiring bfqq is as follows.\n\t\t\t *\n\t\t\t * Here bfqq->dispatched > 0 holds, but\n\t\t\t * bfq_bfqq_must_idle() returned true. This\n\t\t\t * implies that, even if no request arrives\n\t\t\t * for bfqq before bfqq->dispatched reaches 0,\n\t\t\t * bfqq will, however, not be expired on the\n\t\t\t * completion event that causes bfqq->dispatch\n\t\t\t * to reach zero. In contrast, on this event,\n\t\t\t * bfqq will start enjoying device idling\n\t\t\t * (I/O-dispatch plugging).\n\t\t\t *\n\t\t\t * But, if we expired bfqq here, bfqq would\n\t\t\t * not have the chance to enjoy device idling\n\t\t\t * when bfqq->dispatched finally reaches\n\t\t\t * zero. This would expose bfqq to violation\n\t\t\t * of its reserved service guarantees.\n\t\t\t */\n\t\t\treturn;\n\t\t} else if (bfq_may_expire_for_budg_timeout(bfqq))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t\telse if (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t (bfqq->dispatched == 0 ||\n\t\t\t  !bfq_better_to_idle(bfqq)))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_NO_MORE_REQUESTS);\n\t}\n\n\tif (!bfqd->rq_in_driver)\n\t\tbfq_schedule_dispatch(bfqd);\n}\n\nstatic void bfq_finish_requeue_request_body(struct bfq_queue *bfqq)\n{\n\tbfqq->allocated--;\n\n\tbfq_put_queue(bfqq);\n}\n\n/*\n * The processes associated with bfqq may happen to generate their\n * cumulative I/O at a lower rate than the rate at which the device\n * could serve the same I/O. This is rather probable, e.g., if only\n * one process is associated with bfqq and the device is an SSD. It\n * results in bfqq becoming often empty while in service. In this\n * respect, if BFQ is allowed to switch to another queue when bfqq\n * remains empty, then the device goes on being fed with I/O requests,\n * and the throughput is not affected. In contrast, if BFQ is not\n * allowed to switch to another queue---because bfqq is sync and\n * I/O-dispatch needs to be plugged while bfqq is temporarily\n * empty---then, during the service of bfqq, there will be frequent\n * \"service holes\", i.e., time intervals during which bfqq gets empty\n * and the device can only consume the I/O already queued in its\n * hardware queues. During service holes, the device may even get to\n * remaining idle. In the end, during the service of bfqq, the device\n * is driven at a lower speed than the one it can reach with the kind\n * of I/O flowing through bfqq.\n *\n * To counter this loss of throughput, BFQ implements a \"request\n * injection mechanism\", which tries to fill the above service holes\n * with I/O requests taken from other queues. The hard part in this\n * mechanism is finding the right amount of I/O to inject, so as to\n * both boost throughput and not break bfqq's bandwidth and latency\n * guarantees. In this respect, the mechanism maintains a per-queue\n * inject limit, computed as below. While bfqq is empty, the injection\n * mechanism dispatches extra I/O requests only until the total number\n * of I/O requests in flight---i.e., already dispatched but not yet\n * completed---remains lower than this limit.\n *\n * A first definition comes in handy to introduce the algorithm by\n * which the inject limit is computed.  We define as first request for\n * bfqq, an I/O request for bfqq that arrives while bfqq is in\n * service, and causes bfqq to switch from empty to non-empty. The\n * algorithm updates the limit as a function of the effect of\n * injection on the service times of only the first requests of\n * bfqq. The reason for this restriction is that these are the\n * requests whose service time is affected most, because they are the\n * first to arrive after injection possibly occurred.\n *\n * To evaluate the effect of injection, the algorithm measures the\n * \"total service time\" of first requests. We define as total service\n * time of an I/O request, the time that elapses since when the\n * request is enqueued into bfqq, to when it is completed. This\n * quantity allows the whole effect of injection to be measured. It is\n * easy to see why. Suppose that some requests of other queues are\n * actually injected while bfqq is empty, and that a new request R\n * then arrives for bfqq. If the device does start to serve all or\n * part of the injected requests during the service hole, then,\n * because of this extra service, it may delay the next invocation of\n * the dispatch hook of BFQ. Then, even after R gets eventually\n * dispatched, the device may delay the actual service of R if it is\n * still busy serving the extra requests, or if it decides to serve,\n * before R, some extra request still present in its queues. As a\n * conclusion, the cumulative extra delay caused by injection can be\n * easily evaluated by just comparing the total service time of first\n * requests with and without injection.\n *\n * The limit-update algorithm works as follows. On the arrival of a\n * first request of bfqq, the algorithm measures the total time of the\n * request only if one of the three cases below holds, and, for each\n * case, it updates the limit as described below:\n *\n * (1) If there is no in-flight request. This gives a baseline for the\n *     total service time of the requests of bfqq. If the baseline has\n *     not been computed yet, then, after computing it, the limit is\n *     set to 1, to start boosting throughput, and to prepare the\n *     ground for the next case. If the baseline has already been\n *     computed, then it is updated, in case it results to be lower\n *     than the previous value.\n *\n * (2) If the limit is higher than 0 and there are in-flight\n *     requests. By comparing the total service time in this case with\n *     the above baseline, it is possible to know at which extent the\n *     current value of the limit is inflating the total service\n *     time. If the inflation is below a certain threshold, then bfqq\n *     is assumed to be suffering from no perceivable loss of its\n *     service guarantees, and the limit is even tentatively\n *     increased. If the inflation is above the threshold, then the\n *     limit is decreased. Due to the lack of any hysteresis, this\n *     logic makes the limit oscillate even in steady workload\n *     conditions. Yet we opted for it, because it is fast in reaching\n *     the best value for the limit, as a function of the current I/O\n *     workload. To reduce oscillations, this step is disabled for a\n *     short time interval after the limit happens to be decreased.\n *\n * (3) Periodically, after resetting the limit, to make sure that the\n *     limit eventually drops in case the workload changes. This is\n *     needed because, after the limit has gone safely up for a\n *     certain workload, it is impossible to guess whether the\n *     baseline total service time may have changed, without measuring\n *     it again without injection. A more effective version of this\n *     step might be to just sample the baseline, by interrupting\n *     injection only once, and then to reset/lower the limit only if\n *     the total service time with the current limit does happen to be\n *     too large.\n *\n * More details on each step are provided in the comments on the\n * pieces of code that implement these steps: the branch handling the\n * transition from empty to non empty in bfq_add_request(), the branch\n * handling injection in bfq_select_queue(), and the function\n * bfq_choose_bfqq_for_injection(). These comments also explain some\n * exceptions, made by the injection mechanism in some special cases.\n */\nstatic void bfq_update_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tu64 tot_time_ns = ktime_get_ns() - bfqd->last_empty_occupied_ns;\n\tunsigned int old_limit = bfqq->inject_limit;\n\n\tif (bfqq->last_serv_time_ns > 0 && bfqd->rqs_injected) {\n\t\tu64 threshold = (bfqq->last_serv_time_ns * 3)>>1;\n\n\t\tif (tot_time_ns >= threshold && old_limit > 0) {\n\t\t\tbfqq->inject_limit--;\n\t\t\tbfqq->decrease_time_jif = jiffies;\n\t\t} else if (tot_time_ns < threshold &&\n\t\t\t   old_limit <= bfqd->max_rq_in_driver)\n\t\t\tbfqq->inject_limit++;\n\t}\n\n\t/*\n\t * Either we still have to compute the base value for the\n\t * total service time, and there seem to be the right\n\t * conditions to do it, or we can lower the last base value\n\t * computed.\n\t *\n\t * NOTE: (bfqd->rq_in_driver == 1) means that there is no I/O\n\t * request in flight, because this function is in the code\n\t * path that handles the completion of a request of bfqq, and,\n\t * in particular, this function is executed before\n\t * bfqd->rq_in_driver is decremented in such a code path.\n\t */\n\tif ((bfqq->last_serv_time_ns == 0 && bfqd->rq_in_driver == 1) ||\n\t    tot_time_ns < bfqq->last_serv_time_ns) {\n\t\tif (bfqq->last_serv_time_ns == 0) {\n\t\t\t/*\n\t\t\t * Now we certainly have a base value: make sure we\n\t\t\t * start trying injection.\n\t\t\t */\n\t\t\tbfqq->inject_limit = max_t(unsigned int, 1, old_limit);\n\t\t}\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\t} else if (!bfqd->rqs_injected && bfqd->rq_in_driver == 1)\n\t\t/*\n\t\t * No I/O injected and no request still in service in\n\t\t * the drive: these are the exact conditions for\n\t\t * computing the base value of the total service time\n\t\t * for bfqq. So let's update this value, because it is\n\t\t * rather variable. For example, it varies if the size\n\t\t * or the spatial locality of the I/O requests in bfqq\n\t\t * change.\n\t\t */\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\n\n\t/* update complete, not waiting for any request completion any longer */\n\tbfqd->waited_rq = NULL;\n\tbfqd->rqs_injected = false;\n}\n\n/*\n * Handle either a requeue or a finish for rq. The things to do are\n * the same in both cases: all references to rq are to be dropped. In\n * particular, rq is considered completed from the point of view of\n * the scheduler.\n */\nstatic void bfq_finish_requeue_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd;\n\tunsigned long flags;\n\n\t/*\n\t * rq either is not associated with any icq, or is an already\n\t * requeued request that has not (yet) been re-inserted into\n\t * a bfq_queue.\n\t */\n\tif (!rq->elv.icq || !bfqq)\n\t\treturn;\n\n\tbfqd = bfqq->bfqd;\n\n\tif (rq->rq_flags & RQF_STARTED)\n\t\tbfqg_stats_update_completion(bfqq_group(bfqq),\n\t\t\t\t\t     rq->start_time_ns,\n\t\t\t\t\t     rq->io_start_time_ns,\n\t\t\t\t\t     rq->cmd_flags);\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\tif (likely(rq->rq_flags & RQF_STARTED)) {\n\t\tif (rq == bfqd->waited_rq)\n\t\t\tbfq_update_inject_limit(bfqd, bfqq);\n\n\t\tbfq_completed_request(bfqq, bfqd);\n\t}\n\tbfq_finish_requeue_request_body(bfqq);\n\tRQ_BIC(rq)->requests--;\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\n\t/*\n\t * Reset private fields. In case of a requeue, this allows\n\t * this function to correctly do nothing if it is spuriously\n\t * invoked again on this same request (see the check at the\n\t * beginning of the function). Probably, a better general\n\t * design would be to prevent blk-mq from invoking the requeue\n\t * or finish hooks of an elevator, for a request that is not\n\t * referred by that elevator.\n\t *\n\t * Resetting the following fields would break the\n\t * request-insertion logic if rq is re-inserted into a bfq\n\t * internal queue, without a re-preparation. Here we assume\n\t * that re-insertions of requeued requests, without\n\t * re-preparation, can happen only for pass_through or at_head\n\t * requests (which are not re-inserted into bfq internal\n\t * queues).\n\t */\n\trq->elv.priv[0] = NULL;\n\trq->elv.priv[1] = NULL;\n}\n\n/*\n * Removes the association between the current task and bfqq, assuming\n * that bic points to the bfq iocontext of the task.\n * Returns NULL if a new bfqq should be allocated, or the old bfqq if this\n * was the last process referring to that bfqq.\n */\nstatic struct bfq_queue *\nbfq_split_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"splitting queue\");\n\n\tif (bfqq_process_refs(bfqq) == 1 && !bfqq->new_bfqq) {\n\t\tbfqq->pid = current->pid;\n\t\tbfq_clear_bfqq_coop(bfqq);\n\t\tbfq_clear_bfqq_split_coop(bfqq);\n\t\treturn bfqq;\n\t}\n\n\tbic_set_bfqq(bic, NULL, true);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqq->bfqd, bfqq);\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_get_bfqq_handle_split(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_io_cq *bic,\n\t\t\t\t\t\t   struct bio *bio,\n\t\t\t\t\t\t   bool split, bool is_sync,\n\t\t\t\t\t\t   bool *new_queue)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync);\n\n\tif (likely(bfqq && bfqq != &bfqd->oom_bfqq))\n\t\treturn bfqq;\n\n\tif (new_queue)\n\t\t*new_queue = true;\n\n\tif (bfqq)\n\t\tbfq_put_queue(bfqq);\n\tbfqq = bfq_get_queue(bfqd, bio, is_sync, bic, split);\n\n\tbic_set_bfqq(bic, bfqq, is_sync);\n\tif (split && is_sync) {\n\t\tif ((bic->was_in_burst_list && bfqd->large_burst) ||\n\t\t    bic->saved_in_large_burst)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\telse {\n\t\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t\t\tif (bic->was_in_burst_list)\n\t\t\t\t/*\n\t\t\t\t * If bfqq was in the current\n\t\t\t\t * burst list before being\n\t\t\t\t * merged, then we have to add\n\t\t\t\t * it back. And we do not need\n\t\t\t\t * to increase burst_size, as\n\t\t\t\t * we did not decrement\n\t\t\t\t * burst_size when we removed\n\t\t\t\t * bfqq from the burst list as\n\t\t\t\t * a consequence of a merge\n\t\t\t\t * (see comments in\n\t\t\t\t * bfq_put_queue). In this\n\t\t\t\t * respect, it would be rather\n\t\t\t\t * costly to know whether the\n\t\t\t\t * current burst list is still\n\t\t\t\t * the same burst list from\n\t\t\t\t * which bfqq was removed on\n\t\t\t\t * the merge. To avoid this\n\t\t\t\t * cost, if bfqq was in a\n\t\t\t\t * burst list, then we add\n\t\t\t\t * bfqq to the current burst\n\t\t\t\t * list without any further\n\t\t\t\t * check. This can cause\n\t\t\t\t * inappropriate insertions,\n\t\t\t\t * but rarely enough to not\n\t\t\t\t * harm the detection of large\n\t\t\t\t * bursts significantly.\n\t\t\t\t */\n\t\t\t\thlist_add_head(&bfqq->burst_list_node,\n\t\t\t\t\t       &bfqd->burst_list);\n\t\t}\n\t\tbfqq->split_time = jiffies;\n\t}\n\n\treturn bfqq;\n}\n\n/*\n * Only reset private fields. The actual request preparation will be\n * performed by bfq_init_rq, when rq is either inserted or merged. See\n * comments on bfq_init_rq for the reason behind this delayed\n * preparation.\n */\nstatic void bfq_prepare_request(struct request *rq)\n{\n\t/*\n\t * Regardless of whether we have an icq attached, we have to\n\t * clear the scheduler pointers, as they might point to\n\t * previously allocated bic/bfqq structs.\n\t */\n\trq->elv.priv[0] = rq->elv.priv[1] = NULL;\n}\n\nstatic struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\tstruct bfq_queue *waker_bfqq = bfqq->waker_bfqq;\n\n\tif (!waker_bfqq)\n\t\treturn NULL;\n\n\twhile (new_bfqq) {\n\t\tif (new_bfqq == waker_bfqq) {\n\t\t\t/*\n\t\t\t * If waker_bfqq is in the merge chain, and current\n\t\t\t * is the only procress.\n\t\t\t */\n\t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n\t\t\t\treturn NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t}\n\n\treturn waker_bfqq;\n}\n\n/*\n * If needed, init rq, allocate bfq data structures associated with\n * rq, and increment reference counters in the destination bfq_queue\n * for rq. Return the destination bfq_queue for rq, or NULL is rq is\n * not associated with any bfq_queue.\n *\n * This function is invoked by the functions that perform rq insertion\n * or merging. One may have expected the above preparation operations\n * to be performed in bfq_prepare_request, and not delayed to when rq\n * is inserted or merged. The rationale behind this delayed\n * preparation is that, after the prepare_request hook is invoked for\n * rq, rq may still be transformed into a request with no icq, i.e., a\n * request not associated with any queue. No bfq hook is invoked to\n * signal this transformation. As a consequence, should these\n * preparation operations be performed when the prepare_request hook\n * is invoked, and should rq be transformed one moment later, bfq\n * would end up in an inconsistent state, because it would have\n * incremented some queue counters for an rq destined to\n * transformation, without any chance to correctly lower these\n * counters back. In contrast, no transformation can still happen for\n * rq after rq has been inserted or merged. So, it is safe to execute\n * these preparation operations when rq is finally inserted or merged.\n */\nstatic struct bfq_queue *bfq_init_rq(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio = rq->bio;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic;\n\tconst int is_sync = rq_is_sync(rq);\n\tstruct bfq_queue *bfqq;\n\tbool new_queue = false;\n\tbool bfqq_already_existing = false, split = false;\n\n\tif (unlikely(!rq->elv.icq))\n\t\treturn NULL;\n\n\t/*\n\t * Assuming that elv.priv[1] is set only if everything is set\n\t * for this rq. This holds true, because this function is\n\t * invoked only for insertion or merging, and, after such\n\t * events, a request cannot be manipulated any longer before\n\t * being removed from bfq.\n\t */\n\tif (rq->elv.priv[1])\n\t\treturn rq->elv.priv[1];\n\n\tbic = icq_to_bic(rq->elv.icq);\n\n\tbfq_check_ioprio_change(bic, bio);\n\n\tbfq_bic_update_cgroup(bic, bio);\n\n\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync,\n\t\t\t\t\t &new_queue);\n\n\tif (likely(!new_queue)) {\n\t\t/* If the queue was seeky for too long, break it apart. */\n\t\tif (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) &&\n\t\t\t!bic->stably_merged) {\n\t\t\tstruct bfq_queue *waker_bfqq = bfq_waker_bfqq(bfqq);\n\n\t\t\t/* Update bic before losing reference to bfqq */\n\t\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\t\tbic->saved_in_large_burst = true;\n\n\t\t\tbfqq = bfq_split_bfqq(bic, bfqq);\n\t\t\tsplit = true;\n\n\t\t\tif (!bfqq) {\n\t\t\t\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio,\n\t\t\t\t\t\t\t\t true, is_sync,\n\t\t\t\t\t\t\t\t NULL);\n\t\t\t\tif (unlikely(bfqq == &bfqd->oom_bfqq))\n\t\t\t\t\tbfqq_already_existing = true;\n\t\t\t} else\n\t\t\t\tbfqq_already_existing = true;\n\n\t\t\tif (!bfqq_already_existing) {\n\t\t\t\tbfqq->waker_bfqq = waker_bfqq;\n\t\t\t\tbfqq->tentative_waker_bfqq = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * If the waker queue disappears, then\n\t\t\t\t * new_bfqq->waker_bfqq must be\n\t\t\t\t * reset. So insert new_bfqq into the\n\t\t\t\t * woken_list of the waker. See\n\t\t\t\t * bfq_check_waker for details.\n\t\t\t\t */\n\t\t\t\tif (waker_bfqq)\n\t\t\t\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t\t\t\t       &bfqq->waker_bfqq->woken_list);\n\t\t\t}\n\t\t}\n\t}\n\n\tbfqq->allocated++;\n\tbfqq->ref++;\n\tbic->requests++;\n\tbfq_log_bfqq(bfqd, bfqq, \"get_request %p: bfqq %p, %d\",\n\t\t     rq, bfqq, bfqq->ref);\n\n\trq->elv.priv[0] = bic;\n\trq->elv.priv[1] = bfqq;\n\n\t/*\n\t * If a bfq_queue has only one process reference, it is owned\n\t * by only this bic: we can then set bfqq->bic = bic. in\n\t * addition, if the queue has also just been split, we have to\n\t * resume its state.\n\t */\n\tif (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq &&\n\t    bfqq_process_refs(bfqq) == 1) {\n\t\tbfqq->bic = bic;\n\t\tif (split) {\n\t\t\t/*\n\t\t\t * The queue has just been split from a shared\n\t\t\t * queue: restore the idle window and the\n\t\t\t * possible weight raising period.\n\t\t\t */\n\t\t\tbfq_bfqq_resume_state(bfqq, bfqd, bic,\n\t\t\t\t\t      bfqq_already_existing);\n\t\t}\n\t}\n\n\t/*\n\t * Consider bfqq as possibly belonging to a burst of newly\n\t * created queues only if:\n\t * 1) A burst is actually happening (bfqd->burst_size > 0)\n\t * or\n\t * 2) There is no other active queue. In fact, if, in\n\t *    contrast, there are active queues not belonging to the\n\t *    possible burst bfqq may belong to, then there is no gain\n\t *    in considering bfqq as belonging to a burst, and\n\t *    therefore in not weight-raising bfqq. See comments on\n\t *    bfq_handle_burst().\n\t *\n\t * This filtering also helps eliminating false positives,\n\t * occurring when bfqq does not belong to an actual large\n\t * burst, but some background task (e.g., a service) happens\n\t * to trigger the creation of new queues very close to when\n\t * bfqq and its possible companion queues are created. See\n\t * comments on bfq_handle_burst() for further details also on\n\t * this issue.\n\t */\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     (bfqd->burst_size > 0 ||\n\t\t      bfq_tot_busy_queues(bfqd) == 0)))\n\t\tbfq_handle_burst(bfqd, bfqq);\n\n\treturn bfqq;\n}\n\nstatic void\nbfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\n\t/*\n\t * Considering that bfqq may be in race, we should firstly check\n\t * whether bfqq is in service before doing something on it. If\n\t * the bfqq in race is not in service, it has already been expired\n\t * through __bfq_bfqq_expire func and its wait_request flags has\n\t * been cleared in __bfq_bfqd_reset_in_service func.\n\t */\n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t/*\n\t\t * Also here the queue can be safely expired\n\t\t * for budget timeout without wasting\n\t\t * guarantees\n\t\t */\n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t/*\n\t\t * The queue may not be empty upon timer expiration,\n\t\t * because we may not disable the timer when the\n\t\t * first request of the in-service queue arrives\n\t\t * during disk idling.\n\t\t */\n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tbfq_schedule_dispatch(bfqd);\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n/*\n * Handler of the expiration of the timer running if the in-service queue\n * is idling inside its time slice.\n */\nstatic enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t/*\n\t * Theoretical race here: the in-service queue can be NULL or\n\t * different from the queue that was idling if a new request\n\t * arrives for the current queue and there is a full dispatch\n\t * cycle that changes the in-service queue.  This can hardly\n\t * happen, but in the worst case we just expire a queue too\n\t * early.\n\t */\n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __bfq_put_async_bfqq(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue **bfqq_ptr)\n{\n\tstruct bfq_queue *bfqq = *bfqq_ptr;\n\n\tbfq_log(bfqd, \"put_async_bfqq: %p\", bfqq);\n\tif (bfqq) {\n\t\tbfq_bfqq_move(bfqd, bfqq, bfqd->root_group);\n\n\t\tbfq_log_bfqq(bfqd, bfqq, \"put_async_bfqq: putting %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\tbfq_put_queue(bfqq);\n\t\t*bfqq_ptr = NULL;\n\t}\n}\n\n/*\n * Release all the bfqg references to its async queues.  If we are\n * deallocating the group these queues may still contain requests, so\n * we reparent them to the root cgroup (i.e., the only one that will\n * exist for sure until all the requests on a device are gone).\n */\nvoid bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 2; i++)\n\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_bfqq[i][j]);\n\n\t__bfq_put_async_bfqq(bfqd, &bfqg->async_idle_bfqq);\n}\n\n/*\n * See the comments on bfq_limit_depth for the purpose of\n * the depths set in the function. Return minimum shallow depth we'll use.\n */\nstatic unsigned int bfq_update_depths(struct bfq_data *bfqd,\n\t\t\t\t      struct sbitmap_queue *bt)\n{\n\tunsigned int i, j, min_shallow = UINT_MAX;\n\n\t/*\n\t * In-word depths if no bfq_queue is being weight-raised:\n\t * leaving 25% of tags only for sync reads.\n\t *\n\t * In next formulas, right-shift the value\n\t * (1U<<bt->sb.shift), instead of computing directly\n\t * (1U<<(bt->sb.shift - something)), to be robust against\n\t * any possible value of bt->sb.shift, without having to\n\t * limit 'something'.\n\t */\n\t/* no more than 50% of tags for async I/O */\n\tbfqd->word_depths[0][0] = max((1U << bt->sb.shift) >> 1, 1U);\n\t/*\n\t * no more than 75% of tags for sync writes (25% extra tags\n\t * w.r.t. async I/O, to prevent async I/O from starving sync\n\t * writes)\n\t */\n\tbfqd->word_depths[0][1] = max(((1U << bt->sb.shift) * 3) >> 2, 1U);\n\n\t/*\n\t * In-word depths in case some bfq_queue is being weight-\n\t * raised: leaving ~63% of tags for sync reads. This is the\n\t * highest percentage for which, in our tests, application\n\t * start-up times didn't suffer from any regression due to tag\n\t * shortage.\n\t */\n\t/* no more than ~18% of tags for async I/O */\n\tbfqd->word_depths[1][0] = max(((1U << bt->sb.shift) * 3) >> 4, 1U);\n\t/* no more than ~37% of tags for sync writes (~20% extra tags) */\n\tbfqd->word_depths[1][1] = max(((1U << bt->sb.shift) * 6) >> 4, 1U);\n\n\tfor (i = 0; i < 2; i++)\n\t\tfor (j = 0; j < 2; j++)\n\t\t\tmin_shallow = min(min_shallow, bfqd->word_depths[i][j]);\n\n\treturn min_shallow;\n}\n\nstatic void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\tunsigned int min_shallow;\n\n\tmin_shallow = bfq_update_depths(bfqd, tags->bitmap_tags);\n\tsbitmap_queue_min_shallow_depth(tags->bitmap_tags, min_shallow);\n}\n\nstatic int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)\n{\n\tbfq_depth_updated(hctx);\n\treturn 0;\n}\n\nstatic void bfq_exit_queue(struct elevator_queue *e)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tstruct bfq_queue *bfqq, *n;\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\tspin_lock_irq(&bfqd->lock);\n\tlist_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)\n\t\tbfq_deactivate_bfqq(bfqd, bfqq, false, false);\n\tspin_unlock_irq(&bfqd->lock);\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\t/* release oom-queue reference to root group */\n\tbfqg_and_blkg_put(bfqd->root_group);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_deactivate_policy(bfqd->queue, &blkcg_policy_bfq);\n#else\n\tspin_lock_irq(&bfqd->lock);\n\tbfq_put_async_queues(bfqd, bfqd->root_group);\n\tkfree(bfqd->root_group);\n\tspin_unlock_irq(&bfqd->lock);\n#endif\n\n\twbt_enable_default(bfqd->queue);\n\n\tkfree(bfqd);\n}\n\nstatic void bfq_init_root_group(struct bfq_group *root_group,\n\t\t\t\tstruct bfq_data *bfqd)\n{\n\tint i;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\troot_group->entity.parent = NULL;\n\troot_group->my_entity = NULL;\n\troot_group->bfqd = bfqd;\n#endif\n\troot_group->rq_pos_tree = RB_ROOT;\n\tfor (i = 0; i < BFQ_IOPRIO_CLASSES; i++)\n\t\troot_group->sched_data.service_tree[i] = BFQ_SERVICE_TREE_INIT;\n\troot_group->sched_data.bfq_class_idle_last_service = jiffies;\n}\n\nstatic int bfq_init_queue(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct bfq_data *bfqd;\n\tstruct elevator_queue *eq;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn -ENOMEM;\n\n\tbfqd = kzalloc_node(sizeof(*bfqd), GFP_KERNEL, q->node);\n\tif (!bfqd) {\n\t\tkobject_put(&eq->kobj);\n\t\treturn -ENOMEM;\n\t}\n\teq->elevator_data = bfqd;\n\n\tspin_lock_irq(&q->queue_lock);\n\tq->elevator = eq;\n\tspin_unlock_irq(&q->queue_lock);\n\n\t/*\n\t * Our fallback bfqq if bfq_find_alloc_queue() runs into OOM issues.\n\t * Grab a permanent reference to it, so that the normal code flow\n\t * will not attempt to free it.\n\t */\n\tbfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0);\n\tbfqd->oom_bfqq.ref++;\n\tbfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;\n\tbfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;\n\tbfqd->oom_bfqq.entity.new_weight =\n\t\tbfq_ioprio_to_weight(bfqd->oom_bfqq.new_ioprio);\n\n\t/* oom_bfqq does not participate to bursts */\n\tbfq_clear_bfqq_just_created(&bfqd->oom_bfqq);\n\n\t/*\n\t * Trigger weight initialization, according to ioprio, at the\n\t * oom_bfqq's first activation. The oom_bfqq's ioprio and ioprio\n\t * class won't be changed any more.\n\t */\n\tbfqd->oom_bfqq.entity.prio_changed = 1;\n\n\tbfqd->queue = q;\n\n\tINIT_LIST_HEAD(&bfqd->dispatch);\n\n\thrtimer_init(&bfqd->idle_slice_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL);\n\tbfqd->idle_slice_timer.function = bfq_idle_slice_timer;\n\n\tbfqd->queue_weights_tree = RB_ROOT_CACHED;\n\tbfqd->num_groups_with_pending_reqs = 0;\n\n\tINIT_LIST_HEAD(&bfqd->active_list);\n\tINIT_LIST_HEAD(&bfqd->idle_list);\n\tINIT_HLIST_HEAD(&bfqd->burst_list);\n\n\tbfqd->hw_tag = -1;\n\tbfqd->nonrot_with_queueing = blk_queue_nonrot(bfqd->queue);\n\n\tbfqd->bfq_max_budget = bfq_default_max_budget;\n\n\tbfqd->bfq_fifo_expire[0] = bfq_fifo_expire[0];\n\tbfqd->bfq_fifo_expire[1] = bfq_fifo_expire[1];\n\tbfqd->bfq_back_max = bfq_back_max;\n\tbfqd->bfq_back_penalty = bfq_back_penalty;\n\tbfqd->bfq_slice_idle = bfq_slice_idle;\n\tbfqd->bfq_timeout = bfq_timeout;\n\n\tbfqd->bfq_large_burst_thresh = 8;\n\tbfqd->bfq_burst_interval = msecs_to_jiffies(180);\n\n\tbfqd->low_latency = true;\n\n\t/*\n\t * Trade-off between responsiveness and fairness.\n\t */\n\tbfqd->bfq_wr_coeff = 30;\n\tbfqd->bfq_wr_rt_max_time = msecs_to_jiffies(300);\n\tbfqd->bfq_wr_max_time = 0;\n\tbfqd->bfq_wr_min_idle_time = msecs_to_jiffies(2000);\n\tbfqd->bfq_wr_min_inter_arr_async = msecs_to_jiffies(500);\n\tbfqd->bfq_wr_max_softrt_rate = 7000; /*\n\t\t\t\t\t      * Approximate rate required\n\t\t\t\t\t      * to playback or record a\n\t\t\t\t\t      * high-definition compressed\n\t\t\t\t\t      * video.\n\t\t\t\t\t      */\n\tbfqd->wr_busy_queues = 0;\n\n\t/*\n\t * Begin by assuming, optimistically, that the device peak\n\t * rate is equal to 2/3 of the highest reference rate.\n\t */\n\tbfqd->rate_dur_prod = ref_rate[blk_queue_nonrot(bfqd->queue)] *\n\t\tref_wr_duration[blk_queue_nonrot(bfqd->queue)];\n\tbfqd->peak_rate = ref_rate[blk_queue_nonrot(bfqd->queue)] * 2 / 3;\n\n\tspin_lock_init(&bfqd->lock);\n\n\t/*\n\t * The invocation of the next bfq_create_group_hierarchy\n\t * function is the head of a chain of function calls\n\t * (bfq_create_group_hierarchy->blkcg_activate_policy->\n\t * blk_mq_freeze_queue) that may lead to the invocation of the\n\t * has_work hook function. For this reason,\n\t * bfq_create_group_hierarchy is invoked only after all\n\t * scheduler data has been initialized, apart from the fields\n\t * that can be initialized only after invoking\n\t * bfq_create_group_hierarchy. This, in particular, enables\n\t * has_work to correctly return false. Of course, to avoid\n\t * other inconsistencies, the blk-mq stack must then refrain\n\t * from invoking further scheduler hooks before this init\n\t * function is finished.\n\t */\n\tbfqd->root_group = bfq_create_group_hierarchy(bfqd, q->node);\n\tif (!bfqd->root_group)\n\t\tgoto out_free;\n\tbfq_init_root_group(bfqd->root_group, bfqd);\n\tbfq_init_entity(&bfqd->oom_bfqq.entity, bfqd->root_group);\n\n\twbt_disable_default(q);\n\treturn 0;\n\nout_free:\n\tkfree(bfqd);\n\tkobject_put(&eq->kobj);\n\treturn -ENOMEM;\n}\n\nstatic void bfq_slab_kill(void)\n{\n\tkmem_cache_destroy(bfq_pool);\n}\n\nstatic int __init bfq_slab_setup(void)\n{\n\tbfq_pool = KMEM_CACHE(bfq_queue, 0);\n\tif (!bfq_pool)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic ssize_t bfq_var_show(unsigned int var, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", var);\n}\n\nstatic int bfq_var_store(unsigned long *var, const char *page)\n{\n\tunsigned long new_val;\n\tint ret = kstrtoul(page, 10, &new_val);\n\n\tif (ret)\n\t\treturn ret;\n\t*var = new_val;\n\treturn 0;\n}\n\n#define SHOW_FUNCTION(__FUNC, __VAR, __CONV)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t__data = jiffies_to_msecs(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t__data = div_u64(__data, NSEC_PER_MSEC);\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nSHOW_FUNCTION(bfq_fifo_expire_sync_show, bfqd->bfq_fifo_expire[1], 2);\nSHOW_FUNCTION(bfq_fifo_expire_async_show, bfqd->bfq_fifo_expire[0], 2);\nSHOW_FUNCTION(bfq_back_seek_max_show, bfqd->bfq_back_max, 0);\nSHOW_FUNCTION(bfq_back_seek_penalty_show, bfqd->bfq_back_penalty, 0);\nSHOW_FUNCTION(bfq_slice_idle_show, bfqd->bfq_slice_idle, 2);\nSHOW_FUNCTION(bfq_max_budget_show, bfqd->bfq_user_max_budget, 0);\nSHOW_FUNCTION(bfq_timeout_sync_show, bfqd->bfq_timeout, 1);\nSHOW_FUNCTION(bfq_strict_guarantees_show, bfqd->strict_guarantees, 0);\nSHOW_FUNCTION(bfq_low_latency_show, bfqd->low_latency, 0);\n#undef SHOW_FUNCTION\n\n#define USEC_SHOW_FUNCTION(__FUNC, __VAR)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\t__data = div_u64(__data, NSEC_PER_USEC);\t\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nUSEC_SHOW_FUNCTION(bfq_slice_idle_us_show, bfqd->bfq_slice_idle);\n#undef USEC_SHOW_FUNCTION\n\n#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX, __CONV)\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n__FUNC(struct elevator_queue *e, const char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t*(__PTR) = msecs_to_jiffies(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t*(__PTR) = (u64)__data * NSEC_PER_MSEC;\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t*(__PTR) = __data;\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nSTORE_FUNCTION(bfq_fifo_expire_sync_store, &bfqd->bfq_fifo_expire[1], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_fifo_expire_async_store, &bfqd->bfq_fifo_expire[0], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_back_seek_max_store, &bfqd->bfq_back_max, 0, INT_MAX, 0);\nSTORE_FUNCTION(bfq_back_seek_penalty_store, &bfqd->bfq_back_penalty, 1,\n\t\tINT_MAX, 0);\nSTORE_FUNCTION(bfq_slice_idle_store, &bfqd->bfq_slice_idle, 0, INT_MAX, 2);\n#undef STORE_FUNCTION\n\n#define USEC_STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\t*(__PTR) = (u64)__data * NSEC_PER_USEC;\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nUSEC_STORE_FUNCTION(bfq_slice_idle_us_store, &bfqd->bfq_slice_idle, 0,\n\t\t    UINT_MAX);\n#undef USEC_STORE_FUNCTION\n\nstatic ssize_t bfq_max_budget_store(struct elevator_queue *e,\n\t\t\t\t    const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\telse {\n\t\tif (__data > INT_MAX)\n\t\t\t__data = INT_MAX;\n\t\tbfqd->bfq_max_budget = __data;\n\t}\n\n\tbfqd->bfq_user_max_budget = __data;\n\n\treturn count;\n}\n\n/*\n * Leaving this name to preserve name compatibility with cfq\n * parameters, but this timeout is used for both sync and async.\n */\nstatic ssize_t bfq_timeout_sync_store(struct elevator_queue *e,\n\t\t\t\t      const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data < 1)\n\t\t__data = 1;\n\telse if (__data > INT_MAX)\n\t\t__data = INT_MAX;\n\n\tbfqd->bfq_timeout = msecs_to_jiffies(__data);\n\tif (bfqd->bfq_user_max_budget == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\n\treturn count;\n}\n\nstatic ssize_t bfq_strict_guarantees_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (!bfqd->strict_guarantees && __data == 1\n\t    && bfqd->bfq_slice_idle < 8 * NSEC_PER_MSEC)\n\t\tbfqd->bfq_slice_idle = 8 * NSEC_PER_MSEC;\n\n\tbfqd->strict_guarantees = __data;\n\n\treturn count;\n}\n\nstatic ssize_t bfq_low_latency_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (__data == 0 && bfqd->low_latency != 0)\n\t\tbfq_end_wr(bfqd);\n\tbfqd->low_latency = __data;\n\n\treturn count;\n}\n\n#define BFQ_ATTR(name) \\\n\t__ATTR(name, 0644, bfq_##name##_show, bfq_##name##_store)\n\nstatic struct elv_fs_entry bfq_attrs[] = {\n\tBFQ_ATTR(fifo_expire_sync),\n\tBFQ_ATTR(fifo_expire_async),\n\tBFQ_ATTR(back_seek_max),\n\tBFQ_ATTR(back_seek_penalty),\n\tBFQ_ATTR(slice_idle),\n\tBFQ_ATTR(slice_idle_us),\n\tBFQ_ATTR(max_budget),\n\tBFQ_ATTR(timeout_sync),\n\tBFQ_ATTR(strict_guarantees),\n\tBFQ_ATTR(low_latency),\n\t__ATTR_NULL\n};\n\nstatic struct elevator_type iosched_bfq_mq = {\n\t.ops = {\n\t\t.limit_depth\t\t= bfq_limit_depth,\n\t\t.prepare_request\t= bfq_prepare_request,\n\t\t.requeue_request        = bfq_finish_requeue_request,\n\t\t.finish_request\t\t= bfq_finish_requeue_request,\n\t\t.exit_icq\t\t= bfq_exit_icq,\n\t\t.insert_requests\t= bfq_insert_requests,\n\t\t.dispatch_request\t= bfq_dispatch_request,\n\t\t.next_request\t\t= elv_rb_latter_request,\n\t\t.former_request\t\t= elv_rb_former_request,\n\t\t.allow_merge\t\t= bfq_allow_bio_merge,\n\t\t.bio_merge\t\t= bfq_bio_merge,\n\t\t.request_merge\t\t= bfq_request_merge,\n\t\t.requests_merged\t= bfq_requests_merged,\n\t\t.request_merged\t\t= bfq_request_merged,\n\t\t.has_work\t\t= bfq_has_work,\n\t\t.depth_updated\t\t= bfq_depth_updated,\n\t\t.init_hctx\t\t= bfq_init_hctx,\n\t\t.init_sched\t\t= bfq_init_queue,\n\t\t.exit_sched\t\t= bfq_exit_queue,\n\t},\n\n\t.icq_size =\t\tsizeof(struct bfq_io_cq),\n\t.icq_align =\t\t__alignof__(struct bfq_io_cq),\n\t.elevator_attrs =\tbfq_attrs,\n\t.elevator_name =\t\"bfq\",\n\t.elevator_owner =\tTHIS_MODULE,\n};\nMODULE_ALIAS(\"bfq-iosched\");\n\nstatic int __init bfq_init(void)\n{\n\tint ret;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tret = blkcg_policy_register(&blkcg_policy_bfq);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = -ENOMEM;\n\tif (bfq_slab_setup())\n\t\tgoto err_pol_unreg;\n\n\t/*\n\t * Times to load large popular applications for the typical\n\t * systems installed on the reference devices (see the\n\t * comments before the definition of the next\n\t * array). Actually, we use slightly lower values, as the\n\t * estimated peak rate tends to be smaller than the actual\n\t * peak rate.  The reason for this last fact is that estimates\n\t * are computed over much shorter time intervals than the long\n\t * intervals typically used for benchmarking. Why? First, to\n\t * adapt more quickly to variations. Second, because an I/O\n\t * scheduler cannot rely on a peak-rate-evaluation workload to\n\t * be run for a long time.\n\t */\n\tref_wr_duration[0] = msecs_to_jiffies(7000); /* actually 8 sec */\n\tref_wr_duration[1] = msecs_to_jiffies(2500); /* actually 3 sec */\n\n\tret = elv_register(&iosched_bfq_mq);\n\tif (ret)\n\t\tgoto slab_kill;\n\n\treturn 0;\n\nslab_kill:\n\tbfq_slab_kill();\nerr_pol_unreg:\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\treturn ret;\n}\n\nstatic void __exit bfq_exit(void)\n{\n\telv_unregister(&iosched_bfq_mq);\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\tbfq_slab_kill();\n}\n\nmodule_init(bfq_init);\nmodule_exit(bfq_exit);\n\nMODULE_AUTHOR(\"Paolo Valente\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MQ Budget Fair Queueing I/O Scheduler\");\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From fcede1f0a043ccefe9bc6ad57f12718e42f63f1d Mon Sep 17 00:00:00 2001\nFrom: Yu Kuai <yukuai3@huawei.com>\nDate: Wed, 8 Jan 2025 16:41:48 +0800\nSubject: [PATCH] block, bfq: fix waker_bfqq UAF after bfq_split_bfqq()\n\nOur syzkaller report a following UAF for v6.6:\n\nBUG: KASAN: slab-use-after-free in bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\nRead of size 8 at addr ffff8881b57147d8 by task fsstress/232726\n\nCPU: 2 PID: 232726 Comm: fsstress Not tainted 6.6.0-g3629d1885222 #39\nCall Trace:\n <TASK>\n __dump_stack lib/dump_stack.c:88 [inline]\n dump_stack_lvl+0x91/0xf0 lib/dump_stack.c:106\n print_address_description.constprop.0+0x66/0x300 mm/kasan/report.c:364\n print_report+0x3e/0x70 mm/kasan/report.c:475\n kasan_report+0xb8/0xf0 mm/kasan/report.c:588\n hlist_add_head include/linux/list.h:1023 [inline]\n bfq_init_rq+0x175d/0x17a0 block/bfq-iosched.c:6958\n bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n __ext4_read_bh fs/ext4/super.c:205 [inline]\n ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n __do_sys_ioctl fs/ioctl.c:869 [inline]\n __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n entry_SYSCALL_64_after_hwframe+0x78/0xe2\n\nAllocated by task 232719:\n kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n __kasan_slab_alloc+0x87/0x90 mm/kasan/common.c:328\n kasan_slab_alloc include/linux/kasan.h:188 [inline]\n slab_post_alloc_hook mm/slab.h:768 [inline]\n slab_alloc_node mm/slub.c:3492 [inline]\n kmem_cache_alloc_node+0x1b8/0x6f0 mm/slub.c:3537\n bfq_get_queue+0x215/0x1f00 block/bfq-iosched.c:5869\n bfq_get_bfqq_handle_split+0x167/0x5f0 block/bfq-iosched.c:6776\n bfq_init_rq+0x13a4/0x17a0 block/bfq-iosched.c:6938\n bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n __ext4_read_bh fs/ext4/super.c:205 [inline]\n ext4_read_bh_nowait+0x15a/0x240 fs/ext4/super.c:217\n ext4_read_bh_lock+0xac/0xd0 fs/ext4/super.c:242\n ext4_bread_batch+0x268/0x500 fs/ext4/inode.c:958\n __ext4_find_entry+0x448/0x10f0 fs/ext4/namei.c:1671\n ext4_lookup_entry fs/ext4/namei.c:1774 [inline]\n ext4_lookup.part.0+0x359/0x6f0 fs/ext4/namei.c:1842\n ext4_lookup+0x72/0x90 fs/ext4/namei.c:1839\n __lookup_slow+0x257/0x480 fs/namei.c:1696\n lookup_slow fs/namei.c:1713 [inline]\n walk_component+0x454/0x5c0 fs/namei.c:2004\n link_path_walk.part.0+0x773/0xda0 fs/namei.c:2331\n link_path_walk fs/namei.c:3826 [inline]\n path_openat+0x1b9/0x520 fs/namei.c:3826\n do_filp_open+0x1b7/0x400 fs/namei.c:3857\n do_sys_openat2+0x5dc/0x6e0 fs/open.c:1428\n do_sys_open fs/open.c:1443 [inline]\n __do_sys_openat fs/open.c:1459 [inline]\n __se_sys_openat fs/open.c:1454 [inline]\n __x64_sys_openat+0x148/0x200 fs/open.c:1454\n do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n entry_SYSCALL_64_after_hwframe+0x78/0xe2\n\nFreed by task 232726:\n kasan_save_stack+0x22/0x50 mm/kasan/common.c:45\n kasan_set_track+0x25/0x30 mm/kasan/common.c:52\n kasan_save_free_info+0x2b/0x50 mm/kasan/generic.c:522\n ____kasan_slab_free mm/kasan/common.c:236 [inline]\n __kasan_slab_free+0x12a/0x1b0 mm/kasan/common.c:244\n kasan_slab_free include/linux/kasan.h:164 [inline]\n slab_free_hook mm/slub.c:1827 [inline]\n slab_free_freelist_hook mm/slub.c:1853 [inline]\n slab_free mm/slub.c:3820 [inline]\n kmem_cache_free+0x110/0x760 mm/slub.c:3842\n bfq_put_queue+0x6a7/0xfb0 block/bfq-iosched.c:5428\n bfq_forget_entity block/bfq-wf2q.c:634 [inline]\n bfq_put_idle_entity+0x142/0x240 block/bfq-wf2q.c:645\n bfq_forget_idle+0x189/0x1e0 block/bfq-wf2q.c:671\n bfq_update_vtime block/bfq-wf2q.c:1280 [inline]\n __bfq_lookup_next_entity block/bfq-wf2q.c:1374 [inline]\n bfq_lookup_next_entity+0x350/0x480 block/bfq-wf2q.c:1433\n bfq_update_next_in_service+0x1c0/0x4f0 block/bfq-wf2q.c:128\n bfq_deactivate_entity+0x10a/0x240 block/bfq-wf2q.c:1188\n bfq_deactivate_bfqq block/bfq-wf2q.c:1592 [inline]\n bfq_del_bfqq_busy+0x2e8/0xad0 block/bfq-wf2q.c:1659\n bfq_release_process_ref+0x1cc/0x220 block/bfq-iosched.c:3139\n bfq_split_bfqq+0x481/0xdf0 block/bfq-iosched.c:6754\n bfq_init_rq+0xf29/0x17a0 block/bfq-iosched.c:6934\n bfq_insert_request.isra.0+0xe8/0xa20 block/bfq-iosched.c:6271\n bfq_insert_requests+0x27f/0x390 block/bfq-iosched.c:6323\n blk_mq_insert_request+0x290/0x8f0 block/blk-mq.c:2660\n blk_mq_submit_bio+0x1021/0x15e0 block/blk-mq.c:3143\n __submit_bio+0xa0/0x6b0 block/blk-core.c:639\n __submit_bio_noacct_mq block/blk-core.c:718 [inline]\n submit_bio_noacct_nocheck+0x5b7/0x810 block/blk-core.c:747\n submit_bio_noacct+0xca0/0x1990 block/blk-core.c:847\n __ext4_read_bh fs/ext4/super.c:205 [inline]\n ext4_read_bh+0x15e/0x2e0 fs/ext4/super.c:230\n __read_extent_tree_block+0x304/0x6f0 fs/ext4/extents.c:567\n ext4_find_extent+0x479/0xd20 fs/ext4/extents.c:947\n ext4_ext_map_blocks+0x1a3/0x2680 fs/ext4/extents.c:4182\n ext4_map_blocks+0x929/0x15a0 fs/ext4/inode.c:660\n ext4_iomap_begin_report+0x298/0x480 fs/ext4/inode.c:3569\n iomap_iter+0x3dd/0x1010 fs/iomap/iter.c:91\n iomap_fiemap+0x1f4/0x360 fs/iomap/fiemap.c:80\n ext4_fiemap+0x181/0x210 fs/ext4/extents.c:5051\n ioctl_fiemap.isra.0+0x1b4/0x290 fs/ioctl.c:220\n do_vfs_ioctl+0x31c/0x11a0 fs/ioctl.c:811\n __do_sys_ioctl fs/ioctl.c:869 [inline]\n __se_sys_ioctl+0xae/0x190 fs/ioctl.c:857\n do_syscall_x64 arch/x86/entry/common.c:51 [inline]\n do_syscall_64+0x70/0x120 arch/x86/entry/common.c:81\n entry_SYSCALL_64_after_hwframe+0x78/0xe2\n\ncommit 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after\nsplitting\") fix the problem that if waker_bfqq is in the merge chain,\nand current is the only procress, waker_bfqq can be freed from\nbfq_split_bfqq(). However, the case that waker_bfqq is not in the merge\nchain is missed, and if the procress reference of waker_bfqq is 0,\nwaker_bfqq can be freed as well.\n\nFix the problem by checking procress reference if waker_bfqq is not in\nthe merge_chain.\n\nFixes: 1ba0403ac644 (\"block, bfq: fix uaf for accessing waker_bfqq after splitting\")\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yu Kuai <yukuai3@huawei.com>\nReviewed-by: Jan Kara <jack@suse.cz>\nLink: https://lore.kernel.org/r/20250108084148.1549973-1-yukuai1@huaweicloud.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\n---\n block/bfq-iosched.c | 12 ++++++++++--\n 1 file changed, 10 insertions(+), 2 deletions(-)\n\ndiff --git a/block/bfq-iosched.c b/block/bfq-iosched.c\nindex 95dd7b795935..cad16c163611 100644\n--- a/block/bfq-iosched.c\n+++ b/block/bfq-iosched.c\n@@ -6844,16 +6844,24 @@ static struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n \t\tif (new_bfqq == waker_bfqq) {\n \t\t\t/*\n \t\t\t * If waker_bfqq is in the merge chain, and current\n-\t\t\t * is the only procress.\n+\t\t\t * is the only process, waker_bfqq can be freed.\n \t\t\t */\n \t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n \t\t\t\treturn NULL;\n-\t\t\tbreak;\n+\n+\t\t\treturn waker_bfqq;\n \t\t}\n \n \t\tnew_bfqq = new_bfqq->new_bfqq;\n \t}\n \n+\t/*\n+\t * If waker_bfqq is not in the merge chain, and it's procress reference\n+\t * is 0, waker_bfqq can be freed.\n+\t */\n+\tif (bfqq_process_refs(waker_bfqq) == 0)\n+\t\treturn NULL;\n+\n \treturn waker_bfqq;\n }\n \n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "block/bfq-iosched.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Budget Fair Queueing (BFQ) I/O scheduler.\n *\n * Based on ideas and code from CFQ:\n * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk>\n *\n * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it>\n *\t\t      Paolo Valente <paolo.valente@unimore.it>\n *\n * Copyright (C) 2010 Paolo Valente <paolo.valente@unimore.it>\n *                    Arianna Avanzini <avanzini@google.com>\n *\n * Copyright (C) 2017 Paolo Valente <paolo.valente@linaro.org>\n *\n * BFQ is a proportional-share I/O scheduler, with some extra\n * low-latency capabilities. BFQ also supports full hierarchical\n * scheduling through cgroups. Next paragraphs provide an introduction\n * on BFQ inner workings. Details on BFQ benefits, usage and\n * limitations can be found in Documentation/block/bfq-iosched.rst.\n *\n * BFQ is a proportional-share storage-I/O scheduling algorithm based\n * on the slice-by-slice service scheme of CFQ. But BFQ assigns\n * budgets, measured in number of sectors, to processes instead of\n * time slices. The device is not granted to the in-service process\n * for a given time slice, but until it has exhausted its assigned\n * budget. This change from the time to the service domain enables BFQ\n * to distribute the device throughput among processes as desired,\n * without any distortion due to throughput fluctuations, or to device\n * internal queueing. BFQ uses an ad hoc internal scheduler, called\n * B-WF2Q+, to schedule processes according to their budgets. More\n * precisely, BFQ schedules queues associated with processes. Each\n * process/queue is assigned a user-configurable weight, and B-WF2Q+\n * guarantees that each queue receives a fraction of the throughput\n * proportional to its weight. Thanks to the accurate policy of\n * B-WF2Q+, BFQ can afford to assign high budgets to I/O-bound\n * processes issuing sequential requests (to boost the throughput),\n * and yet guarantee a low latency to interactive and soft real-time\n * applications.\n *\n * In particular, to provide these low-latency guarantees, BFQ\n * explicitly privileges the I/O of two classes of time-sensitive\n * applications: interactive and soft real-time. In more detail, BFQ\n * behaves this way if the low_latency parameter is set (default\n * configuration). This feature enables BFQ to provide applications in\n * these classes with a very low latency.\n *\n * To implement this feature, BFQ constantly tries to detect whether\n * the I/O requests in a bfq_queue come from an interactive or a soft\n * real-time application. For brevity, in these cases, the queue is\n * said to be interactive or soft real-time. In both cases, BFQ\n * privileges the service of the queue, over that of non-interactive\n * and non-soft-real-time queues. This privileging is performed,\n * mainly, by raising the weight of the queue. So, for brevity, we\n * call just weight-raising periods the time periods during which a\n * queue is privileged, because deemed interactive or soft real-time.\n *\n * The detection of soft real-time queues/applications is described in\n * detail in the comments on the function\n * bfq_bfqq_softrt_next_start. On the other hand, the detection of an\n * interactive queue works as follows: a queue is deemed interactive\n * if it is constantly non empty only for a limited time interval,\n * after which it does become empty. The queue may be deemed\n * interactive again (for a limited time), if it restarts being\n * constantly non empty, provided that this happens only after the\n * queue has remained empty for a given minimum idle time.\n *\n * By default, BFQ computes automatically the above maximum time\n * interval, i.e., the time interval after which a constantly\n * non-empty queue stops being deemed interactive. Since a queue is\n * weight-raised while it is deemed interactive, this maximum time\n * interval happens to coincide with the (maximum) duration of the\n * weight-raising for interactive queues.\n *\n * Finally, BFQ also features additional heuristics for\n * preserving both a low latency and a high throughput on NCQ-capable,\n * rotational or flash-based devices, and to get the job done quickly\n * for applications consisting in many I/O-bound processes.\n *\n * NOTE: if the main or only goal, with a given device, is to achieve\n * the maximum-possible throughput at all times, then do switch off\n * all low-latency heuristics for that device, by setting low_latency\n * to 0.\n *\n * BFQ is described in [1], where also a reference to the initial,\n * more theoretical paper on BFQ can be found. The interested reader\n * can find in the latter paper full details on the main algorithm, as\n * well as formulas of the guarantees and formal proofs of all the\n * properties.  With respect to the version of BFQ presented in these\n * papers, this implementation adds a few more heuristics, such as the\n * ones that guarantee a low latency to interactive and soft real-time\n * applications, and a hierarchical extension based on H-WF2Q+.\n *\n * B-WF2Q+ is based on WF2Q+, which is described in [2], together with\n * H-WF2Q+, while the augmented tree used here to implement B-WF2Q+\n * with O(log N) complexity derives from the one introduced with EEVDF\n * in [3].\n *\n * [1] P. Valente, A. Avanzini, \"Evolution of the BFQ Storage I/O\n *     Scheduler\", Proceedings of the First Workshop on Mobile System\n *     Technologies (MST-2015), May 2015.\n *     http://algogroup.unimore.it/people/paolo/disk_sched/mst-2015.pdf\n *\n * [2] Jon C.R. Bennett and H. Zhang, \"Hierarchical Packet Fair Queueing\n *     Algorithms\", IEEE/ACM Transactions on Networking, 5(5):675-689,\n *     Oct 1997.\n *\n * http://www.cs.cmu.edu/~hzhang/papers/TON-97-Oct.ps.gz\n *\n * [3] I. Stoica and H. Abdel-Wahab, \"Earliest Eligible Virtual Deadline\n *     First: A Flexible and Accurate Mechanism for Proportional Share\n *     Resource Allocation\", technical report.\n *\n * http://www.cs.berkeley.edu/~istoica/papers/eevdf-tr-95.pdf\n */\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/cgroup.h>\n#include <linux/ktime.h>\n#include <linux/rbtree.h>\n#include <linux/ioprio.h>\n#include <linux/sbitmap.h>\n#include <linux/delay.h>\n#include <linux/backing-dev.h>\n\n#include <trace/events/block.h>\n\n#include \"elevator.h\"\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-tag.h\"\n#include \"blk-mq-sched.h\"\n#include \"bfq-iosched.h\"\n#include \"blk-wbt.h\"\n\n#define BFQ_BFQQ_FNS(name)\t\t\t\t\t\t\\\nvoid bfq_mark_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__set_bit(BFQQF_##name, &(bfqq)->flags);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nvoid bfq_clear_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__clear_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nint bfq_bfqq_##name(const struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\n\nBFQ_BFQQ_FNS(just_created);\nBFQ_BFQQ_FNS(busy);\nBFQ_BFQQ_FNS(wait_request);\nBFQ_BFQQ_FNS(non_blocking_wait_rq);\nBFQ_BFQQ_FNS(fifo_expire);\nBFQ_BFQQ_FNS(has_short_ttime);\nBFQ_BFQQ_FNS(sync);\nBFQ_BFQQ_FNS(IO_bound);\nBFQ_BFQQ_FNS(in_large_burst);\nBFQ_BFQQ_FNS(coop);\nBFQ_BFQQ_FNS(split_coop);\nBFQ_BFQQ_FNS(softrt_update);\n#undef BFQ_BFQQ_FNS\t\t\t\t\t\t\\\n\n/* Expiration time of async (0) and sync (1) requests, in ns. */\nstatic const u64 bfq_fifo_expire[2] = { NSEC_PER_SEC / 4, NSEC_PER_SEC / 8 };\n\n/* Maximum backwards seek (magic number lifted from CFQ), in KiB. */\nstatic const int bfq_back_max = 16 * 1024;\n\n/* Penalty of a backwards seek, in number of sectors. */\nstatic const int bfq_back_penalty = 2;\n\n/* Idling period duration, in ns. */\nstatic u64 bfq_slice_idle = NSEC_PER_SEC / 125;\n\n/* Minimum number of assigned budgets for which stats are safe to compute. */\nstatic const int bfq_stats_min_budgets = 194;\n\n/* Default maximum budget values, in sectors and number of requests. */\nstatic const int bfq_default_max_budget = 16 * 1024;\n\n/*\n * When a sync request is dispatched, the queue that contains that\n * request, and all the ancestor entities of that queue, are charged\n * with the number of sectors of the request. In contrast, if the\n * request is async, then the queue and its ancestor entities are\n * charged with the number of sectors of the request, multiplied by\n * the factor below. This throttles the bandwidth for async I/O,\n * w.r.t. to sync I/O, and it is done to counter the tendency of async\n * writes to steal I/O throughput to reads.\n *\n * The current value of this parameter is the result of a tuning with\n * several hardware and software configurations. We tried to find the\n * lowest value for which writes do not cause noticeable problems to\n * reads. In fact, the lower this parameter, the stabler I/O control,\n * in the following respect.  The lower this parameter is, the less\n * the bandwidth enjoyed by a group decreases\n * - when the group does writes, w.r.t. to when it does reads;\n * - when other groups do reads, w.r.t. to when they do writes.\n */\nstatic const int bfq_async_charge_factor = 3;\n\n/* Default timeout values, in jiffies, approximating CFQ defaults. */\nconst int bfq_timeout = HZ / 8;\n\n/*\n * Time limit for merging (see comments in bfq_setup_cooperator). Set\n * to the slowest value that, in our tests, proved to be effective in\n * removing false positives, while not causing true positives to miss\n * queue merging.\n *\n * As can be deduced from the low time limit below, queue merging, if\n * successful, happens at the very beginning of the I/O of the involved\n * cooperating processes, as a consequence of the arrival of the very\n * first requests from each cooperator.  After that, there is very\n * little chance to find cooperators.\n */\nstatic const unsigned long bfq_merge_time_limit = HZ/10;\n\nstatic struct kmem_cache *bfq_pool;\n\n/* Below this threshold (in ns), we consider thinktime immediate. */\n#define BFQ_MIN_TT\t\t(2 * NSEC_PER_MSEC)\n\n/* hw_tag detection: parallel requests threshold and min samples needed. */\n#define BFQ_HW_QUEUE_THRESHOLD\t3\n#define BFQ_HW_QUEUE_SAMPLES\t32\n\n#define BFQQ_SEEK_THR\t\t(sector_t)(8 * 100)\n#define BFQQ_SECT_THR_NONROT\t(sector_t)(2 * 32)\n#define BFQ_RQ_SEEKY(bfqd, last_pos, rq) \\\n\t(get_sdist(last_pos, rq) >\t\t\t\\\n\t BFQQ_SEEK_THR &&\t\t\t\t\\\n\t (!blk_queue_nonrot(bfqd->queue) ||\t\t\\\n\t  blk_rq_sectors(rq) < BFQQ_SECT_THR_NONROT))\n#define BFQQ_CLOSE_THR\t\t(sector_t)(8 * 1024)\n#define BFQQ_SEEKY(bfqq)\t(hweight32(bfqq->seek_history) > 19)\n/*\n * Sync random I/O is likely to be confused with soft real-time I/O,\n * because it is characterized by limited throughput and apparently\n * isochronous arrival pattern. To avoid false positives, queues\n * containing only random (seeky) I/O are prevented from being tagged\n * as soft real-time.\n */\n#define BFQQ_TOTALLY_SEEKY(bfqq)\t(bfqq->seek_history == -1)\n\n/* Min number of samples required to perform peak-rate update */\n#define BFQ_RATE_MIN_SAMPLES\t32\n/* Min observation time interval required to perform a peak-rate update (ns) */\n#define BFQ_RATE_MIN_INTERVAL\t(300*NSEC_PER_MSEC)\n/* Target observation time interval for a peak-rate update (ns) */\n#define BFQ_RATE_REF_INTERVAL\tNSEC_PER_SEC\n\n/*\n * Shift used for peak-rate fixed precision calculations.\n * With\n * - the current shift: 16 positions\n * - the current type used to store rate: u32\n * - the current unit of measure for rate: [sectors/usec], or, more precisely,\n *   [(sectors/usec) / 2^BFQ_RATE_SHIFT] to take into account the shift,\n * the range of rates that can be stored is\n * [1 / 2^BFQ_RATE_SHIFT, 2^(32 - BFQ_RATE_SHIFT)] sectors/usec =\n * [1 / 2^16, 2^16] sectors/usec = [15e-6, 65536] sectors/usec =\n * [15, 65G] sectors/sec\n * Which, assuming a sector size of 512B, corresponds to a range of\n * [7.5K, 33T] B/sec\n */\n#define BFQ_RATE_SHIFT\t\t16\n\n/*\n * When configured for computing the duration of the weight-raising\n * for interactive queues automatically (see the comments at the\n * beginning of this file), BFQ does it using the following formula:\n * duration = (ref_rate / r) * ref_wr_duration,\n * where r is the peak rate of the device, and ref_rate and\n * ref_wr_duration are two reference parameters.  In particular,\n * ref_rate is the peak rate of the reference storage device (see\n * below), and ref_wr_duration is about the maximum time needed, with\n * BFQ and while reading two files in parallel, to load typical large\n * applications on the reference device (see the comments on\n * max_service_from_wr below, for more details on how ref_wr_duration\n * is obtained).  In practice, the slower/faster the device at hand\n * is, the more/less it takes to load applications with respect to the\n * reference device.  Accordingly, the longer/shorter BFQ grants\n * weight raising to interactive applications.\n *\n * BFQ uses two different reference pairs (ref_rate, ref_wr_duration),\n * depending on whether the device is rotational or non-rotational.\n *\n * In the following definitions, ref_rate[0] and ref_wr_duration[0]\n * are the reference values for a rotational device, whereas\n * ref_rate[1] and ref_wr_duration[1] are the reference values for a\n * non-rotational device. The reference rates are not the actual peak\n * rates of the devices used as a reference, but slightly lower\n * values. The reason for using slightly lower values is that the\n * peak-rate estimator tends to yield slightly lower values than the\n * actual peak rate (it can yield the actual peak rate only if there\n * is only one process doing I/O, and the process does sequential\n * I/O).\n *\n * The reference peak rates are measured in sectors/usec, left-shifted\n * by BFQ_RATE_SHIFT.\n */\nstatic int ref_rate[2] = {14000, 33000};\n/*\n * To improve readability, a conversion function is used to initialize\n * the following array, which entails that the array can be\n * initialized only in a function.\n */\nstatic int ref_wr_duration[2];\n\n/*\n * BFQ uses the above-detailed, time-based weight-raising mechanism to\n * privilege interactive tasks. This mechanism is vulnerable to the\n * following false positives: I/O-bound applications that will go on\n * doing I/O for much longer than the duration of weight\n * raising. These applications have basically no benefit from being\n * weight-raised at the beginning of their I/O. On the opposite end,\n * while being weight-raised, these applications\n * a) unjustly steal throughput to applications that may actually need\n * low latency;\n * b) make BFQ uselessly perform device idling; device idling results\n * in loss of device throughput with most flash-based storage, and may\n * increase latencies when used purposelessly.\n *\n * BFQ tries to reduce these problems, by adopting the following\n * countermeasure. To introduce this countermeasure, we need first to\n * finish explaining how the duration of weight-raising for\n * interactive tasks is computed.\n *\n * For a bfq_queue deemed as interactive, the duration of weight\n * raising is dynamically adjusted, as a function of the estimated\n * peak rate of the device, so as to be equal to the time needed to\n * execute the 'largest' interactive task we benchmarked so far. By\n * largest task, we mean the task for which each involved process has\n * to do more I/O than for any of the other tasks we benchmarked. This\n * reference interactive task is the start-up of LibreOffice Writer,\n * and in this task each process/bfq_queue needs to have at most ~110K\n * sectors transferred.\n *\n * This last piece of information enables BFQ to reduce the actual\n * duration of weight-raising for at least one class of I/O-bound\n * applications: those doing sequential or quasi-sequential I/O. An\n * example is file copy. In fact, once started, the main I/O-bound\n * processes of these applications usually consume the above 110K\n * sectors in much less time than the processes of an application that\n * is starting, because these I/O-bound processes will greedily devote\n * almost all their CPU cycles only to their target,\n * throughput-friendly I/O operations. This is even more true if BFQ\n * happens to be underestimating the device peak rate, and thus\n * overestimating the duration of weight raising. But, according to\n * our measurements, once transferred 110K sectors, these processes\n * have no right to be weight-raised any longer.\n *\n * Basing on the last consideration, BFQ ends weight-raising for a\n * bfq_queue if the latter happens to have received an amount of\n * service at least equal to the following constant. The constant is\n * set to slightly more than 110K, to have a minimum safety margin.\n *\n * This early ending of weight-raising reduces the amount of time\n * during which interactive false positives cause the two problems\n * described at the beginning of these comments.\n */\nstatic const unsigned long max_service_from_wr = 120000;\n\n/*\n * Maximum time between the creation of two queues, for stable merge\n * to be activated (in ms)\n */\nstatic const unsigned long bfq_activation_stable_merging = 600;\n/*\n * Minimum time to be waited before evaluating delayed stable merge (in ms)\n */\nstatic const unsigned long bfq_late_stable_merging = 600;\n\n#define RQ_BIC(rq)\t\t((struct bfq_io_cq *)((rq)->elv.priv[0]))\n#define RQ_BFQQ(rq)\t\t((rq)->elv.priv[1])\n\nstruct bfq_queue *bic_to_bfqq(struct bfq_io_cq *bic, bool is_sync)\n{\n\treturn bic->bfqq[is_sync];\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq);\n\nvoid bic_set_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq, bool is_sync)\n{\n\tstruct bfq_queue *old_bfqq = bic->bfqq[is_sync];\n\n\t/* Clear bic pointer if bfqq is detached from this bic */\n\tif (old_bfqq && old_bfqq->bic == bic)\n\t\told_bfqq->bic = NULL;\n\n\t/*\n\t * If bfqq != NULL, then a non-stable queue merge between\n\t * bic->bfqq and bfqq is happening here. This causes troubles\n\t * in the following case: bic->bfqq has also been scheduled\n\t * for a possible stable merge with bic->stable_merge_bfqq,\n\t * and bic->stable_merge_bfqq == bfqq happens to\n\t * hold. Troubles occur because bfqq may then undergo a split,\n\t * thereby becoming eligible for a stable merge. Yet, if\n\t * bic->stable_merge_bfqq points exactly to bfqq, then bfqq\n\t * would be stably merged with itself. To avoid this anomaly,\n\t * we cancel the stable merge if\n\t * bic->stable_merge_bfqq == bfqq.\n\t */\n\tbic->bfqq[is_sync] = bfqq;\n\n\tif (bfqq && bic->stable_merge_bfqq == bfqq) {\n\t\t/*\n\t\t * Actually, these same instructions are executed also\n\t\t * in bfq_setup_cooperator, in case of abort or actual\n\t\t * execution of a stable merge. We could avoid\n\t\t * repeating these instructions there too, but if we\n\t\t * did so, we would nest even more complexity in this\n\t\t * function.\n\t\t */\n\t\tbfq_put_stable_ref(bic->stable_merge_bfqq);\n\n\t\tbic->stable_merge_bfqq = NULL;\n\t}\n}\n\nstruct bfq_data *bic_to_bfqd(struct bfq_io_cq *bic)\n{\n\treturn bic->icq.q->elevator->elevator_data;\n}\n\n/**\n * icq_to_bic - convert iocontext queue structure to bfq_io_cq.\n * @icq: the iocontext queue.\n */\nstatic struct bfq_io_cq *icq_to_bic(struct io_cq *icq)\n{\n\t/* bic->icq is the first member, %NULL will convert to %NULL */\n\treturn container_of(icq, struct bfq_io_cq, icq);\n}\n\n/**\n * bfq_bic_lookup - search into @ioc a bic associated to @bfqd.\n * @q: the request queue.\n */\nstatic struct bfq_io_cq *bfq_bic_lookup(struct request_queue *q)\n{\n\tstruct bfq_io_cq *icq;\n\tunsigned long flags;\n\n\tif (!current->io_context)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\ticq = icq_to_bic(ioc_lookup_icq(q));\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\n\treturn icq;\n}\n\n/*\n * Scheduler run of queue, if there are requests pending and no one in the\n * driver that will restart queueing.\n */\nvoid bfq_schedule_dispatch(struct bfq_data *bfqd)\n{\n\tlockdep_assert_held(&bfqd->lock);\n\n\tif (bfqd->queued != 0) {\n\t\tbfq_log(bfqd, \"schedule dispatch\");\n\t\tblk_mq_run_hw_queues(bfqd->queue, true);\n\t}\n}\n\n#define bfq_class_idle(bfqq)\t((bfqq)->ioprio_class == IOPRIO_CLASS_IDLE)\n\n#define bfq_sample_valid(samples)\t((samples) > 80)\n\n/*\n * Lifted from AS - choose which of rq1 and rq2 that is best served now.\n * We choose the request that is closer to the head right now.  Distance\n * behind the head is penalized and only allowed to a certain extent.\n */\nstatic struct request *bfq_choose_req(struct bfq_data *bfqd,\n\t\t\t\t      struct request *rq1,\n\t\t\t\t      struct request *rq2,\n\t\t\t\t      sector_t last)\n{\n\tsector_t s1, s2, d1 = 0, d2 = 0;\n\tunsigned long back_max;\n#define BFQ_RQ1_WRAP\t0x01 /* request 1 wraps */\n#define BFQ_RQ2_WRAP\t0x02 /* request 2 wraps */\n\tunsigned int wrap = 0; /* bit mask: requests behind the disk head? */\n\n\tif (!rq1 || rq1 == rq2)\n\t\treturn rq2;\n\tif (!rq2)\n\t\treturn rq1;\n\n\tif (rq_is_sync(rq1) && !rq_is_sync(rq2))\n\t\treturn rq1;\n\telse if (rq_is_sync(rq2) && !rq_is_sync(rq1))\n\t\treturn rq2;\n\tif ((rq1->cmd_flags & REQ_META) && !(rq2->cmd_flags & REQ_META))\n\t\treturn rq1;\n\telse if ((rq2->cmd_flags & REQ_META) && !(rq1->cmd_flags & REQ_META))\n\t\treturn rq2;\n\n\ts1 = blk_rq_pos(rq1);\n\ts2 = blk_rq_pos(rq2);\n\n\t/*\n\t * By definition, 1KiB is 2 sectors.\n\t */\n\tback_max = bfqd->bfq_back_max * 2;\n\n\t/*\n\t * Strict one way elevator _except_ in the case where we allow\n\t * short backward seeks which are biased as twice the cost of a\n\t * similar forward seek.\n\t */\n\tif (s1 >= last)\n\t\td1 = s1 - last;\n\telse if (s1 + back_max >= last)\n\t\td1 = (last - s1) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ1_WRAP;\n\n\tif (s2 >= last)\n\t\td2 = s2 - last;\n\telse if (s2 + back_max >= last)\n\t\td2 = (last - s2) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ2_WRAP;\n\n\t/* Found required data */\n\n\t/*\n\t * By doing switch() on the bit mask \"wrap\" we avoid having to\n\t * check two variables for all permutations: --> faster!\n\t */\n\tswitch (wrap) {\n\tcase 0: /* common case for CFQ: rq1 and rq2 not wrapped */\n\t\tif (d1 < d2)\n\t\t\treturn rq1;\n\t\telse if (d2 < d1)\n\t\t\treturn rq2;\n\n\t\tif (s1 >= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\n\tcase BFQ_RQ2_WRAP:\n\t\treturn rq1;\n\tcase BFQ_RQ1_WRAP:\n\t\treturn rq2;\n\tcase BFQ_RQ1_WRAP|BFQ_RQ2_WRAP: /* both rqs wrapped */\n\tdefault:\n\t\t/*\n\t\t * Since both rqs are wrapped,\n\t\t * start with the one that's further behind head\n\t\t * (--> only *one* back seek required),\n\t\t * since back seek takes more time than forward.\n\t\t */\n\t\tif (s1 <= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\t}\n}\n\n#define BFQ_LIMIT_INLINE_DEPTH 16\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\nstatic bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct bfq_entity *inline_entities[BFQ_LIMIT_INLINE_DEPTH];\n\tstruct bfq_entity **entities = inline_entities;\n\tint depth, level, alloc_depth = BFQ_LIMIT_INLINE_DEPTH;\n\tint class_idx = bfqq->ioprio_class - 1;\n\tstruct bfq_sched_data *sched_data;\n\tunsigned long wsum;\n\tbool ret = false;\n\n\tif (!entity->on_st_or_in_serv)\n\t\treturn false;\n\nretry:\n\tspin_lock_irq(&bfqd->lock);\n\t/* +1 for bfqq entity, root cgroup not included */\n\tdepth = bfqg_to_blkg(bfqq_group(bfqq))->blkcg->css.cgroup->level + 1;\n\tif (depth > alloc_depth) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tif (entities != inline_entities)\n\t\t\tkfree(entities);\n\t\tentities = kmalloc_array(depth, sizeof(*entities), GFP_NOIO);\n\t\tif (!entities)\n\t\t\treturn false;\n\t\talloc_depth = depth;\n\t\tgoto retry;\n\t}\n\n\tsched_data = entity->sched_data;\n\t/* Gather our ancestors as we need to traverse them in reverse order */\n\tlevel = 0;\n\tfor_each_entity(entity) {\n\t\t/*\n\t\t * If at some level entity is not even active, allow request\n\t\t * queueing so that BFQ knows there's work to do and activate\n\t\t * entities.\n\t\t */\n\t\tif (!entity->on_st_or_in_serv)\n\t\t\tgoto out;\n\t\t/* Uh, more parents than cgroup subsystem thinks? */\n\t\tif (WARN_ON_ONCE(level >= depth))\n\t\t\tbreak;\n\t\tentities[level++] = entity;\n\t}\n\tWARN_ON_ONCE(level != depth);\n\tfor (level--; level >= 0; level--) {\n\t\tentity = entities[level];\n\t\tif (level > 0) {\n\t\t\twsum = bfq_entity_service_tree(entity)->wsum;\n\t\t} else {\n\t\t\tint i;\n\t\t\t/*\n\t\t\t * For bfqq itself we take into account service trees\n\t\t\t * of all higher priority classes and multiply their\n\t\t\t * weights so that low prio queue from higher class\n\t\t\t * gets more requests than high prio queue from lower\n\t\t\t * class.\n\t\t\t */\n\t\t\twsum = 0;\n\t\t\tfor (i = 0; i <= class_idx; i++) {\n\t\t\t\twsum = wsum * IOPRIO_BE_NR +\n\t\t\t\t\tsched_data->service_tree[i].wsum;\n\t\t\t}\n\t\t}\n\t\tif (!wsum)\n\t\t\tcontinue;\n\t\tlimit = DIV_ROUND_CLOSEST(limit * entity->weight, wsum);\n\t\tif (entity->allocated >= limit) {\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t\"too many requests: allocated %d limit %d level %d\",\n\t\t\t\tentity->allocated, limit, level);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tspin_unlock_irq(&bfqd->lock);\n\tif (entities != inline_entities)\n\t\tkfree(entities);\n\treturn ret;\n}\n#else\nstatic bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)\n{\n\treturn false;\n}\n#endif\n\n/*\n * Async I/O can easily starve sync I/O (both sync reads and sync\n * writes), by consuming all tags. Similarly, storms of sync writes,\n * such as those that sync(2) may trigger, can starve sync reads.\n * Limit depths of async I/O and sync writes so as to counter both\n * problems.\n *\n * Also if a bfq queue or its parent cgroup consume more tags than would be\n * appropriate for their weight, we trim the available tag depth to 1. This\n * avoids a situation where one cgroup can starve another cgroup from tags and\n * thus block service differentiation among cgroups. Note that because the\n * queue / cgroup already has many requests allocated and queued, this does not\n * significantly affect service guarantees coming from the BFQ scheduling\n * algorithm.\n */\nstatic void bfq_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data)\n{\n\tstruct bfq_data *bfqd = data->q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(data->q);\n\tstruct bfq_queue *bfqq = bic ? bic_to_bfqq(bic, op_is_sync(opf)) : NULL;\n\tint depth;\n\tunsigned limit = data->q->nr_requests;\n\n\t/* Sync reads have full depth available */\n\tif (op_is_sync(opf) && !op_is_write(opf)) {\n\t\tdepth = 0;\n\t} else {\n\t\tdepth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(opf)];\n\t\tlimit = (limit * depth) >> bfqd->full_depth_shift;\n\t}\n\n\t/*\n\t * Does queue (or any parent entity) exceed number of requests that\n\t * should be available to it? Heavily limit depth so that it cannot\n\t * consume more available requests and thus starve other entities.\n\t */\n\tif (bfqq && bfqq_request_over_limit(bfqq, limit))\n\t\tdepth = 1;\n\n\tbfq_log(bfqd, \"[%s] wr_busy %d sync %d depth %u\",\n\t\t__func__, bfqd->wr_busy_queues, op_is_sync(opf), depth);\n\tif (depth)\n\t\tdata->shallow_depth = depth;\n}\n\nstatic struct bfq_queue *\nbfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,\n\t\t     sector_t sector, struct rb_node **ret_parent,\n\t\t     struct rb_node ***rb_link)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tparent = NULL;\n\tp = &root->rb_node;\n\twhile (*p) {\n\t\tstruct rb_node **n;\n\n\t\tparent = *p;\n\t\tbfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\n\t\t/*\n\t\t * Sort strictly based on sector. Smallest to the left,\n\t\t * largest to the right.\n\t\t */\n\t\tif (sector > blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_right;\n\t\telse if (sector < blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_left;\n\t\telse\n\t\t\tbreak;\n\t\tp = n;\n\t\tbfqq = NULL;\n\t}\n\n\t*ret_parent = parent;\n\tif (rb_link)\n\t\t*rb_link = p;\n\n\tbfq_log(bfqd, \"rq_pos_tree_lookup %llu: returning %d\",\n\t\t(unsigned long long)sector,\n\t\tbfqq ? bfqq->pid : 0);\n\n\treturn bfqq;\n}\n\nstatic bool bfq_too_late_for_merging(struct bfq_queue *bfqq)\n{\n\treturn bfqq->service_from_backlogged > 0 &&\n\t\ttime_is_before_jiffies(bfqq->first_IO_time +\n\t\t\t\t       bfq_merge_time_limit);\n}\n\n/*\n * The following function is not marked as __cold because it is\n * actually cold, but for the same performance goal described in the\n * comments on the likely() at the beginning of\n * bfq_setup_cooperator(). Unexpectedly, to reach an even lower\n * execution time for the case where this function is not invoked, we\n * had to add an unlikely() in each involved if().\n */\nvoid __cold\nbfq_pos_tree_add_move(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *__bfqq;\n\n\tif (bfqq->pos_root) {\n\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\tbfqq->pos_root = NULL;\n\t}\n\n\t/* oom_bfqq does not participate in queue merging */\n\tif (bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq cannot be merged any longer (see comments in\n\t * bfq_setup_cooperator): no point in adding bfqq into the\n\t * position tree.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn;\n\n\tif (bfq_class_idle(bfqq))\n\t\treturn;\n\tif (!bfqq->next_rq)\n\t\treturn;\n\n\tbfqq->pos_root = &bfqq_group(bfqq)->rq_pos_tree;\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, bfqq->pos_root,\n\t\t\tblk_rq_pos(bfqq->next_rq), &parent, &p);\n\tif (!__bfqq) {\n\t\trb_link_node(&bfqq->pos_node, parent, p);\n\t\trb_insert_color(&bfqq->pos_node, bfqq->pos_root);\n\t} else\n\t\tbfqq->pos_root = NULL;\n}\n\n/*\n * The following function returns false either if every active queue\n * must receive the same share of the throughput (symmetric scenario),\n * or, as a special case, if bfqq must receive a share of the\n * throughput lower than or equal to the share that every other active\n * queue must receive.  If bfqq does sync I/O, then these are the only\n * two cases where bfqq happens to be guaranteed its share of the\n * throughput even if I/O dispatching is not plugged when bfqq remains\n * temporarily empty (for more details, see the comments in the\n * function bfq_better_to_idle()). For this reason, the return value\n * of this function is used to check whether I/O-dispatch plugging can\n * be avoided.\n *\n * The above first case (symmetric scenario) occurs when:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) all active groups at the same level in the groups tree have the same\n *    weight,\n * 4) all active groups at the same level in the groups tree have the same\n *    number of children.\n *\n * Unfortunately, keeping the necessary state for evaluating exactly\n * the last two symmetry sub-conditions above would be quite complex\n * and time consuming. Therefore this function evaluates, instead,\n * only the following stronger three sub-conditions, for which it is\n * much easier to maintain the needed state:\n * 1) all active queues have the same weight,\n * 2) all active queues belong to the same I/O-priority class,\n * 3) there are no active groups.\n * In particular, the last condition is always true if hierarchical\n * support or the cgroups interface are not enabled, thus no state\n * needs to be maintained in this case.\n */\nstatic bool bfq_asymmetric_scenario(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tbool smallest_weight = bfqq &&\n\t\tbfqq->weight_counter &&\n\t\tbfqq->weight_counter ==\n\t\tcontainer_of(\n\t\t\trb_first_cached(&bfqd->queue_weights_tree),\n\t\t\tstruct bfq_weight_counter,\n\t\t\tweights_node);\n\n\t/*\n\t * For queue weights to differ, queue_weights_tree must contain\n\t * at least two nodes.\n\t */\n\tbool varied_queue_weights = !smallest_weight &&\n\t\t!RB_EMPTY_ROOT(&bfqd->queue_weights_tree.rb_root) &&\n\t\t(bfqd->queue_weights_tree.rb_root.rb_node->rb_left ||\n\t\t bfqd->queue_weights_tree.rb_root.rb_node->rb_right);\n\n\tbool multiple_classes_busy =\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[1]) ||\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[2]) ||\n\t\t(bfqd->busy_queues[1] && bfqd->busy_queues[2]);\n\n\treturn varied_queue_weights || multiple_classes_busy\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\t       || bfqd->num_groups_with_pending_reqs > 0\n#endif\n\t\t;\n}\n\n/*\n * If the weight-counter tree passed as input contains no counter for\n * the weight of the input queue, then add that counter; otherwise just\n * increment the existing counter.\n *\n * Note that weight-counter trees contain few nodes in mostly symmetric\n * scenarios. For example, if all queues have the same weight, then the\n * weight-counter tree for the queues may contain at most one node.\n * This holds even if low_latency is on, because weight-raised queues\n * are not inserted in the tree.\n * In most scenarios, the rate at which nodes are created/destroyed\n * should be low too.\n */\nvoid bfq_weights_tree_add(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct rb_root_cached *root)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tbool leftmost = true;\n\n\t/*\n\t * Do not insert if the queue is already associated with a\n\t * counter, which happens if:\n\t *   1) a request arrival has caused the queue to become both\n\t *      non-weight-raised, and hence change its weight, and\n\t *      backlogged; in this respect, each of the two events\n\t *      causes an invocation of this function,\n\t *   2) this is the invocation of this function caused by the\n\t *      second event. This second invocation is actually useless,\n\t *      and we handle this fact by exiting immediately. More\n\t *      efficient or clearer solutions might possibly be adopted.\n\t */\n\tif (bfqq->weight_counter)\n\t\treturn;\n\n\twhile (*new) {\n\t\tstruct bfq_weight_counter *__counter = container_of(*new,\n\t\t\t\t\t\tstruct bfq_weight_counter,\n\t\t\t\t\t\tweights_node);\n\t\tparent = *new;\n\n\t\tif (entity->weight == __counter->weight) {\n\t\t\tbfqq->weight_counter = __counter;\n\t\t\tgoto inc_counter;\n\t\t}\n\t\tif (entity->weight < __counter->weight)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\tbfqq->weight_counter = kzalloc(sizeof(struct bfq_weight_counter),\n\t\t\t\t       GFP_ATOMIC);\n\n\t/*\n\t * In the unlucky event of an allocation failure, we just\n\t * exit. This will cause the weight of queue to not be\n\t * considered in bfq_asymmetric_scenario, which, in its turn,\n\t * causes the scenario to be deemed wrongly symmetric in case\n\t * bfqq's weight would have been the only weight making the\n\t * scenario asymmetric.  On the bright side, no unbalance will\n\t * however occur when bfqq becomes inactive again (the\n\t * invocation of this function is triggered by an activation\n\t * of queue).  In fact, bfq_weights_tree_remove does nothing\n\t * if !bfqq->weight_counter.\n\t */\n\tif (unlikely(!bfqq->weight_counter))\n\t\treturn;\n\n\tbfqq->weight_counter->weight = entity->weight;\n\trb_link_node(&bfqq->weight_counter->weights_node, parent, new);\n\trb_insert_color_cached(&bfqq->weight_counter->weights_node, root,\n\t\t\t\tleftmost);\n\ninc_counter:\n\tbfqq->weight_counter->num_active++;\n\tbfqq->ref++;\n}\n\n/*\n * Decrement the weight counter associated with the queue, and, if the\n * counter reaches 0, remove the counter from the tree.\n * See the comments to the function bfq_weights_tree_add() for considerations\n * about overhead.\n */\nvoid __bfq_weights_tree_remove(struct bfq_data *bfqd,\n\t\t\t       struct bfq_queue *bfqq,\n\t\t\t       struct rb_root_cached *root)\n{\n\tif (!bfqq->weight_counter)\n\t\treturn;\n\n\tbfqq->weight_counter->num_active--;\n\tif (bfqq->weight_counter->num_active > 0)\n\t\tgoto reset_entity_pointer;\n\n\trb_erase_cached(&bfqq->weight_counter->weights_node, root);\n\tkfree(bfqq->weight_counter);\n\nreset_entity_pointer:\n\tbfqq->weight_counter = NULL;\n\tbfq_put_queue(bfqq);\n}\n\n/*\n * Invoke __bfq_weights_tree_remove on bfqq and decrement the number\n * of active groups for each queue's inactive parent entity.\n */\nvoid bfq_weights_tree_remove(struct bfq_data *bfqd,\n\t\t\t     struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = bfqq->entity.parent;\n\n\tfor_each_entity(entity) {\n\t\tstruct bfq_sched_data *sd = entity->my_sched_data;\n\n\t\tif (sd->next_in_service || sd->in_service_entity) {\n\t\t\t/*\n\t\t\t * entity is still active, because either\n\t\t\t * next_in_service or in_service_entity is not\n\t\t\t * NULL (see the comments on the definition of\n\t\t\t * next_in_service for details on why\n\t\t\t * in_service_entity must be checked too).\n\t\t\t *\n\t\t\t * As a consequence, its parent entities are\n\t\t\t * active as well, and thus this loop must\n\t\t\t * stop here.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * The decrement of num_groups_with_pending_reqs is\n\t\t * not performed immediately upon the deactivation of\n\t\t * entity, but it is delayed to when it also happens\n\t\t * that the first leaf descendant bfqq of entity gets\n\t\t * all its pending requests completed. The following\n\t\t * instructions perform this delayed decrement, if\n\t\t * needed. See the comments on\n\t\t * num_groups_with_pending_reqs for details.\n\t\t */\n\t\tif (entity->in_groups_with_pending_reqs) {\n\t\t\tentity->in_groups_with_pending_reqs = false;\n\t\t\tbfqd->num_groups_with_pending_reqs--;\n\t\t}\n\t}\n\n\t/*\n\t * Next function is invoked last, because it causes bfqq to be\n\t * freed if the following holds: bfqq is not in service and\n\t * has no dispatched request. DO NOT use bfqq after the next\n\t * function invocation.\n\t */\n\t__bfq_weights_tree_remove(bfqd, bfqq,\n\t\t\t\t  &bfqd->queue_weights_tree);\n}\n\n/*\n * Return expired entry, or NULL to just start from scratch in rbtree.\n */\nstatic struct request *bfq_check_fifo(struct bfq_queue *bfqq,\n\t\t\t\t      struct request *last)\n{\n\tstruct request *rq;\n\n\tif (bfq_bfqq_fifo_expire(bfqq))\n\t\treturn NULL;\n\n\tbfq_mark_bfqq_fifo_expire(bfqq);\n\n\trq = rq_entry_fifo(bfqq->fifo.next);\n\n\tif (rq == last || ktime_get_ns() < rq->fifo_time)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"check_fifo: returned %p\", rq);\n\treturn rq;\n}\n\nstatic struct request *bfq_find_next_rq(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\tstruct request *last)\n{\n\tstruct rb_node *rbnext = rb_next(&last->rb_node);\n\tstruct rb_node *rbprev = rb_prev(&last->rb_node);\n\tstruct request *next, *prev = NULL;\n\n\t/* Follow expired path, else get first next available. */\n\tnext = bfq_check_fifo(bfqq, last);\n\tif (next)\n\t\treturn next;\n\n\tif (rbprev)\n\t\tprev = rb_entry_rq(rbprev);\n\n\tif (rbnext)\n\t\tnext = rb_entry_rq(rbnext);\n\telse {\n\t\trbnext = rb_first(&bfqq->sort_list);\n\t\tif (rbnext && rbnext != &last->rb_node)\n\t\t\tnext = rb_entry_rq(rbnext);\n\t}\n\n\treturn bfq_choose_req(bfqd, next, prev, blk_rq_pos(last));\n}\n\n/* see the definition of bfq_async_charge_factor for details */\nstatic unsigned long bfq_serv_to_charge(struct request *rq,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\tif (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||\n\t    bfq_asymmetric_scenario(bfqq->bfqd, bfqq))\n\t\treturn blk_rq_sectors(rq);\n\n\treturn blk_rq_sectors(rq) * bfq_async_charge_factor;\n}\n\n/**\n * bfq_updated_next_req - update the queue after a new next_rq selection.\n * @bfqd: the device data the queue belongs to.\n * @bfqq: the queue to update.\n *\n * If the first request of a queue changes we make sure that the queue\n * has enough budget to serve at least its first request (if the\n * request has grown).  We do this because if the queue has not enough\n * budget for its first request, it has to go through two dispatch\n * rounds to actually get it dispatched.\n */\nstatic void bfq_updated_next_req(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct request *next_rq = bfqq->next_rq;\n\tunsigned long new_budget;\n\n\tif (!next_rq)\n\t\treturn;\n\n\tif (bfqq == bfqd->in_service_queue)\n\t\t/*\n\t\t * In order not to break guarantees, budgets cannot be\n\t\t * changed after an entity has been selected.\n\t\t */\n\t\treturn;\n\n\tnew_budget = max_t(unsigned long,\n\t\t\t   max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t bfq_serv_to_charge(next_rq, bfqq)),\n\t\t\t   entity->service);\n\tif (entity->budget != new_budget) {\n\t\tentity->budget = new_budget;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"updated next rq: new budget %lu\",\n\t\t\t\t\t new_budget);\n\t\tbfq_requeue_bfqq(bfqd, bfqq, false);\n\t}\n}\n\nstatic unsigned int bfq_wr_duration(struct bfq_data *bfqd)\n{\n\tu64 dur;\n\n\tif (bfqd->bfq_wr_max_time > 0)\n\t\treturn bfqd->bfq_wr_max_time;\n\n\tdur = bfqd->rate_dur_prod;\n\tdo_div(dur, bfqd->peak_rate);\n\n\t/*\n\t * Limit duration between 3 and 25 seconds. The upper limit\n\t * has been conservatively set after the following worst case:\n\t * on a QEMU/KVM virtual machine\n\t * - running in a slow PC\n\t * - with a virtual disk stacked on a slow low-end 5400rpm HDD\n\t * - serving a heavy I/O workload, such as the sequential reading\n\t *   of several files\n\t * mplayer took 23 seconds to start, if constantly weight-raised.\n\t *\n\t * As for higher values than that accommodating the above bad\n\t * scenario, tests show that higher values would often yield\n\t * the opposite of the desired result, i.e., would worsen\n\t * responsiveness by allowing non-interactive applications to\n\t * preserve weight raising for too long.\n\t *\n\t * On the other end, lower values than 3 seconds make it\n\t * difficult for most interactive tasks to complete their jobs\n\t * before weight-raising finishes.\n\t */\n\treturn clamp_val(dur, msecs_to_jiffies(3000), msecs_to_jiffies(25000));\n}\n\n/* switch back from soft real-time to interactive weight raising */\nstatic void switch_back_to_interactive_wr(struct bfq_queue *bfqq,\n\t\t\t\t\t  struct bfq_data *bfqd)\n{\n\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\tbfqq->last_wr_start_finish = bfqq->wr_start_at_switch_to_srt;\n}\n\nstatic void\nbfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,\n\t\t      struct bfq_io_cq *bic, bool bfq_already_existing)\n{\n\tunsigned int old_wr_coeff = 1;\n\tbool busy = bfq_already_existing && bfq_bfqq_busy(bfqq);\n\n\tif (bic->saved_has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\tif (bic->saved_IO_bound)\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\tbfqq->last_serv_time_ns = bic->saved_last_serv_time_ns;\n\tbfqq->inject_limit = bic->saved_inject_limit;\n\tbfqq->decrease_time_jif = bic->saved_decrease_time_jif;\n\n\tbfqq->entity.new_weight = bic->saved_weight;\n\tbfqq->ttime = bic->saved_ttime;\n\tbfqq->io_start_time = bic->saved_io_start_time;\n\tbfqq->tot_idle_time = bic->saved_tot_idle_time;\n\t/*\n\t * Restore weight coefficient only if low_latency is on\n\t */\n\tif (bfqd->low_latency) {\n\t\told_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq->wr_coeff = bic->saved_wr_coeff;\n\t}\n\tbfqq->service_from_wr = bic->saved_service_from_wr;\n\tbfqq->wr_start_at_switch_to_srt = bic->saved_wr_start_at_switch_to_srt;\n\tbfqq->last_wr_start_finish = bic->saved_last_wr_start_finish;\n\tbfqq->wr_cur_max_time = bic->saved_wr_cur_max_time;\n\n\tif (bfqq->wr_coeff > 1 && (bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t   bfqq->wr_cur_max_time))) {\n\t\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t    time_is_after_eq_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t     bfq_wr_duration(bfqd))) {\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t} else {\n\t\t\tbfqq->wr_coeff = 1;\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t     \"resume state: switching off wr\");\n\t\t}\n\t}\n\n\t/* make sure weight will be updated, however we got here */\n\tbfqq->entity.prio_changed = 1;\n\n\tif (likely(!busy))\n\t\treturn;\n\n\tif (old_wr_coeff == 1 && bfqq->wr_coeff > 1)\n\t\tbfqd->wr_busy_queues++;\n\telse if (old_wr_coeff > 1 && bfqq->wr_coeff == 1)\n\t\tbfqd->wr_busy_queues--;\n}\n\nstatic int bfqq_process_refs(struct bfq_queue *bfqq)\n{\n\treturn bfqq->ref - bfqq->entity.allocated -\n\t\tbfqq->entity.on_st_or_in_serv -\n\t\t(bfqq->weight_counter != NULL) - bfqq->stable_ref;\n}\n\n/* Empty burst list and add just bfqq (see comments on bfq_handle_burst) */\nstatic void bfq_reset_burst_list(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(item, n, &bfqd->burst_list, burst_list_node)\n\t\thlist_del_init(&item->burst_list_node);\n\n\t/*\n\t * Start the creation of a new burst list only if there is no\n\t * active queue. See comments on the conditional invocation of\n\t * bfq_handle_burst().\n\t */\n\tif (bfq_tot_busy_queues(bfqd) == 0) {\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n\t\tbfqd->burst_size = 1;\n\t} else\n\t\tbfqd->burst_size = 0;\n\n\tbfqd->burst_parent_entity = bfqq->entity.parent;\n}\n\n/* Add bfqq to the list of queues in current burst (see bfq_handle_burst) */\nstatic void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/* Increment burst size to take into account also bfqq */\n\tbfqd->burst_size++;\n\n\tif (bfqd->burst_size == bfqd->bfq_large_burst_thresh) {\n\t\tstruct bfq_queue *pos, *bfqq_item;\n\t\tstruct hlist_node *n;\n\n\t\t/*\n\t\t * Enough queues have been activated shortly after each\n\t\t * other to consider this burst as large.\n\t\t */\n\t\tbfqd->large_burst = true;\n\n\t\t/*\n\t\t * We can now mark all queues in the burst list as\n\t\t * belonging to a large burst.\n\t\t */\n\t\thlist_for_each_entry(bfqq_item, &bfqd->burst_list,\n\t\t\t\t     burst_list_node)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq_item);\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\n\t\t/*\n\t\t * From now on, and until the current burst finishes, any\n\t\t * new queue being activated shortly after the last queue\n\t\t * was inserted in the burst can be immediately marked as\n\t\t * belonging to a large burst. So the burst list is not\n\t\t * needed any more. Remove it.\n\t\t */\n\t\thlist_for_each_entry_safe(pos, n, &bfqd->burst_list,\n\t\t\t\t\t  burst_list_node)\n\t\t\thlist_del_init(&pos->burst_list_node);\n\t} else /*\n\t\t* Burst not yet large: add bfqq to the burst list. Do\n\t\t* not increment the ref counter for bfqq, because bfqq\n\t\t* is removed from the burst list before freeing bfqq\n\t\t* in put_queue.\n\t\t*/\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n}\n\n/*\n * If many queues belonging to the same group happen to be created\n * shortly after each other, then the processes associated with these\n * queues have typically a common goal. In particular, bursts of queue\n * creations are usually caused by services or applications that spawn\n * many parallel threads/processes. Examples are systemd during boot,\n * or git grep. To help these processes get their job done as soon as\n * possible, it is usually better to not grant either weight-raising\n * or device idling to their queues, unless these queues must be\n * protected from the I/O flowing through other active queues.\n *\n * In this comment we describe, firstly, the reasons why this fact\n * holds, and, secondly, the next function, which implements the main\n * steps needed to properly mark these queues so that they can then be\n * treated in a different way.\n *\n * The above services or applications benefit mostly from a high\n * throughput: the quicker the requests of the activated queues are\n * cumulatively served, the sooner the target job of these queues gets\n * completed. As a consequence, weight-raising any of these queues,\n * which also implies idling the device for it, is almost always\n * counterproductive, unless there are other active queues to isolate\n * these new queues from. If there no other active queues, then\n * weight-raising these new queues just lowers throughput in most\n * cases.\n *\n * On the other hand, a burst of queue creations may be caused also by\n * the start of an application that does not consist of a lot of\n * parallel I/O-bound threads. In fact, with a complex application,\n * several short processes may need to be executed to start-up the\n * application. In this respect, to start an application as quickly as\n * possible, the best thing to do is in any case to privilege the I/O\n * related to the application with respect to all other\n * I/O. Therefore, the best strategy to start as quickly as possible\n * an application that causes a burst of queue creations is to\n * weight-raise all the queues created during the burst. This is the\n * exact opposite of the best strategy for the other type of bursts.\n *\n * In the end, to take the best action for each of the two cases, the\n * two types of bursts need to be distinguished. Fortunately, this\n * seems relatively easy, by looking at the sizes of the bursts. In\n * particular, we found a threshold such that only bursts with a\n * larger size than that threshold are apparently caused by\n * services or commands such as systemd or git grep. For brevity,\n * hereafter we call just 'large' these bursts. BFQ *does not*\n * weight-raise queues whose creation occurs in a large burst. In\n * addition, for each of these queues BFQ performs or does not perform\n * idling depending on which choice boosts the throughput more. The\n * exact choice depends on the device and request pattern at\n * hand.\n *\n * Unfortunately, false positives may occur while an interactive task\n * is starting (e.g., an application is being started). The\n * consequence is that the queues associated with the task do not\n * enjoy weight raising as expected. Fortunately these false positives\n * are very rare. They typically occur if some service happens to\n * start doing I/O exactly when the interactive task starts.\n *\n * Turning back to the next function, it is invoked only if there are\n * no active queues (apart from active queues that would belong to the\n * same, possible burst bfqq would belong to), and it implements all\n * the steps needed to detect the occurrence of a large burst and to\n * properly mark all the queues belonging to it (so that they can then\n * be treated in a different way). This goal is achieved by\n * maintaining a \"burst list\" that holds, temporarily, the queues that\n * belong to the burst in progress. The list is then used to mark\n * these queues as belonging to a large burst if the burst does become\n * large. The main steps are the following.\n *\n * . when the very first queue is created, the queue is inserted into the\n *   list (as it could be the first queue in a possible burst)\n *\n * . if the current burst has not yet become large, and a queue Q that does\n *   not yet belong to the burst is activated shortly after the last time\n *   at which a new queue entered the burst list, then the function appends\n *   Q to the burst list\n *\n * . if, as a consequence of the previous step, the burst size reaches\n *   the large-burst threshold, then\n *\n *     . all the queues in the burst list are marked as belonging to a\n *       large burst\n *\n *     . the burst list is deleted; in fact, the burst list already served\n *       its purpose (keeping temporarily track of the queues in a burst,\n *       so as to be able to mark them as belonging to a large burst in the\n *       previous sub-step), and now is not needed any more\n *\n *     . the device enters a large-burst mode\n *\n * . if a queue Q that does not belong to the burst is created while\n *   the device is in large-burst mode and shortly after the last time\n *   at which a queue either entered the burst list or was marked as\n *   belonging to the current large burst, then Q is immediately marked\n *   as belonging to a large burst.\n *\n * . if a queue Q that does not belong to the burst is created a while\n *   later, i.e., not shortly after, than the last time at which a queue\n *   either entered the burst list or was marked as belonging to the\n *   current large burst, then the current burst is deemed as finished and:\n *\n *        . the large-burst mode is reset if set\n *\n *        . the burst list is emptied\n *\n *        . Q is inserted in the burst list, as Q may be the first queue\n *          in a possible new burst (then the burst list contains just Q\n *          after this step).\n */\nstatic void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq is already in the burst list or is part of a large\n\t * burst, or finally has just been split, then there is\n\t * nothing else to do.\n\t */\n\tif (!hlist_unhashed(&bfqq->burst_list_node) ||\n\t    bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     msecs_to_jiffies(10)))\n\t\treturn;\n\n\t/*\n\t * If bfqq's creation happens late enough, or bfqq belongs to\n\t * a different group than the burst group, then the current\n\t * burst is finished, and related data structures must be\n\t * reset.\n\t *\n\t * In this respect, consider the special case where bfqq is\n\t * the very first queue created after BFQ is selected for this\n\t * device. In this case, last_ins_in_burst and\n\t * burst_parent_entity are not yet significant when we get\n\t * here. But it is easy to verify that, whether or not the\n\t * following condition is true, bfqq will end up being\n\t * inserted into the burst list. In particular the list will\n\t * happen to contain only bfqq. And this is exactly what has\n\t * to happen, as bfqq may be the first queue of the first\n\t * burst.\n\t */\n\tif (time_is_before_jiffies(bfqd->last_ins_in_burst +\n\t    bfqd->bfq_burst_interval) ||\n\t    bfqq->entity.parent != bfqd->burst_parent_entity) {\n\t\tbfqd->large_burst = false;\n\t\tbfq_reset_burst_list(bfqd, bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then bfqq is being activated shortly after the\n\t * last queue. So, if the current burst is also large, we can mark\n\t * bfqq as belonging to this large burst immediately.\n\t */\n\tif (bfqd->large_burst) {\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\tgoto end;\n\t}\n\n\t/*\n\t * If we get here, then a large-burst state has not yet been\n\t * reached, but bfqq is being activated shortly after the last\n\t * queue. Then we add bfqq to the burst.\n\t */\n\tbfq_add_to_burst(bfqd, bfqq);\nend:\n\t/*\n\t * At this point, bfqq either has been added to the current\n\t * burst or has caused the current burst to terminate and a\n\t * possible new burst to start. In particular, in the second\n\t * case, bfqq has become the first queue in the possible new\n\t * burst.  In both cases last_ins_in_burst needs to be moved\n\t * forward.\n\t */\n\tbfqd->last_ins_in_burst = jiffies;\n}\n\nstatic int bfq_bfqq_budget_left(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\treturn entity->budget - entity->service;\n}\n\n/*\n * If enough samples have been computed, return the current max budget\n * stored in bfqd, which is dynamically updated according to the\n * estimated disk peak rate; otherwise return the default max budget\n */\nstatic int bfq_max_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget;\n\telse\n\t\treturn bfqd->bfq_max_budget;\n}\n\n/*\n * Return min budget, which is a fraction of the current or default\n * max budget (trying with 1/32)\n */\nstatic int bfq_min_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget / 32;\n\telse\n\t\treturn bfqd->bfq_max_budget / 32;\n}\n\n/*\n * The next function, invoked after the input queue bfqq switches from\n * idle to busy, updates the budget of bfqq. The function also tells\n * whether the in-service queue should be expired, by returning\n * true. The purpose of expiring the in-service queue is to give bfqq\n * the chance to possibly preempt the in-service queue, and the reason\n * for preempting the in-service queue is to achieve one of the two\n * goals below.\n *\n * 1. Guarantee to bfqq its reserved bandwidth even if bfqq has\n * expired because it has remained idle. In particular, bfqq may have\n * expired for one of the following two reasons:\n *\n * - BFQQE_NO_MORE_REQUESTS bfqq did not enjoy any device idling\n *   and did not make it to issue a new request before its last\n *   request was served;\n *\n * - BFQQE_TOO_IDLE bfqq did enjoy device idling, but did not issue\n *   a new request before the expiration of the idling-time.\n *\n * Even if bfqq has expired for one of the above reasons, the process\n * associated with the queue may be however issuing requests greedily,\n * and thus be sensitive to the bandwidth it receives (bfqq may have\n * remained idle for other reasons: CPU high load, bfqq not enjoying\n * idling, I/O throttling somewhere in the path from the process to\n * the I/O scheduler, ...). But if, after every expiration for one of\n * the above two reasons, bfqq has to wait for the service of at least\n * one full budget of another queue before being served again, then\n * bfqq is likely to get a much lower bandwidth or resource time than\n * its reserved ones. To address this issue, two countermeasures need\n * to be taken.\n *\n * First, the budget and the timestamps of bfqq need to be updated in\n * a special way on bfqq reactivation: they need to be updated as if\n * bfqq did not remain idle and did not expire. In fact, if they are\n * computed as if bfqq expired and remained idle until reactivation,\n * then the process associated with bfqq is treated as if, instead of\n * being greedy, it stopped issuing requests when bfqq remained idle,\n * and restarts issuing requests only on this reactivation. In other\n * words, the scheduler does not help the process recover the \"service\n * hole\" between bfqq expiration and reactivation. As a consequence,\n * the process receives a lower bandwidth than its reserved one. In\n * contrast, to recover this hole, the budget must be updated as if\n * bfqq was not expired at all before this reactivation, i.e., it must\n * be set to the value of the remaining budget when bfqq was\n * expired. Along the same line, timestamps need to be assigned the\n * value they had the last time bfqq was selected for service, i.e.,\n * before last expiration. Thus timestamps need to be back-shifted\n * with respect to their normal computation (see [1] for more details\n * on this tricky aspect).\n *\n * Secondly, to allow the process to recover the hole, the in-service\n * queue must be expired too, to give bfqq the chance to preempt it\n * immediately. In fact, if bfqq has to wait for a full budget of the\n * in-service queue to be completed, then it may become impossible to\n * let the process recover the hole, even if the back-shifted\n * timestamps of bfqq are lower than those of the in-service queue. If\n * this happens for most or all of the holes, then the process may not\n * receive its reserved bandwidth. In this respect, it is worth noting\n * that, being the service of outstanding requests unpreemptible, a\n * little fraction of the holes may however be unrecoverable, thereby\n * causing a little loss of bandwidth.\n *\n * The last important point is detecting whether bfqq does need this\n * bandwidth recovery. In this respect, the next function deems the\n * process associated with bfqq greedy, and thus allows it to recover\n * the hole, if: 1) the process is waiting for the arrival of a new\n * request (which implies that bfqq expired for one of the above two\n * reasons), and 2) such a request has arrived soon. The first\n * condition is controlled through the flag non_blocking_wait_rq,\n * while the second through the flag arrived_in_time. If both\n * conditions hold, then the function computes the budget in the\n * above-described special way, and signals that the in-service queue\n * should be expired. Timestamp back-shifting is done later in\n * __bfq_activate_entity.\n *\n * 2. Reduce latency. Even if timestamps are not backshifted to let\n * the process associated with bfqq recover a service hole, bfqq may\n * however happen to have, after being (re)activated, a lower finish\n * timestamp than the in-service queue.\t That is, the next budget of\n * bfqq may have to be completed before the one of the in-service\n * queue. If this is the case, then preempting the in-service queue\n * allows this goal to be achieved, apart from the unpreemptible,\n * outstanding requests mentioned above.\n *\n * Unfortunately, regardless of which of the above two goals one wants\n * to achieve, service trees need first to be updated to know whether\n * the in-service queue must be preempted. To have service trees\n * correctly updated, the in-service queue must be expired and\n * rescheduled, and bfqq must be scheduled too. This is one of the\n * most costly operations (in future versions, the scheduling\n * mechanism may be re-designed in such a way to make it possible to\n * know whether preemption is needed without needing to update service\n * trees). In addition, queue preemptions almost always cause random\n * I/O, which may in turn cause loss of throughput. Finally, there may\n * even be no in-service queue when the next function is invoked (so,\n * no queue to compare timestamps with). Because of these facts, the\n * next function adopts the following simple scheme to avoid costly\n * operations, too frequent preemptions and too many dependencies on\n * the state of the scheduler: it requests the expiration of the\n * in-service queue (unconditionally) only for queues that need to\n * recover a hole. Then it delegates to other parts of the code the\n * responsibility of handling the above case 2.\n */\nstatic bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\t\tbool arrived_in_time)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * In the next compound condition, we check also whether there\n\t * is some budget left, because otherwise there is no point in\n\t * trying to go on serving bfqq with this same budget: bfqq\n\t * would be expired immediately after being selected for\n\t * service. This would only cause useless overhead.\n\t */\n\tif (bfq_bfqq_non_blocking_wait_rq(bfqq) && arrived_in_time &&\n\t    bfq_bfqq_budget_left(bfqq) > 0) {\n\t\t/*\n\t\t * We do not clear the flag non_blocking_wait_rq here, as\n\t\t * the latter is used in bfq_activate_bfqq to signal\n\t\t * that timestamps need to be back-shifted (and is\n\t\t * cleared right after).\n\t\t */\n\n\t\t/*\n\t\t * In next assignment we rely on that either\n\t\t * entity->service or entity->budget are not updated\n\t\t * on expiration if bfqq is empty (see\n\t\t * __bfq_bfqq_recalc_budget). Thus both quantities\n\t\t * remain unchanged after such an expiration, and the\n\t\t * following statement therefore assigns to\n\t\t * entity->budget the remaining budget on such an\n\t\t * expiration.\n\t\t */\n\t\tentity->budget = min_t(unsigned long,\n\t\t\t\t       bfq_bfqq_budget_left(bfqq),\n\t\t\t\t       bfqq->max_budget);\n\n\t\t/*\n\t\t * At this point, we have used entity->service to get\n\t\t * the budget left (needed for updating\n\t\t * entity->budget). Thus we finally can, and have to,\n\t\t * reset entity->service. The latter must be reset\n\t\t * because bfqq would otherwise be charged again for\n\t\t * the service it has received during its previous\n\t\t * service slot(s).\n\t\t */\n\t\tentity->service = 0;\n\n\t\treturn true;\n\t}\n\n\t/*\n\t * We can finally complete expiration, by setting service to 0.\n\t */\n\tentity->service = 0;\n\tentity->budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t       bfq_serv_to_charge(bfqq->next_rq, bfqq));\n\tbfq_clear_bfqq_non_blocking_wait_rq(bfqq);\n\treturn false;\n}\n\n/*\n * Return the farthest past time instant according to jiffies\n * macros.\n */\nstatic unsigned long bfq_smallest_from_now(void)\n{\n\treturn jiffies - MAX_JIFFY_OFFSET;\n}\n\nstatic void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     unsigned int old_wr_coeff,\n\t\t\t\t\t     bool wr_or_deserves_wr,\n\t\t\t\t\t     bool interactive,\n\t\t\t\t\t     bool in_burst,\n\t\t\t\t\t     bool soft_rt)\n{\n\tif (old_wr_coeff == 1 && wr_or_deserves_wr) {\n\t\t/* start a weight-raising period */\n\t\tif (interactive) {\n\t\t\tbfqq->service_from_wr = 0;\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else {\n\t\t\t/*\n\t\t\t * No interactive weight raising in progress\n\t\t\t * here: assign minus infinity to\n\t\t\t * wr_start_at_switch_to_srt, to make sure\n\t\t\t * that, at the end of the soft-real-time\n\t\t\t * weight raising periods that is starting\n\t\t\t * now, no interactive weight-raising period\n\t\t\t * may be wrongly considered as still in\n\t\t\t * progress (and thus actually started by\n\t\t\t * mistake).\n\t\t\t */\n\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\tbfq_smallest_from_now();\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t}\n\n\t\t/*\n\t\t * If needed, further reduce budget to make sure it is\n\t\t * close to bfqq's backlog, so as to reduce the\n\t\t * scheduling-error component due to a too large\n\t\t * budget. Do not care about throughput consequences,\n\t\t * but only about latency. Finally, do not assign a\n\t\t * too small budget either, to avoid increasing\n\t\t * latency by causing too frequent expirations.\n\t\t */\n\t\tbfqq->entity.budget = min_t(unsigned long,\n\t\t\t\t\t    bfqq->entity.budget,\n\t\t\t\t\t    2 * bfq_min_budget(bfqd));\n\t} else if (old_wr_coeff > 1) {\n\t\tif (interactive) { /* update wr coeff and duration */\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else if (in_burst)\n\t\t\tbfqq->wr_coeff = 1;\n\t\telse if (soft_rt) {\n\t\t\t/*\n\t\t\t * The application is now or still meeting the\n\t\t\t * requirements for being deemed soft rt.  We\n\t\t\t * can then correctly and safely (re)charge\n\t\t\t * the weight-raising duration for the\n\t\t\t * application with the weight-raising\n\t\t\t * duration for soft rt applications.\n\t\t\t *\n\t\t\t * In particular, doing this recharge now, i.e.,\n\t\t\t * before the weight-raising period for the\n\t\t\t * application finishes, reduces the probability\n\t\t\t * of the following negative scenario:\n\t\t\t * 1) the weight of a soft rt application is\n\t\t\t *    raised at startup (as for any newly\n\t\t\t *    created application),\n\t\t\t * 2) since the application is not interactive,\n\t\t\t *    at a certain time weight-raising is\n\t\t\t *    stopped for the application,\n\t\t\t * 3) at that time the application happens to\n\t\t\t *    still have pending requests, and hence\n\t\t\t *    is destined to not have a chance to be\n\t\t\t *    deemed soft rt before these requests are\n\t\t\t *    completed (see the comments to the\n\t\t\t *    function bfq_bfqq_softrt_next_start()\n\t\t\t *    for details on soft rt detection),\n\t\t\t * 4) these pending requests experience a high\n\t\t\t *    latency because the application is not\n\t\t\t *    weight-raised while they are pending.\n\t\t\t */\n\t\t\tif (bfqq->wr_cur_max_time !=\n\t\t\t\tbfqd->bfq_wr_rt_max_time) {\n\t\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\t\tbfqq->last_wr_start_finish;\n\n\t\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\t}\n\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\t}\n}\n\nstatic bool bfq_bfqq_idle_for_long_time(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn bfqq->dispatched == 0 &&\n\t\ttime_is_before_jiffies(\n\t\t\tbfqq->budget_timeout +\n\t\t\tbfqd->bfq_wr_min_idle_time);\n}\n\n\n/*\n * Return true if bfqq is in a higher priority class, or has a higher\n * weight than the in-service queue.\n */\nstatic bool bfq_bfqq_higher_class_or_weight(struct bfq_queue *bfqq,\n\t\t\t\t\t    struct bfq_queue *in_serv_bfqq)\n{\n\tint bfqq_weight, in_serv_weight;\n\n\tif (bfqq->ioprio_class < in_serv_bfqq->ioprio_class)\n\t\treturn true;\n\n\tif (in_serv_bfqq->entity.parent == bfqq->entity.parent) {\n\t\tbfqq_weight = bfqq->entity.weight;\n\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t} else {\n\t\tif (bfqq->entity.parent)\n\t\t\tbfqq_weight = bfqq->entity.parent->weight;\n\t\telse\n\t\t\tbfqq_weight = bfqq->entity.weight;\n\t\tif (in_serv_bfqq->entity.parent)\n\t\t\tin_serv_weight = in_serv_bfqq->entity.parent->weight;\n\t\telse\n\t\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t}\n\n\treturn bfqq_weight > in_serv_weight;\n}\n\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq);\n\nstatic void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     int old_wr_coeff,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     bool *interactive)\n{\n\tbool soft_rt, in_burst,\twr_or_deserves_wr,\n\t\tbfqq_wants_to_preempt,\n\t\tidle_for_long_time = bfq_bfqq_idle_for_long_time(bfqd, bfqq),\n\t\t/*\n\t\t * See the comments on\n\t\t * bfq_bfqq_update_budg_for_activation for\n\t\t * details on the usage of the next variable.\n\t\t */\n\t\tarrived_in_time =  ktime_get_ns() <=\n\t\t\tbfqq->ttime.last_end_request +\n\t\t\tbfqd->bfq_slice_idle * 3;\n\n\n\t/*\n\t * bfqq deserves to be weight-raised if:\n\t * - it is sync,\n\t * - it does not belong to a large burst,\n\t * - it has been idle for enough time or is soft real-time,\n\t * - is linked to a bfq_io_cq (it is not shared in any sense),\n\t * - has a default weight (otherwise we assume the user wanted\n\t *   to control its weight explicitly)\n\t */\n\tin_burst = bfq_bfqq_in_large_burst(bfqq);\n\tsoft_rt = bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t\t!BFQQ_TOTALLY_SEEKY(bfqq) &&\n\t\t!in_burst &&\n\t\ttime_is_before_jiffies(bfqq->soft_rt_next_start) &&\n\t\tbfqq->dispatched == 0 &&\n\t\tbfqq->entity.new_weight == 40;\n\t*interactive = !in_burst && idle_for_long_time &&\n\t\tbfqq->entity.new_weight == 40;\n\t/*\n\t * Merged bfq_queues are kept out of weight-raising\n\t * (low-latency) mechanisms. The reason is that these queues\n\t * are usually created for non-interactive and\n\t * non-soft-real-time tasks. Yet this is not the case for\n\t * stably-merged queues. These queues are merged just because\n\t * they are created shortly after each other. So they may\n\t * easily serve the I/O of an interactive or soft-real time\n\t * application, if the application happens to spawn multiple\n\t * processes. So let also stably-merged queued enjoy weight\n\t * raising.\n\t */\n\twr_or_deserves_wr = bfqd->low_latency &&\n\t\t(bfqq->wr_coeff > 1 ||\n\t\t (bfq_bfqq_sync(bfqq) &&\n\t\t  (bfqq->bic || RQ_BIC(rq)->stably_merged) &&\n\t\t   (*interactive || soft_rt)));\n\n\t/*\n\t * Using the last flag, update budget and check whether bfqq\n\t * may want to preempt the in-service queue.\n\t */\n\tbfqq_wants_to_preempt =\n\t\tbfq_bfqq_update_budg_for_activation(bfqd, bfqq,\n\t\t\t\t\t\t    arrived_in_time);\n\n\t/*\n\t * If bfqq happened to be activated in a burst, but has been\n\t * idle for much more than an interactive queue, then we\n\t * assume that, in the overall I/O initiated in the burst, the\n\t * I/O associated with bfqq is finished. So bfqq does not need\n\t * to be treated as a queue belonging to a burst\n\t * anymore. Accordingly, we reset bfqq's in_large_burst flag\n\t * if set, and remove bfqq from the burst list if it's\n\t * there. We do not decrement burst_size, because the fact\n\t * that bfqq does not need to belong to the burst list any\n\t * more does not invalidate the fact that bfqq was created in\n\t * a burst.\n\t */\n\tif (likely(!bfq_bfqq_just_created(bfqq)) &&\n\t    idle_for_long_time &&\n\t    time_is_before_jiffies(\n\t\t    bfqq->budget_timeout +\n\t\t    msecs_to_jiffies(10000))) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t}\n\n\tbfq_clear_bfqq_just_created(bfqq);\n\n\tif (bfqd->low_latency) {\n\t\tif (unlikely(time_is_after_jiffies(bfqq->split_time)))\n\t\t\t/* wraparound */\n\t\t\tbfqq->split_time =\n\t\t\t\tjiffies - bfqd->bfq_wr_min_idle_time - 1;\n\n\t\tif (time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t   bfqd->bfq_wr_min_idle_time)) {\n\t\t\tbfq_update_bfqq_wr_on_rq_arrival(bfqd, bfqq,\n\t\t\t\t\t\t\t old_wr_coeff,\n\t\t\t\t\t\t\t wr_or_deserves_wr,\n\t\t\t\t\t\t\t *interactive,\n\t\t\t\t\t\t\t in_burst,\n\t\t\t\t\t\t\t soft_rt);\n\n\t\t\tif (old_wr_coeff != bfqq->wr_coeff)\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n\n\tbfqq->last_idle_bklogged = jiffies;\n\tbfqq->service_from_backlogged = 0;\n\tbfq_clear_bfqq_softrt_update(bfqq);\n\n\tbfq_add_bfqq_busy(bfqq);\n\n\t/*\n\t * Expire in-service queue if preemption may be needed for\n\t * guarantees or throughput. As for guarantees, we care\n\t * explicitly about two cases. The first is that bfqq has to\n\t * recover a service hole, as explained in the comments on\n\t * bfq_bfqq_update_budg_for_activation(), i.e., that\n\t * bfqq_wants_to_preempt is true. However, if bfqq does not\n\t * carry time-critical I/O, then bfqq's bandwidth is less\n\t * important than that of queues that carry time-critical I/O.\n\t * So, as a further constraint, we consider this case only if\n\t * bfqq is at least as weight-raised, i.e., at least as time\n\t * critical, as the in-service queue.\n\t *\n\t * The second case is that bfqq is in a higher priority class,\n\t * or has a higher weight than the in-service queue. If this\n\t * condition does not hold, we don't care because, even if\n\t * bfqq does not start to be served immediately, the resulting\n\t * delay for bfqq's I/O is however lower or much lower than\n\t * the ideal completion time to be guaranteed to bfqq's I/O.\n\t *\n\t * In both cases, preemption is needed only if, according to\n\t * the timestamps of both bfqq and of the in-service queue,\n\t * bfqq actually is the next queue to serve. So, to reduce\n\t * useless preemptions, the return value of\n\t * next_queue_may_preempt() is considered in the next compound\n\t * condition too. Yet next_queue_may_preempt() just checks a\n\t * simple, necessary condition for bfqq to be the next queue\n\t * to serve. In fact, to evaluate a sufficient condition, the\n\t * timestamps of the in-service queue would need to be\n\t * updated, and this operation is quite costly (see the\n\t * comments on bfq_bfqq_update_budg_for_activation()).\n\t *\n\t * As for throughput, we ask bfq_better_to_idle() whether we\n\t * still need to plug I/O dispatching. If bfq_better_to_idle()\n\t * says no, then plugging is not needed any longer, either to\n\t * boost throughput or to perserve service guarantees. Then\n\t * the best option is to stop plugging I/O, as not doing so\n\t * would certainly lower throughput. We may end up in this\n\t * case if: (1) upon a dispatch attempt, we detected that it\n\t * was better to plug I/O dispatch, and to wait for a new\n\t * request to arrive for the currently in-service queue, but\n\t * (2) this switch of bfqq to busy changes the scenario.\n\t */\n\tif (bfqd->in_service_queue &&\n\t    ((bfqq_wants_to_preempt &&\n\t      bfqq->wr_coeff >= bfqd->in_service_queue->wr_coeff) ||\n\t     bfq_bfqq_higher_class_or_weight(bfqq, bfqd->in_service_queue) ||\n\t     !bfq_better_to_idle(bfqd->in_service_queue)) &&\n\t    next_queue_may_preempt(bfqd))\n\t\tbfq_bfqq_expire(bfqd, bfqd->in_service_queue,\n\t\t\t\tfalse, BFQQE_PREEMPTED);\n}\n\nstatic void bfq_reset_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\t/* invalidate baseline total service time */\n\tbfqq->last_serv_time_ns = 0;\n\n\t/*\n\t * Reset pointer in case we are waiting for\n\t * some request completion.\n\t */\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * If bfqq has a short think time, then start by setting the\n\t * inject limit to 0 prudentially, because the service time of\n\t * an injected I/O request may be higher than the think time\n\t * of bfqq, and therefore, if one request was injected when\n\t * bfqq remains empty, this injected request might delay the\n\t * service of the next I/O request for bfqq significantly. In\n\t * case bfqq can actually tolerate some injection, then the\n\t * adaptive update will however raise the limit soon. This\n\t * lucky circumstance holds exactly because bfqq has a short\n\t * think time, and thus, after remaining empty, is likely to\n\t * get new I/O enqueued---and then completed---before being\n\t * expired. This is the very pattern that gives the\n\t * limit-update algorithm the chance to measure the effect of\n\t * injection on request service times, and then to update the\n\t * limit accordingly.\n\t *\n\t * However, in the following special case, the inject limit is\n\t * left to 1 even if the think time is short: bfqq's I/O is\n\t * synchronized with that of some other queue, i.e., bfqq may\n\t * receive new I/O only after the I/O of the other queue is\n\t * completed. Keeping the inject limit to 1 allows the\n\t * blocking I/O to be served while bfqq is in service. And\n\t * this is very convenient both for bfqq and for overall\n\t * throughput, as explained in detail in the comments in\n\t * bfq_update_has_short_ttime().\n\t *\n\t * On the opposite end, if bfqq has a long think time, then\n\t * start directly by 1, because:\n\t * a) on the bright side, keeping at most one request in\n\t * service in the drive is unlikely to cause any harm to the\n\t * latency of bfqq's requests, as the service time of a single\n\t * request is likely to be lower than the think time of bfqq;\n\t * b) on the downside, after becoming empty, bfqq is likely to\n\t * expire before getting its next request. With this request\n\t * arrival pattern, it is very hard to sample total service\n\t * times and update the inject limit accordingly (see comments\n\t * on bfq_update_inject_limit()). So the limit is likely to be\n\t * never, or at least seldom, updated.  As a consequence, by\n\t * setting the limit to 1, we avoid that no injection ever\n\t * occurs with bfqq. On the downside, this proactive step\n\t * further reduces chances to actually compute the baseline\n\t * total service time. Thus it reduces chances to execute the\n\t * limit-update algorithm and possibly raise the limit to more\n\t * than 1.\n\t */\n\tif (bfq_bfqq_has_short_ttime(bfqq))\n\t\tbfqq->inject_limit = 0;\n\telse\n\t\tbfqq->inject_limit = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)\n{\n\tu64 tot_io_time = now_ns - bfqq->io_start_time;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) && bfqq->dispatched == 0)\n\t\tbfqq->tot_idle_time +=\n\t\t\tnow_ns - bfqq->ttime.last_end_request;\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq)))\n\t\treturn;\n\n\t/*\n\t * Must be busy for at least about 80% of the time to be\n\t * considered I/O bound.\n\t */\n\tif (bfqq->tot_idle_time * 5 > tot_io_time)\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * Keep an observation window of at most 200 ms in the past\n\t * from now.\n\t */\n\tif (tot_io_time > 200 * NSEC_PER_MSEC) {\n\t\tbfqq->io_start_time = now_ns - (tot_io_time>>1);\n\t\tbfqq->tot_idle_time >>= 1;\n\t}\n}\n\n/*\n * Detect whether bfqq's I/O seems synchronized with that of some\n * other queue, i.e., whether bfqq, after remaining empty, happens to\n * receive new I/O only right after some I/O request of the other\n * queue has been completed. We call waker queue the other queue, and\n * we assume, for simplicity, that bfqq may have at most one waker\n * queue.\n *\n * A remarkable throughput boost can be reached by unconditionally\n * injecting the I/O of the waker queue, every time a new\n * bfq_dispatch_request happens to be invoked while I/O is being\n * plugged for bfqq.  In addition to boosting throughput, this\n * unblocks bfqq's I/O, thereby improving bandwidth and latency for\n * bfqq. Note that these same results may be achieved with the general\n * injection mechanism, but less effectively. For details on this\n * aspect, see the comments on the choice of the queue for injection\n * in bfq_select_queue().\n *\n * Turning back to the detection of a waker queue, a queue Q is deemed as a\n * waker queue for bfqq if, for three consecutive times, bfqq happens to become\n * non empty right after a request of Q has been completed within given\n * timeout. In this respect, even if bfqq is empty, we do not check for a waker\n * if it still has some in-flight I/O. In fact, in this case bfqq is actually\n * still being served by the drive, and may receive new I/O on the completion\n * of some of the in-flight requests. In particular, on the first time, Q is\n * tentatively set as a candidate waker queue, while on the third consecutive\n * time that Q is detected, the field waker_bfqq is set to Q, to confirm that Q\n * is a waker queue for bfqq. These detection steps are performed only if bfqq\n * has a long think time, so as to make it more likely that bfqq's I/O is\n * actually being blocked by a synchronization. This last filter, plus the\n * above three-times requirement and time limit for detection, make false\n * positives less likely.\n *\n * NOTE\n *\n * The sooner a waker queue is detected, the sooner throughput can be\n * boosted by injecting I/O from the waker queue. Fortunately,\n * detection is likely to be actually fast, for the following\n * reasons. While blocked by synchronization, bfqq has a long think\n * time. This implies that bfqq's inject limit is at least equal to 1\n * (see the comments in bfq_update_inject_limit()). So, thanks to\n * injection, the waker queue is likely to be served during the very\n * first I/O-plugging time interval for bfqq. This triggers the first\n * step of the detection mechanism. Thanks again to injection, the\n * candidate waker queue is then likely to be confirmed no later than\n * during the next I/O-plugging interval for bfqq.\n *\n * ISSUE\n *\n * On queue merging all waker information is lost.\n */\nstatic void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    u64 now_ns)\n{\n\tchar waker_name[MAX_BFQQ_NAME_LENGTH];\n\n\tif (!bfqd->last_completed_rq_bfqq ||\n\t    bfqd->last_completed_rq_bfqq == bfqq ||\n\t    bfq_bfqq_has_short_ttime(bfqq) ||\n\t    now_ns - bfqd->last_completion >= 4 * NSEC_PER_MSEC)\n\t\treturn;\n\n\t/*\n\t * We reset waker detection logic also if too much time has passed\n \t * since the first detection. If wakeups are rare, pointless idling\n\t * doesn't hurt throughput that much. The condition below makes sure\n\t * we do not uselessly idle blocking waker in more than 1/64 cases. \n\t */\n\tif (bfqd->last_completed_rq_bfqq !=\n\t    bfqq->tentative_waker_bfqq ||\n\t    now_ns > bfqq->waker_detection_started +\n\t\t\t\t\t128 * (u64)bfqd->bfq_slice_idle) {\n\t\t/*\n\t\t * First synchronization detected with a\n\t\t * candidate waker queue, or with a different\n\t\t * candidate waker queue from the current one.\n\t\t */\n\t\tbfqq->tentative_waker_bfqq =\n\t\t\tbfqd->last_completed_rq_bfqq;\n\t\tbfqq->num_waker_detections = 1;\n\t\tbfqq->waker_detection_started = now_ns;\n\t\tbfq_bfqq_name(bfqq->tentative_waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set tentative waker %s\", waker_name);\n\t} else /* Same tentative waker queue detected again */\n\t\tbfqq->num_waker_detections++;\n\n\tif (bfqq->num_waker_detections == 3) {\n\t\tbfqq->waker_bfqq = bfqd->last_completed_rq_bfqq;\n\t\tbfqq->tentative_waker_bfqq = NULL;\n\t\tbfq_bfqq_name(bfqq->waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set waker %s\", waker_name);\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * bfqq->waker_bfqq must be reset. To\n\t\t * this goal, we maintain in each\n\t\t * waker queue a list, woken_list, of\n\t\t * all the queues that reference the\n\t\t * waker queue through their\n\t\t * waker_bfqq pointer. When the waker\n\t\t * queue exits, the waker_bfqq pointer\n\t\t * of all the queues in the woken_list\n\t\t * is reset.\n\t\t *\n\t\t * In addition, if bfqq is already in\n\t\t * the woken_list of a waker queue,\n\t\t * then, before being inserted into\n\t\t * the woken_list of a new waker\n\t\t * queue, bfqq must be removed from\n\t\t * the woken_list of the old waker\n\t\t * queue.\n\t\t */\n\t\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\t\thlist_del_init(&bfqq->woken_list_node);\n\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t       &bfqd->last_completed_rq_bfqq->woken_list);\n\t}\n}\n\nstatic void bfq_add_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct request *next_rq, *prev;\n\tunsigned int old_wr_coeff = bfqq->wr_coeff;\n\tbool interactive = false;\n\tu64 now_ns = ktime_get_ns();\n\n\tbfq_log_bfqq(bfqd, bfqq, \"add_request %d\", rq_is_sync(rq));\n\tbfqq->queued[rq_is_sync(rq)]++;\n\t/*\n\t * Updating of 'bfqd->queued' is protected by 'bfqd->lock', however, it\n\t * may be read without holding the lock in bfq_has_work().\n\t */\n\tWRITE_ONCE(bfqd->queued, bfqd->queued + 1);\n\n\tif (bfq_bfqq_sync(bfqq) && RQ_BIC(rq)->requests <= 1) {\n\t\tbfq_check_waker(bfqd, bfqq, now_ns);\n\n\t\t/*\n\t\t * Periodically reset inject limit, to make sure that\n\t\t * the latter eventually drops in case workload\n\t\t * changes, see step (3) in the comments on\n\t\t * bfq_update_inject_limit().\n\t\t */\n\t\tif (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t     msecs_to_jiffies(1000)))\n\t\t\tbfq_reset_inject_limit(bfqd, bfqq);\n\n\t\t/*\n\t\t * The following conditions must hold to setup a new\n\t\t * sampling of total service time, and then a new\n\t\t * update of the inject limit:\n\t\t * - bfqq is in service, because the total service\n\t\t *   time is evaluated only for the I/O requests of\n\t\t *   the queues in service;\n\t\t * - this is the right occasion to compute or to\n\t\t *   lower the baseline total service time, because\n\t\t *   there are actually no requests in the drive,\n\t\t *   or\n\t\t *   the baseline total service time is available, and\n\t\t *   this is the right occasion to compute the other\n\t\t *   quantity needed to update the inject limit, i.e.,\n\t\t *   the total service time caused by the amount of\n\t\t *   injection allowed by the current value of the\n\t\t *   limit. It is the right occasion because injection\n\t\t *   has actually been performed during the service\n\t\t *   hole, and there are still in-flight requests,\n\t\t *   which are very likely to be exactly the injected\n\t\t *   requests, or part of them;\n\t\t * - the minimum interval for sampling the total\n\t\t *   service time and updating the inject limit has\n\t\t *   elapsed.\n\t\t */\n\t\tif (bfqq == bfqd->in_service_queue &&\n\t\t    (bfqd->rq_in_driver == 0 ||\n\t\t     (bfqq->last_serv_time_ns > 0 &&\n\t\t      bfqd->rqs_injected && bfqd->rq_in_driver > 0)) &&\n\t\t    time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t      msecs_to_jiffies(10))) {\n\t\t\tbfqd->last_empty_occupied_ns = ktime_get_ns();\n\t\t\t/*\n\t\t\t * Start the state machine for measuring the\n\t\t\t * total service time of rq: setting\n\t\t\t * wait_dispatch will cause bfqd->waited_rq to\n\t\t\t * be set when rq will be dispatched.\n\t\t\t */\n\t\t\tbfqd->wait_dispatch = true;\n\t\t\t/*\n\t\t\t * If there is no I/O in service in the drive,\n\t\t\t * then possible injection occurred before the\n\t\t\t * arrival of rq will not affect the total\n\t\t\t * service time of rq. So the injection limit\n\t\t\t * must not be updated as a function of such\n\t\t\t * total service time, unless new injection\n\t\t\t * occurs before rq is completed. To have the\n\t\t\t * injection limit updated only in the latter\n\t\t\t * case, reset rqs_injected here (rqs_injected\n\t\t\t * will be set in case injection is performed\n\t\t\t * on bfqq before rq is completed).\n\t\t\t */\n\t\t\tif (bfqd->rq_in_driver == 0)\n\t\t\t\tbfqd->rqs_injected = false;\n\t\t}\n\t}\n\n\tif (bfq_bfqq_sync(bfqq))\n\t\tbfq_update_io_intensity(bfqq, now_ns);\n\n\telv_rb_add(&bfqq->sort_list, rq);\n\n\t/*\n\t * Check if this request is a better next-serve candidate.\n\t */\n\tprev = bfqq->next_rq;\n\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, rq, bfqd->last_position);\n\tbfqq->next_rq = next_rq;\n\n\t/*\n\t * Adjust priority tree position, if next_rq changes.\n\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing && prev != bfqq->next_rq))\n\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\n\tif (!bfq_bfqq_busy(bfqq)) /* switching to busy ... */\n\t\tbfq_bfqq_handle_idle_busy_switch(bfqd, bfqq, old_wr_coeff,\n\t\t\t\t\t\t rq, &interactive);\n\telse {\n\t\tif (bfqd->low_latency && old_wr_coeff == 1 && !rq_is_sync(rq) &&\n\t\t    time_is_before_jiffies(\n\t\t\t\tbfqq->last_wr_start_finish +\n\t\t\t\tbfqd->bfq_wr_min_inter_arr_async)) {\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\n\t\t\tbfqd->wr_busy_queues++;\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t\tif (prev != bfqq->next_rq)\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\t/*\n\t * Assign jiffies to last_wr_start_finish in the following\n\t * cases:\n\t *\n\t * . if bfqq is not going to be weight-raised, because, for\n\t *   non weight-raised queues, last_wr_start_finish stores the\n\t *   arrival time of the last request; as of now, this piece\n\t *   of information is used only for deciding whether to\n\t *   weight-raise async queues\n\t *\n\t * . if bfqq is not weight-raised, because, if bfqq is now\n\t *   switching to weight-raised, then last_wr_start_finish\n\t *   stores the time when weight-raising starts\n\t *\n\t * . if bfqq is interactive, because, regardless of whether\n\t *   bfqq is currently weight-raised, the weight-raising\n\t *   period must start or restart (this case is considered\n\t *   separately because it is not detected by the above\n\t *   conditions, if bfqq is already weight-raised)\n\t *\n\t * last_wr_start_finish has to be updated also if bfqq is soft\n\t * real-time, because the weight-raising period is constantly\n\t * restarted on idle-to-busy transitions for these queues, but\n\t * this is already done in bfq_bfqq_handle_idle_busy_switch if\n\t * needed.\n\t */\n\tif (bfqd->low_latency &&\n\t\t(old_wr_coeff == 1 || bfqq->wr_coeff == 1 || interactive))\n\t\tbfqq->last_wr_start_finish = jiffies;\n}\n\nstatic struct request *bfq_find_rq_fmerge(struct bfq_data *bfqd,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq;\n\n\n\tif (bfqq)\n\t\treturn elv_rb_find(&bfqq->sort_list, bio_end_sector(bio));\n\n\treturn NULL;\n}\n\nstatic sector_t get_sdist(sector_t last_pos, struct request *rq)\n{\n\tif (last_pos)\n\t\treturn abs(blk_rq_pos(rq) - last_pos);\n\n\treturn 0;\n}\n\n#if 0 /* Still not clear if we can do without next two functions */\nstatic void bfq_activate_request(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\n\tbfqd->rq_in_driver++;\n}\n\nstatic void bfq_deactivate_request(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\n\tbfqd->rq_in_driver--;\n}\n#endif\n\nstatic void bfq_remove_request(struct request_queue *q,\n\t\t\t       struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tconst int sync = rq_is_sync(rq);\n\n\tif (bfqq->next_rq == rq) {\n\t\tbfqq->next_rq = bfq_find_next_rq(bfqd, bfqq, rq);\n\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\tif (rq->queuelist.prev != &rq->queuelist)\n\t\tlist_del_init(&rq->queuelist);\n\tbfqq->queued[sync]--;\n\t/*\n\t * Updating of 'bfqd->queued' is protected by 'bfqd->lock', however, it\n\t * may be read without holding the lock in bfq_has_work().\n\t */\n\tWRITE_ONCE(bfqd->queued, bfqd->queued - 1);\n\telv_rb_del(&bfqq->sort_list, rq);\n\n\telv_rqhash_del(q, rq);\n\tif (q->last_merge == rq)\n\t\tq->last_merge = NULL;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\tbfqq->next_rq = NULL;\n\n\t\tif (bfq_bfqq_busy(bfqq) && bfqq != bfqd->in_service_queue) {\n\t\t\tbfq_del_bfqq_busy(bfqq, false);\n\t\t\t/*\n\t\t\t * bfqq emptied. In normal operation, when\n\t\t\t * bfqq is empty, bfqq->entity.service and\n\t\t\t * bfqq->entity.budget must contain,\n\t\t\t * respectively, the service received and the\n\t\t\t * budget used last time bfqq emptied. These\n\t\t\t * facts do not hold in this case, as at least\n\t\t\t * this last removal occurred while bfqq is\n\t\t\t * not in service. To avoid inconsistencies,\n\t\t\t * reset both bfqq->entity.service and\n\t\t\t * bfqq->entity.budget, if bfqq has still a\n\t\t\t * process that may issue I/O requests to it.\n\t\t\t */\n\t\t\tbfqq->entity.budget = bfqq->entity.service = 0;\n\t\t}\n\n\t\t/*\n\t\t * Remove queue from request-position tree as it is empty.\n\t\t */\n\t\tif (bfqq->pos_root) {\n\t\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\t\tbfqq->pos_root = NULL;\n\t\t}\n\t} else {\n\t\t/* see comments on bfq_pos_tree_add_move() for the unlikely() */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending--;\n\n}\n\nstatic bool bfq_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *free = NULL;\n\t/*\n\t * bfq_bic_lookup grabs the queue_lock: invoke it now and\n\t * store its return value for later use, to avoid nesting\n\t * queue_lock inside the bfqd->lock. We assume that the bic\n\t * returned by bfq_bic_lookup does not go away before\n\t * bfqd->lock is taken.\n\t */\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(q);\n\tbool ret;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tif (bic) {\n\t\t/*\n\t\t * Make sure cgroup info is uptodate for current process before\n\t\t * considering the merge.\n\t\t */\n\t\tbfq_bic_update_cgroup(bic, bio);\n\n\t\tbfqd->bio_bfqq = bic_to_bfqq(bic, op_is_sync(bio->bi_opf));\n\t} else {\n\t\tbfqd->bio_bfqq = NULL;\n\t}\n\tbfqd->bio_bic = bic;\n\n\tret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);\n\n\tspin_unlock_irq(&bfqd->lock);\n\tif (free)\n\t\tblk_mq_free_request(free);\n\n\treturn ret;\n}\n\nstatic int bfq_request_merge(struct request_queue *q, struct request **req,\n\t\t\t     struct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *__rq;\n\n\t__rq = bfq_find_rq_fmerge(bfqd, bio, q);\n\tif (__rq && elv_bio_merge_ok(__rq, bio)) {\n\t\t*req = __rq;\n\n\t\tif (blk_discard_mergable(__rq))\n\t\t\treturn ELEVATOR_DISCARD_MERGE;\n\t\treturn ELEVATOR_FRONT_MERGE;\n\t}\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\nstatic void bfq_request_merged(struct request_queue *q, struct request *req,\n\t\t\t       enum elv_merge type)\n{\n\tif (type == ELEVATOR_FRONT_MERGE &&\n\t    rb_prev(&req->rb_node) &&\n\t    blk_rq_pos(req) <\n\t    blk_rq_pos(container_of(rb_prev(&req->rb_node),\n\t\t\t\t    struct request, rb_node))) {\n\t\tstruct bfq_queue *bfqq = RQ_BFQQ(req);\n\t\tstruct bfq_data *bfqd;\n\t\tstruct request *prev, *next_rq;\n\n\t\tif (!bfqq)\n\t\t\treturn;\n\n\t\tbfqd = bfqq->bfqd;\n\n\t\t/* Reposition request in its sort_list */\n\t\telv_rb_del(&bfqq->sort_list, req);\n\t\telv_rb_add(&bfqq->sort_list, req);\n\n\t\t/* Choose next request to be served for bfqq */\n\t\tprev = bfqq->next_rq;\n\t\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, req,\n\t\t\t\t\t bfqd->last_position);\n\t\tbfqq->next_rq = next_rq;\n\t\t/*\n\t\t * If next_rq changes, update both the queue's budget to\n\t\t * fit the new request and the queue's position in its\n\t\t * rq_pos_tree.\n\t\t */\n\t\tif (prev != bfqq->next_rq) {\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t\t\t/*\n\t\t\t * See comments on bfq_pos_tree_add_move() for\n\t\t\t * the unlikely().\n\t\t\t */\n\t\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t\t}\n\t}\n}\n\n/*\n * This function is called to notify the scheduler that the requests\n * rq and 'next' have been merged, with 'next' going away.  BFQ\n * exploits this hook to address the following issue: if 'next' has a\n * fifo_time lower that rq, then the fifo_time of rq must be set to\n * the value of 'next', to not forget the greater age of 'next'.\n *\n * NOTE: in this function we assume that rq is in a bfq_queue, basing\n * on that rq is picked from the hash table q->elevator->hash, which,\n * in its turn, is filled only with I/O requests present in\n * bfq_queues, while BFQ is in use for the request queue q. In fact,\n * the function that fills this hash table (elv_rqhash_add) is called\n * only by bfq_insert_request.\n */\nstatic void bfq_requests_merged(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*next_bfqq = RQ_BFQQ(next);\n\n\tif (!bfqq)\n\t\tgoto remove;\n\n\t/*\n\t * If next and rq belong to the same bfq_queue and next is older\n\t * than rq, then reposition rq in the fifo (by substituting next\n\t * with rq). Otherwise, if next and rq belong to different\n\t * bfq_queues, never reposition rq: in fact, we would have to\n\t * reposition it with respect to next's position in its own fifo,\n\t * which would most certainly be too expensive with respect to\n\t * the benefits.\n\t */\n\tif (bfqq == next_bfqq &&\n\t    !list_empty(&rq->queuelist) && !list_empty(&next->queuelist) &&\n\t    next->fifo_time < rq->fifo_time) {\n\t\tlist_del_init(&rq->queuelist);\n\t\tlist_replace_init(&next->queuelist, &rq->queuelist);\n\t\trq->fifo_time = next->fifo_time;\n\t}\n\n\tif (bfqq->next_rq == next)\n\t\tbfqq->next_rq = rq;\n\n\tbfqg_stats_update_io_merged(bfqq_group(bfqq), next->cmd_flags);\nremove:\n\t/* Merged request may be in the IO scheduler. Remove it. */\n\tif (!RB_EMPTY_NODE(&next->rb_node)) {\n\t\tbfq_remove_request(next->q, next);\n\t\tif (next_bfqq)\n\t\t\tbfqg_stats_update_io_remove(bfqq_group(next_bfqq),\n\t\t\t\t\t\t    next->cmd_flags);\n\t}\n}\n\n/* Must be called with bfqq != NULL */\nstatic void bfq_bfqq_end_wr(struct bfq_queue *bfqq)\n{\n\t/*\n\t * If bfqq has been enjoying interactive weight-raising, then\n\t * reset soft_rt_next_start. We do it for the following\n\t * reason. bfqq may have been conveying the I/O needed to load\n\t * a soft real-time application. Such an application actually\n\t * exhibits a soft real-time I/O pattern after it finishes\n\t * loading, and finally starts doing its job. But, if bfqq has\n\t * been receiving a lot of bandwidth so far (likely to happen\n\t * on a fast device), then soft_rt_next_start now contains a\n\t * high value that. So, without this reset, bfqq would be\n\t * prevented from being possibly considered as soft_rt for a\n\t * very long time.\n\t */\n\n\tif (bfqq->wr_cur_max_time !=\n\t    bfqq->bfqd->bfq_wr_rt_max_time)\n\t\tbfqq->soft_rt_next_start = jiffies;\n\n\tif (bfq_bfqq_busy(bfqq))\n\t\tbfqq->bfqd->wr_busy_queues--;\n\tbfqq->wr_coeff = 1;\n\tbfqq->wr_cur_max_time = 0;\n\tbfqq->last_wr_start_finish = jiffies;\n\t/*\n\t * Trigger a weight change on the next invocation of\n\t * __bfq_entity_update_weight_prio.\n\t */\n\tbfqq->entity.prio_changed = 1;\n}\n\nvoid bfq_end_wr_async_queues(struct bfq_data *bfqd,\n\t\t\t     struct bfq_group *bfqg)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 2; i++)\n\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\tif (bfqg->async_bfqq[i][j])\n\t\t\t\tbfq_bfqq_end_wr(bfqg->async_bfqq[i][j]);\n\tif (bfqg->async_idle_bfqq)\n\t\tbfq_bfqq_end_wr(bfqg->async_idle_bfqq);\n}\n\nstatic void bfq_end_wr(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tlist_for_each_entry(bfqq, &bfqd->active_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tlist_for_each_entry(bfqq, &bfqd->idle_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tbfq_end_wr_async(bfqd);\n\n\tspin_unlock_irq(&bfqd->lock);\n}\n\nstatic sector_t bfq_io_struct_pos(void *io_struct, bool request)\n{\n\tif (request)\n\t\treturn blk_rq_pos(io_struct);\n\telse\n\t\treturn ((struct bio *)io_struct)->bi_iter.bi_sector;\n}\n\nstatic int bfq_rq_close_to_sector(void *io_struct, bool request,\n\t\t\t\t  sector_t sector)\n{\n\treturn abs(bfq_io_struct_pos(io_struct, request) - sector) <=\n\t       BFQQ_CLOSE_THR;\n}\n\nstatic struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_queue *bfqq,\n\t\t\t\t\t sector_t sector)\n{\n\tstruct rb_root *root = &bfqq_group(bfqq)->rq_pos_tree;\n\tstruct rb_node *parent, *node;\n\tstruct bfq_queue *__bfqq;\n\n\tif (RB_EMPTY_ROOT(root))\n\t\treturn NULL;\n\n\t/*\n\t * First, if we find a request starting at the end of the last\n\t * request, choose it.\n\t */\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, root, sector, &parent, NULL);\n\tif (__bfqq)\n\t\treturn __bfqq;\n\n\t/*\n\t * If the exact sector wasn't found, the parent of the NULL leaf\n\t * will contain the closest sector (rq_pos_tree sorted by\n\t * next_request position).\n\t */\n\t__bfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\tif (blk_rq_pos(__bfqq->next_rq) < sector)\n\t\tnode = rb_next(&__bfqq->pos_node);\n\telse\n\t\tnode = rb_prev(&__bfqq->pos_node);\n\tif (!node)\n\t\treturn NULL;\n\n\t__bfqq = rb_entry(node, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_find_close_cooperator(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_queue *cur_bfqq,\n\t\t\t\t\t\t   sector_t sector)\n{\n\tstruct bfq_queue *bfqq;\n\n\t/*\n\t * We shall notice if some of the queues are cooperating,\n\t * e.g., working closely on the same area of the device. In\n\t * that case, we can group them together and: 1) don't waste\n\t * time idling, and 2) serve the union of their requests in\n\t * the best possible order for throughput.\n\t */\n\tbfqq = bfqq_find_close(bfqd, cur_bfqq, sector);\n\tif (!bfqq || bfqq == cur_bfqq)\n\t\treturn NULL;\n\n\treturn bfqq;\n}\n\nstatic struct bfq_queue *\nbfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)\n{\n\tint process_refs, new_process_refs;\n\tstruct bfq_queue *__bfqq;\n\n\t/*\n\t * If there are no process references on the new_bfqq, then it is\n\t * unsafe to follow the ->new_bfqq chain as other bfqq's in the chain\n\t * may have dropped their last reference (not just their last process\n\t * reference).\n\t */\n\tif (!bfqq_process_refs(new_bfqq))\n\t\treturn NULL;\n\n\t/* Avoid a circular list and skip interim queue merges. */\n\twhile ((__bfqq = new_bfqq->new_bfqq)) {\n\t\tif (__bfqq == bfqq)\n\t\t\treturn NULL;\n\t\tnew_bfqq = __bfqq;\n\t}\n\n\tprocess_refs = bfqq_process_refs(bfqq);\n\tnew_process_refs = bfqq_process_refs(new_bfqq);\n\t/*\n\t * If the process for the bfqq has gone away, there is no\n\t * sense in merging the queues.\n\t */\n\tif (process_refs == 0 || new_process_refs == 0)\n\t\treturn NULL;\n\n\t/*\n\t * Make sure merged queues belong to the same parent. Parents could\n\t * have changed since the time we decided the two queues are suitable\n\t * for merging.\n\t */\n\tif (new_bfqq->entity.parent != bfqq->entity.parent)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"scheduling merge with queue %d\",\n\t\tnew_bfqq->pid);\n\n\t/*\n\t * Merging is just a redirection: the requests of the process\n\t * owning one of the two queues are redirected to the other queue.\n\t * The latter queue, in its turn, is set as shared if this is the\n\t * first time that the requests of some process are redirected to\n\t * it.\n\t *\n\t * We redirect bfqq to new_bfqq and not the opposite, because\n\t * we are in the context of the process owning bfqq, thus we\n\t * have the io_cq of this process. So we can immediately\n\t * configure this io_cq to redirect the requests of the\n\t * process to new_bfqq. In contrast, the io_cq of new_bfqq is\n\t * not available any more (new_bfqq->bic == NULL).\n\t *\n\t * Anyway, even in case new_bfqq coincides with the in-service\n\t * queue, redirecting requests the in-service queue is the\n\t * best option, as we feed the in-service queue with new\n\t * requests close to the last request served and, by doing so,\n\t * are likely to increase the throughput.\n\t */\n\tbfqq->new_bfqq = new_bfqq;\n\t/*\n\t * The above assignment schedules the following redirections:\n\t * each time some I/O for bfqq arrives, the process that\n\t * generated that I/O is disassociated from bfqq and\n\t * associated with new_bfqq. Here we increases new_bfqq->ref\n\t * in advance, adding the number of processes that are\n\t * expected to be associated with new_bfqq as they happen to\n\t * issue I/O.\n\t */\n\tnew_bfqq->ref += process_refs;\n\treturn new_bfqq;\n}\n\nstatic bool bfq_may_be_close_cooperator(struct bfq_queue *bfqq,\n\t\t\t\t\tstruct bfq_queue *new_bfqq)\n{\n\tif (bfq_too_late_for_merging(new_bfqq))\n\t\treturn false;\n\n\tif (bfq_class_idle(bfqq) || bfq_class_idle(new_bfqq) ||\n\t    (bfqq->ioprio_class != new_bfqq->ioprio_class))\n\t\treturn false;\n\n\t/*\n\t * If either of the queues has already been detected as seeky,\n\t * then merging it with the other queue is unlikely to lead to\n\t * sequential I/O.\n\t */\n\tif (BFQQ_SEEKY(bfqq) || BFQQ_SEEKY(new_bfqq))\n\t\treturn false;\n\n\t/*\n\t * Interleaved I/O is known to be done by (some) applications\n\t * only for reads, so it does not make sense to merge async\n\t * queues.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || !bfq_bfqq_sync(new_bfqq))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq);\n\n/*\n * Attempt to schedule a merge of bfqq with the currently in-service\n * queue or with a close queue among the scheduled queues.  Return\n * NULL if no merge was scheduled, a pointer to the shared bfq_queue\n * structure otherwise.\n *\n * The OOM queue is not allowed to participate to cooperation: in fact, since\n * the requests temporarily redirected to the OOM queue could be redirected\n * again to dedicated queues at any time, the state needed to correctly\n * handle merging with the OOM queue would be quite complex and expensive\n * to maintain. Besides, in such a critical condition as an out of memory,\n * the benefits of queue merging may be little relevant, or even negligible.\n *\n * WARNING: queue merging may impair fairness among non-weight raised\n * queues, for at least two reasons: 1) the original weight of a\n * merged queue may change during the merged state, 2) even being the\n * weight the same, a merged queue may be bloated with many more\n * requests than the ones produced by its originally-associated\n * process.\n */\nstatic struct bfq_queue *\nbfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t     void *io_struct, bool request, struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue *in_service_bfqq, *new_bfqq;\n\n\t/* if a merge has already been setup, then proceed with that first */\n\tnew_bfqq = bfqq->new_bfqq;\n\tif (new_bfqq) {\n\t\twhile (new_bfqq->new_bfqq)\n\t\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t\treturn new_bfqq;\n\t}\n\n\t/*\n\t * Check delayed stable merge for rotational or non-queueing\n\t * devs. For this branch to be executed, bfqq must not be\n\t * currently merged with some other queue (i.e., bfqq->bic\n\t * must be non null). If we considered also merged queues,\n\t * then we should also check whether bfqq has already been\n\t * merged with bic->stable_merge_bfqq. But this would be\n\t * costly and complicated.\n\t */\n\tif (unlikely(!bfqd->nonrot_with_queueing)) {\n\t\t/*\n\t\t * Make sure also that bfqq is sync, because\n\t\t * bic->stable_merge_bfqq may point to some queue (for\n\t\t * stable merging) also if bic is associated with a\n\t\t * sync queue, but this bfqq is async\n\t\t */\n\t\tif (bfq_bfqq_sync(bfqq) && bic->stable_merge_bfqq &&\n\t\t    !bfq_bfqq_just_created(bfqq) &&\n\t\t    time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t  msecs_to_jiffies(bfq_late_stable_merging)) &&\n\t\t    time_is_before_jiffies(bfqq->creation_time +\n\t\t\t\t\t   msecs_to_jiffies(bfq_late_stable_merging))) {\n\t\t\tstruct bfq_queue *stable_merge_bfqq =\n\t\t\t\tbic->stable_merge_bfqq;\n\t\t\tint proc_ref = min(bfqq_process_refs(bfqq),\n\t\t\t\t\t   bfqq_process_refs(stable_merge_bfqq));\n\n\t\t\t/* deschedule stable merge, because done or aborted here */\n\t\t\tbfq_put_stable_ref(stable_merge_bfqq);\n\n\t\t\tbic->stable_merge_bfqq = NULL;\n\n\t\t\tif (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t    proc_ref > 0) {\n\t\t\t\t/* next function will take at least one ref */\n\t\t\t\tstruct bfq_queue *new_bfqq =\n\t\t\t\t\tbfq_setup_merge(bfqq, stable_merge_bfqq);\n\n\t\t\t\tif (new_bfqq) {\n\t\t\t\t\tbic->stably_merged = true;\n\t\t\t\t\tif (new_bfqq->bic)\n\t\t\t\t\t\tnew_bfqq->bic->stably_merged =\n\t\t\t\t\t\t\t\t\ttrue;\n\t\t\t\t}\n\t\t\t\treturn new_bfqq;\n\t\t\t} else\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\t/*\n\t * Do not perform queue merging if the device is non\n\t * rotational and performs internal queueing. In fact, such a\n\t * device reaches a high speed through internal parallelism\n\t * and pipelining. This means that, to reach a high\n\t * throughput, it must have many requests enqueued at the same\n\t * time. But, in this configuration, the internal scheduling\n\t * algorithm of the device does exactly the job of queue\n\t * merging: it reorders requests so as to obtain as much as\n\t * possible a sequential I/O pattern. As a consequence, with\n\t * the workload generated by processes doing interleaved I/O,\n\t * the throughput reached by the device is likely to be the\n\t * same, with and without queue merging.\n\t *\n\t * Disabling merging also provides a remarkable benefit in\n\t * terms of throughput. Merging tends to make many workloads\n\t * artificially more uneven, because of shared queues\n\t * remaining non empty for incomparably more time than\n\t * non-merged queues. This may accentuate workload\n\t * asymmetries. For example, if one of the queues in a set of\n\t * merged queues has a higher weight than a normal queue, then\n\t * the shared queue may inherit such a high weight and, by\n\t * staying almost always active, may force BFQ to perform I/O\n\t * plugging most of the time. This evidently makes it harder\n\t * for BFQ to let the device reach a high throughput.\n\t *\n\t * Finally, the likely() macro below is not used because one\n\t * of the two branches is more likely than the other, but to\n\t * have the code path after the following if() executed as\n\t * fast as possible for the case of a non rotational device\n\t * with queueing. We want it because this is the fastest kind\n\t * of device. On the opposite end, the likely() may lengthen\n\t * the execution time of BFQ for the case of slower devices\n\t * (rotational or at least without queueing). But in this case\n\t * the execution time of BFQ matters very little, if not at\n\t * all.\n\t */\n\tif (likely(bfqd->nonrot_with_queueing))\n\t\treturn NULL;\n\n\t/*\n\t * Prevent bfqq from being merged if it has been created too\n\t * long ago. The idea is that true cooperating processes, and\n\t * thus their associated bfq_queues, are supposed to be\n\t * created shortly after each other. This is the case, e.g.,\n\t * for KVM/QEMU and dump I/O threads. Basing on this\n\t * assumption, the following filtering greatly reduces the\n\t * probability that two non-cooperating processes, which just\n\t * happen to do close I/O for some short time interval, have\n\t * their queues merged by mistake.\n\t */\n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn NULL;\n\n\tif (!io_struct || unlikely(bfqq == &bfqd->oom_bfqq))\n\t\treturn NULL;\n\n\t/* If there is only one backlogged queue, don't search. */\n\tif (bfq_tot_busy_queues(bfqd) == 1)\n\t\treturn NULL;\n\n\tin_service_bfqq = bfqd->in_service_queue;\n\n\tif (in_service_bfqq && in_service_bfqq != bfqq &&\n\t    likely(in_service_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_rq_close_to_sector(io_struct, request,\n\t\t\t\t   bfqd->in_serv_last_pos) &&\n\t    bfqq->entity.parent == in_service_bfqq->entity.parent &&\n\t    bfq_may_be_close_cooperator(bfqq, in_service_bfqq)) {\n\t\tnew_bfqq = bfq_setup_merge(bfqq, in_service_bfqq);\n\t\tif (new_bfqq)\n\t\t\treturn new_bfqq;\n\t}\n\t/*\n\t * Check whether there is a cooperator among currently scheduled\n\t * queues. The only thing we need is that the bio/request is not\n\t * NULL, as we need it to establish whether a cooperator exists.\n\t */\n\tnew_bfqq = bfq_find_close_cooperator(bfqd, bfqq,\n\t\t\tbfq_io_struct_pos(io_struct, request));\n\n\tif (new_bfqq && likely(new_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_may_be_close_cooperator(bfqq, new_bfqq))\n\t\treturn bfq_setup_merge(bfqq, new_bfqq);\n\n\treturn NULL;\n}\n\nstatic void bfq_bfqq_save_state(struct bfq_queue *bfqq)\n{\n\tstruct bfq_io_cq *bic = bfqq->bic;\n\n\t/*\n\t * If !bfqq->bic, the queue is already shared or its requests\n\t * have already been redirected to a shared queue; both idle window\n\t * and weight raising state have already been saved. Do nothing.\n\t */\n\tif (!bic)\n\t\treturn;\n\n\tbic->saved_last_serv_time_ns = bfqq->last_serv_time_ns;\n\tbic->saved_inject_limit = bfqq->inject_limit;\n\tbic->saved_decrease_time_jif = bfqq->decrease_time_jif;\n\n\tbic->saved_weight = bfqq->entity.orig_weight;\n\tbic->saved_ttime = bfqq->ttime;\n\tbic->saved_has_short_ttime = bfq_bfqq_has_short_ttime(bfqq);\n\tbic->saved_IO_bound = bfq_bfqq_IO_bound(bfqq);\n\tbic->saved_io_start_time = bfqq->io_start_time;\n\tbic->saved_tot_idle_time = bfqq->tot_idle_time;\n\tbic->saved_in_large_burst = bfq_bfqq_in_large_burst(bfqq);\n\tbic->was_in_burst_list = !hlist_unhashed(&bfqq->burst_list_node);\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t     bfqq->bfqd->low_latency)) {\n\t\t/*\n\t\t * bfqq being merged right after being created: bfqq\n\t\t * would have deserved interactive weight raising, but\n\t\t * did not make it to be set in a weight-raised state,\n\t\t * because of this early merge.\tStore directly the\n\t\t * weight-raising state that would have been assigned\n\t\t * to bfqq, so that to avoid that bfqq unjustly fails\n\t\t * to enjoy weight raising if split soon.\n\t\t */\n\t\tbic->saved_wr_coeff = bfqq->bfqd->bfq_wr_coeff;\n\t\tbic->saved_wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\t\tbic->saved_wr_cur_max_time = bfq_wr_duration(bfqq->bfqd);\n\t\tbic->saved_last_wr_start_finish = jiffies;\n\t} else {\n\t\tbic->saved_wr_coeff = bfqq->wr_coeff;\n\t\tbic->saved_wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tbic->saved_service_from_wr = bfqq->service_from_wr;\n\t\tbic->saved_last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tbic->saved_wr_cur_max_time = bfqq->wr_cur_max_time;\n\t}\n}\n\n\nstatic void\nbfq_reassign_last_bfqq(struct bfq_queue *cur_bfqq, struct bfq_queue *new_bfqq)\n{\n\tif (cur_bfqq->entity.parent &&\n\t    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->entity.parent->last_bfqq_created = new_bfqq;\n\telse if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->bfqd->last_bfqq_created = new_bfqq;\n}\n\nvoid bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t/*\n\t * To prevent bfqq's service guarantees from being violated,\n\t * bfqq may be left busy, i.e., queued for service, even if\n\t * empty (see comments in __bfq_bfqq_expire() for\n\t * details). But, if no process will send requests to bfqq any\n\t * longer, then there is no point in keeping bfqq queued for\n\t * service. In addition, keeping bfqq queued for service, but\n\t * with no process ref any longer, may have caused bfqq to be\n\t * freed when dequeued from service. But this is assumed to\n\t * never happen.\n\t */\n\tif (bfq_bfqq_busy(bfqq) && RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq != bfqd->in_service_queue)\n\t\tbfq_del_bfqq_busy(bfqq, false);\n\n\tbfq_reassign_last_bfqq(bfqq, NULL);\n\n\tbfq_put_queue(bfqq);\n}\n\nstatic struct bfq_queue *bfq_merge_bfqqs(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_io_cq *bic,\n\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"merging with queue %lu\",\n\t\t(unsigned long)new_bfqq->pid);\n\t/* Save weight raising and idle window of the merged queues */\n\tbfq_bfqq_save_state(bfqq);\n\tbfq_bfqq_save_state(new_bfqq);\n\tif (bfq_bfqq_IO_bound(bfqq))\n\t\tbfq_mark_bfqq_IO_bound(new_bfqq);\n\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\t/*\n\t * The processes associated with bfqq are cooperators of the\n\t * processes associated with new_bfqq. So, if bfqq has a\n\t * waker, then assume that all these processes will be happy\n\t * to let bfqq's waker freely inject I/O when they have no\n\t * I/O.\n\t */\n\tif (bfqq->waker_bfqq && !new_bfqq->waker_bfqq &&\n\t    bfqq->waker_bfqq != new_bfqq) {\n\t\tnew_bfqq->waker_bfqq = bfqq->waker_bfqq;\n\t\tnew_bfqq->tentative_waker_bfqq = NULL;\n\n\t\t/*\n\t\t * If the waker queue disappears, then\n\t\t * new_bfqq->waker_bfqq must be reset. So insert\n\t\t * new_bfqq into the woken_list of the waker. See\n\t\t * bfq_check_waker for details.\n\t\t */\n\t\thlist_add_head(&new_bfqq->woken_list_node,\n\t\t\t       &new_bfqq->waker_bfqq->woken_list);\n\n\t}\n\n\t/*\n\t * If bfqq is weight-raised, then let new_bfqq inherit\n\t * weight-raising. To reduce false positives, neglect the case\n\t * where bfqq has just been created, but has not yet made it\n\t * to be weight-raised (which may happen because EQM may merge\n\t * bfqq even before bfq_add_request is executed for the first\n\t * time for bfqq). Handling this case would however be very\n\t * easy, thanks to the flag just_created.\n\t */\n\tif (new_bfqq->wr_coeff == 1 && bfqq->wr_coeff > 1) {\n\t\tnew_bfqq->wr_coeff = bfqq->wr_coeff;\n\t\tnew_bfqq->wr_cur_max_time = bfqq->wr_cur_max_time;\n\t\tnew_bfqq->last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tnew_bfqq->wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tif (bfq_bfqq_busy(new_bfqq))\n\t\t\tbfqd->wr_busy_queues++;\n\t\tnew_bfqq->entity.prio_changed = 1;\n\t}\n\n\tif (bfqq->wr_coeff > 1) { /* bfqq has given its wr to new_bfqq */\n\t\tbfqq->wr_coeff = 1;\n\t\tbfqq->entity.prio_changed = 1;\n\t\tif (bfq_bfqq_busy(bfqq))\n\t\t\tbfqd->wr_busy_queues--;\n\t}\n\n\tbfq_log_bfqq(bfqd, new_bfqq, \"merge_bfqqs: wr_busy %d\",\n\t\t     bfqd->wr_busy_queues);\n\n\t/*\n\t * Merge queues (that is, let bic redirect its requests to new_bfqq)\n\t */\n\tbic_set_bfqq(bic, new_bfqq, true);\n\tbfq_mark_bfqq_coop(new_bfqq);\n\t/*\n\t * new_bfqq now belongs to at least two bics (it is a shared queue):\n\t * set new_bfqq->bic to NULL. bfqq either:\n\t * - does not belong to any bic any more, and hence bfqq->bic must\n\t *   be set to NULL, or\n\t * - is a queue whose owning bics have already been redirected to a\n\t *   different queue, hence the queue is destined to not belong to\n\t *   any bic soon and bfqq->bic is already NULL (therefore the next\n\t *   assignment causes no harm).\n\t */\n\tnew_bfqq->bic = NULL;\n\t/*\n\t * If the queue is shared, the pid is the pid of one of the associated\n\t * processes. Which pid depends on the exact sequence of merge events\n\t * the queue underwent. So printing such a pid is useless and confusing\n\t * because it reports a random pid between those of the associated\n\t * processes.\n\t * We mark such a queue with a pid -1, and then print SHARED instead of\n\t * a pid in logging messages.\n\t */\n\tnew_bfqq->pid = -1;\n\tbfqq->bic = NULL;\n\n\tbfq_reassign_last_bfqq(bfqq, new_bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n\n\treturn new_bfqq;\n}\n\nstatic bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tbool is_sync = op_is_sync(bio->bi_opf);\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq, *new_bfqq;\n\n\t/*\n\t * Disallow merge of a sync bio into an async request.\n\t */\n\tif (is_sync && !rq_is_sync(rq))\n\t\treturn false;\n\n\t/*\n\t * Lookup the bfqq that this bio will be queued with. Allow\n\t * merge only if rq is queued there.\n\t */\n\tif (!bfqq)\n\t\treturn false;\n\n\t/*\n\t * We take advantage of this function to perform an early merge\n\t * of the queues of possible cooperating processes.\n\t */\n\tnew_bfqq = bfq_setup_cooperator(bfqd, bfqq, bio, false, bfqd->bio_bic);\n\tif (new_bfqq) {\n\t\t/*\n\t\t * bic still points to bfqq, then it has not yet been\n\t\t * redirected to some other bfq_queue, and a queue\n\t\t * merge between bfqq and new_bfqq can be safely\n\t\t * fulfilled, i.e., bic can be redirected to new_bfqq\n\t\t * and bfqq can be put.\n\t\t */\n\t\twhile (bfqq != new_bfqq)\n\t\t\tbfqq = bfq_merge_bfqqs(bfqd, bfqd->bio_bic, bfqq);\n\n\t\t/*\n\t\t * Change also bqfd->bio_bfqq, as\n\t\t * bfqd->bio_bic now points to new_bfqq, and\n\t\t * this function may be invoked again (and then may\n\t\t * use again bqfd->bio_bfqq).\n\t\t */\n\t\tbfqd->bio_bfqq = bfqq;\n\t}\n\n\treturn bfqq == RQ_BFQQ(rq);\n}\n\n/*\n * Set the maximum time for the in-service queue to consume its\n * budget. This prevents seeky processes from lowering the throughput.\n * In practice, a time-slice service scheme is used with seeky\n * processes.\n */\nstatic void bfq_set_budget_timeout(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tunsigned int timeout_coeff;\n\n\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time)\n\t\ttimeout_coeff = 1;\n\telse\n\t\ttimeout_coeff = bfqq->entity.weight / bfqq->entity.orig_weight;\n\n\tbfqd->last_budget_start = ktime_get();\n\n\tbfqq->budget_timeout = jiffies +\n\t\tbfqd->bfq_timeout * timeout_coeff;\n}\n\nstatic void __bfq_set_in_service_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq)\n{\n\tif (bfqq) {\n\t\tbfq_clear_bfqq_fifo_expire(bfqq);\n\n\t\tbfqd->budgets_assigned = (bfqd->budgets_assigned * 7 + 256) / 8;\n\n\t\tif (time_is_before_jiffies(bfqq->last_wr_start_finish) &&\n\t\t    bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    time_is_before_jiffies(bfqq->budget_timeout)) {\n\t\t\t/*\n\t\t\t * For soft real-time queues, move the start\n\t\t\t * of the weight-raising period forward by the\n\t\t\t * time the queue has not received any\n\t\t\t * service. Otherwise, a relatively long\n\t\t\t * service delay is likely to cause the\n\t\t\t * weight-raising period of the queue to end,\n\t\t\t * because of the short duration of the\n\t\t\t * weight-raising period of a soft real-time\n\t\t\t * queue.  It is worth noting that this move\n\t\t\t * is not so dangerous for the other queues,\n\t\t\t * because soft real-time queues are not\n\t\t\t * greedy.\n\t\t\t *\n\t\t\t * To not add a further variable, we use the\n\t\t\t * overloaded field budget_timeout to\n\t\t\t * determine for how long the queue has not\n\t\t\t * received service, i.e., how much time has\n\t\t\t * elapsed since the queue expired. However,\n\t\t\t * this is a little imprecise, because\n\t\t\t * budget_timeout is set to jiffies if bfqq\n\t\t\t * not only expires, but also remains with no\n\t\t\t * request.\n\t\t\t */\n\t\t\tif (time_after(bfqq->budget_timeout,\n\t\t\t\t       bfqq->last_wr_start_finish))\n\t\t\t\tbfqq->last_wr_start_finish +=\n\t\t\t\t\tjiffies - bfqq->budget_timeout;\n\t\t\telse\n\t\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\n\t\tbfq_set_budget_timeout(bfqd, bfqq);\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t     \"set_in_service_queue, cur-budget = %d\",\n\t\t\t     bfqq->entity.budget);\n\t}\n\n\tbfqd->in_service_queue = bfqq;\n\tbfqd->in_serv_last_pos = 0;\n}\n\n/*\n * Get and set a new queue for service.\n */\nstatic struct bfq_queue *bfq_set_in_service_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfq_get_next_queue(bfqd);\n\n\t__bfq_set_in_service_queue(bfqd, bfqq);\n\treturn bfqq;\n}\n\nstatic void bfq_arm_slice_timer(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\tu32 sl;\n\n\tbfq_mark_bfqq_wait_request(bfqq);\n\n\t/*\n\t * We don't want to idle for seeks, but we do want to allow\n\t * fair distribution of slice time for a process doing back-to-back\n\t * seeks. So allow a little bit of time for him to submit a new rq.\n\t */\n\tsl = bfqd->bfq_slice_idle;\n\t/*\n\t * Unless the queue is being weight-raised or the scenario is\n\t * asymmetric, grant only minimum idle time if the queue\n\t * is seeky. A long idling is preserved for a weight-raised\n\t * queue, or, more in general, in an asymmetric scenario,\n\t * because a long idling is needed for guaranteeing to a queue\n\t * its reserved share of the throughput (in particular, it is\n\t * needed if the queue has a higher weight than some other\n\t * queue).\n\t */\n\tif (BFQQ_SEEKY(bfqq) && bfqq->wr_coeff == 1 &&\n\t    !bfq_asymmetric_scenario(bfqd, bfqq))\n\t\tsl = min_t(u64, sl, BFQ_MIN_TT);\n\telse if (bfqq->wr_coeff > 1)\n\t\tsl = max_t(u32, sl, 20ULL * NSEC_PER_MSEC);\n\n\tbfqd->last_idling_start = ktime_get();\n\tbfqd->last_idling_start_jiffies = jiffies;\n\n\thrtimer_start(&bfqd->idle_slice_timer, ns_to_ktime(sl),\n\t\t      HRTIMER_MODE_REL);\n\tbfqg_stats_set_start_idle_time(bfqq_group(bfqq));\n}\n\n/*\n * In autotuning mode, max_budget is dynamically recomputed as the\n * amount of sectors transferred in timeout at the estimated peak\n * rate. This enables BFQ to utilize a full timeslice with a full\n * budget, even if the in-service queue is served at peak rate. And\n * this maximises throughput with sequential workloads.\n */\nstatic unsigned long bfq_calc_max_budget(struct bfq_data *bfqd)\n{\n\treturn (u64)bfqd->peak_rate * USEC_PER_MSEC *\n\t\tjiffies_to_msecs(bfqd->bfq_timeout)>>BFQ_RATE_SHIFT;\n}\n\n/*\n * Update parameters related to throughput and responsiveness, as a\n * function of the estimated peak rate. See comments on\n * bfq_calc_max_budget(), and on the ref_wr_duration array.\n */\nstatic void update_thr_responsiveness_params(struct bfq_data *bfqd)\n{\n\tif (bfqd->bfq_user_max_budget == 0) {\n\t\tbfqd->bfq_max_budget =\n\t\t\tbfq_calc_max_budget(bfqd);\n\t\tbfq_log(bfqd, \"new max_budget = %d\", bfqd->bfq_max_budget);\n\t}\n}\n\nstatic void bfq_reset_rate_computation(struct bfq_data *bfqd,\n\t\t\t\t       struct request *rq)\n{\n\tif (rq != NULL) { /* new rq dispatch now, reset accordingly */\n\t\tbfqd->last_dispatch = bfqd->first_dispatch = ktime_get_ns();\n\t\tbfqd->peak_rate_samples = 1;\n\t\tbfqd->sequential_samples = 0;\n\t\tbfqd->tot_sectors_dispatched = bfqd->last_rq_max_size =\n\t\t\tblk_rq_sectors(rq);\n\t} else /* no new rq dispatched, just reset the number of samples */\n\t\tbfqd->peak_rate_samples = 0; /* full re-init on next disp. */\n\n\tbfq_log(bfqd,\n\t\t\"reset_rate_computation at end, sample %u/%u tot_sects %llu\",\n\t\tbfqd->peak_rate_samples, bfqd->sequential_samples,\n\t\tbfqd->tot_sectors_dispatched);\n}\n\nstatic void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)\n{\n\tu32 rate, weight, divisor;\n\n\t/*\n\t * For the convergence property to hold (see comments on\n\t * bfq_update_peak_rate()) and for the assessment to be\n\t * reliable, a minimum number of samples must be present, and\n\t * a minimum amount of time must have elapsed. If not so, do\n\t * not compute new rate. Just reset parameters, to get ready\n\t * for a new evaluation attempt.\n\t */\n\tif (bfqd->peak_rate_samples < BFQ_RATE_MIN_SAMPLES ||\n\t    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL)\n\t\tgoto reset_computation;\n\n\t/*\n\t * If a new request completion has occurred after last\n\t * dispatch, then, to approximate the rate at which requests\n\t * have been served by the device, it is more precise to\n\t * extend the observation interval to the last completion.\n\t */\n\tbfqd->delta_from_first =\n\t\tmax_t(u64, bfqd->delta_from_first,\n\t\t      bfqd->last_completion - bfqd->first_dispatch);\n\n\t/*\n\t * Rate computed in sects/usec, and not sects/nsec, for\n\t * precision issues.\n\t */\n\trate = div64_ul(bfqd->tot_sectors_dispatched<<BFQ_RATE_SHIFT,\n\t\t\tdiv_u64(bfqd->delta_from_first, NSEC_PER_USEC));\n\n\t/*\n\t * Peak rate not updated if:\n\t * - the percentage of sequential dispatches is below 3/4 of the\n\t *   total, and rate is below the current estimated peak rate\n\t * - rate is unreasonably high (> 20M sectors/sec)\n\t */\n\tif ((bfqd->sequential_samples < (3 * bfqd->peak_rate_samples)>>2 &&\n\t     rate <= bfqd->peak_rate) ||\n\t\trate > 20<<BFQ_RATE_SHIFT)\n\t\tgoto reset_computation;\n\n\t/*\n\t * We have to update the peak rate, at last! To this purpose,\n\t * we use a low-pass filter. We compute the smoothing constant\n\t * of the filter as a function of the 'weight' of the new\n\t * measured rate.\n\t *\n\t * As can be seen in next formulas, we define this weight as a\n\t * quantity proportional to how sequential the workload is,\n\t * and to how long the observation time interval is.\n\t *\n\t * The weight runs from 0 to 8. The maximum value of the\n\t * weight, 8, yields the minimum value for the smoothing\n\t * constant. At this minimum value for the smoothing constant,\n\t * the measured rate contributes for half of the next value of\n\t * the estimated peak rate.\n\t *\n\t * So, the first step is to compute the weight as a function\n\t * of how sequential the workload is. Note that the weight\n\t * cannot reach 9, because bfqd->sequential_samples cannot\n\t * become equal to bfqd->peak_rate_samples, which, in its\n\t * turn, holds true because bfqd->sequential_samples is not\n\t * incremented for the first sample.\n\t */\n\tweight = (9 * bfqd->sequential_samples) / bfqd->peak_rate_samples;\n\n\t/*\n\t * Second step: further refine the weight as a function of the\n\t * duration of the observation interval.\n\t */\n\tweight = min_t(u32, 8,\n\t\t       div_u64(weight * bfqd->delta_from_first,\n\t\t\t       BFQ_RATE_REF_INTERVAL));\n\n\t/*\n\t * Divisor ranging from 10, for minimum weight, to 2, for\n\t * maximum weight.\n\t */\n\tdivisor = 10 - weight;\n\n\t/*\n\t * Finally, update peak rate:\n\t *\n\t * peak_rate = peak_rate * (divisor-1) / divisor  +  rate / divisor\n\t */\n\tbfqd->peak_rate *= divisor-1;\n\tbfqd->peak_rate /= divisor;\n\trate /= divisor; /* smoothing constant alpha = 1/divisor */\n\n\tbfqd->peak_rate += rate;\n\n\t/*\n\t * For a very slow device, bfqd->peak_rate can reach 0 (see\n\t * the minimum representable values reported in the comments\n\t * on BFQ_RATE_SHIFT). Push to 1 if this happens, to avoid\n\t * divisions by zero where bfqd->peak_rate is used as a\n\t * divisor.\n\t */\n\tbfqd->peak_rate = max_t(u32, 1, bfqd->peak_rate);\n\n\tupdate_thr_responsiveness_params(bfqd);\n\nreset_computation:\n\tbfq_reset_rate_computation(bfqd, rq);\n}\n\n/*\n * Update the read/write peak rate (the main quantity used for\n * auto-tuning, see update_thr_responsiveness_params()).\n *\n * It is not trivial to estimate the peak rate (correctly): because of\n * the presence of sw and hw queues between the scheduler and the\n * device components that finally serve I/O requests, it is hard to\n * say exactly when a given dispatched request is served inside the\n * device, and for how long. As a consequence, it is hard to know\n * precisely at what rate a given set of requests is actually served\n * by the device.\n *\n * On the opposite end, the dispatch time of any request is trivially\n * available, and, from this piece of information, the \"dispatch rate\"\n * of requests can be immediately computed. So, the idea in the next\n * function is to use what is known, namely request dispatch times\n * (plus, when useful, request completion times), to estimate what is\n * unknown, namely in-device request service rate.\n *\n * The main issue is that, because of the above facts, the rate at\n * which a certain set of requests is dispatched over a certain time\n * interval can vary greatly with respect to the rate at which the\n * same requests are then served. But, since the size of any\n * intermediate queue is limited, and the service scheme is lossless\n * (no request is silently dropped), the following obvious convergence\n * property holds: the number of requests dispatched MUST become\n * closer and closer to the number of requests completed as the\n * observation interval grows. This is the key property used in\n * the next function to estimate the peak service rate as a function\n * of the observed dispatch rate. The function assumes to be invoked\n * on every request dispatch.\n */\nstatic void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tif (bfqd->peak_rate_samples == 0) { /* first dispatch */\n\t\tbfq_log(bfqd, \"update_peak_rate: goto reset, samples %d\",\n\t\t\tbfqd->peak_rate_samples);\n\t\tbfq_reset_rate_computation(bfqd, rq);\n\t\tgoto update_last_values; /* will add one sample */\n\t}\n\n\t/*\n\t * Device idle for very long: the observation interval lasting\n\t * up to this dispatch cannot be a valid observation interval\n\t * for computing a new peak rate (similarly to the late-\n\t * completion event in bfq_completed_request()). Go to\n\t * update_rate_and_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - start a new observation interval with this dispatch\n\t */\n\tif (now_ns - bfqd->last_dispatch > 100*NSEC_PER_MSEC &&\n\t    bfqd->rq_in_driver == 0)\n\t\tgoto update_rate_and_reset;\n\n\t/* Update sampling information */\n\tbfqd->peak_rate_samples++;\n\n\tif ((bfqd->rq_in_driver > 0 ||\n\t\tnow_ns - bfqd->last_completion < BFQ_MIN_TT)\n\t    && !BFQ_RQ_SEEKY(bfqd, bfqd->last_position, rq))\n\t\tbfqd->sequential_samples++;\n\n\tbfqd->tot_sectors_dispatched += blk_rq_sectors(rq);\n\n\t/* Reset max observed rq size every 32 dispatches */\n\tif (likely(bfqd->peak_rate_samples % 32))\n\t\tbfqd->last_rq_max_size =\n\t\t\tmax_t(u32, blk_rq_sectors(rq), bfqd->last_rq_max_size);\n\telse\n\t\tbfqd->last_rq_max_size = blk_rq_sectors(rq);\n\n\tbfqd->delta_from_first = now_ns - bfqd->first_dispatch;\n\n\t/* Target observation interval not yet reached, go on sampling */\n\tif (bfqd->delta_from_first < BFQ_RATE_REF_INTERVAL)\n\t\tgoto update_last_values;\n\nupdate_rate_and_reset:\n\tbfq_update_rate_reset(bfqd, rq);\nupdate_last_values:\n\tbfqd->last_position = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\tif (RQ_BFQQ(rq) == bfqd->in_service_queue)\n\t\tbfqd->in_serv_last_pos = bfqd->last_position;\n\tbfqd->last_dispatch = now_ns;\n}\n\n/*\n * Remove request from internal lists.\n */\nstatic void bfq_dispatch_remove(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\n\t/*\n\t * For consistency, the next instruction should have been\n\t * executed after removing the request from the queue and\n\t * dispatching it.  We execute instead this instruction before\n\t * bfq_remove_request() (and hence introduce a temporary\n\t * inconsistency), for efficiency.  In fact, should this\n\t * dispatch occur for a non in-service bfqq, this anticipated\n\t * increment prevents two counters related to bfqq->dispatched\n\t * from risking to be, first, uselessly decremented, and then\n\t * incremented again when the (new) value of bfqq->dispatched\n\t * happens to be taken into account.\n\t */\n\tbfqq->dispatched++;\n\tbfq_update_peak_rate(q->elevator->elevator_data, rq);\n\n\tbfq_remove_request(q, rq);\n}\n\n/*\n * There is a case where idling does not have to be performed for\n * throughput concerns, but to preserve the throughput share of\n * the process associated with bfqq.\n *\n * To introduce this case, we can note that allowing the drive\n * to enqueue more than one request at a time, and hence\n * delegating de facto final scheduling decisions to the\n * drive's internal scheduler, entails loss of control on the\n * actual request service order. In particular, the critical\n * situation is when requests from different processes happen\n * to be present, at the same time, in the internal queue(s)\n * of the drive. In such a situation, the drive, by deciding\n * the service order of the internally-queued requests, does\n * determine also the actual throughput distribution among\n * these processes. But the drive typically has no notion or\n * concern about per-process throughput distribution, and\n * makes its decisions only on a per-request basis. Therefore,\n * the service distribution enforced by the drive's internal\n * scheduler is likely to coincide with the desired throughput\n * distribution only in a completely symmetric, or favorably\n * skewed scenario where:\n * (i-a) each of these processes must get the same throughput as\n *\t the others,\n * (i-b) in case (i-a) does not hold, it holds that the process\n *       associated with bfqq must receive a lower or equal\n *\t throughput than any of the other processes;\n * (ii)  the I/O of each process has the same properties, in\n *       terms of locality (sequential or random), direction\n *       (reads or writes), request sizes, greediness\n *       (from I/O-bound to sporadic), and so on;\n\n * In fact, in such a scenario, the drive tends to treat the requests\n * of each process in about the same way as the requests of the\n * others, and thus to provide each of these processes with about the\n * same throughput.  This is exactly the desired throughput\n * distribution if (i-a) holds, or, if (i-b) holds instead, this is an\n * even more convenient distribution for (the process associated with)\n * bfqq.\n *\n * In contrast, in any asymmetric or unfavorable scenario, device\n * idling (I/O-dispatch plugging) is certainly needed to guarantee\n * that bfqq receives its assigned fraction of the device throughput\n * (see [1] for details).\n *\n * The problem is that idling may significantly reduce throughput with\n * certain combinations of types of I/O and devices. An important\n * example is sync random I/O on flash storage with command\n * queueing. So, unless bfqq falls in cases where idling also boosts\n * throughput, it is important to check conditions (i-a), i(-b) and\n * (ii) accurately, so as to avoid idling when not strictly needed for\n * service guarantees.\n *\n * Unfortunately, it is extremely difficult to thoroughly check\n * condition (ii). And, in case there are active groups, it becomes\n * very difficult to check conditions (i-a) and (i-b) too.  In fact,\n * if there are active groups, then, for conditions (i-a) or (i-b) to\n * become false 'indirectly', it is enough that an active group\n * contains more active processes or sub-groups than some other active\n * group. More precisely, for conditions (i-a) or (i-b) to become\n * false because of such a group, it is not even necessary that the\n * group is (still) active: it is sufficient that, even if the group\n * has become inactive, some of its descendant processes still have\n * some request already dispatched but still waiting for\n * completion. In fact, requests have still to be guaranteed their\n * share of the throughput even after being dispatched. In this\n * respect, it is easy to show that, if a group frequently becomes\n * inactive while still having in-flight requests, and if, when this\n * happens, the group is not considered in the calculation of whether\n * the scenario is asymmetric, then the group may fail to be\n * guaranteed its fair share of the throughput (basically because\n * idling may not be performed for the descendant processes of the\n * group, but it had to be).  We address this issue with the following\n * bi-modal behavior, implemented in the function\n * bfq_asymmetric_scenario().\n *\n * If there are groups with requests waiting for completion\n * (as commented above, some of these groups may even be\n * already inactive), then the scenario is tagged as\n * asymmetric, conservatively, without checking any of the\n * conditions (i-a), (i-b) or (ii). So the device is idled for bfqq.\n * This behavior matches also the fact that groups are created\n * exactly if controlling I/O is a primary concern (to\n * preserve bandwidth and latency guarantees).\n *\n * On the opposite end, if there are no groups with requests waiting\n * for completion, then only conditions (i-a) and (i-b) are actually\n * controlled, i.e., provided that conditions (i-a) or (i-b) holds,\n * idling is not performed, regardless of whether condition (ii)\n * holds.  In other words, only if conditions (i-a) and (i-b) do not\n * hold, then idling is allowed, and the device tends to be prevented\n * from queueing many requests, possibly of several processes. Since\n * there are no groups with requests waiting for completion, then, to\n * control conditions (i-a) and (i-b) it is enough to check just\n * whether all the queues with requests waiting for completion also\n * have the same weight.\n *\n * Not checking condition (ii) evidently exposes bfqq to the\n * risk of getting less throughput than its fair share.\n * However, for queues with the same weight, a further\n * mechanism, preemption, mitigates or even eliminates this\n * problem. And it does so without consequences on overall\n * throughput. This mechanism and its benefits are explained\n * in the next three paragraphs.\n *\n * Even if a queue, say Q, is expired when it remains idle, Q\n * can still preempt the new in-service queue if the next\n * request of Q arrives soon (see the comments on\n * bfq_bfqq_update_budg_for_activation). If all queues and\n * groups have the same weight, this form of preemption,\n * combined with the hole-recovery heuristic described in the\n * comments on function bfq_bfqq_update_budg_for_activation,\n * are enough to preserve a correct bandwidth distribution in\n * the mid term, even without idling. In fact, even if not\n * idling allows the internal queues of the device to contain\n * many requests, and thus to reorder requests, we can rather\n * safely assume that the internal scheduler still preserves a\n * minimum of mid-term fairness.\n *\n * More precisely, this preemption-based, idleless approach\n * provides fairness in terms of IOPS, and not sectors per\n * second. This can be seen with a simple example. Suppose\n * that there are two queues with the same weight, but that\n * the first queue receives requests of 8 sectors, while the\n * second queue receives requests of 1024 sectors. In\n * addition, suppose that each of the two queues contains at\n * most one request at a time, which implies that each queue\n * always remains idle after it is served. Finally, after\n * remaining idle, each queue receives very quickly a new\n * request. It follows that the two queues are served\n * alternatively, preempting each other if needed. This\n * implies that, although both queues have the same weight,\n * the queue with large requests receives a service that is\n * 1024/8 times as high as the service received by the other\n * queue.\n *\n * The motivation for using preemption instead of idling (for\n * queues with the same weight) is that, by not idling,\n * service guarantees are preserved (completely or at least in\n * part) without minimally sacrificing throughput. And, if\n * there is no active group, then the primary expectation for\n * this device is probably a high throughput.\n *\n * We are now left only with explaining the two sub-conditions in the\n * additional compound condition that is checked below for deciding\n * whether the scenario is asymmetric. To explain the first\n * sub-condition, we need to add that the function\n * bfq_asymmetric_scenario checks the weights of only\n * non-weight-raised queues, for efficiency reasons (see comments on\n * bfq_weights_tree_add()). Then the fact that bfqq is weight-raised\n * is checked explicitly here. More precisely, the compound condition\n * below takes into account also the fact that, even if bfqq is being\n * weight-raised, the scenario is still symmetric if all queues with\n * requests waiting for completion happen to be\n * weight-raised. Actually, we should be even more precise here, and\n * differentiate between interactive weight raising and soft real-time\n * weight raising.\n *\n * The second sub-condition checked in the compound condition is\n * whether there is a fair amount of already in-flight I/O not\n * belonging to bfqq. If so, I/O dispatching is to be plugged, for the\n * following reason. The drive may decide to serve in-flight\n * non-bfqq's I/O requests before bfqq's ones, thereby delaying the\n * arrival of new I/O requests for bfqq (recall that bfqq is sync). If\n * I/O-dispatching is not plugged, then, while bfqq remains empty, a\n * basically uncontrolled amount of I/O from other queues may be\n * dispatched too, possibly causing the service of bfqq's I/O to be\n * delayed even longer in the drive. This problem gets more and more\n * serious as the speed and the queue depth of the drive grow,\n * because, as these two quantities grow, the probability to find no\n * queue busy but many requests in flight grows too. By contrast,\n * plugging I/O dispatching minimizes the delay induced by already\n * in-flight I/O, and enables bfqq to recover the bandwidth it may\n * lose because of this delay.\n *\n * As a side note, it is worth considering that the above\n * device-idling countermeasures may however fail in the following\n * unlucky scenario: if I/O-dispatch plugging is (correctly) disabled\n * in a time period during which all symmetry sub-conditions hold, and\n * therefore the device is allowed to enqueue many requests, but at\n * some later point in time some sub-condition stops to hold, then it\n * may become impossible to make requests be served in the desired\n * order until all the requests already queued in the device have been\n * served. The last sub-condition commented above somewhat mitigates\n * this problem for weight-raised queues.\n *\n * However, as an additional mitigation for this problem, we preserve\n * plugging for a special symmetric case that may suddenly turn into\n * asymmetric: the case where only bfqq is busy. In this case, not\n * expiring bfqq does not cause any harm to any other queues in terms\n * of service guarantees. In contrast, it avoids the following unlucky\n * sequence of events: (1) bfqq is expired, (2) a new queue with a\n * lower weight than bfqq becomes busy (or more queues), (3) the new\n * queue is served until a new request arrives for bfqq, (4) when bfqq\n * is finally served, there are so many requests of the new queue in\n * the drive that the pending requests for bfqq take a lot of time to\n * be served. In particular, event (2) may case even already\n * dispatched requests of bfqq to be delayed, inside the drive. So, to\n * avoid this series of events, the scenario is preventively declared\n * as asymmetric also if bfqq is the only busy queues\n */\nstatic bool idling_needed_for_service_guarantees(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tint tot_busy_queues = bfq_tot_busy_queues(bfqd);\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\treturn (bfqq->wr_coeff > 1 &&\n\t\t(bfqd->wr_busy_queues <\n\t\t tot_busy_queues ||\n\t\t bfqd->rq_in_driver >=\n\t\t bfqq->dispatched + 4)) ||\n\t\tbfq_asymmetric_scenario(bfqd, bfqq) ||\n\t\ttot_busy_queues == 1;\n}\n\nstatic bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t      enum bfqq_expiration reason)\n{\n\t/*\n\t * If this bfqq is shared between multiple processes, check\n\t * to make sure that those processes are still issuing I/Os\n\t * within the mean seek distance. If not, it may be time to\n\t * break the queues apart again.\n\t */\n\tif (bfq_bfqq_coop(bfqq) && BFQQ_SEEKY(bfqq))\n\t\tbfq_mark_bfqq_split_coop(bfqq);\n\n\t/*\n\t * Consider queues with a higher finish virtual time than\n\t * bfqq. If idling_needed_for_service_guarantees(bfqq) returns\n\t * true, then bfqq's bandwidth would be violated if an\n\t * uncontrolled amount of I/O from these queues were\n\t * dispatched while bfqq is waiting for its new I/O to\n\t * arrive. This is exactly what may happen if this is a forced\n\t * expiration caused by a preemption attempt, and if bfqq is\n\t * not re-scheduled. To prevent this from happening, re-queue\n\t * bfqq if it needs I/O-dispatch plugging, even if it is\n\t * empty. By doing so, bfqq is granted to be served before the\n\t * above queues (provided that bfqq is of course eligible).\n\t */\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    !(reason == BFQQE_PREEMPTED &&\n\t      idling_needed_for_service_guarantees(bfqd, bfqq))) {\n\t\tif (bfqq->dispatched == 0)\n\t\t\t/*\n\t\t\t * Overloading budget_timeout field to store\n\t\t\t * the time at which the queue remains with no\n\t\t\t * backlog and no outstanding request; used by\n\t\t\t * the weight-raising mechanism.\n\t\t\t */\n\t\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_busy(bfqq, true);\n\t} else {\n\t\tbfq_requeue_bfqq(bfqd, bfqq, true);\n\t\t/*\n\t\t * Resort priority tree of potential close cooperators.\n\t\t * See comments on bfq_pos_tree_add_move() for the unlikely().\n\t\t */\n\t\tif (unlikely(!bfqd->nonrot_with_queueing &&\n\t\t\t     !RB_EMPTY_ROOT(&bfqq->sort_list)))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\t/*\n\t * All in-service entities must have been properly deactivated\n\t * or requeued before executing the next function, which\n\t * resets all in-service entities as no more in service. This\n\t * may cause bfqq to be freed. If this happens, the next\n\t * function returns true.\n\t */\n\treturn __bfq_bfqd_reset_in_service(bfqd);\n}\n\n/**\n * __bfq_bfqq_recalc_budget - try to adapt the budget to the @bfqq behavior.\n * @bfqd: device data.\n * @bfqq: queue to update.\n * @reason: reason for expiration.\n *\n * Handle the feedback on @bfqq budget at queue expiration.\n * See the body for detailed comments.\n */\nstatic void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,\n\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t     enum bfqq_expiration reason)\n{\n\tstruct request *next_rq;\n\tint budget, min_budget;\n\n\tmin_budget = bfq_min_budget(bfqd);\n\n\tif (bfqq->wr_coeff == 1)\n\t\tbudget = bfqq->max_budget;\n\telse /*\n\t      * Use a constant, low budget for weight-raised queues,\n\t      * to help achieve a low latency. Keep it slightly higher\n\t      * than the minimum possible budget, to cause a little\n\t      * bit fewer expirations.\n\t      */\n\t\tbudget = 2 * min_budget;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last budg %d, budg left %d\",\n\t\tbfqq->entity.budget, bfq_bfqq_budget_left(bfqq));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last max_budg %d, min budg %d\",\n\t\tbudget, bfq_min_budget(bfqd));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: sync %d, seeky %d\",\n\t\tbfq_bfqq_sync(bfqq), BFQQ_SEEKY(bfqd->in_service_queue));\n\n\tif (bfq_bfqq_sync(bfqq) && bfqq->wr_coeff == 1) {\n\t\tswitch (reason) {\n\t\t/*\n\t\t * Caveat: in all the following cases we trade latency\n\t\t * for throughput.\n\t\t */\n\t\tcase BFQQE_TOO_IDLE:\n\t\t\t/*\n\t\t\t * This is the only case where we may reduce\n\t\t\t * the budget: if there is no request of the\n\t\t\t * process still waiting for completion, then\n\t\t\t * we assume (tentatively) that the timer has\n\t\t\t * expired because the batch of requests of\n\t\t\t * the process could have been served with a\n\t\t\t * smaller budget.  Hence, betting that\n\t\t\t * process will behave in the same way when it\n\t\t\t * becomes backlogged again, we reduce its\n\t\t\t * next budget.  As long as we guess right,\n\t\t\t * this budget cut reduces the latency\n\t\t\t * experienced by the process.\n\t\t\t *\n\t\t\t * However, if there are still outstanding\n\t\t\t * requests, then the process may have not yet\n\t\t\t * issued its next request just because it is\n\t\t\t * still waiting for the completion of some of\n\t\t\t * the still outstanding ones.  So in this\n\t\t\t * subcase we do not reduce its budget, on the\n\t\t\t * contrary we increase it to possibly boost\n\t\t\t * the throughput, as discussed in the\n\t\t\t * comments to the BUDGET_TIMEOUT case.\n\t\t\t */\n\t\t\tif (bfqq->dispatched > 0) /* still outstanding reqs */\n\t\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\telse {\n\t\t\t\tif (budget > 5 * min_budget)\n\t\t\t\t\tbudget -= 4 * min_budget;\n\t\t\t\telse\n\t\t\t\t\tbudget = min_budget;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_TIMEOUT:\n\t\t\t/*\n\t\t\t * We double the budget here because it gives\n\t\t\t * the chance to boost the throughput if this\n\t\t\t * is not a seeky process (and has bumped into\n\t\t\t * this timeout because of, e.g., ZBR).\n\t\t\t */\n\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_EXHAUSTED:\n\t\t\t/*\n\t\t\t * The process still has backlog, and did not\n\t\t\t * let either the budget timeout or the disk\n\t\t\t * idling timeout expire. Hence it is not\n\t\t\t * seeky, has a short thinktime and may be\n\t\t\t * happy with a higher budget too. So\n\t\t\t * definitely increase the budget of this good\n\t\t\t * candidate to boost the disk throughput.\n\t\t\t */\n\t\t\tbudget = min(budget * 4, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_NO_MORE_REQUESTS:\n\t\t\t/*\n\t\t\t * For queues that expire for this reason, it\n\t\t\t * is particularly important to keep the\n\t\t\t * budget close to the actual service they\n\t\t\t * need. Doing so reduces the timestamp\n\t\t\t * misalignment problem described in the\n\t\t\t * comments in the body of\n\t\t\t * __bfq_activate_entity. In fact, suppose\n\t\t\t * that a queue systematically expires for\n\t\t\t * BFQQE_NO_MORE_REQUESTS and presents a\n\t\t\t * new request in time to enjoy timestamp\n\t\t\t * back-shifting. The larger the budget of the\n\t\t\t * queue is with respect to the service the\n\t\t\t * queue actually requests in each service\n\t\t\t * slot, the more times the queue can be\n\t\t\t * reactivated with the same virtual finish\n\t\t\t * time. It follows that, even if this finish\n\t\t\t * time is pushed to the system virtual time\n\t\t\t * to reduce the consequent timestamp\n\t\t\t * misalignment, the queue unjustly enjoys for\n\t\t\t * many re-activations a lower finish time\n\t\t\t * than all newly activated queues.\n\t\t\t *\n\t\t\t * The service needed by bfqq is measured\n\t\t\t * quite precisely by bfqq->entity.service.\n\t\t\t * Since bfqq does not enjoy device idling,\n\t\t\t * bfqq->entity.service is equal to the number\n\t\t\t * of sectors that the process associated with\n\t\t\t * bfqq requested to read/write before waiting\n\t\t\t * for request completions, or blocking for\n\t\t\t * other reasons.\n\t\t\t */\n\t\t\tbudget = max_t(int, bfqq->entity.service, min_budget);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else if (!bfq_bfqq_sync(bfqq)) {\n\t\t/*\n\t\t * Async queues get always the maximum possible\n\t\t * budget, as for them we do not care about latency\n\t\t * (in addition, their ability to dispatch is limited\n\t\t * by the charging factor).\n\t\t */\n\t\tbudget = bfqd->bfq_max_budget;\n\t}\n\n\tbfqq->max_budget = budget;\n\n\tif (bfqd->budgets_assigned >= bfq_stats_min_budgets &&\n\t    !bfqd->bfq_user_max_budget)\n\t\tbfqq->max_budget = min(bfqq->max_budget, bfqd->bfq_max_budget);\n\n\t/*\n\t * If there is still backlog, then assign a new budget, making\n\t * sure that it is large enough for the next request.  Since\n\t * the finish time of bfqq must be kept in sync with the\n\t * budget, be sure to call __bfq_bfqq_expire() *after* this\n\t * update.\n\t *\n\t * If there is no backlog, then no need to update the budget;\n\t * it will be updated on the arrival of a new request.\n\t */\n\tnext_rq = bfqq->next_rq;\n\tif (next_rq)\n\t\tbfqq->entity.budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t\t    bfq_serv_to_charge(next_rq, bfqq));\n\n\tbfq_log_bfqq(bfqd, bfqq, \"head sect: %u, new budget %d\",\n\t\t\tnext_rq ? blk_rq_sectors(next_rq) : 0,\n\t\t\tbfqq->entity.budget);\n}\n\n/*\n * Return true if the process associated with bfqq is \"slow\". The slow\n * flag is used, in addition to the budget timeout, to reduce the\n * amount of service provided to seeky processes, and thus reduce\n * their chances to lower the throughput. More details in the comments\n * on the function bfq_bfqq_expire().\n *\n * An important observation is in order: as discussed in the comments\n * on the function bfq_update_peak_rate(), with devices with internal\n * queues, it is hard if ever possible to know when and for how long\n * an I/O request is processed by the device (apart from the trivial\n * I/O pattern where a new request is dispatched only after the\n * previous one has been completed). This makes it hard to evaluate\n * the real rate at which the I/O requests of each bfq_queue are\n * served.  In fact, for an I/O scheduler like BFQ, serving a\n * bfq_queue means just dispatching its requests during its service\n * slot (i.e., until the budget of the queue is exhausted, or the\n * queue remains idle, or, finally, a timeout fires). But, during the\n * service slot of a bfq_queue, around 100 ms at most, the device may\n * be even still processing requests of bfq_queues served in previous\n * service slots. On the opposite end, the requests of the in-service\n * bfq_queue may be completed after the service slot of the queue\n * finishes.\n *\n * Anyway, unless more sophisticated solutions are used\n * (where possible), the sum of the sizes of the requests dispatched\n * during the service slot of a bfq_queue is probably the only\n * approximation available for the service received by the bfq_queue\n * during its service slot. And this sum is the quantity used in this\n * function to evaluate the I/O speed of a process.\n */\nstatic bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t\t bool compensate, enum bfqq_expiration reason,\n\t\t\t\t unsigned long *delta_ms)\n{\n\tktime_t delta_ktime;\n\tu32 delta_usecs;\n\tbool slow = BFQQ_SEEKY(bfqq); /* if delta too short, use seekyness */\n\n\tif (!bfq_bfqq_sync(bfqq))\n\t\treturn false;\n\n\tif (compensate)\n\t\tdelta_ktime = bfqd->last_idling_start;\n\telse\n\t\tdelta_ktime = ktime_get();\n\tdelta_ktime = ktime_sub(delta_ktime, bfqd->last_budget_start);\n\tdelta_usecs = ktime_to_us(delta_ktime);\n\n\t/* don't use too short time intervals */\n\tif (delta_usecs < 1000) {\n\t\tif (blk_queue_nonrot(bfqd->queue))\n\t\t\t /*\n\t\t\t  * give same worst-case guarantees as idling\n\t\t\t  * for seeky\n\t\t\t  */\n\t\t\t*delta_ms = BFQ_MIN_TT / NSEC_PER_MSEC;\n\t\telse /* charge at least one seek */\n\t\t\t*delta_ms = bfq_slice_idle / NSEC_PER_MSEC;\n\n\t\treturn slow;\n\t}\n\n\t*delta_ms = delta_usecs / USEC_PER_MSEC;\n\n\t/*\n\t * Use only long (> 20ms) intervals to filter out excessive\n\t * spikes in service rate estimation.\n\t */\n\tif (delta_usecs > 20000) {\n\t\t/*\n\t\t * Caveat for rotational devices: processes doing I/O\n\t\t * in the slower disk zones tend to be slow(er) even\n\t\t * if not seeky. In this respect, the estimated peak\n\t\t * rate is likely to be an average over the disk\n\t\t * surface. Accordingly, to not be too harsh with\n\t\t * unlucky processes, a process is deemed slow only if\n\t\t * its rate has been lower than half of the estimated\n\t\t * peak rate.\n\t\t */\n\t\tslow = bfqq->entity.service < bfqd->bfq_max_budget / 2;\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"bfq_bfqq_is_slow: slow %d\", slow);\n\n\treturn slow;\n}\n\n/*\n * To be deemed as soft real-time, an application must meet two\n * requirements. First, the application must not require an average\n * bandwidth higher than the approximate bandwidth required to playback or\n * record a compressed high-definition video.\n * The next function is invoked on the completion of the last request of a\n * batch, to compute the next-start time instant, soft_rt_next_start, such\n * that, if the next request of the application does not arrive before\n * soft_rt_next_start, then the above requirement on the bandwidth is met.\n *\n * The second requirement is that the request pattern of the application is\n * isochronous, i.e., that, after issuing a request or a batch of requests,\n * the application stops issuing new requests until all its pending requests\n * have been completed. After that, the application may issue a new batch,\n * and so on.\n * For this reason the next function is invoked to compute\n * soft_rt_next_start only for applications that meet this requirement,\n * whereas soft_rt_next_start is set to infinity for applications that do\n * not.\n *\n * Unfortunately, even a greedy (i.e., I/O-bound) application may\n * happen to meet, occasionally or systematically, both the above\n * bandwidth and isochrony requirements. This may happen at least in\n * the following circumstances. First, if the CPU load is high. The\n * application may stop issuing requests while the CPUs are busy\n * serving other processes, then restart, then stop again for a while,\n * and so on. The other circumstances are related to the storage\n * device: the storage device is highly loaded or reaches a low-enough\n * throughput with the I/O of the application (e.g., because the I/O\n * is random and/or the device is slow). In all these cases, the\n * I/O of the application may be simply slowed down enough to meet\n * the bandwidth and isochrony requirements. To reduce the probability\n * that greedy applications are deemed as soft real-time in these\n * corner cases, a further rule is used in the computation of\n * soft_rt_next_start: the return value of this function is forced to\n * be higher than the maximum between the following two quantities.\n *\n * (a) Current time plus: (1) the maximum time for which the arrival\n *     of a request is waited for when a sync queue becomes idle,\n *     namely bfqd->bfq_slice_idle, and (2) a few extra jiffies. We\n *     postpone for a moment the reason for adding a few extra\n *     jiffies; we get back to it after next item (b).  Lower-bounding\n *     the return value of this function with the current time plus\n *     bfqd->bfq_slice_idle tends to filter out greedy applications,\n *     because the latter issue their next request as soon as possible\n *     after the last one has been completed. In contrast, a soft\n *     real-time application spends some time processing data, after a\n *     batch of its requests has been completed.\n *\n * (b) Current value of bfqq->soft_rt_next_start. As pointed out\n *     above, greedy applications may happen to meet both the\n *     bandwidth and isochrony requirements under heavy CPU or\n *     storage-device load. In more detail, in these scenarios, these\n *     applications happen, only for limited time periods, to do I/O\n *     slowly enough to meet all the requirements described so far,\n *     including the filtering in above item (a). These slow-speed\n *     time intervals are usually interspersed between other time\n *     intervals during which these applications do I/O at a very high\n *     speed. Fortunately, exactly because of the high speed of the\n *     I/O in the high-speed intervals, the values returned by this\n *     function happen to be so high, near the end of any such\n *     high-speed interval, to be likely to fall *after* the end of\n *     the low-speed time interval that follows. These high values are\n *     stored in bfqq->soft_rt_next_start after each invocation of\n *     this function. As a consequence, if the last value of\n *     bfqq->soft_rt_next_start is constantly used to lower-bound the\n *     next value that this function may return, then, from the very\n *     beginning of a low-speed interval, bfqq->soft_rt_next_start is\n *     likely to be constantly kept so high that any I/O request\n *     issued during the low-speed interval is considered as arriving\n *     to soon for the application to be deemed as soft\n *     real-time. Then, in the high-speed interval that follows, the\n *     application will not be deemed as soft real-time, just because\n *     it will do I/O at a high speed. And so on.\n *\n * Getting back to the filtering in item (a), in the following two\n * cases this filtering might be easily passed by a greedy\n * application, if the reference quantity was just\n * bfqd->bfq_slice_idle:\n * 1) HZ is so low that the duration of a jiffy is comparable to or\n *    higher than bfqd->bfq_slice_idle. This happens, e.g., on slow\n *    devices with HZ=100. The time granularity may be so coarse\n *    that the approximation, in jiffies, of bfqd->bfq_slice_idle\n *    is rather lower than the exact value.\n * 2) jiffies, instead of increasing at a constant rate, may stop increasing\n *    for a while, then suddenly 'jump' by several units to recover the lost\n *    increments. This seems to happen, e.g., inside virtual machines.\n * To address this issue, in the filtering in (a) we do not use as a\n * reference time interval just bfqd->bfq_slice_idle, but\n * bfqd->bfq_slice_idle plus a few jiffies. In particular, we add the\n * minimum number of jiffies for which the filter seems to be quite\n * precise also in embedded systems and KVM/QEMU virtual machines.\n */\nstatic unsigned long bfq_bfqq_softrt_next_start(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn max3(bfqq->soft_rt_next_start,\n\t\t    bfqq->last_idle_bklogged +\n\t\t    HZ * bfqq->service_from_backlogged /\n\t\t    bfqd->bfq_wr_max_softrt_rate,\n\t\t    jiffies + nsecs_to_jiffies(bfqq->bfqd->bfq_slice_idle) + 4);\n}\n\n/**\n * bfq_bfqq_expire - expire a queue.\n * @bfqd: device owning the queue.\n * @bfqq: the queue to expire.\n * @compensate: if true, compensate for the time spent idling.\n * @reason: the reason causing the expiration.\n *\n * If the process associated with bfqq does slow I/O (e.g., because it\n * issues random requests), we charge bfqq with the time it has been\n * in service instead of the service it has received (see\n * bfq_bfqq_charge_time for details on how this goal is achieved). As\n * a consequence, bfqq will typically get higher timestamps upon\n * reactivation, and hence it will be rescheduled as if it had\n * received more service than what it has actually received. In the\n * end, bfqq receives less service in proportion to how slowly its\n * associated process consumes its budgets (and hence how seriously it\n * tends to lower the throughput). In addition, this time-charging\n * strategy guarantees time fairness among slow processes. In\n * contrast, if the process associated with bfqq is not slow, we\n * charge bfqq exactly with the service it has received.\n *\n * Charging time to the first type of queues and the exact service to\n * the other has the effect of using the WF2Q+ policy to schedule the\n * former on a timeslice basis, without violating service domain\n * guarantees among the latter.\n */\nvoid bfq_bfqq_expire(struct bfq_data *bfqd,\n\t\t     struct bfq_queue *bfqq,\n\t\t     bool compensate,\n\t\t     enum bfqq_expiration reason)\n{\n\tbool slow;\n\tunsigned long delta = 0;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t/*\n\t * Check whether the process is slow (see bfq_bfqq_is_slow).\n\t */\n\tslow = bfq_bfqq_is_slow(bfqd, bfqq, compensate, reason, &delta);\n\n\t/*\n\t * As above explained, charge slow (typically seeky) and\n\t * timed-out queues with the time and not the service\n\t * received, to favor sequential workloads.\n\t *\n\t * Processes doing I/O in the slower disk zones will tend to\n\t * be slow(er) even if not seeky. Therefore, since the\n\t * estimated peak rate is actually an average over the disk\n\t * surface, these processes may timeout just for bad luck. To\n\t * avoid punishing them, do not charge time to processes that\n\t * succeeded in consuming at least 2/3 of their budget. This\n\t * allows BFQ to preserve enough elasticity to still perform\n\t * bandwidth, and not time, distribution with little unlucky\n\t * or quasi-sequential processes.\n\t */\n\tif (bfqq->wr_coeff == 1 &&\n\t    (slow ||\n\t     (reason == BFQQE_BUDGET_TIMEOUT &&\n\t      bfq_bfqq_budget_left(bfqq) >=  entity->budget / 3)))\n\t\tbfq_bfqq_charge_time(bfqd, bfqq, delta);\n\n\tif (bfqd->low_latency && bfqq->wr_coeff == 1)\n\t\tbfqq->last_wr_start_finish = jiffies;\n\n\tif (bfqd->low_latency && bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\t/*\n\t\t * If we get here, and there are no outstanding\n\t\t * requests, then the request pattern is isochronous\n\t\t * (see the comments on the function\n\t\t * bfq_bfqq_softrt_next_start()). Therefore we can\n\t\t * compute soft_rt_next_start.\n\t\t *\n\t\t * If, instead, the queue still has outstanding\n\t\t * requests, then we have to wait for the completion\n\t\t * of all the outstanding requests to discover whether\n\t\t * the request pattern is actually isochronous.\n\t\t */\n\t\tif (bfqq->dispatched == 0)\n\t\t\tbfqq->soft_rt_next_start =\n\t\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\t\telse if (bfqq->dispatched > 0) {\n\t\t\t/*\n\t\t\t * Schedule an update of soft_rt_next_start to when\n\t\t\t * the task may be discovered to be isochronous.\n\t\t\t */\n\t\t\tbfq_mark_bfqq_softrt_update(bfqq);\n\t\t}\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\"expire (%d, slow %d, num_disp %d, short_ttime %d)\", reason,\n\t\tslow, bfqq->dispatched, bfq_bfqq_has_short_ttime(bfqq));\n\n\t/*\n\t * bfqq expired, so no total service time needs to be computed\n\t * any longer: reset state machine for measuring total service\n\t * times.\n\t */\n\tbfqd->rqs_injected = bfqd->wait_dispatch = false;\n\tbfqd->waited_rq = NULL;\n\n\t/*\n\t * Increase, decrease or leave budget unchanged according to\n\t * reason.\n\t */\n\t__bfq_bfqq_recalc_budget(bfqd, bfqq, reason);\n\tif (__bfq_bfqq_expire(bfqd, bfqq, reason))\n\t\t/* bfqq is gone, no more actions on it */\n\t\treturn;\n\n\t/* mark bfqq as waiting a request only if a bic still points to it */\n\tif (!bfq_bfqq_busy(bfqq) &&\n\t    reason != BFQQE_BUDGET_TIMEOUT &&\n\t    reason != BFQQE_BUDGET_EXHAUSTED) {\n\t\tbfq_mark_bfqq_non_blocking_wait_rq(bfqq);\n\t\t/*\n\t\t * Not setting service to 0, because, if the next rq\n\t\t * arrives in time, the queue will go on receiving\n\t\t * service with this same budget (as if it never expired)\n\t\t */\n\t} else\n\t\tentity->service = 0;\n\n\t/*\n\t * Reset the received-service counter for every parent entity.\n\t * Differently from what happens with bfqq->entity.service,\n\t * the resetting of this counter never needs to be postponed\n\t * for parent entities. In fact, in case bfqq may have a\n\t * chance to go on being served using the last, partially\n\t * consumed budget, bfqq->entity.service needs to be kept,\n\t * because if bfqq then actually goes on being served using\n\t * the same budget, the last value of bfqq->entity.service is\n\t * needed to properly decrement bfqq->entity.budget by the\n\t * portion already consumed. In contrast, it is not necessary\n\t * to keep entity->service for parent entities too, because\n\t * the bubble up of the new value of bfqq->entity.budget will\n\t * make sure that the budgets of parent entities are correct,\n\t * even in case bfqq and thus parent entities go on receiving\n\t * service with the same budget.\n\t */\n\tentity = entity->parent;\n\tfor_each_entity(entity)\n\t\tentity->service = 0;\n}\n\n/*\n * Budget timeout is not implemented through a dedicated timer, but\n * just checked on request arrivals and completions, as well as on\n * idle timer expirations.\n */\nstatic bool bfq_bfqq_budget_timeout(struct bfq_queue *bfqq)\n{\n\treturn time_is_before_eq_jiffies(bfqq->budget_timeout);\n}\n\n/*\n * If we expire a queue that is actively waiting (i.e., with the\n * device idled) for the arrival of a new request, then we may incur\n * the timestamp misalignment problem described in the body of the\n * function __bfq_activate_entity. Hence we return true only if this\n * condition does not hold, or if the queue is slow enough to deserve\n * only to be kicked off for preserving a high throughput.\n */\nstatic bool bfq_may_expire_for_budg_timeout(struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\"may_budget_timeout: wait_request %d left %d timeout %d\",\n\t\tbfq_bfqq_wait_request(bfqq),\n\t\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3,\n\t\tbfq_bfqq_budget_timeout(bfqq));\n\n\treturn (!bfq_bfqq_wait_request(bfqq) ||\n\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3)\n\t\t&&\n\t\tbfq_bfqq_budget_timeout(bfqq);\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq)\n{\n\tbool rot_without_queueing =\n\t\t!blk_queue_nonrot(bfqd->queue) && !bfqd->hw_tag,\n\t\tbfqq_sequential_and_IO_bound,\n\t\tidling_boosts_thr;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tbfqq_sequential_and_IO_bound = !BFQQ_SEEKY(bfqq) &&\n\t\tbfq_bfqq_IO_bound(bfqq) && bfq_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * The next variable takes into account the cases where idling\n\t * boosts the throughput.\n\t *\n\t * The value of the variable is computed considering, first, that\n\t * idling is virtually always beneficial for the throughput if:\n\t * (a) the device is not NCQ-capable and rotational, or\n\t * (b) regardless of the presence of NCQ, the device is rotational and\n\t *     the request pattern for bfqq is I/O-bound and sequential, or\n\t * (c) regardless of whether it is rotational, the device is\n\t *     not NCQ-capable and the request pattern for bfqq is\n\t *     I/O-bound and sequential.\n\t *\n\t * Secondly, and in contrast to the above item (b), idling an\n\t * NCQ-capable flash-based device would not boost the\n\t * throughput even with sequential I/O; rather it would lower\n\t * the throughput in proportion to how fast the device\n\t * is. Accordingly, the next variable is true if any of the\n\t * above conditions (a), (b) or (c) is true, and, in\n\t * particular, happens to be false if bfqd is an NCQ-capable\n\t * flash-based device.\n\t */\n\tidling_boosts_thr = rot_without_queueing ||\n\t\t((!blk_queue_nonrot(bfqd->queue) || !bfqd->hw_tag) &&\n\t\t bfqq_sequential_and_IO_bound);\n\n\t/*\n\t * The return value of this function is equal to that of\n\t * idling_boosts_thr, unless a special case holds. In this\n\t * special case, described below, idling may cause problems to\n\t * weight-raised queues.\n\t *\n\t * When the request pool is saturated (e.g., in the presence\n\t * of write hogs), if the processes associated with\n\t * non-weight-raised queues ask for requests at a lower rate,\n\t * then processes associated with weight-raised queues have a\n\t * higher probability to get a request from the pool\n\t * immediately (or at least soon) when they need one. Thus\n\t * they have a higher probability to actually get a fraction\n\t * of the device throughput proportional to their high\n\t * weight. This is especially true with NCQ-capable drives,\n\t * which enqueue several requests in advance, and further\n\t * reorder internally-queued requests.\n\t *\n\t * For this reason, we force to false the return value if\n\t * there are weight-raised busy queues. In this case, and if\n\t * bfqq is not weight-raised, this guarantees that the device\n\t * is not idled for bfqq (if, instead, bfqq is weight-raised,\n\t * then idling will be guaranteed by another variable, see\n\t * below). Combined with the timestamping rules of BFQ (see\n\t * [1] for details), this behavior causes bfqq, and hence any\n\t * sync non-weight-raised queue, to get a lower number of\n\t * requests served, and thus to ask for a lower number of\n\t * requests from the request pool, before the busy\n\t * weight-raised queues get served again. This often mitigates\n\t * starvation problems in the presence of heavy write\n\t * workloads and NCQ, thereby guaranteeing a higher\n\t * application and system responsiveness in these hostile\n\t * scenarios.\n\t */\n\treturn idling_boosts_thr &&\n\t\tbfqd->wr_busy_queues == 0;\n}\n\n/*\n * For a queue that becomes empty, device idling is allowed only if\n * this function returns true for that queue. As a consequence, since\n * device idling plays a critical role for both throughput boosting\n * and service guarantees, the return value of this function plays a\n * critical role as well.\n *\n * In a nutshell, this function returns true only if idling is\n * beneficial for throughput or, even if detrimental for throughput,\n * idling is however necessary to preserve service guarantees (low\n * latency, desired throughput distribution, ...). In particular, on\n * NCQ-capable devices, this function tries to return false, so as to\n * help keep the drives' internal queues full, whenever this helps the\n * device boost the throughput without causing any service-guarantee\n * issue.\n *\n * Most of the issues taken into account to get the return value of\n * this function are not trivial. We discuss these issues in the two\n * functions providing the main pieces of information needed by this\n * function.\n */\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tbool idling_boosts_thr_with_no_issue, idling_needed_for_service_guar;\n\n\t/* No point in idling for bfqq if it won't get requests any longer */\n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tif (unlikely(bfqd->strict_guarantees))\n\t\treturn true;\n\n\t/*\n\t * Idling is performed only if slice_idle > 0. In addition, we\n\t * do not idle if\n\t * (a) bfqq is async\n\t * (b) bfqq is in the idle io prio class: in this case we do\n\t * not idle because we want to minimize the bandwidth that\n\t * queues in this class can steal to higher-priority queues\n\t */\n\tif (bfqd->bfq_slice_idle == 0 || !bfq_bfqq_sync(bfqq) ||\n\t   bfq_class_idle(bfqq))\n\t\treturn false;\n\n\tidling_boosts_thr_with_no_issue =\n\t\tidling_boosts_thr_without_issues(bfqd, bfqq);\n\n\tidling_needed_for_service_guar =\n\t\tidling_needed_for_service_guarantees(bfqd, bfqq);\n\n\t/*\n\t * We have now the two components we need to compute the\n\t * return value of the function, which is true only if idling\n\t * either boosts the throughput (without issues), or is\n\t * necessary to preserve service guarantees.\n\t */\n\treturn idling_boosts_thr_with_no_issue ||\n\t\tidling_needed_for_service_guar;\n}\n\n/*\n * If the in-service queue is empty but the function bfq_better_to_idle\n * returns true, then:\n * 1) the queue must remain in service and cannot be expired, and\n * 2) the device must be idled to wait for the possible arrival of a new\n *    request for the queue.\n * See the comments on the function bfq_better_to_idle for the reasons\n * why performing device idling is the best choice to boost the throughput\n * and preserve service guarantees when bfq_better_to_idle itself\n * returns true.\n */\nstatic bool bfq_bfqq_must_idle(struct bfq_queue *bfqq)\n{\n\treturn RB_EMPTY_ROOT(&bfqq->sort_list) && bfq_better_to_idle(bfqq);\n}\n\n/*\n * This function chooses the queue from which to pick the next extra\n * I/O request to inject, if it finds a compatible queue. See the\n * comments on bfq_update_inject_limit() for details on the injection\n * mechanism, and for the definitions of the quantities mentioned\n * below.\n */\nstatic struct bfq_queue *\nbfq_choose_bfqq_for_injection(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *in_serv_bfqq = bfqd->in_service_queue;\n\tunsigned int limit = in_serv_bfqq->inject_limit;\n\t/*\n\t * If\n\t * - bfqq is not weight-raised and therefore does not carry\n\t *   time-critical I/O,\n\t * or\n\t * - regardless of whether bfqq is weight-raised, bfqq has\n\t *   however a long think time, during which it can absorb the\n\t *   effect of an appropriate number of extra I/O requests\n\t *   from other queues (see bfq_update_inject_limit for\n\t *   details on the computation of this number);\n\t * then injection can be performed without restrictions.\n\t */\n\tbool in_serv_always_inject = in_serv_bfqq->wr_coeff == 1 ||\n\t\t!bfq_bfqq_has_short_ttime(in_serv_bfqq);\n\n\t/*\n\t * If\n\t * - the baseline total service time could not be sampled yet,\n\t *   so the inject limit happens to be still 0, and\n\t * - a lot of time has elapsed since the plugging of I/O\n\t *   dispatching started, so drive speed is being wasted\n\t *   significantly;\n\t * then temporarily raise inject limit to one request.\n\t */\n\tif (limit == 0 && in_serv_bfqq->last_serv_time_ns == 0 &&\n\t    bfq_bfqq_wait_request(in_serv_bfqq) &&\n\t    time_is_before_eq_jiffies(bfqd->last_idling_start_jiffies +\n\t\t\t\t      bfqd->bfq_slice_idle)\n\t\t)\n\t\tlimit = 1;\n\n\tif (bfqd->rq_in_driver >= limit)\n\t\treturn NULL;\n\n\t/*\n\t * Linear search of the source queue for injection; but, with\n\t * a high probability, very few steps are needed to find a\n\t * candidate queue, i.e., a queue with enough budget left for\n\t * its next request. In fact:\n\t * - BFQ dynamically updates the budget of every queue so as\n\t *   to accommodate the expected backlog of the queue;\n\t * - if a queue gets all its requests dispatched as injected\n\t *   service, then the queue is removed from the active list\n\t *   (and re-added only if it gets new requests, but then it\n\t *   is assigned again enough budget for its new backlog).\n\t */\n\tlist_for_each_entry(bfqq, &bfqd->active_list, bfqq_list)\n\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t    (in_serv_always_inject || bfqq->wr_coeff > 1) &&\n\t\t    bfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t    bfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Allow for only one large in-flight request\n\t\t\t * on non-rotational devices, for the\n\t\t\t * following reason. On non-rotationl drives,\n\t\t\t * large requests take much longer than\n\t\t\t * smaller requests to be served. In addition,\n\t\t\t * the drive prefers to serve large requests\n\t\t\t * w.r.t. to small ones, if it can choose. So,\n\t\t\t * having more than one large requests queued\n\t\t\t * in the drive may easily make the next first\n\t\t\t * request of the in-service queue wait for so\n\t\t\t * long to break bfqq's service guarantees. On\n\t\t\t * the bright side, large requests let the\n\t\t\t * drive reach a very high throughput, even if\n\t\t\t * there is only one in-flight large request\n\t\t\t * at a time.\n\t\t\t */\n\t\t\tif (blk_queue_nonrot(bfqd->queue) &&\n\t\t\t    blk_rq_sectors(bfqq->next_rq) >=\n\t\t\t    BFQQ_SECT_THR_NONROT)\n\t\t\t\tlimit = min_t(unsigned int, 1, limit);\n\t\t\telse\n\t\t\t\tlimit = in_serv_bfqq->inject_limit;\n\n\t\t\tif (bfqd->rq_in_driver < limit) {\n\t\t\t\tbfqd->rqs_injected = true;\n\t\t\t\treturn bfqq;\n\t\t\t}\n\t\t}\n\n\treturn NULL;\n}\n\n/*\n * Select a queue for service.  If we have a current queue in service,\n * check whether to continue servicing it, or retrieve and set a new one.\n */\nstatic struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\tstruct request *next_rq;\n\tenum bfqq_expiration reason = BFQQE_BUDGET_TIMEOUT;\n\n\tbfqq = bfqd->in_service_queue;\n\tif (!bfqq)\n\t\tgoto new_queue;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: already in-service queue\");\n\n\t/*\n\t * Do not expire bfqq for budget timeout if bfqq may be about\n\t * to enjoy device idling. The reason why, in this case, we\n\t * prevent bfqq from expiring is the same as in the comments\n\t * on the case where bfq_bfqq_must_idle() returns true, in\n\t * bfq_completed_request().\n\t */\n\tif (bfq_may_expire_for_budg_timeout(bfqq) &&\n\t    !bfq_bfqq_must_idle(bfqq))\n\t\tgoto expire;\n\ncheck_queue:\n\t/*\n\t * This loop is rarely executed more than once. Even when it\n\t * happens, it is much more convenient to re-execute this loop\n\t * than to return NULL and trigger a new dispatch to get a\n\t * request served.\n\t */\n\tnext_rq = bfqq->next_rq;\n\t/*\n\t * If bfqq has requests queued and it has enough budget left to\n\t * serve them, keep the queue, otherwise expire it.\n\t */\n\tif (next_rq) {\n\t\tif (bfq_serv_to_charge(next_rq, bfqq) >\n\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t/*\n\t\t\t * Expire the queue for budget exhaustion,\n\t\t\t * which makes sure that the next budget is\n\t\t\t * enough to serve the next request, even if\n\t\t\t * it comes from the fifo expired path.\n\t\t\t */\n\t\t\treason = BFQQE_BUDGET_EXHAUSTED;\n\t\t\tgoto expire;\n\t\t} else {\n\t\t\t/*\n\t\t\t * The idle timer may be pending because we may\n\t\t\t * not disable disk idling even when a new request\n\t\t\t * arrives.\n\t\t\t */\n\t\t\tif (bfq_bfqq_wait_request(bfqq)) {\n\t\t\t\t/*\n\t\t\t\t * If we get here: 1) at least a new request\n\t\t\t\t * has arrived but we have not disabled the\n\t\t\t\t * timer because the request was too small,\n\t\t\t\t * 2) then the block layer has unplugged\n\t\t\t\t * the device, causing the dispatch to be\n\t\t\t\t * invoked.\n\t\t\t\t *\n\t\t\t\t * Since the device is unplugged, now the\n\t\t\t\t * requests are probably large enough to\n\t\t\t\t * provide a reasonable throughput.\n\t\t\t\t * So we disable idling.\n\t\t\t\t */\n\t\t\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\t\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\t\t\t}\n\t\t\tgoto keep_queue;\n\t\t}\n\t}\n\n\t/*\n\t * No requests pending. However, if the in-service queue is idling\n\t * for a new request, or has requests waiting for a completion and\n\t * may idle after their completion, then keep it anyway.\n\t *\n\t * Yet, inject service from other queues if it boosts\n\t * throughput and is possible.\n\t */\n\tif (bfq_bfqq_wait_request(bfqq) ||\n\t    (bfqq->dispatched != 0 && bfq_better_to_idle(bfqq))) {\n\t\tstruct bfq_queue *async_bfqq =\n\t\t\tbfqq->bic && bfqq->bic->bfqq[0] &&\n\t\t\tbfq_bfqq_busy(bfqq->bic->bfqq[0]) &&\n\t\t\tbfqq->bic->bfqq[0]->next_rq ?\n\t\t\tbfqq->bic->bfqq[0] : NULL;\n\t\tstruct bfq_queue *blocked_bfqq =\n\t\t\t!hlist_empty(&bfqq->woken_list) ?\n\t\t\tcontainer_of(bfqq->woken_list.first,\n\t\t\t\t     struct bfq_queue,\n\t\t\t\t     woken_list_node)\n\t\t\t: NULL;\n\n\t\t/*\n\t\t * The next four mutually-exclusive ifs decide\n\t\t * whether to try injection, and choose the queue to\n\t\t * pick an I/O request from.\n\t\t *\n\t\t * The first if checks whether the process associated\n\t\t * with bfqq has also async I/O pending. If so, it\n\t\t * injects such I/O unconditionally. Injecting async\n\t\t * I/O from the same process can cause no harm to the\n\t\t * process. On the contrary, it can only increase\n\t\t * bandwidth and reduce latency for the process.\n\t\t *\n\t\t * The second if checks whether there happens to be a\n\t\t * non-empty waker queue for bfqq, i.e., a queue whose\n\t\t * I/O needs to be completed for bfqq to receive new\n\t\t * I/O. This happens, e.g., if bfqq is associated with\n\t\t * a process that does some sync. A sync generates\n\t\t * extra blocking I/O, which must be completed before\n\t\t * the process associated with bfqq can go on with its\n\t\t * I/O. If the I/O of the waker queue is not served,\n\t\t * then bfqq remains empty, and no I/O is dispatched,\n\t\t * until the idle timeout fires for bfqq. This is\n\t\t * likely to result in lower bandwidth and higher\n\t\t * latencies for bfqq, and in a severe loss of total\n\t\t * throughput. The best action to take is therefore to\n\t\t * serve the waker queue as soon as possible. So do it\n\t\t * (without relying on the third alternative below for\n\t\t * eventually serving waker_bfqq's I/O; see the last\n\t\t * paragraph for further details). This systematic\n\t\t * injection of I/O from the waker queue does not\n\t\t * cause any delay to bfqq's I/O. On the contrary,\n\t\t * next bfqq's I/O is brought forward dramatically,\n\t\t * for it is not blocked for milliseconds.\n\t\t *\n\t\t * The third if checks whether there is a queue woken\n\t\t * by bfqq, and currently with pending I/O. Such a\n\t\t * woken queue does not steal bandwidth from bfqq,\n\t\t * because it remains soon without I/O if bfqq is not\n\t\t * served. So there is virtually no risk of loss of\n\t\t * bandwidth for bfqq if this woken queue has I/O\n\t\t * dispatched while bfqq is waiting for new I/O.\n\t\t *\n\t\t * The fourth if checks whether bfqq is a queue for\n\t\t * which it is better to avoid injection. It is so if\n\t\t * bfqq delivers more throughput when served without\n\t\t * any further I/O from other queues in the middle, or\n\t\t * if the service times of bfqq's I/O requests both\n\t\t * count more than overall throughput, and may be\n\t\t * easily increased by injection (this happens if bfqq\n\t\t * has a short think time). If none of these\n\t\t * conditions holds, then a candidate queue for\n\t\t * injection is looked for through\n\t\t * bfq_choose_bfqq_for_injection(). Note that the\n\t\t * latter may return NULL (for example if the inject\n\t\t * limit for bfqq is currently 0).\n\t\t *\n\t\t * NOTE: motivation for the second alternative\n\t\t *\n\t\t * Thanks to the way the inject limit is updated in\n\t\t * bfq_update_has_short_ttime(), it is rather likely\n\t\t * that, if I/O is being plugged for bfqq and the\n\t\t * waker queue has pending I/O requests that are\n\t\t * blocking bfqq's I/O, then the fourth alternative\n\t\t * above lets the waker queue get served before the\n\t\t * I/O-plugging timeout fires. So one may deem the\n\t\t * second alternative superfluous. It is not, because\n\t\t * the fourth alternative may be way less effective in\n\t\t * case of a synchronization. For two main\n\t\t * reasons. First, throughput may be low because the\n\t\t * inject limit may be too low to guarantee the same\n\t\t * amount of injected I/O, from the waker queue or\n\t\t * other queues, that the second alternative\n\t\t * guarantees (the second alternative unconditionally\n\t\t * injects a pending I/O request of the waker queue\n\t\t * for each bfq_dispatch_request()). Second, with the\n\t\t * fourth alternative, the duration of the plugging,\n\t\t * i.e., the time before bfqq finally receives new I/O,\n\t\t * may not be minimized, because the waker queue may\n\t\t * happen to be served only after other queues.\n\t\t */\n\t\tif (async_bfqq &&\n\t\t    icq_to_bic(async_bfqq->next_rq->elv.icq) == bfqq->bic &&\n\t\t    bfq_serv_to_charge(async_bfqq->next_rq, async_bfqq) <=\n\t\t    bfq_bfqq_budget_left(async_bfqq))\n\t\t\tbfqq = bfqq->bic->bfqq[0];\n\t\telse if (bfqq->waker_bfqq &&\n\t\t\t   bfq_bfqq_busy(bfqq->waker_bfqq) &&\n\t\t\t   bfqq->waker_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(bfqq->waker_bfqq->next_rq,\n\t\t\t\t\t      bfqq->waker_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(bfqq->waker_bfqq)\n\t\t\t)\n\t\t\tbfqq = bfqq->waker_bfqq;\n\t\telse if (blocked_bfqq &&\n\t\t\t   bfq_bfqq_busy(blocked_bfqq) &&\n\t\t\t   blocked_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(blocked_bfqq->next_rq,\n\t\t\t\t\t      blocked_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(blocked_bfqq)\n\t\t\t)\n\t\t\tbfqq = blocked_bfqq;\n\t\telse if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t (bfqq->wr_coeff == 1 || bfqd->wr_busy_queues > 1 ||\n\t\t\t  !bfq_bfqq_has_short_ttime(bfqq)))\n\t\t\tbfqq = bfq_choose_bfqq_for_injection(bfqd);\n\t\telse\n\t\t\tbfqq = NULL;\n\n\t\tgoto keep_queue;\n\t}\n\n\treason = BFQQE_NO_MORE_REQUESTS;\nexpire:\n\tbfq_bfqq_expire(bfqd, bfqq, false, reason);\nnew_queue:\n\tbfqq = bfq_set_in_service_queue(bfqd);\n\tif (bfqq) {\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: checking new queue\");\n\t\tgoto check_queue;\n\t}\nkeep_queue:\n\tif (bfqq)\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: returned this queue\");\n\telse\n\t\tbfq_log(bfqd, \"select_queue: no queue returned\");\n\n\treturn bfqq;\n}\n\nstatic void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tif (bfqq->wr_coeff > 1) { /* queue is being weight-raised */\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t\"raising period dur %u/%u msec, old coeff %u, w %d(%d)\",\n\t\t\tjiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),\n\t\t\tjiffies_to_msecs(bfqq->wr_cur_max_time),\n\t\t\tbfqq->wr_coeff,\n\t\t\tbfqq->entity.weight, bfqq->entity.orig_weight);\n\n\t\tif (entity->prio_changed)\n\t\t\tbfq_log_bfqq(bfqd, bfqq, \"WARN: pending prio change\");\n\n\t\t/*\n\t\t * If the queue was activated in a burst, or too much\n\t\t * time has elapsed from the beginning of this\n\t\t * weight-raising period, then end weight raising.\n\t\t */\n\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\telse if (time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t\t\tbfqq->wr_cur_max_time)) {\n\t\t\tif (bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time ||\n\t\t\ttime_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t       bfq_wr_duration(bfqd))) {\n\t\t\t\t/*\n\t\t\t\t * Either in interactive weight\n\t\t\t\t * raising, or in soft_rt weight\n\t\t\t\t * raising with the\n\t\t\t\t * interactive-weight-raising period\n\t\t\t\t * elapsed (so no switch back to\n\t\t\t\t * interactive weight raising).\n\t\t\t\t */\n\t\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t\t} else { /*\n\t\t\t\t  * soft_rt finishing while still in\n\t\t\t\t  * interactive period, switch back to\n\t\t\t\t  * interactive weight raising\n\t\t\t\t  */\n\t\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t\t}\n\t\t}\n\t\tif (bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time &&\n\t\t    bfqq->service_from_wr > max_service_from_wr) {\n\t\t\t/* see comments on max_service_from_wr */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t}\n\t}\n\t/*\n\t * To improve latency (for this or other queues), immediately\n\t * update weight both if it must be raised and if it must be\n\t * lowered. Since, entity may be on some active tree here, and\n\t * might have a pending change of its ioprio class, invoke\n\t * next function with the last parameter unset (see the\n\t * comments on the function).\n\t */\n\tif ((entity->weight > entity->orig_weight) != (bfqq->wr_coeff > 1))\n\t\t__bfq_entity_update_weight_prio(bfq_entity_service_tree(entity),\n\t\t\t\t\t\tentity, false);\n}\n\n/*\n * Dispatch next request from bfqq.\n */\nstatic struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct request *rq = bfqq->next_rq;\n\tunsigned long service_to_charge;\n\n\tservice_to_charge = bfq_serv_to_charge(rq, bfqq);\n\n\tbfq_bfqq_served(bfqq, service_to_charge);\n\n\tif (bfqq == bfqd->in_service_queue && bfqd->wait_dispatch) {\n\t\tbfqd->wait_dispatch = false;\n\t\tbfqd->waited_rq = rq;\n\t}\n\n\tbfq_dispatch_remove(bfqd->queue, rq);\n\n\tif (bfqq != bfqd->in_service_queue)\n\t\tgoto return_rq;\n\n\t/*\n\t * If weight raising has to terminate for bfqq, then next\n\t * function causes an immediate update of bfqq's weight,\n\t * without waiting for next activation. As a consequence, on\n\t * expiration, bfqq will be timestamped as if has never been\n\t * weight-raised during this service slot, even if it has\n\t * received part or even most of the service as a\n\t * weight-raised queue. This inflates bfqq's timestamps, which\n\t * is beneficial, as bfqq is then more willing to leave the\n\t * device immediately to possible other weight-raised queues.\n\t */\n\tbfq_update_wr_data(bfqd, bfqq);\n\n\t/*\n\t * Expire bfqq, pretending that its budget expired, if bfqq\n\t * belongs to CLASS_IDLE and other queues are waiting for\n\t * service.\n\t */\n\tif (!(bfq_tot_busy_queues(bfqd) > 1 && bfq_class_idle(bfqq)))\n\t\tgoto return_rq;\n\n\tbfq_bfqq_expire(bfqd, bfqq, false, BFQQE_BUDGET_EXHAUSTED);\n\nreturn_rq:\n\treturn rq;\n}\n\nstatic bool bfq_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\n\t/*\n\t * Avoiding lock: a race on bfqd->queued should cause at\n\t * most a call to dispatch for nothing\n\t */\n\treturn !list_empty_careful(&bfqd->dispatch) ||\n\t\tREAD_ONCE(bfqd->queued);\n}\n\nstatic struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq = NULL;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tif (!list_empty(&bfqd->dispatch)) {\n\t\trq = list_first_entry(&bfqd->dispatch, struct request,\n\t\t\t\t      queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (bfqq) {\n\t\t\t/*\n\t\t\t * Increment counters here, because this\n\t\t\t * dispatch does not follow the standard\n\t\t\t * dispatch flow (where counters are\n\t\t\t * incremented)\n\t\t\t */\n\t\t\tbfqq->dispatched++;\n\n\t\t\tgoto inc_in_driver_start_rq;\n\t\t}\n\n\t\t/*\n\t\t * We exploit the bfq_finish_requeue_request hook to\n\t\t * decrement rq_in_driver, but\n\t\t * bfq_finish_requeue_request will not be invoked on\n\t\t * this request. So, to avoid unbalance, just start\n\t\t * this request, without incrementing rq_in_driver. As\n\t\t * a negative consequence, rq_in_driver is deceptively\n\t\t * lower than it should be while this request is in\n\t\t * service. This may cause bfq_schedule_dispatch to be\n\t\t * invoked uselessly.\n\t\t *\n\t\t * As for implementing an exact solution, the\n\t\t * bfq_finish_requeue_request hook, if defined, is\n\t\t * probably invoked also on this request. So, by\n\t\t * exploiting this hook, we could 1) increment\n\t\t * rq_in_driver here, and 2) decrement it in\n\t\t * bfq_finish_requeue_request. Such a solution would\n\t\t * let the value of the counter be always accurate,\n\t\t * but it would entail using an extra interface\n\t\t * function. This cost seems higher than the benefit,\n\t\t * being the frequency of non-elevator-private\n\t\t * requests very low.\n\t\t */\n\t\tgoto start_rq;\n\t}\n\n\tbfq_log(bfqd, \"dispatch requests: %d busy queues\",\n\t\tbfq_tot_busy_queues(bfqd));\n\n\tif (bfq_tot_busy_queues(bfqd) == 0)\n\t\tgoto exit;\n\n\t/*\n\t * Force device to serve one request at a time if\n\t * strict_guarantees is true. Forcing this service scheme is\n\t * currently the ONLY way to guarantee that the request\n\t * service order enforced by the scheduler is respected by a\n\t * queueing device. Otherwise the device is free even to make\n\t * some unlucky request wait for as long as the device\n\t * wishes.\n\t *\n\t * Of course, serving one request at a time may cause loss of\n\t * throughput.\n\t */\n\tif (bfqd->strict_guarantees && bfqd->rq_in_driver > 0)\n\t\tgoto exit;\n\n\tbfqq = bfq_select_queue(bfqd);\n\tif (!bfqq)\n\t\tgoto exit;\n\n\trq = bfq_dispatch_rq_from_bfqq(bfqd, bfqq);\n\n\tif (rq) {\ninc_in_driver_start_rq:\n\t\tbfqd->rq_in_driver++;\nstart_rq:\n\t\trq->rq_flags |= RQF_STARTED;\n\t}\nexit:\n\treturn rq;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t      struct request *rq,\n\t\t\t\t      struct bfq_queue *in_serv_queue,\n\t\t\t\t      bool idle_timer_disabled)\n{\n\tstruct bfq_queue *bfqq = rq ? RQ_BFQQ(rq) : NULL;\n\n\tif (!idle_timer_disabled && !bfqq)\n\t\treturn;\n\n\t/*\n\t * rq and bfqq are guaranteed to exist until this function\n\t * ends, for the following reasons. First, rq can be\n\t * dispatched to the device, and then can be completed and\n\t * freed, only after this function ends. Second, rq cannot be\n\t * merged (and thus freed because of a merge) any longer,\n\t * because it has already started. Thus rq cannot be freed\n\t * before this function ends, and, since rq has a reference to\n\t * bfqq, the same guarantee holds for bfqq too.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tif (idle_timer_disabled)\n\t\t/*\n\t\t * Since the idle timer has been disabled,\n\t\t * in_serv_queue contained some request when\n\t\t * __bfq_dispatch_request was invoked above, which\n\t\t * implies that rq was picked exactly from\n\t\t * in_serv_queue. Thus in_serv_queue == bfqq, and is\n\t\t * therefore guaranteed to exist because of the above\n\t\t * arguments.\n\t\t */\n\t\tbfqg_stats_update_idle_time(bfqq_group(in_serv_queue));\n\tif (bfqq) {\n\t\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\t\tbfqg_stats_update_avg_queue_size(bfqg);\n\t\tbfqg_stats_set_start_empty_time(bfqg);\n\t\tbfqg_stats_update_io_remove(bfqg, rq->cmd_flags);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     struct bfq_queue *in_serv_queue,\n\t\t\t\t\t     bool idle_timer_disabled) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq;\n\tstruct bfq_queue *in_serv_queue;\n\tbool waiting_rq, idle_timer_disabled = false;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tin_serv_queue = bfqd->in_service_queue;\n\twaiting_rq = in_serv_queue && bfq_bfqq_wait_request(in_serv_queue);\n\n\trq = __bfq_dispatch_request(hctx);\n\tif (in_serv_queue == bfqd->in_service_queue) {\n\t\tidle_timer_disabled =\n\t\t\twaiting_rq && !bfq_bfqq_wait_request(in_serv_queue);\n\t}\n\n\tspin_unlock_irq(&bfqd->lock);\n\tbfq_update_dispatch_stats(hctx->queue, rq,\n\t\t\tidle_timer_disabled ? in_serv_queue : NULL,\n\t\t\t\tidle_timer_disabled);\n\n\treturn rq;\n}\n\n/*\n * Task holds one reference to the queue, dropped when task exits.  Each rq\n * in-flight on this queue also holds a reference, dropped when rq is freed.\n *\n * Scheduler lock must be held here. Recall not to use bfqq after calling\n * this function on it.\n */\nvoid bfq_put_queue(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"put_queue: %p %d\", bfqq, bfqq->ref);\n\n\tbfqq->ref--;\n\tif (bfqq->ref)\n\t\treturn;\n\n\tif (!hlist_unhashed(&bfqq->burst_list_node)) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\t/*\n\t\t * Decrement also burst size after the removal, if the\n\t\t * process associated with bfqq is exiting, and thus\n\t\t * does not contribute to the burst any longer. This\n\t\t * decrement helps filter out false positives of large\n\t\t * bursts, when some short-lived process (often due to\n\t\t * the execution of commands by some service) happens\n\t\t * to start and exit while a complex application is\n\t\t * starting, and thus spawning several processes that\n\t\t * do I/O (and that *must not* be treated as a large\n\t\t * burst, see comments on bfq_handle_burst).\n\t\t *\n\t\t * In particular, the decrement is performed only if:\n\t\t * 1) bfqq is not a merged queue, because, if it is,\n\t\t * then this free of bfqq is not triggered by the exit\n\t\t * of the process bfqq is associated with, but exactly\n\t\t * by the fact that bfqq has just been merged.\n\t\t * 2) burst_size is greater than 0, to handle\n\t\t * unbalanced decrements. Unbalanced decrements may\n\t\t * happen in te following case: bfqq is inserted into\n\t\t * the current burst list--without incrementing\n\t\t * bust_size--because of a split, but the current\n\t\t * burst list is not the burst list bfqq belonged to\n\t\t * (see comments on the case of a split in\n\t\t * bfq_set_request).\n\t\t */\n\t\tif (bfqq->bic && bfqq->bfqd->burst_size > 0)\n\t\t\tbfqq->bfqd->burst_size--;\n\t}\n\n\t/*\n\t * bfqq does not exist any longer, so it cannot be woken by\n\t * any other queue, and cannot wake any other queue. Then bfqq\n\t * must be removed from the woken list of its possible waker\n\t * queue, and all queues in the woken list of bfqq must stop\n\t * having a waker queue. Strictly speaking, these updates\n\t * should be performed when bfqq remains with no I/O source\n\t * attached to it, which happens before bfqq gets freed. In\n\t * particular, this happens when the last process associated\n\t * with bfqq exits or gets associated with a different\n\t * queue. However, both events lead to bfqq being freed soon,\n\t * and dangling references would come out only after bfqq gets\n\t * freed. So these updates are done here, as a simple and safe\n\t * way to handle all cases.\n\t */\n\t/* remove bfqq from woken list */\n\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\thlist_del_init(&bfqq->woken_list_node);\n\n\t/* reset waker for all queues in woken list */\n\thlist_for_each_entry_safe(item, n, &bfqq->woken_list,\n\t\t\t\t  woken_list_node) {\n\t\titem->waker_bfqq = NULL;\n\t\thlist_del_init(&item->woken_list_node);\n\t}\n\n\tif (bfqq->bfqd->last_completed_rq_bfqq == bfqq)\n\t\tbfqq->bfqd->last_completed_rq_bfqq = NULL;\n\n\tkmem_cache_free(bfq_pool, bfqq);\n\tbfqg_and_blkg_put(bfqg);\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq)\n{\n\tbfqq->stable_ref--;\n\tbfq_put_queue(bfqq);\n}\n\nvoid bfq_put_cooperator(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *__bfqq, *next;\n\n\t/*\n\t * If this queue was scheduled to merge with another queue, be\n\t * sure to drop the reference taken on that queue (and others in\n\t * the merge chain). See bfq_setup_merge and bfq_merge_bfqqs.\n\t */\n\t__bfqq = bfqq->new_bfqq;\n\twhile (__bfqq) {\n\t\tif (__bfqq == bfqq)\n\t\t\tbreak;\n\t\tnext = __bfqq->new_bfqq;\n\t\tbfq_put_queue(__bfqq);\n\t\t__bfqq = next;\n\t}\n}\n\nstatic void bfq_exit_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tif (bfqq == bfqd->in_service_queue) {\n\t\t__bfq_bfqq_expire(bfqd, bfqq, BFQQE_BUDGET_TIMEOUT);\n\t\tbfq_schedule_dispatch(bfqd);\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"exit_bfqq: %p, %d\", bfqq, bfqq->ref);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n}\n\nstatic void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync);\n\tstruct bfq_data *bfqd;\n\n\tif (bfqq)\n\t\tbfqd = bfqq->bfqd; /* NULL if scheduler already exited */\n\n\tif (bfqq && bfqd) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\tbic_set_bfqq(bic, NULL, is_sync);\n\t\tbfq_exit_bfqq(bfqd, bfqq);\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t}\n}\n\nstatic void bfq_exit_icq(struct io_cq *icq)\n{\n\tstruct bfq_io_cq *bic = icq_to_bic(icq);\n\n\tif (bic->stable_merge_bfqq) {\n\t\tstruct bfq_data *bfqd = bic->stable_merge_bfqq->bfqd;\n\n\t\t/*\n\t\t * bfqd is NULL if scheduler already exited, and in\n\t\t * that case this is the last time bfqq is accessed.\n\t\t */\n\t\tif (bfqd) {\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\t\tbfq_put_stable_ref(bic->stable_merge_bfqq);\n\t\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\t} else {\n\t\t\tbfq_put_stable_ref(bic->stable_merge_bfqq);\n\t\t}\n\t}\n\n\tbfq_exit_icq_bfqq(bic, true);\n\tbfq_exit_icq_bfqq(bic, false);\n}\n\n/*\n * Update the entity prio values; note that the new values will not\n * be used until the next (re)activation.\n */\nstatic void\nbfq_set_next_ioprio_data(struct bfq_queue *bfqq, struct bfq_io_cq *bic)\n{\n\tstruct task_struct *tsk = current;\n\tint ioprio_class;\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\n\tif (!bfqd)\n\t\treturn;\n\n\tioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tswitch (ioprio_class) {\n\tdefault:\n\t\tpr_err(\"bdi %s: bfq: bad prio class %d\\n\",\n\t\t\tbdi_dev_name(bfqq->bfqd->queue->disk->bdi),\n\t\t\tioprio_class);\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_NONE:\n\t\t/*\n\t\t * No prio set, inherit CPU scheduling settings.\n\t\t */\n\t\tbfqq->new_ioprio = task_nice_ioprio(tsk);\n\t\tbfqq->new_ioprio_class = task_nice_ioclass(tsk);\n\t\tbreak;\n\tcase IOPRIO_CLASS_RT:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_DATA(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_RT;\n\t\tbreak;\n\tcase IOPRIO_CLASS_BE:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_DATA(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_BE;\n\t\tbreak;\n\tcase IOPRIO_CLASS_IDLE:\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_IDLE;\n\t\tbfqq->new_ioprio = 7;\n\t\tbreak;\n\t}\n\n\tif (bfqq->new_ioprio >= IOPRIO_NR_LEVELS) {\n\t\tpr_crit(\"bfq_set_next_ioprio_data: new_ioprio %d\\n\",\n\t\t\tbfqq->new_ioprio);\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t}\n\n\tbfqq->entity.new_weight = bfq_ioprio_to_weight(bfqq->new_ioprio);\n\tbfq_log_bfqq(bfqd, bfqq, \"new_ioprio %d new_weight %d\",\n\t\t     bfqq->new_ioprio, bfqq->entity.new_weight);\n\tbfqq->entity.prio_changed = 1;\n}\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn);\n\nstatic void bfq_check_ioprio_change(struct bfq_io_cq *bic, struct bio *bio)\n{\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tstruct bfq_queue *bfqq;\n\tint ioprio = bic->icq.ioc->ioprio;\n\n\t/*\n\t * This condition may trigger on a newly created bic, be sure to\n\t * drop the lock before returning.\n\t */\n\tif (unlikely(!bfqd) || likely(bic->ioprio == ioprio))\n\t\treturn;\n\n\tbic->ioprio = ioprio;\n\n\tbfqq = bic_to_bfqq(bic, false);\n\tif (bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\n\t\tbfqq = bfq_get_queue(bfqd, bio, false, bic, true);\n\t\tbic_set_bfqq(bic, bfqq, false);\n\t\tbfq_release_process_ref(bfqd, old_bfqq);\n\t}\n\n\tbfqq = bic_to_bfqq(bic, true);\n\tif (bfqq)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n}\n\nstatic void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic, pid_t pid, int is_sync)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tRB_CLEAR_NODE(&bfqq->entity.rb_node);\n\tINIT_LIST_HEAD(&bfqq->fifo);\n\tINIT_HLIST_NODE(&bfqq->burst_list_node);\n\tINIT_HLIST_NODE(&bfqq->woken_list_node);\n\tINIT_HLIST_HEAD(&bfqq->woken_list);\n\n\tbfqq->ref = 0;\n\tbfqq->bfqd = bfqd;\n\n\tif (bic)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n\n\tif (is_sync) {\n\t\t/*\n\t\t * No need to mark as has_short_ttime if in\n\t\t * idle_class, because no device idling is performed\n\t\t * for queues in idle class\n\t\t */\n\t\tif (!bfq_class_idle(bfqq))\n\t\t\t/* tentatively mark as has_short_ttime */\n\t\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\t\tbfq_mark_bfqq_sync(bfqq);\n\t\tbfq_mark_bfqq_just_created(bfqq);\n\t} else\n\t\tbfq_clear_bfqq_sync(bfqq);\n\n\t/* set end request to minus infinity from now */\n\tbfqq->ttime.last_end_request = now_ns + 1;\n\n\tbfqq->creation_time = jiffies;\n\n\tbfqq->io_start_time = now_ns;\n\n\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\tbfqq->pid = pid;\n\n\t/* Tentative initial value to trade off between thr and lat */\n\tbfqq->max_budget = (2 * bfq_max_budget(bfqd)) / 3;\n\tbfqq->budget_timeout = bfq_smallest_from_now();\n\n\tbfqq->wr_coeff = 1;\n\tbfqq->last_wr_start_finish = jiffies;\n\tbfqq->wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\tbfqq->split_time = bfq_smallest_from_now();\n\n\t/*\n\t * To not forget the possibly high bandwidth consumed by a\n\t * process/queue in the recent past,\n\t * bfq_bfqq_softrt_next_start() returns a value at least equal\n\t * to the current value of bfqq->soft_rt_next_start (see\n\t * comments on bfq_bfqq_softrt_next_start).  Set\n\t * soft_rt_next_start to now, to mean that bfqq has consumed\n\t * no bandwidth so far.\n\t */\n\tbfqq->soft_rt_next_start = jiffies;\n\n\t/* first request is almost certainly seeky */\n\tbfqq->seek_history = 1;\n}\n\nstatic struct bfq_queue **bfq_async_queue_prio(struct bfq_data *bfqd,\n\t\t\t\t\t       struct bfq_group *bfqg,\n\t\t\t\t\t       int ioprio_class, int ioprio)\n{\n\tswitch (ioprio_class) {\n\tcase IOPRIO_CLASS_RT:\n\t\treturn &bfqg->async_bfqq[0][ioprio];\n\tcase IOPRIO_CLASS_NONE:\n\t\tioprio = IOPRIO_BE_NORM;\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_BE:\n\t\treturn &bfqg->async_bfqq[1][ioprio];\n\tcase IOPRIO_CLASS_IDLE:\n\t\treturn &bfqg->async_idle_bfqq;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bfq_queue *\nbfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic,\n\t\t\t  struct bfq_queue *last_bfqq_created)\n{\n\tstruct bfq_queue *new_bfqq =\n\t\tbfq_setup_merge(bfqq, last_bfqq_created);\n\n\tif (!new_bfqq)\n\t\treturn bfqq;\n\n\tif (new_bfqq->bic)\n\t\tnew_bfqq->bic->stably_merged = true;\n\tbic->stably_merged = true;\n\n\t/*\n\t * Reusing merge functions. This implies that\n\t * bfqq->bic must be set too, for\n\t * bfq_merge_bfqqs to correctly save bfqq's\n\t * state before killing it.\n\t */\n\tbfqq->bic = bic;\n\treturn bfq_merge_bfqqs(bfqd, bic, bfqq);\n}\n\n/*\n * Many throughput-sensitive workloads are made of several parallel\n * I/O flows, with all flows generated by the same application, or\n * more generically by the same task (e.g., system boot). The most\n * counterproductive action with these workloads is plugging I/O\n * dispatch when one of the bfq_queues associated with these flows\n * remains temporarily empty.\n *\n * To avoid this plugging, BFQ has been using a burst-handling\n * mechanism for years now. This mechanism has proven effective for\n * throughput, and not detrimental for service guarantees. The\n * following function pushes this mechanism a little bit further,\n * basing on the following two facts.\n *\n * First, all the I/O flows of a the same application or task\n * contribute to the execution/completion of that common application\n * or task. So the performance figures that matter are total\n * throughput of the flows and task-wide I/O latency.  In particular,\n * these flows do not need to be protected from each other, in terms\n * of individual bandwidth or latency.\n *\n * Second, the above fact holds regardless of the number of flows.\n *\n * Putting these two facts together, this commits merges stably the\n * bfq_queues associated with these I/O flows, i.e., with the\n * processes that generate these IO/ flows, regardless of how many the\n * involved processes are.\n *\n * To decide whether a set of bfq_queues is actually associated with\n * the I/O flows of a common application or task, and to merge these\n * queues stably, this function operates as follows: given a bfq_queue,\n * say Q2, currently being created, and the last bfq_queue, say Q1,\n * created before Q2, Q2 is merged stably with Q1 if\n * - very little time has elapsed since when Q1 was created\n * - Q2 has the same ioprio as Q1\n * - Q2 belongs to the same group as Q1\n *\n * Merging bfq_queues also reduces scheduling overhead. A fio test\n * with ten random readers on /dev/nullb shows a throughput boost of\n * 40%, with a quadcore. Since BFQ's execution time amounts to ~50% of\n * the total per-request processing time, the above throughput boost\n * implies that BFQ's overhead is reduced by more than 50%.\n *\n * This new mechanism most certainly obsoletes the current\n * burst-handling heuristics. We keep those heuristics for the moment.\n */\nstatic struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,\n\t\t\t\t\t\t      struct bfq_queue *bfqq,\n\t\t\t\t\t\t      struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue **source_bfqq = bfqq->entity.parent ?\n\t\t&bfqq->entity.parent->last_bfqq_created :\n\t\t&bfqd->last_bfqq_created;\n\n\tstruct bfq_queue *last_bfqq_created = *source_bfqq;\n\n\t/*\n\t * If last_bfqq_created has not been set yet, then init it. If\n\t * it has been set already, but too long ago, then move it\n\t * forward to bfqq. Finally, move also if bfqq belongs to a\n\t * different group than last_bfqq_created, or if bfqq has a\n\t * different ioprio or ioprio_class. If none of these\n\t * conditions holds true, then try an early stable merge or\n\t * schedule a delayed stable merge.\n\t *\n\t * A delayed merge is scheduled (instead of performing an\n\t * early merge), in case bfqq might soon prove to be more\n\t * throughput-beneficial if not merged. Currently this is\n\t * possible only if bfqd is rotational with no queueing. For\n\t * such a drive, not merging bfqq is better for throughput if\n\t * bfqq happens to contain sequential I/O. So, we wait a\n\t * little bit for enough I/O to flow through bfqq. After that,\n\t * if such an I/O is sequential, then the merge is\n\t * canceled. Otherwise the merge is finally performed.\n\t */\n\tif (!last_bfqq_created ||\n\t    time_before(last_bfqq_created->creation_time +\n\t\t\tmsecs_to_jiffies(bfq_activation_stable_merging),\n\t\t\tbfqq->creation_time) ||\n\t\tbfqq->entity.parent != last_bfqq_created->entity.parent ||\n\t\tbfqq->ioprio != last_bfqq_created->ioprio ||\n\t\tbfqq->ioprio_class != last_bfqq_created->ioprio_class)\n\t\t*source_bfqq = bfqq;\n\telse if (time_after_eq(last_bfqq_created->creation_time +\n\t\t\t\t bfqd->bfq_burst_interval,\n\t\t\t\t bfqq->creation_time)) {\n\t\tif (likely(bfqd->nonrot_with_queueing))\n\t\t\t/*\n\t\t\t * With this type of drive, leaving\n\t\t\t * bfqq alone may provide no\n\t\t\t * throughput benefits compared with\n\t\t\t * merging bfqq. So merge bfqq now.\n\t\t\t */\n\t\t\tbfqq = bfq_do_early_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t\t bic,\n\t\t\t\t\t\t\t last_bfqq_created);\n\t\telse { /* schedule tentative stable merge */\n\t\t\t/*\n\t\t\t * get reference on last_bfqq_created,\n\t\t\t * to prevent it from being freed,\n\t\t\t * until we decide whether to merge\n\t\t\t */\n\t\t\tlast_bfqq_created->ref++;\n\t\t\t/*\n\t\t\t * need to keep track of stable refs, to\n\t\t\t * compute process refs correctly\n\t\t\t */\n\t\t\tlast_bfqq_created->stable_ref++;\n\t\t\t/*\n\t\t\t * Record the bfqq to merge to.\n\t\t\t */\n\t\t\tbic->stable_merge_bfqq = last_bfqq_created;\n\t\t}\n\t}\n\n\treturn bfqq;\n}\n\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn)\n{\n\tconst int ioprio = IOPRIO_PRIO_DATA(bic->ioprio);\n\tconst int ioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tstruct bfq_queue **async_bfqq = NULL;\n\tstruct bfq_queue *bfqq;\n\tstruct bfq_group *bfqg;\n\n\tbfqg = bfq_bio_bfqg(bfqd, bio);\n\tif (!is_sync) {\n\t\tasync_bfqq = bfq_async_queue_prio(bfqd, bfqg, ioprio_class,\n\t\t\t\t\t\t  ioprio);\n\t\tbfqq = *async_bfqq;\n\t\tif (bfqq)\n\t\t\tgoto out;\n\t}\n\n\tbfqq = kmem_cache_alloc_node(bfq_pool,\n\t\t\t\t     GFP_NOWAIT | __GFP_ZERO | __GFP_NOWARN,\n\t\t\t\t     bfqd->queue->node);\n\n\tif (bfqq) {\n\t\tbfq_init_bfqq(bfqd, bfqq, bic, current->pid,\n\t\t\t      is_sync);\n\t\tbfq_init_entity(&bfqq->entity, bfqg);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"allocated\");\n\t} else {\n\t\tbfqq = &bfqd->oom_bfqq;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"using oom bfqq\");\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Pin the queue now that it's allocated, scheduler exit will\n\t * prune it.\n\t */\n\tif (async_bfqq) {\n\t\tbfqq->ref++; /*\n\t\t\t      * Extra group reference, w.r.t. sync\n\t\t\t      * queue. This extra reference is removed\n\t\t\t      * only if bfqq->bfqg disappears, to\n\t\t\t      * guarantee that this queue is not freed\n\t\t\t      * until its group goes away.\n\t\t\t      */\n\t\tbfq_log_bfqq(bfqd, bfqq, \"get_queue, bfqq not in async: %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\t*async_bfqq = bfqq;\n\t}\n\nout:\n\tbfqq->ref++; /* get a process reference to this queue */\n\n\tif (bfqq != &bfqd->oom_bfqq && is_sync && !respawn)\n\t\tbfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);\n\treturn bfqq;\n}\n\nstatic void bfq_update_io_thinktime(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tstruct bfq_ttime *ttime = &bfqq->ttime;\n\tu64 elapsed;\n\n\t/*\n\t * We are really interested in how long it takes for the queue to\n\t * become busy when there is no outstanding IO for this queue. So\n\t * ignore cases when the bfq queue has already IO queued.\n\t */\n\tif (bfqq->dispatched || bfq_bfqq_busy(bfqq))\n\t\treturn;\n\telapsed = ktime_get_ns() - bfqq->ttime.last_end_request;\n\telapsed = min_t(u64, elapsed, 2ULL * bfqd->bfq_slice_idle);\n\n\tttime->ttime_samples = (7*ttime->ttime_samples + 256) / 8;\n\tttime->ttime_total = div_u64(7*ttime->ttime_total + 256*elapsed,  8);\n\tttime->ttime_mean = div64_ul(ttime->ttime_total + 128,\n\t\t\t\t     ttime->ttime_samples);\n}\n\nstatic void\nbfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct request *rq)\n{\n\tbfqq->seek_history <<= 1;\n\tbfqq->seek_history |= BFQ_RQ_SEEKY(bfqd, bfqq->last_request_pos, rq);\n\n\tif (bfqq->wr_coeff > 1 &&\n\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t    BFQQ_TOTALLY_SEEKY(bfqq)) {\n\t\tif (time_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t   bfq_wr_duration(bfqd))) {\n\t\t\t/*\n\t\t\t * In soft_rt weight raising with the\n\t\t\t * interactive-weight-raising period\n\t\t\t * elapsed (so no switch back to\n\t\t\t * interactive weight raising).\n\t\t\t */\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t} else { /*\n\t\t\t  * stopping soft_rt weight raising\n\t\t\t  * while still in interactive period,\n\t\t\t  * switch back to interactive weight\n\t\t\t  * raising\n\t\t\t  */\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n}\n\nstatic void bfq_update_has_short_ttime(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq,\n\t\t\t\t       struct bfq_io_cq *bic)\n{\n\tbool has_short_ttime = true, state_changed;\n\n\t/*\n\t * No need to update has_short_ttime if bfqq is async or in\n\t * idle io prio class, or if bfq_slice_idle is zero, because\n\t * no device idling is performed for bfqq in this case.\n\t */\n\tif (!bfq_bfqq_sync(bfqq) || bfq_class_idle(bfqq) ||\n\t    bfqd->bfq_slice_idle == 0)\n\t\treturn;\n\n\t/* Idle window just restored, statistics are meaningless. */\n\tif (time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     bfqd->bfq_wr_min_idle_time))\n\t\treturn;\n\n\t/* Think time is infinite if no process is linked to\n\t * bfqq. Otherwise check average think time to decide whether\n\t * to mark as has_short_ttime. To this goal, compare average\n\t * think time with half the I/O-plugging timeout.\n\t */\n\tif (atomic_read(&bic->icq.ioc->active_ref) == 0 ||\n\t    (bfq_sample_valid(bfqq->ttime.ttime_samples) &&\n\t     bfqq->ttime.ttime_mean > bfqd->bfq_slice_idle>>1))\n\t\thas_short_ttime = false;\n\n\tstate_changed = has_short_ttime != bfq_bfqq_has_short_ttime(bfqq);\n\n\tif (has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\t/*\n\t * Until the base value for the total service time gets\n\t * finally computed for bfqq, the inject limit does depend on\n\t * the think-time state (short|long). In particular, the limit\n\t * is 0 or 1 if the think time is deemed, respectively, as\n\t * short or long (details in the comments in\n\t * bfq_update_inject_limit()). Accordingly, the next\n\t * instructions reset the inject limit if the think-time state\n\t * has changed and the above base value is still to be\n\t * computed.\n\t *\n\t * However, the reset is performed only if more than 100 ms\n\t * have elapsed since the last update of the inject limit, or\n\t * (inclusive) if the change is from short to long think\n\t * time. The reason for this waiting is as follows.\n\t *\n\t * bfqq may have a long think time because of a\n\t * synchronization with some other queue, i.e., because the\n\t * I/O of some other queue may need to be completed for bfqq\n\t * to receive new I/O. Details in the comments on the choice\n\t * of the queue for injection in bfq_select_queue().\n\t *\n\t * As stressed in those comments, if such a synchronization is\n\t * actually in place, then, without injection on bfqq, the\n\t * blocking I/O cannot happen to served while bfqq is in\n\t * service. As a consequence, if bfqq is granted\n\t * I/O-dispatch-plugging, then bfqq remains empty, and no I/O\n\t * is dispatched, until the idle timeout fires. This is likely\n\t * to result in lower bandwidth and higher latencies for bfqq,\n\t * and in a severe loss of total throughput.\n\t *\n\t * On the opposite end, a non-zero inject limit may allow the\n\t * I/O that blocks bfqq to be executed soon, and therefore\n\t * bfqq to receive new I/O soon.\n\t *\n\t * But, if the blocking gets actually eliminated, then the\n\t * next think-time sample for bfqq may be very low. This in\n\t * turn may cause bfqq's think time to be deemed\n\t * short. Without the 100 ms barrier, this new state change\n\t * would cause the body of the next if to be executed\n\t * immediately. But this would set to 0 the inject\n\t * limit. Without injection, the blocking I/O would cause the\n\t * think time of bfqq to become long again, and therefore the\n\t * inject limit to be raised again, and so on. The only effect\n\t * of such a steady oscillation between the two think-time\n\t * states would be to prevent effective injection on bfqq.\n\t *\n\t * In contrast, if the inject limit is not reset during such a\n\t * long time interval as 100 ms, then the number of short\n\t * think time samples can grow significantly before the reset\n\t * is performed. As a consequence, the think time state can\n\t * become stable before the reset. Therefore there will be no\n\t * state change when the 100 ms elapse, and no reset of the\n\t * inject limit. The inject limit remains steadily equal to 1\n\t * both during and after the 100 ms. So injection can be\n\t * performed at all times, and throughput gets boosted.\n\t *\n\t * An inject limit equal to 1 is however in conflict, in\n\t * general, with the fact that the think time of bfqq is\n\t * short, because injection may be likely to delay bfqq's I/O\n\t * (as explained in the comments in\n\t * bfq_update_inject_limit()). But this does not happen in\n\t * this special case, because bfqq's low think time is due to\n\t * an effective handling of a synchronization, through\n\t * injection. In this special case, bfqq's I/O does not get\n\t * delayed by injection; on the contrary, bfqq's I/O is\n\t * brought forward, because it is not blocked for\n\t * milliseconds.\n\t *\n\t * In addition, serving the blocking I/O much sooner, and much\n\t * more frequently than once per I/O-plugging timeout, makes\n\t * it much quicker to detect a waker queue (the concept of\n\t * waker queue is defined in the comments in\n\t * bfq_add_request()). This makes it possible to start sooner\n\t * to boost throughput more effectively, by injecting the I/O\n\t * of the waker queue unconditionally on every\n\t * bfq_dispatch_request().\n\t *\n\t * One last, important benefit of not resetting the inject\n\t * limit before 100 ms is that, during this time interval, the\n\t * base value for the total service time is likely to get\n\t * finally computed for bfqq, freeing the inject limit from\n\t * its relation with the think time.\n\t */\n\tif (state_changed && bfqq->last_serv_time_ns == 0 &&\n\t    (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t      msecs_to_jiffies(100)) ||\n\t     !has_short_ttime))\n\t\tbfq_reset_inject_limit(bfqd, bfqq);\n}\n\n/*\n * Called when a new fs request (rq) is added to bfqq.  Check if there's\n * something we should do about it.\n */\nstatic void bfq_rq_enqueued(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending++;\n\n\tbfqq->last_request_pos = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\n\tif (bfqq == bfqd->in_service_queue && bfq_bfqq_wait_request(bfqq)) {\n\t\tbool small_req = bfqq->queued[rq_is_sync(rq)] == 1 &&\n\t\t\t\t blk_rq_sectors(rq) < 32;\n\t\tbool budget_timeout = bfq_bfqq_budget_timeout(bfqq);\n\n\t\t/*\n\t\t * There is just this request queued: if\n\t\t * - the request is small, and\n\t\t * - we are idling to boost throughput, and\n\t\t * - the queue is not to be expired,\n\t\t * then just exit.\n\t\t *\n\t\t * In this way, if the device is being idled to wait\n\t\t * for a new request from the in-service queue, we\n\t\t * avoid unplugging the device and committing the\n\t\t * device to serve just a small request. In contrast\n\t\t * we wait for the block layer to decide when to\n\t\t * unplug the device: hopefully, new requests will be\n\t\t * merged to this one quickly, then the device will be\n\t\t * unplugged and larger requests will be dispatched.\n\t\t */\n\t\tif (small_req && idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t    !budget_timeout)\n\t\t\treturn;\n\n\t\t/*\n\t\t * A large enough request arrived, or idling is being\n\t\t * performed to preserve service guarantees, or\n\t\t * finally the queue is to be expired: in all these\n\t\t * cases disk idling is to be stopped, so clear\n\t\t * wait_request flag and reset timer.\n\t\t */\n\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\n\t\t/*\n\t\t * The queue is not empty, because a new request just\n\t\t * arrived. Hence we can safely expire the queue, in\n\t\t * case of budget timeout, without risking that the\n\t\t * timestamps of the queue are not updated correctly.\n\t\t * See [1] for more details.\n\t\t */\n\t\tif (budget_timeout)\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t}\n}\n\nstatic void bfqq_request_allocated(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated++;\n}\n\nstatic void bfqq_request_freed(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated--;\n}\n\n/* returns true if it causes the idle timer to be disabled */\nstatic bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,\n\t\t\t\t\t\t RQ_BIC(rq));\n\tbool waiting, idle_timer_disabled = false;\n\n\tif (new_bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\t\t/*\n\t\t * Release the request's reference to the old bfqq\n\t\t * and make sure one is taken to the shared queue.\n\t\t */\n\t\tbfqq_request_allocated(new_bfqq);\n\t\tbfqq_request_freed(bfqq);\n\t\tnew_bfqq->ref++;\n\t\t/*\n\t\t * If the bic associated with the process\n\t\t * issuing this request still points to bfqq\n\t\t * (and thus has not been already redirected\n\t\t * to new_bfqq or even some other bfq_queue),\n\t\t * then complete the merge and redirect it to\n\t\t * new_bfqq.\n\t\t */\n\t\tif (bic_to_bfqq(RQ_BIC(rq), 1) == bfqq) {\n\t\t\twhile (bfqq != new_bfqq)\n\t\t\t\tbfqq = bfq_merge_bfqqs(bfqd, RQ_BIC(rq), bfqq);\n\t\t}\n\n\t\tbfq_clear_bfqq_just_created(old_bfqq);\n\t\t/*\n\t\t * rq is about to be enqueued into new_bfqq,\n\t\t * release rq reference on bfqq\n\t\t */\n\t\tbfq_put_queue(old_bfqq);\n\t\trq->elv.priv[1] = new_bfqq;\n\t}\n\n\tbfq_update_io_thinktime(bfqd, bfqq);\n\tbfq_update_has_short_ttime(bfqd, bfqq, RQ_BIC(rq));\n\tbfq_update_io_seektime(bfqd, bfqq, rq);\n\n\twaiting = bfqq && bfq_bfqq_wait_request(bfqq);\n\tbfq_add_request(rq);\n\tidle_timer_disabled = waiting && !bfq_bfqq_wait_request(bfqq);\n\n\trq->fifo_time = ktime_get_ns() + bfqd->bfq_fifo_expire[rq_is_sync(rq)];\n\tlist_add_tail(&rq->queuelist, &bfqq->fifo);\n\n\tbfq_rq_enqueued(bfqd, bfqq, rq);\n\n\treturn idle_timer_disabled;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t    struct bfq_queue *bfqq,\n\t\t\t\t    bool idle_timer_disabled,\n\t\t\t\t    blk_opf_t cmd_flags)\n{\n\tif (!bfqq)\n\t\treturn;\n\n\t/*\n\t * bfqq still exists, because it can disappear only after\n\t * either it is merged with another queue, or the process it\n\t * is associated with exits. But both actions must be taken by\n\t * the same process currently executing this flow of\n\t * instructions.\n\t *\n\t * In addition, the following queue lock guarantees that\n\t * bfqq_group(bfqq) exists as well.\n\t */\n\tspin_lock_irq(&q->queue_lock);\n\tbfqg_stats_update_io_add(bfqq_group(bfqq), bfqq, cmd_flags);\n\tif (idle_timer_disabled)\n\t\tbfqg_stats_update_idle_time(bfqq_group(bfqq));\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t\t   struct bfq_queue *bfqq,\n\t\t\t\t\t   bool idle_timer_disabled,\n\t\t\t\t\t   blk_opf_t cmd_flags) {}\n#endif /* CONFIG_BFQ_CGROUP_DEBUG */\n\nstatic struct bfq_queue *bfq_init_rq(struct request *rq);\n\nstatic void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t       bool at_head)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_queue *bfqq;\n\tbool idle_timer_disabled = false;\n\tblk_opf_t cmd_flags;\n\tLIST_HEAD(free);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)\n\t\tbfqg_stats_update_legacy_io(q, rq);\n#endif\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bfq_init_rq(rq);\n\tif (blk_mq_sched_try_insert_merge(q, rq, &free)) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tblk_mq_free_requests(&free);\n\t\treturn;\n\t}\n\n\ttrace_block_rq_insert(rq);\n\n\tif (!bfqq || at_head) {\n\t\tif (at_head)\n\t\t\tlist_add(&rq->queuelist, &bfqd->dispatch);\n\t\telse\n\t\t\tlist_add_tail(&rq->queuelist, &bfqd->dispatch);\n\t} else {\n\t\tidle_timer_disabled = __bfq_insert_request(bfqd, rq);\n\t\t/*\n\t\t * Update bfqq, because, if a queue merge has occurred\n\t\t * in __bfq_insert_request, then rq has been\n\t\t * redirected into a new queue.\n\t\t */\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (rq_mergeable(rq)) {\n\t\t\telv_rqhash_add(q, rq);\n\t\t\tif (!q->last_merge)\n\t\t\t\tq->last_merge = rq;\n\t\t}\n\t}\n\n\t/*\n\t * Cache cmd_flags before releasing scheduler lock, because rq\n\t * may disappear afterwards (for example, because of a request\n\t * merge).\n\t */\n\tcmd_flags = rq->cmd_flags;\n\tspin_unlock_irq(&bfqd->lock);\n\n\tbfq_update_insert_stats(q, bfqq, idle_timer_disabled,\n\t\t\t\tcmd_flags);\n}\n\nstatic void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t\tstruct list_head *list, bool at_head)\n{\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tbfq_insert_request(hctx, rq, at_head);\n\t}\n}\n\nstatic void bfq_update_hw_tag(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\tbfqd->max_rq_in_driver = max_t(int, bfqd->max_rq_in_driver,\n\t\t\t\t       bfqd->rq_in_driver);\n\n\tif (bfqd->hw_tag == 1)\n\t\treturn;\n\n\t/*\n\t * This sample is valid if the number of outstanding requests\n\t * is large enough to allow a queueing behavior.  Note that the\n\t * sum is not exact, as it's not taking into account deactivated\n\t * requests.\n\t */\n\tif (bfqd->rq_in_driver + bfqd->queued <= BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\t/*\n\t * If active queue hasn't enough requests and can idle, bfq might not\n\t * dispatch sufficient requests to hardware. Don't zero hw_tag in this\n\t * case\n\t */\n\tif (bfqq && bfq_bfqq_has_short_ttime(bfqq) &&\n\t    bfqq->dispatched + bfqq->queued[0] + bfqq->queued[1] <\n\t    BFQ_HW_QUEUE_THRESHOLD &&\n\t    bfqd->rq_in_driver < BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\tif (bfqd->hw_tag_samples++ < BFQ_HW_QUEUE_SAMPLES)\n\t\treturn;\n\n\tbfqd->hw_tag = bfqd->max_rq_in_driver > BFQ_HW_QUEUE_THRESHOLD;\n\tbfqd->max_rq_in_driver = 0;\n\tbfqd->hw_tag_samples = 0;\n\n\tbfqd->nonrot_with_queueing =\n\t\tblk_queue_nonrot(bfqd->queue) && bfqd->hw_tag;\n}\n\nstatic void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)\n{\n\tu64 now_ns;\n\tu32 delta_us;\n\n\tbfq_update_hw_tag(bfqd);\n\n\tbfqd->rq_in_driver--;\n\tbfqq->dispatched--;\n\n\tif (!bfqq->dispatched && !bfq_bfqq_busy(bfqq)) {\n\t\t/*\n\t\t * Set budget_timeout (which we overload to store the\n\t\t * time at which the queue remains with no backlog and\n\t\t * no outstanding request; used by the weight-raising\n\t\t * mechanism).\n\t\t */\n\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_weights_tree_remove(bfqd, bfqq);\n\t}\n\n\tnow_ns = ktime_get_ns();\n\n\tbfqq->ttime.last_end_request = now_ns;\n\n\t/*\n\t * Using us instead of ns, to get a reasonable precision in\n\t * computing rate in next check.\n\t */\n\tdelta_us = div_u64(now_ns - bfqd->last_completion, NSEC_PER_USEC);\n\n\t/*\n\t * If the request took rather long to complete, and, according\n\t * to the maximum request size recorded, this completion latency\n\t * implies that the request was certainly served at a very low\n\t * rate (less than 1M sectors/sec), then the whole observation\n\t * interval that lasts up to this time instant cannot be a\n\t * valid time interval for computing a new peak rate.  Invoke\n\t * bfq_update_rate_reset to have the following three steps\n\t * taken:\n\t * - close the observation interval at the last (previous)\n\t *   request dispatch or completion\n\t * - compute rate, if possible, for that observation interval\n\t * - reset to zero samples, which will trigger a proper\n\t *   re-initialization of the observation interval on next\n\t *   dispatch\n\t */\n\tif (delta_us > BFQ_MIN_TT/NSEC_PER_USEC &&\n\t   (bfqd->last_rq_max_size<<BFQ_RATE_SHIFT)/delta_us <\n\t\t\t1UL<<(BFQ_RATE_SHIFT - 10))\n\t\tbfq_update_rate_reset(bfqd, NULL);\n\tbfqd->last_completion = now_ns;\n\t/*\n\t * Shared queues are likely to receive I/O at a high\n\t * rate. This may deceptively let them be considered as wakers\n\t * of other queues. But a false waker will unjustly steal\n\t * bandwidth to its supposedly woken queue. So considering\n\t * also shared queues in the waking mechanism may cause more\n\t * control troubles than throughput benefits. Then reset\n\t * last_completed_rq_bfqq if bfqq is a shared queue.\n\t */\n\tif (!bfq_bfqq_coop(bfqq))\n\t\tbfqd->last_completed_rq_bfqq = bfqq;\n\telse\n\t\tbfqd->last_completed_rq_bfqq = NULL;\n\n\t/*\n\t * If we are waiting to discover whether the request pattern\n\t * of the task associated with the queue is actually\n\t * isochronous, and both requisites for this condition to hold\n\t * are now satisfied, then compute soft_rt_next_start (see the\n\t * comments on the function bfq_bfqq_softrt_next_start()). We\n\t * do not compute soft_rt_next_start if bfqq is in interactive\n\t * weight raising (see the comments in bfq_bfqq_expire() for\n\t * an explanation). We schedule this delayed update when bfqq\n\t * expires, if it still has in-flight requests.\n\t */\n\tif (bfq_bfqq_softrt_update(bfqq) && bfqq->dispatched == 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq->wr_coeff != bfqd->bfq_wr_coeff)\n\t\tbfqq->soft_rt_next_start =\n\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\n\t/*\n\t * If this is the in-service queue, check if it needs to be expired,\n\t * or if we want to idle in case it has no pending requests.\n\t */\n\tif (bfqd->in_service_queue == bfqq) {\n\t\tif (bfq_bfqq_must_idle(bfqq)) {\n\t\t\tif (bfqq->dispatched == 0)\n\t\t\t\tbfq_arm_slice_timer(bfqd);\n\t\t\t/*\n\t\t\t * If we get here, we do not expire bfqq, even\n\t\t\t * if bfqq was in budget timeout or had no\n\t\t\t * more requests (as controlled in the next\n\t\t\t * conditional instructions). The reason for\n\t\t\t * not expiring bfqq is as follows.\n\t\t\t *\n\t\t\t * Here bfqq->dispatched > 0 holds, but\n\t\t\t * bfq_bfqq_must_idle() returned true. This\n\t\t\t * implies that, even if no request arrives\n\t\t\t * for bfqq before bfqq->dispatched reaches 0,\n\t\t\t * bfqq will, however, not be expired on the\n\t\t\t * completion event that causes bfqq->dispatch\n\t\t\t * to reach zero. In contrast, on this event,\n\t\t\t * bfqq will start enjoying device idling\n\t\t\t * (I/O-dispatch plugging).\n\t\t\t *\n\t\t\t * But, if we expired bfqq here, bfqq would\n\t\t\t * not have the chance to enjoy device idling\n\t\t\t * when bfqq->dispatched finally reaches\n\t\t\t * zero. This would expose bfqq to violation\n\t\t\t * of its reserved service guarantees.\n\t\t\t */\n\t\t\treturn;\n\t\t} else if (bfq_may_expire_for_budg_timeout(bfqq))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t\telse if (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t (bfqq->dispatched == 0 ||\n\t\t\t  !bfq_better_to_idle(bfqq)))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_NO_MORE_REQUESTS);\n\t}\n\n\tif (!bfqd->rq_in_driver)\n\t\tbfq_schedule_dispatch(bfqd);\n}\n\n/*\n * The processes associated with bfqq may happen to generate their\n * cumulative I/O at a lower rate than the rate at which the device\n * could serve the same I/O. This is rather probable, e.g., if only\n * one process is associated with bfqq and the device is an SSD. It\n * results in bfqq becoming often empty while in service. In this\n * respect, if BFQ is allowed to switch to another queue when bfqq\n * remains empty, then the device goes on being fed with I/O requests,\n * and the throughput is not affected. In contrast, if BFQ is not\n * allowed to switch to another queue---because bfqq is sync and\n * I/O-dispatch needs to be plugged while bfqq is temporarily\n * empty---then, during the service of bfqq, there will be frequent\n * \"service holes\", i.e., time intervals during which bfqq gets empty\n * and the device can only consume the I/O already queued in its\n * hardware queues. During service holes, the device may even get to\n * remaining idle. In the end, during the service of bfqq, the device\n * is driven at a lower speed than the one it can reach with the kind\n * of I/O flowing through bfqq.\n *\n * To counter this loss of throughput, BFQ implements a \"request\n * injection mechanism\", which tries to fill the above service holes\n * with I/O requests taken from other queues. The hard part in this\n * mechanism is finding the right amount of I/O to inject, so as to\n * both boost throughput and not break bfqq's bandwidth and latency\n * guarantees. In this respect, the mechanism maintains a per-queue\n * inject limit, computed as below. While bfqq is empty, the injection\n * mechanism dispatches extra I/O requests only until the total number\n * of I/O requests in flight---i.e., already dispatched but not yet\n * completed---remains lower than this limit.\n *\n * A first definition comes in handy to introduce the algorithm by\n * which the inject limit is computed.  We define as first request for\n * bfqq, an I/O request for bfqq that arrives while bfqq is in\n * service, and causes bfqq to switch from empty to non-empty. The\n * algorithm updates the limit as a function of the effect of\n * injection on the service times of only the first requests of\n * bfqq. The reason for this restriction is that these are the\n * requests whose service time is affected most, because they are the\n * first to arrive after injection possibly occurred.\n *\n * To evaluate the effect of injection, the algorithm measures the\n * \"total service time\" of first requests. We define as total service\n * time of an I/O request, the time that elapses since when the\n * request is enqueued into bfqq, to when it is completed. This\n * quantity allows the whole effect of injection to be measured. It is\n * easy to see why. Suppose that some requests of other queues are\n * actually injected while bfqq is empty, and that a new request R\n * then arrives for bfqq. If the device does start to serve all or\n * part of the injected requests during the service hole, then,\n * because of this extra service, it may delay the next invocation of\n * the dispatch hook of BFQ. Then, even after R gets eventually\n * dispatched, the device may delay the actual service of R if it is\n * still busy serving the extra requests, or if it decides to serve,\n * before R, some extra request still present in its queues. As a\n * conclusion, the cumulative extra delay caused by injection can be\n * easily evaluated by just comparing the total service time of first\n * requests with and without injection.\n *\n * The limit-update algorithm works as follows. On the arrival of a\n * first request of bfqq, the algorithm measures the total time of the\n * request only if one of the three cases below holds, and, for each\n * case, it updates the limit as described below:\n *\n * (1) If there is no in-flight request. This gives a baseline for the\n *     total service time of the requests of bfqq. If the baseline has\n *     not been computed yet, then, after computing it, the limit is\n *     set to 1, to start boosting throughput, and to prepare the\n *     ground for the next case. If the baseline has already been\n *     computed, then it is updated, in case it results to be lower\n *     than the previous value.\n *\n * (2) If the limit is higher than 0 and there are in-flight\n *     requests. By comparing the total service time in this case with\n *     the above baseline, it is possible to know at which extent the\n *     current value of the limit is inflating the total service\n *     time. If the inflation is below a certain threshold, then bfqq\n *     is assumed to be suffering from no perceivable loss of its\n *     service guarantees, and the limit is even tentatively\n *     increased. If the inflation is above the threshold, then the\n *     limit is decreased. Due to the lack of any hysteresis, this\n *     logic makes the limit oscillate even in steady workload\n *     conditions. Yet we opted for it, because it is fast in reaching\n *     the best value for the limit, as a function of the current I/O\n *     workload. To reduce oscillations, this step is disabled for a\n *     short time interval after the limit happens to be decreased.\n *\n * (3) Periodically, after resetting the limit, to make sure that the\n *     limit eventually drops in case the workload changes. This is\n *     needed because, after the limit has gone safely up for a\n *     certain workload, it is impossible to guess whether the\n *     baseline total service time may have changed, without measuring\n *     it again without injection. A more effective version of this\n *     step might be to just sample the baseline, by interrupting\n *     injection only once, and then to reset/lower the limit only if\n *     the total service time with the current limit does happen to be\n *     too large.\n *\n * More details on each step are provided in the comments on the\n * pieces of code that implement these steps: the branch handling the\n * transition from empty to non empty in bfq_add_request(), the branch\n * handling injection in bfq_select_queue(), and the function\n * bfq_choose_bfqq_for_injection(). These comments also explain some\n * exceptions, made by the injection mechanism in some special cases.\n */\nstatic void bfq_update_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tu64 tot_time_ns = ktime_get_ns() - bfqd->last_empty_occupied_ns;\n\tunsigned int old_limit = bfqq->inject_limit;\n\n\tif (bfqq->last_serv_time_ns > 0 && bfqd->rqs_injected) {\n\t\tu64 threshold = (bfqq->last_serv_time_ns * 3)>>1;\n\n\t\tif (tot_time_ns >= threshold && old_limit > 0) {\n\t\t\tbfqq->inject_limit--;\n\t\t\tbfqq->decrease_time_jif = jiffies;\n\t\t} else if (tot_time_ns < threshold &&\n\t\t\t   old_limit <= bfqd->max_rq_in_driver)\n\t\t\tbfqq->inject_limit++;\n\t}\n\n\t/*\n\t * Either we still have to compute the base value for the\n\t * total service time, and there seem to be the right\n\t * conditions to do it, or we can lower the last base value\n\t * computed.\n\t *\n\t * NOTE: (bfqd->rq_in_driver == 1) means that there is no I/O\n\t * request in flight, because this function is in the code\n\t * path that handles the completion of a request of bfqq, and,\n\t * in particular, this function is executed before\n\t * bfqd->rq_in_driver is decremented in such a code path.\n\t */\n\tif ((bfqq->last_serv_time_ns == 0 && bfqd->rq_in_driver == 1) ||\n\t    tot_time_ns < bfqq->last_serv_time_ns) {\n\t\tif (bfqq->last_serv_time_ns == 0) {\n\t\t\t/*\n\t\t\t * Now we certainly have a base value: make sure we\n\t\t\t * start trying injection.\n\t\t\t */\n\t\t\tbfqq->inject_limit = max_t(unsigned int, 1, old_limit);\n\t\t}\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\t} else if (!bfqd->rqs_injected && bfqd->rq_in_driver == 1)\n\t\t/*\n\t\t * No I/O injected and no request still in service in\n\t\t * the drive: these are the exact conditions for\n\t\t * computing the base value of the total service time\n\t\t * for bfqq. So let's update this value, because it is\n\t\t * rather variable. For example, it varies if the size\n\t\t * or the spatial locality of the I/O requests in bfqq\n\t\t * change.\n\t\t */\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\n\n\t/* update complete, not waiting for any request completion any longer */\n\tbfqd->waited_rq = NULL;\n\tbfqd->rqs_injected = false;\n}\n\n/*\n * Handle either a requeue or a finish for rq. The things to do are\n * the same in both cases: all references to rq are to be dropped. In\n * particular, rq is considered completed from the point of view of\n * the scheduler.\n */\nstatic void bfq_finish_requeue_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd;\n\tunsigned long flags;\n\n\t/*\n\t * rq either is not associated with any icq, or is an already\n\t * requeued request that has not (yet) been re-inserted into\n\t * a bfq_queue.\n\t */\n\tif (!rq->elv.icq || !bfqq)\n\t\treturn;\n\n\tbfqd = bfqq->bfqd;\n\n\tif (rq->rq_flags & RQF_STARTED)\n\t\tbfqg_stats_update_completion(bfqq_group(bfqq),\n\t\t\t\t\t     rq->start_time_ns,\n\t\t\t\t\t     rq->io_start_time_ns,\n\t\t\t\t\t     rq->cmd_flags);\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\tif (likely(rq->rq_flags & RQF_STARTED)) {\n\t\tif (rq == bfqd->waited_rq)\n\t\t\tbfq_update_inject_limit(bfqd, bfqq);\n\n\t\tbfq_completed_request(bfqq, bfqd);\n\t}\n\tbfqq_request_freed(bfqq);\n\tbfq_put_queue(bfqq);\n\tRQ_BIC(rq)->requests--;\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\n\t/*\n\t * Reset private fields. In case of a requeue, this allows\n\t * this function to correctly do nothing if it is spuriously\n\t * invoked again on this same request (see the check at the\n\t * beginning of the function). Probably, a better general\n\t * design would be to prevent blk-mq from invoking the requeue\n\t * or finish hooks of an elevator, for a request that is not\n\t * referred by that elevator.\n\t *\n\t * Resetting the following fields would break the\n\t * request-insertion logic if rq is re-inserted into a bfq\n\t * internal queue, without a re-preparation. Here we assume\n\t * that re-insertions of requeued requests, without\n\t * re-preparation, can happen only for pass_through or at_head\n\t * requests (which are not re-inserted into bfq internal\n\t * queues).\n\t */\n\trq->elv.priv[0] = NULL;\n\trq->elv.priv[1] = NULL;\n}\n\nstatic void bfq_finish_request(struct request *rq)\n{\n\tbfq_finish_requeue_request(rq);\n\n\tif (rq->elv.icq) {\n\t\tput_io_context(rq->elv.icq->ioc);\n\t\trq->elv.icq = NULL;\n\t}\n}\n\n/*\n * Removes the association between the current task and bfqq, assuming\n * that bic points to the bfq iocontext of the task.\n * Returns NULL if a new bfqq should be allocated, or the old bfqq if this\n * was the last process referring to that bfqq.\n */\nstatic struct bfq_queue *\nbfq_split_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"splitting queue\");\n\n\tif (bfqq_process_refs(bfqq) == 1 && !bfqq->new_bfqq) {\n\t\tbfqq->pid = current->pid;\n\t\tbfq_clear_bfqq_coop(bfqq);\n\t\tbfq_clear_bfqq_split_coop(bfqq);\n\t\treturn bfqq;\n\t}\n\n\tbic_set_bfqq(bic, NULL, true);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqq->bfqd, bfqq);\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_get_bfqq_handle_split(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_io_cq *bic,\n\t\t\t\t\t\t   struct bio *bio,\n\t\t\t\t\t\t   bool split, bool is_sync,\n\t\t\t\t\t\t   bool *new_queue)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync);\n\n\tif (likely(bfqq && bfqq != &bfqd->oom_bfqq))\n\t\treturn bfqq;\n\n\tif (new_queue)\n\t\t*new_queue = true;\n\n\tif (bfqq)\n\t\tbfq_put_queue(bfqq);\n\tbfqq = bfq_get_queue(bfqd, bio, is_sync, bic, split);\n\n\tbic_set_bfqq(bic, bfqq, is_sync);\n\tif (split && is_sync) {\n\t\tif ((bic->was_in_burst_list && bfqd->large_burst) ||\n\t\t    bic->saved_in_large_burst)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\telse {\n\t\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t\t\tif (bic->was_in_burst_list)\n\t\t\t\t/*\n\t\t\t\t * If bfqq was in the current\n\t\t\t\t * burst list before being\n\t\t\t\t * merged, then we have to add\n\t\t\t\t * it back. And we do not need\n\t\t\t\t * to increase burst_size, as\n\t\t\t\t * we did not decrement\n\t\t\t\t * burst_size when we removed\n\t\t\t\t * bfqq from the burst list as\n\t\t\t\t * a consequence of a merge\n\t\t\t\t * (see comments in\n\t\t\t\t * bfq_put_queue). In this\n\t\t\t\t * respect, it would be rather\n\t\t\t\t * costly to know whether the\n\t\t\t\t * current burst list is still\n\t\t\t\t * the same burst list from\n\t\t\t\t * which bfqq was removed on\n\t\t\t\t * the merge. To avoid this\n\t\t\t\t * cost, if bfqq was in a\n\t\t\t\t * burst list, then we add\n\t\t\t\t * bfqq to the current burst\n\t\t\t\t * list without any further\n\t\t\t\t * check. This can cause\n\t\t\t\t * inappropriate insertions,\n\t\t\t\t * but rarely enough to not\n\t\t\t\t * harm the detection of large\n\t\t\t\t * bursts significantly.\n\t\t\t\t */\n\t\t\t\thlist_add_head(&bfqq->burst_list_node,\n\t\t\t\t\t       &bfqd->burst_list);\n\t\t}\n\t\tbfqq->split_time = jiffies;\n\t}\n\n\treturn bfqq;\n}\n\n/*\n * Only reset private fields. The actual request preparation will be\n * performed by bfq_init_rq, when rq is either inserted or merged. See\n * comments on bfq_init_rq for the reason behind this delayed\n * preparation.\n */\nstatic void bfq_prepare_request(struct request *rq)\n{\n\trq->elv.icq = ioc_find_get_icq(rq->q);\n\n\t/*\n\t * Regardless of whether we have an icq attached, we have to\n\t * clear the scheduler pointers, as they might point to\n\t * previously allocated bic/bfqq structs.\n\t */\n\trq->elv.priv[0] = rq->elv.priv[1] = NULL;\n}\n\nstatic struct bfq_queue *bfq_waker_bfqq(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *new_bfqq = bfqq->new_bfqq;\n\tstruct bfq_queue *waker_bfqq = bfqq->waker_bfqq;\n\n\tif (!waker_bfqq)\n\t\treturn NULL;\n\n\twhile (new_bfqq) {\n\t\tif (new_bfqq == waker_bfqq) {\n\t\t\t/*\n\t\t\t * If waker_bfqq is in the merge chain, and current\n\t\t\t * is the only procress.\n\t\t\t */\n\t\t\tif (bfqq_process_refs(waker_bfqq) == 1)\n\t\t\t\treturn NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tnew_bfqq = new_bfqq->new_bfqq;\n\t}\n\n\treturn waker_bfqq;\n}\n\n/*\n * If needed, init rq, allocate bfq data structures associated with\n * rq, and increment reference counters in the destination bfq_queue\n * for rq. Return the destination bfq_queue for rq, or NULL is rq is\n * not associated with any bfq_queue.\n *\n * This function is invoked by the functions that perform rq insertion\n * or merging. One may have expected the above preparation operations\n * to be performed in bfq_prepare_request, and not delayed to when rq\n * is inserted or merged. The rationale behind this delayed\n * preparation is that, after the prepare_request hook is invoked for\n * rq, rq may still be transformed into a request with no icq, i.e., a\n * request not associated with any queue. No bfq hook is invoked to\n * signal this transformation. As a consequence, should these\n * preparation operations be performed when the prepare_request hook\n * is invoked, and should rq be transformed one moment later, bfq\n * would end up in an inconsistent state, because it would have\n * incremented some queue counters for an rq destined to\n * transformation, without any chance to correctly lower these\n * counters back. In contrast, no transformation can still happen for\n * rq after rq has been inserted or merged. So, it is safe to execute\n * these preparation operations when rq is finally inserted or merged.\n */\nstatic struct bfq_queue *bfq_init_rq(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio = rq->bio;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic;\n\tconst int is_sync = rq_is_sync(rq);\n\tstruct bfq_queue *bfqq;\n\tbool new_queue = false;\n\tbool bfqq_already_existing = false, split = false;\n\n\tif (unlikely(!rq->elv.icq))\n\t\treturn NULL;\n\n\t/*\n\t * Assuming that elv.priv[1] is set only if everything is set\n\t * for this rq. This holds true, because this function is\n\t * invoked only for insertion or merging, and, after such\n\t * events, a request cannot be manipulated any longer before\n\t * being removed from bfq.\n\t */\n\tif (rq->elv.priv[1])\n\t\treturn rq->elv.priv[1];\n\n\tbic = icq_to_bic(rq->elv.icq);\n\n\tbfq_check_ioprio_change(bic, bio);\n\n\tbfq_bic_update_cgroup(bic, bio);\n\n\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync,\n\t\t\t\t\t &new_queue);\n\n\tif (likely(!new_queue)) {\n\t\t/* If the queue was seeky for too long, break it apart. */\n\t\tif (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) &&\n\t\t\t!bic->stably_merged) {\n\t\t\tstruct bfq_queue *waker_bfqq = bfq_waker_bfqq(bfqq);\n\n\t\t\t/* Update bic before losing reference to bfqq */\n\t\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\t\tbic->saved_in_large_burst = true;\n\n\t\t\tbfqq = bfq_split_bfqq(bic, bfqq);\n\t\t\tsplit = true;\n\n\t\t\tif (!bfqq) {\n\t\t\t\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio,\n\t\t\t\t\t\t\t\t true, is_sync,\n\t\t\t\t\t\t\t\t NULL);\n\t\t\t\tif (unlikely(bfqq == &bfqd->oom_bfqq))\n\t\t\t\t\tbfqq_already_existing = true;\n\t\t\t} else\n\t\t\t\tbfqq_already_existing = true;\n\n\t\t\tif (!bfqq_already_existing) {\n\t\t\t\tbfqq->waker_bfqq = waker_bfqq;\n\t\t\t\tbfqq->tentative_waker_bfqq = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * If the waker queue disappears, then\n\t\t\t\t * new_bfqq->waker_bfqq must be\n\t\t\t\t * reset. So insert new_bfqq into the\n\t\t\t\t * woken_list of the waker. See\n\t\t\t\t * bfq_check_waker for details.\n\t\t\t\t */\n\t\t\t\tif (waker_bfqq)\n\t\t\t\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t\t\t\t       &bfqq->waker_bfqq->woken_list);\n\t\t\t}\n\t\t}\n\t}\n\n\tbfqq_request_allocated(bfqq);\n\tbfqq->ref++;\n\tbic->requests++;\n\tbfq_log_bfqq(bfqd, bfqq, \"get_request %p: bfqq %p, %d\",\n\t\t     rq, bfqq, bfqq->ref);\n\n\trq->elv.priv[0] = bic;\n\trq->elv.priv[1] = bfqq;\n\n\t/*\n\t * If a bfq_queue has only one process reference, it is owned\n\t * by only this bic: we can then set bfqq->bic = bic. in\n\t * addition, if the queue has also just been split, we have to\n\t * resume its state.\n\t */\n\tif (likely(bfqq != &bfqd->oom_bfqq) && !bfqq->new_bfqq &&\n\t    bfqq_process_refs(bfqq) == 1) {\n\t\tbfqq->bic = bic;\n\t\tif (split) {\n\t\t\t/*\n\t\t\t * The queue has just been split from a shared\n\t\t\t * queue: restore the idle window and the\n\t\t\t * possible weight raising period.\n\t\t\t */\n\t\t\tbfq_bfqq_resume_state(bfqq, bfqd, bic,\n\t\t\t\t\t      bfqq_already_existing);\n\t\t}\n\t}\n\n\t/*\n\t * Consider bfqq as possibly belonging to a burst of newly\n\t * created queues only if:\n\t * 1) A burst is actually happening (bfqd->burst_size > 0)\n\t * or\n\t * 2) There is no other active queue. In fact, if, in\n\t *    contrast, there are active queues not belonging to the\n\t *    possible burst bfqq may belong to, then there is no gain\n\t *    in considering bfqq as belonging to a burst, and\n\t *    therefore in not weight-raising bfqq. See comments on\n\t *    bfq_handle_burst().\n\t *\n\t * This filtering also helps eliminating false positives,\n\t * occurring when bfqq does not belong to an actual large\n\t * burst, but some background task (e.g., a service) happens\n\t * to trigger the creation of new queues very close to when\n\t * bfqq and its possible companion queues are created. See\n\t * comments on bfq_handle_burst() for further details also on\n\t * this issue.\n\t */\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     (bfqd->burst_size > 0 ||\n\t\t      bfq_tot_busy_queues(bfqd) == 0)))\n\t\tbfq_handle_burst(bfqd, bfqq);\n\n\treturn bfqq;\n}\n\nstatic void\nbfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\n\t/*\n\t * Considering that bfqq may be in race, we should firstly check\n\t * whether bfqq is in service before doing something on it. If\n\t * the bfqq in race is not in service, it has already been expired\n\t * through __bfq_bfqq_expire func and its wait_request flags has\n\t * been cleared in __bfq_bfqd_reset_in_service func.\n\t */\n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t/*\n\t\t * Also here the queue can be safely expired\n\t\t * for budget timeout without wasting\n\t\t * guarantees\n\t\t */\n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t/*\n\t\t * The queue may not be empty upon timer expiration,\n\t\t * because we may not disable the timer when the\n\t\t * first request of the in-service queue arrives\n\t\t * during disk idling.\n\t\t */\n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tbfq_schedule_dispatch(bfqd);\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n/*\n * Handler of the expiration of the timer running if the in-service queue\n * is idling inside its time slice.\n */\nstatic enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t/*\n\t * Theoretical race here: the in-service queue can be NULL or\n\t * different from the queue that was idling if a new request\n\t * arrives for the current queue and there is a full dispatch\n\t * cycle that changes the in-service queue.  This can hardly\n\t * happen, but in the worst case we just expire a queue too\n\t * early.\n\t */\n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __bfq_put_async_bfqq(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue **bfqq_ptr)\n{\n\tstruct bfq_queue *bfqq = *bfqq_ptr;\n\n\tbfq_log(bfqd, \"put_async_bfqq: %p\", bfqq);\n\tif (bfqq) {\n\t\tbfq_bfqq_move(bfqd, bfqq, bfqd->root_group);\n\n\t\tbfq_log_bfqq(bfqd, bfqq, \"put_async_bfqq: putting %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\tbfq_put_queue(bfqq);\n\t\t*bfqq_ptr = NULL;\n\t}\n}\n\n/*\n * Release all the bfqg references to its async queues.  If we are\n * deallocating the group these queues may still contain requests, so\n * we reparent them to the root cgroup (i.e., the only one that will\n * exist for sure until all the requests on a device are gone).\n */\nvoid bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 2; i++)\n\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_bfqq[i][j]);\n\n\t__bfq_put_async_bfqq(bfqd, &bfqg->async_idle_bfqq);\n}\n\n/*\n * See the comments on bfq_limit_depth for the purpose of\n * the depths set in the function. Return minimum shallow depth we'll use.\n */\nstatic void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)\n{\n\tunsigned int depth = 1U << bt->sb.shift;\n\n\tbfqd->full_depth_shift = bt->sb.shift;\n\t/*\n\t * In-word depths if no bfq_queue is being weight-raised:\n\t * leaving 25% of tags only for sync reads.\n\t *\n\t * In next formulas, right-shift the value\n\t * (1U<<bt->sb.shift), instead of computing directly\n\t * (1U<<(bt->sb.shift - something)), to be robust against\n\t * any possible value of bt->sb.shift, without having to\n\t * limit 'something'.\n\t */\n\t/* no more than 50% of tags for async I/O */\n\tbfqd->word_depths[0][0] = max(depth >> 1, 1U);\n\t/*\n\t * no more than 75% of tags for sync writes (25% extra tags\n\t * w.r.t. async I/O, to prevent async I/O from starving sync\n\t * writes)\n\t */\n\tbfqd->word_depths[0][1] = max((depth * 3) >> 2, 1U);\n\n\t/*\n\t * In-word depths in case some bfq_queue is being weight-\n\t * raised: leaving ~63% of tags for sync reads. This is the\n\t * highest percentage for which, in our tests, application\n\t * start-up times didn't suffer from any regression due to tag\n\t * shortage.\n\t */\n\t/* no more than ~18% of tags for async I/O */\n\tbfqd->word_depths[1][0] = max((depth * 3) >> 4, 1U);\n\t/* no more than ~37% of tags for sync writes (~20% extra tags) */\n\tbfqd->word_depths[1][1] = max((depth * 6) >> 4, 1U);\n}\n\nstatic void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\n\tbfq_update_depths(bfqd, &tags->bitmap_tags);\n\tsbitmap_queue_min_shallow_depth(&tags->bitmap_tags, 1);\n}\n\nstatic int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)\n{\n\tbfq_depth_updated(hctx);\n\treturn 0;\n}\n\nstatic void bfq_exit_queue(struct elevator_queue *e)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tstruct bfq_queue *bfqq, *n;\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\tspin_lock_irq(&bfqd->lock);\n\tlist_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)\n\t\tbfq_deactivate_bfqq(bfqd, bfqq, false, false);\n\tspin_unlock_irq(&bfqd->lock);\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\t/* release oom-queue reference to root group */\n\tbfqg_and_blkg_put(bfqd->root_group);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_deactivate_policy(bfqd->queue, &blkcg_policy_bfq);\n#else\n\tspin_lock_irq(&bfqd->lock);\n\tbfq_put_async_queues(bfqd, bfqd->root_group);\n\tkfree(bfqd->root_group);\n\tspin_unlock_irq(&bfqd->lock);\n#endif\n\n\tblk_stat_disable_accounting(bfqd->queue);\n\twbt_enable_default(bfqd->queue);\n\n\tkfree(bfqd);\n}\n\nstatic void bfq_init_root_group(struct bfq_group *root_group,\n\t\t\t\tstruct bfq_data *bfqd)\n{\n\tint i;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\troot_group->entity.parent = NULL;\n\troot_group->my_entity = NULL;\n\troot_group->bfqd = bfqd;\n#endif\n\troot_group->rq_pos_tree = RB_ROOT;\n\tfor (i = 0; i < BFQ_IOPRIO_CLASSES; i++)\n\t\troot_group->sched_data.service_tree[i] = BFQ_SERVICE_TREE_INIT;\n\troot_group->sched_data.bfq_class_idle_last_service = jiffies;\n}\n\nstatic int bfq_init_queue(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct bfq_data *bfqd;\n\tstruct elevator_queue *eq;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn -ENOMEM;\n\n\tbfqd = kzalloc_node(sizeof(*bfqd), GFP_KERNEL, q->node);\n\tif (!bfqd) {\n\t\tkobject_put(&eq->kobj);\n\t\treturn -ENOMEM;\n\t}\n\teq->elevator_data = bfqd;\n\n\tspin_lock_irq(&q->queue_lock);\n\tq->elevator = eq;\n\tspin_unlock_irq(&q->queue_lock);\n\n\t/*\n\t * Our fallback bfqq if bfq_find_alloc_queue() runs into OOM issues.\n\t * Grab a permanent reference to it, so that the normal code flow\n\t * will not attempt to free it.\n\t */\n\tbfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0);\n\tbfqd->oom_bfqq.ref++;\n\tbfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;\n\tbfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;\n\tbfqd->oom_bfqq.entity.new_weight =\n\t\tbfq_ioprio_to_weight(bfqd->oom_bfqq.new_ioprio);\n\n\t/* oom_bfqq does not participate to bursts */\n\tbfq_clear_bfqq_just_created(&bfqd->oom_bfqq);\n\n\t/*\n\t * Trigger weight initialization, according to ioprio, at the\n\t * oom_bfqq's first activation. The oom_bfqq's ioprio and ioprio\n\t * class won't be changed any more.\n\t */\n\tbfqd->oom_bfqq.entity.prio_changed = 1;\n\n\tbfqd->queue = q;\n\n\tINIT_LIST_HEAD(&bfqd->dispatch);\n\n\thrtimer_init(&bfqd->idle_slice_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL);\n\tbfqd->idle_slice_timer.function = bfq_idle_slice_timer;\n\n\tbfqd->queue_weights_tree = RB_ROOT_CACHED;\n\tbfqd->num_groups_with_pending_reqs = 0;\n\n\tINIT_LIST_HEAD(&bfqd->active_list);\n\tINIT_LIST_HEAD(&bfqd->idle_list);\n\tINIT_HLIST_HEAD(&bfqd->burst_list);\n\n\tbfqd->hw_tag = -1;\n\tbfqd->nonrot_with_queueing = blk_queue_nonrot(bfqd->queue);\n\n\tbfqd->bfq_max_budget = bfq_default_max_budget;\n\n\tbfqd->bfq_fifo_expire[0] = bfq_fifo_expire[0];\n\tbfqd->bfq_fifo_expire[1] = bfq_fifo_expire[1];\n\tbfqd->bfq_back_max = bfq_back_max;\n\tbfqd->bfq_back_penalty = bfq_back_penalty;\n\tbfqd->bfq_slice_idle = bfq_slice_idle;\n\tbfqd->bfq_timeout = bfq_timeout;\n\n\tbfqd->bfq_large_burst_thresh = 8;\n\tbfqd->bfq_burst_interval = msecs_to_jiffies(180);\n\n\tbfqd->low_latency = true;\n\n\t/*\n\t * Trade-off between responsiveness and fairness.\n\t */\n\tbfqd->bfq_wr_coeff = 30;\n\tbfqd->bfq_wr_rt_max_time = msecs_to_jiffies(300);\n\tbfqd->bfq_wr_max_time = 0;\n\tbfqd->bfq_wr_min_idle_time = msecs_to_jiffies(2000);\n\tbfqd->bfq_wr_min_inter_arr_async = msecs_to_jiffies(500);\n\tbfqd->bfq_wr_max_softrt_rate = 7000; /*\n\t\t\t\t\t      * Approximate rate required\n\t\t\t\t\t      * to playback or record a\n\t\t\t\t\t      * high-definition compressed\n\t\t\t\t\t      * video.\n\t\t\t\t\t      */\n\tbfqd->wr_busy_queues = 0;\n\n\t/*\n\t * Begin by assuming, optimistically, that the device peak\n\t * rate is equal to 2/3 of the highest reference rate.\n\t */\n\tbfqd->rate_dur_prod = ref_rate[blk_queue_nonrot(bfqd->queue)] *\n\t\tref_wr_duration[blk_queue_nonrot(bfqd->queue)];\n\tbfqd->peak_rate = ref_rate[blk_queue_nonrot(bfqd->queue)] * 2 / 3;\n\n\tspin_lock_init(&bfqd->lock);\n\n\t/*\n\t * The invocation of the next bfq_create_group_hierarchy\n\t * function is the head of a chain of function calls\n\t * (bfq_create_group_hierarchy->blkcg_activate_policy->\n\t * blk_mq_freeze_queue) that may lead to the invocation of the\n\t * has_work hook function. For this reason,\n\t * bfq_create_group_hierarchy is invoked only after all\n\t * scheduler data has been initialized, apart from the fields\n\t * that can be initialized only after invoking\n\t * bfq_create_group_hierarchy. This, in particular, enables\n\t * has_work to correctly return false. Of course, to avoid\n\t * other inconsistencies, the blk-mq stack must then refrain\n\t * from invoking further scheduler hooks before this init\n\t * function is finished.\n\t */\n\tbfqd->root_group = bfq_create_group_hierarchy(bfqd, q->node);\n\tif (!bfqd->root_group)\n\t\tgoto out_free;\n\tbfq_init_root_group(bfqd->root_group, bfqd);\n\tbfq_init_entity(&bfqd->oom_bfqq.entity, bfqd->root_group);\n\n\t/* We dispatch from request queue wide instead of hw queue */\n\tblk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);\n\n\twbt_disable_default(q);\n\tblk_stat_enable_accounting(q);\n\n\treturn 0;\n\nout_free:\n\tkfree(bfqd);\n\tkobject_put(&eq->kobj);\n\treturn -ENOMEM;\n}\n\nstatic void bfq_slab_kill(void)\n{\n\tkmem_cache_destroy(bfq_pool);\n}\n\nstatic int __init bfq_slab_setup(void)\n{\n\tbfq_pool = KMEM_CACHE(bfq_queue, 0);\n\tif (!bfq_pool)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic ssize_t bfq_var_show(unsigned int var, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", var);\n}\n\nstatic int bfq_var_store(unsigned long *var, const char *page)\n{\n\tunsigned long new_val;\n\tint ret = kstrtoul(page, 10, &new_val);\n\n\tif (ret)\n\t\treturn ret;\n\t*var = new_val;\n\treturn 0;\n}\n\n#define SHOW_FUNCTION(__FUNC, __VAR, __CONV)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t__data = jiffies_to_msecs(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t__data = div_u64(__data, NSEC_PER_MSEC);\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nSHOW_FUNCTION(bfq_fifo_expire_sync_show, bfqd->bfq_fifo_expire[1], 2);\nSHOW_FUNCTION(bfq_fifo_expire_async_show, bfqd->bfq_fifo_expire[0], 2);\nSHOW_FUNCTION(bfq_back_seek_max_show, bfqd->bfq_back_max, 0);\nSHOW_FUNCTION(bfq_back_seek_penalty_show, bfqd->bfq_back_penalty, 0);\nSHOW_FUNCTION(bfq_slice_idle_show, bfqd->bfq_slice_idle, 2);\nSHOW_FUNCTION(bfq_max_budget_show, bfqd->bfq_user_max_budget, 0);\nSHOW_FUNCTION(bfq_timeout_sync_show, bfqd->bfq_timeout, 1);\nSHOW_FUNCTION(bfq_strict_guarantees_show, bfqd->strict_guarantees, 0);\nSHOW_FUNCTION(bfq_low_latency_show, bfqd->low_latency, 0);\n#undef SHOW_FUNCTION\n\n#define USEC_SHOW_FUNCTION(__FUNC, __VAR)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\t__data = div_u64(__data, NSEC_PER_USEC);\t\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nUSEC_SHOW_FUNCTION(bfq_slice_idle_us_show, bfqd->bfq_slice_idle);\n#undef USEC_SHOW_FUNCTION\n\n#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX, __CONV)\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n__FUNC(struct elevator_queue *e, const char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t*(__PTR) = msecs_to_jiffies(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t*(__PTR) = (u64)__data * NSEC_PER_MSEC;\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t*(__PTR) = __data;\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nSTORE_FUNCTION(bfq_fifo_expire_sync_store, &bfqd->bfq_fifo_expire[1], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_fifo_expire_async_store, &bfqd->bfq_fifo_expire[0], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_back_seek_max_store, &bfqd->bfq_back_max, 0, INT_MAX, 0);\nSTORE_FUNCTION(bfq_back_seek_penalty_store, &bfqd->bfq_back_penalty, 1,\n\t\tINT_MAX, 0);\nSTORE_FUNCTION(bfq_slice_idle_store, &bfqd->bfq_slice_idle, 0, INT_MAX, 2);\n#undef STORE_FUNCTION\n\n#define USEC_STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\t*(__PTR) = (u64)__data * NSEC_PER_USEC;\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nUSEC_STORE_FUNCTION(bfq_slice_idle_us_store, &bfqd->bfq_slice_idle, 0,\n\t\t    UINT_MAX);\n#undef USEC_STORE_FUNCTION\n\nstatic ssize_t bfq_max_budget_store(struct elevator_queue *e,\n\t\t\t\t    const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\telse {\n\t\tif (__data > INT_MAX)\n\t\t\t__data = INT_MAX;\n\t\tbfqd->bfq_max_budget = __data;\n\t}\n\n\tbfqd->bfq_user_max_budget = __data;\n\n\treturn count;\n}\n\n/*\n * Leaving this name to preserve name compatibility with cfq\n * parameters, but this timeout is used for both sync and async.\n */\nstatic ssize_t bfq_timeout_sync_store(struct elevator_queue *e,\n\t\t\t\t      const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data < 1)\n\t\t__data = 1;\n\telse if (__data > INT_MAX)\n\t\t__data = INT_MAX;\n\n\tbfqd->bfq_timeout = msecs_to_jiffies(__data);\n\tif (bfqd->bfq_user_max_budget == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\n\treturn count;\n}\n\nstatic ssize_t bfq_strict_guarantees_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (!bfqd->strict_guarantees && __data == 1\n\t    && bfqd->bfq_slice_idle < 8 * NSEC_PER_MSEC)\n\t\tbfqd->bfq_slice_idle = 8 * NSEC_PER_MSEC;\n\n\tbfqd->strict_guarantees = __data;\n\n\treturn count;\n}\n\nstatic ssize_t bfq_low_latency_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (__data == 0 && bfqd->low_latency != 0)\n\t\tbfq_end_wr(bfqd);\n\tbfqd->low_latency = __data;\n\n\treturn count;\n}\n\n#define BFQ_ATTR(name) \\\n\t__ATTR(name, 0644, bfq_##name##_show, bfq_##name##_store)\n\nstatic struct elv_fs_entry bfq_attrs[] = {\n\tBFQ_ATTR(fifo_expire_sync),\n\tBFQ_ATTR(fifo_expire_async),\n\tBFQ_ATTR(back_seek_max),\n\tBFQ_ATTR(back_seek_penalty),\n\tBFQ_ATTR(slice_idle),\n\tBFQ_ATTR(slice_idle_us),\n\tBFQ_ATTR(max_budget),\n\tBFQ_ATTR(timeout_sync),\n\tBFQ_ATTR(strict_guarantees),\n\tBFQ_ATTR(low_latency),\n\t__ATTR_NULL\n};\n\nstatic struct elevator_type iosched_bfq_mq = {\n\t.ops = {\n\t\t.limit_depth\t\t= bfq_limit_depth,\n\t\t.prepare_request\t= bfq_prepare_request,\n\t\t.requeue_request        = bfq_finish_requeue_request,\n\t\t.finish_request\t\t= bfq_finish_request,\n\t\t.exit_icq\t\t= bfq_exit_icq,\n\t\t.insert_requests\t= bfq_insert_requests,\n\t\t.dispatch_request\t= bfq_dispatch_request,\n\t\t.next_request\t\t= elv_rb_latter_request,\n\t\t.former_request\t\t= elv_rb_former_request,\n\t\t.allow_merge\t\t= bfq_allow_bio_merge,\n\t\t.bio_merge\t\t= bfq_bio_merge,\n\t\t.request_merge\t\t= bfq_request_merge,\n\t\t.requests_merged\t= bfq_requests_merged,\n\t\t.request_merged\t\t= bfq_request_merged,\n\t\t.has_work\t\t= bfq_has_work,\n\t\t.depth_updated\t\t= bfq_depth_updated,\n\t\t.init_hctx\t\t= bfq_init_hctx,\n\t\t.init_sched\t\t= bfq_init_queue,\n\t\t.exit_sched\t\t= bfq_exit_queue,\n\t},\n\n\t.icq_size =\t\tsizeof(struct bfq_io_cq),\n\t.icq_align =\t\t__alignof__(struct bfq_io_cq),\n\t.elevator_attrs =\tbfq_attrs,\n\t.elevator_name =\t\"bfq\",\n\t.elevator_owner =\tTHIS_MODULE,\n};\nMODULE_ALIAS(\"bfq-iosched\");\n\nstatic int __init bfq_init(void)\n{\n\tint ret;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tret = blkcg_policy_register(&blkcg_policy_bfq);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = -ENOMEM;\n\tif (bfq_slab_setup())\n\t\tgoto err_pol_unreg;\n\n\t/*\n\t * Times to load large popular applications for the typical\n\t * systems installed on the reference devices (see the\n\t * comments before the definition of the next\n\t * array). Actually, we use slightly lower values, as the\n\t * estimated peak rate tends to be smaller than the actual\n\t * peak rate.  The reason for this last fact is that estimates\n\t * are computed over much shorter time intervals than the long\n\t * intervals typically used for benchmarking. Why? First, to\n\t * adapt more quickly to variations. Second, because an I/O\n\t * scheduler cannot rely on a peak-rate-evaluation workload to\n\t * be run for a long time.\n\t */\n\tref_wr_duration[0] = msecs_to_jiffies(7000); /* actually 8 sec */\n\tref_wr_duration[1] = msecs_to_jiffies(2500); /* actually 3 sec */\n\n\tret = elv_register(&iosched_bfq_mq);\n\tif (ret)\n\t\tgoto slab_kill;\n\n\treturn 0;\n\nslab_kill:\n\tbfq_slab_kill();\nerr_pol_unreg:\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\treturn ret;\n}\n\nstatic void __exit bfq_exit(void)\n{\n\telv_unregister(&iosched_bfq_mq);\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\tbfq_slab_kill();\n}\n\nmodule_init(bfq_init);\nmodule_exit(bfq_exit);\n\nMODULE_AUTHOR(\"Paolo Valente\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MQ Budget Fair Queueing I/O Scheduler\");\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21632",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21632.json",
            "patch_attempts": [
                {
                    "upstream_commit": "5cc2db37124bb33914996d6fdbb2ddb3811f2945",
                    "upstream_commit_date": "2025-01-07 15:55:51 -0800",
                    "upstream_patch": "a9d9c33132d49329ada647e4514d210d15e31d81",
                    "total_versions_tested": 2,
                    "successful_patches": 2,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "0a3a872214188e4268d31581ed0cd44508e038cf",
                            "downstream_commit": "0c50f00cc29948184af05bda31392fff5821f4f3",
                            "commit_date": "2025-01-17 13:36:21 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file arch/x86/kernel/fpu/regset.c",
                            "downstream_patch_content": "commit 0a3a872214188e4268d31581ed0cd44508e038cf\nAuthor: Rick Edgecombe <rick.p.edgecombe@intel.com>\nDate:   Tue Jan 7 15:30:56 2025 -0800\n\n    x86/fpu: Ensure shadow stack is active before \"getting\" registers\n    \n    commit a9d9c33132d49329ada647e4514d210d15e31d81 upstream.\n    \n    The x86 shadow stack support has its own set of registers. Those registers\n    are XSAVE-managed, but they are \"supervisor state components\" which means\n    that userspace can not touch them with XSAVE/XRSTOR.  It also means that\n    they are not accessible from the existing ptrace ABI for XSAVE state.\n    Thus, there is a new ptrace get/set interface for it.\n    \n    The regset code that ptrace uses provides an ->active() handler in\n    addition to the get/set ones. For shadow stack this ->active() handler\n    verifies that shadow stack is enabled via the ARCH_SHSTK_SHSTK bit in the\n    thread struct. The ->active() handler is checked from some call sites of\n    the regset get/set handlers, but not the ptrace ones. This was not\n    understood when shadow stack support was put in place.\n    \n    As a result, both the set/get handlers can be called with\n    XFEATURE_CET_USER in its init state, which would cause get_xsave_addr() to\n    return NULL and trigger a WARN_ON(). The ssp_set() handler luckily has an\n    ssp_active() check to avoid surprising the kernel with shadow stack\n    behavior when the kernel is not ready for it (ARCH_SHSTK_SHSTK==0). That\n    check just happened to avoid the warning.\n    \n    But the ->get() side wasn't so lucky. It can be called with shadow stacks\n    disabled, triggering the warning in practice, as reported by Christina\n    Schimpe:\n    \n    WARNING: CPU: 5 PID: 1773 at arch/x86/kernel/fpu/regset.c:198 ssp_get+0x89/0xa0\n    [...]\n    Call Trace:\n    <TASK>\n    ? show_regs+0x6e/0x80\n    ? ssp_get+0x89/0xa0\n    ? __warn+0x91/0x150\n    ? ssp_get+0x89/0xa0\n    ? report_bug+0x19d/0x1b0\n    ? handle_bug+0x46/0x80\n    ? exc_invalid_op+0x1d/0x80\n    ? asm_exc_invalid_op+0x1f/0x30\n    ? __pfx_ssp_get+0x10/0x10\n    ? ssp_get+0x89/0xa0\n    ? ssp_get+0x52/0xa0\n    __regset_get+0xad/0xf0\n    copy_regset_to_user+0x52/0xc0\n    ptrace_regset+0x119/0x140\n    ptrace_request+0x13c/0x850\n    ? wait_task_inactive+0x142/0x1d0\n    ? do_syscall_64+0x6d/0x90\n    arch_ptrace+0x102/0x300\n    [...]\n    \n    Ensure that shadow stacks are active in a thread before looking them up\n    in the XSAVE buffer. Since ARCH_SHSTK_SHSTK and user_ssp[SHSTK_EN] are\n    set at the same time, the active check ensures that there will be\n    something to find in the XSAVE buffer.\n    \n    [ dhansen: changelog/subject tweaks ]\n    \n    Fixes: 2fab02b25ae7 (\"x86: Add PTRACE interface for shadow stack\")\n    Reported-by: Christina Schimpe <christina.schimpe@intel.com>\n    Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>\n    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>\n    Tested-by: Christina Schimpe <christina.schimpe@intel.com>\n    Cc:stable@vger.kernel.org\n    Link: https://lore.kernel.org/all/20250107233056.235536-1-rick.p.edgecombe%40intel.com\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c\nindex 6bc1eb2a21bd..887b0b8e21e3 100644\n--- a/arch/x86/kernel/fpu/regset.c\n+++ b/arch/x86/kernel/fpu/regset.c\n@@ -190,7 +190,8 @@ int ssp_get(struct task_struct *target, const struct user_regset *regset,\n \tstruct fpu *fpu = &target->thread.fpu;\n \tstruct cet_user_state *cetregs;\n \n-\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK))\n+\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK) ||\n+\t    !ssp_active(target, regset))\n \t\treturn -ENODEV;\n \n \tsync_fpstate(fpu);\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "6bfe1fc22f462bec87422cdcbec4d7a2f43ff01d",
                            "downstream_commit": "8e122d780a0f19aefd700dbd0b0e3ed3af0ae97f",
                            "commit_date": "2025-01-17 13:40:54 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file arch/x86/kernel/fpu/regset.c",
                            "downstream_patch_content": "commit 6bfe1fc22f462bec87422cdcbec4d7a2f43ff01d\nAuthor: Rick Edgecombe <rick.p.edgecombe@intel.com>\nDate:   Tue Jan 7 15:30:56 2025 -0800\n\n    x86/fpu: Ensure shadow stack is active before \"getting\" registers\n    \n    commit a9d9c33132d49329ada647e4514d210d15e31d81 upstream.\n    \n    The x86 shadow stack support has its own set of registers. Those registers\n    are XSAVE-managed, but they are \"supervisor state components\" which means\n    that userspace can not touch them with XSAVE/XRSTOR.  It also means that\n    they are not accessible from the existing ptrace ABI for XSAVE state.\n    Thus, there is a new ptrace get/set interface for it.\n    \n    The regset code that ptrace uses provides an ->active() handler in\n    addition to the get/set ones. For shadow stack this ->active() handler\n    verifies that shadow stack is enabled via the ARCH_SHSTK_SHSTK bit in the\n    thread struct. The ->active() handler is checked from some call sites of\n    the regset get/set handlers, but not the ptrace ones. This was not\n    understood when shadow stack support was put in place.\n    \n    As a result, both the set/get handlers can be called with\n    XFEATURE_CET_USER in its init state, which would cause get_xsave_addr() to\n    return NULL and trigger a WARN_ON(). The ssp_set() handler luckily has an\n    ssp_active() check to avoid surprising the kernel with shadow stack\n    behavior when the kernel is not ready for it (ARCH_SHSTK_SHSTK==0). That\n    check just happened to avoid the warning.\n    \n    But the ->get() side wasn't so lucky. It can be called with shadow stacks\n    disabled, triggering the warning in practice, as reported by Christina\n    Schimpe:\n    \n    WARNING: CPU: 5 PID: 1773 at arch/x86/kernel/fpu/regset.c:198 ssp_get+0x89/0xa0\n    [...]\n    Call Trace:\n    <TASK>\n    ? show_regs+0x6e/0x80\n    ? ssp_get+0x89/0xa0\n    ? __warn+0x91/0x150\n    ? ssp_get+0x89/0xa0\n    ? report_bug+0x19d/0x1b0\n    ? handle_bug+0x46/0x80\n    ? exc_invalid_op+0x1d/0x80\n    ? asm_exc_invalid_op+0x1f/0x30\n    ? __pfx_ssp_get+0x10/0x10\n    ? ssp_get+0x89/0xa0\n    ? ssp_get+0x52/0xa0\n    __regset_get+0xad/0xf0\n    copy_regset_to_user+0x52/0xc0\n    ptrace_regset+0x119/0x140\n    ptrace_request+0x13c/0x850\n    ? wait_task_inactive+0x142/0x1d0\n    ? do_syscall_64+0x6d/0x90\n    arch_ptrace+0x102/0x300\n    [...]\n    \n    Ensure that shadow stacks are active in a thread before looking them up\n    in the XSAVE buffer. Since ARCH_SHSTK_SHSTK and user_ssp[SHSTK_EN] are\n    set at the same time, the active check ensures that there will be\n    something to find in the XSAVE buffer.\n    \n    [ dhansen: changelog/subject tweaks ]\n    \n    Fixes: 2fab02b25ae7 (\"x86: Add PTRACE interface for shadow stack\")\n    Reported-by: Christina Schimpe <christina.schimpe@intel.com>\n    Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>\n    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>\n    Tested-by: Christina Schimpe <christina.schimpe@intel.com>\n    Cc:stable@vger.kernel.org\n    Link: https://lore.kernel.org/all/20250107233056.235536-1-rick.p.edgecombe%40intel.com\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c\nindex 6bc1eb2a21bd..887b0b8e21e3 100644\n--- a/arch/x86/kernel/fpu/regset.c\n+++ b/arch/x86/kernel/fpu/regset.c\n@@ -190,7 +190,8 @@ int ssp_get(struct task_struct *target, const struct user_regset *regset,\n \tstruct fpu *fpu = &target->thread.fpu;\n \tstruct cet_user_state *cetregs;\n \n-\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK))\n+\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK) ||\n+\t    !ssp_active(target, regset))\n \t\treturn -ENODEV;\n \n \tsync_fpstate(fpu);\n",
                            "downstream_file_content": {
                                "arch/x86/kernel/fpu/regset.c": "// SPDX-License-Identifier: GPL-2.0\n/*\n * FPU register's regset abstraction, for ptrace, core dumps, etc.\n */\n#include <linux/sched/task_stack.h>\n#include <linux/vmalloc.h>\n\n#include <asm/fpu/api.h>\n#include <asm/fpu/signal.h>\n#include <asm/fpu/regset.h>\n#include <asm/prctl.h>\n\n#include \"context.h\"\n#include \"internal.h\"\n#include \"legacy.h\"\n#include \"xstate.h\"\n\n/*\n * The xstateregs_active() routine is the same as the regset_fpregs_active() routine,\n * as the \"regset->n\" for the xstate regset will be updated based on the feature\n * capabilities supported by the xsave.\n */\nint regset_fpregs_active(struct task_struct *target, const struct user_regset *regset)\n{\n\treturn regset->n;\n}\n\nint regset_xregset_fpregs_active(struct task_struct *target, const struct user_regset *regset)\n{\n\tif (boot_cpu_has(X86_FEATURE_FXSR))\n\t\treturn regset->n;\n\telse\n\t\treturn 0;\n}\n\n/*\n * The regset get() functions are invoked from:\n *\n *   - coredump to dump the current task's fpstate. If the current task\n *     owns the FPU then the memory state has to be synchronized and the\n *     FPU register state preserved. Otherwise fpstate is already in sync.\n *\n *   - ptrace to dump fpstate of a stopped task, in which case the registers\n *     have already been saved to fpstate on context switch.\n */\nstatic void sync_fpstate(struct fpu *fpu)\n{\n\tif (fpu == &current->thread.fpu)\n\t\tfpu_sync_fpstate(fpu);\n}\n\n/*\n * Invalidate cached FPU registers before modifying the stopped target\n * task's fpstate.\n *\n * This forces the target task on resume to restore the FPU registers from\n * modified fpstate. Otherwise the task might skip the restore and operate\n * with the cached FPU registers which discards the modifications.\n */\nstatic void fpu_force_restore(struct fpu *fpu)\n{\n\t/*\n\t * Only stopped child tasks can be used to modify the FPU\n\t * state in the fpstate buffer:\n\t */\n\tWARN_ON_FPU(fpu == &current->thread.fpu);\n\n\t__fpu_invalidate_fpregs_state(fpu);\n}\n\nint xfpregs_get(struct task_struct *target, const struct user_regset *regset,\n\t\tstruct membuf to)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\treturn -ENODEV;\n\n\tsync_fpstate(fpu);\n\n\tif (!use_xsave()) {\n\t\treturn membuf_write(&to, &fpu->fpstate->regs.fxsave,\n\t\t\t\t    sizeof(fpu->fpstate->regs.fxsave));\n\t}\n\n\tcopy_xstate_to_uabi_buf(to, target, XSTATE_COPY_FX);\n\treturn 0;\n}\n\nint xfpregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\tunsigned int pos, unsigned int count,\n\t\tconst void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct fxregs_state newstate;\n\tint ret;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\treturn -ENODEV;\n\n\t/* No funny business with partial or oversized writes is permitted. */\n\tif (pos != 0 || count != sizeof(newstate))\n\t\treturn -EINVAL;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate, 0, -1);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Do not allow an invalid MXCSR value. */\n\tif (newstate.mxcsr & ~mxcsr_feature_mask)\n\t\treturn -EINVAL;\n\n\tfpu_force_restore(fpu);\n\n\t/* Copy the state  */\n\tmemcpy(&fpu->fpstate->regs.fxsave, &newstate, sizeof(newstate));\n\n\t/* Clear xmm8..15 for 32-bit callers */\n\tBUILD_BUG_ON(sizeof(fpu->__fpstate.regs.fxsave.xmm_space) != 16 * 16);\n\tif (in_ia32_syscall())\n\t\tmemset(&fpu->fpstate->regs.fxsave.xmm_space[8*4], 0, 8 * 16);\n\n\t/* Mark FP and SSE as in use when XSAVE is enabled */\n\tif (use_xsave())\n\t\tfpu->fpstate->regs.xsave.header.xfeatures |= XFEATURE_MASK_FPSSE;\n\n\treturn 0;\n}\n\nint xstateregs_get(struct task_struct *target, const struct user_regset *regset,\n\t\tstruct membuf to)\n{\n\tif (!cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\tsync_fpstate(&target->thread.fpu);\n\n\tcopy_xstate_to_uabi_buf(to, target, XSTATE_COPY_XSAVE);\n\treturn 0;\n}\n\nint xstateregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *tmpbuf = NULL;\n\tint ret;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\t/*\n\t * A whole standard-format XSAVE buffer is needed:\n\t */\n\tif (pos != 0 || count != fpu_user_cfg.max_size)\n\t\treturn -EFAULT;\n\n\tif (!kbuf) {\n\t\ttmpbuf = vmalloc(count);\n\t\tif (!tmpbuf)\n\t\t\treturn -ENOMEM;\n\n\t\tif (copy_from_user(tmpbuf, ubuf, count)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfpu_force_restore(fpu);\n\tret = copy_uabi_from_kernel_to_xstate(fpu->fpstate, kbuf ?: tmpbuf, &target->thread.pkru);\n\nout:\n\tvfree(tmpbuf);\n\treturn ret;\n}\n\n#ifdef CONFIG_X86_USER_SHADOW_STACK\nint ssp_active(struct task_struct *target, const struct user_regset *regset)\n{\n\tif (target->thread.features & ARCH_SHSTK_SHSTK)\n\t\treturn regset->n;\n\n\treturn 0;\n}\n\nint ssp_get(struct task_struct *target, const struct user_regset *regset,\n\t    struct membuf to)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct cet_user_state *cetregs;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK))\n\t\treturn -ENODEV;\n\n\tsync_fpstate(fpu);\n\tcetregs = get_xsave_addr(&fpu->fpstate->regs.xsave, XFEATURE_CET_USER);\n\tif (WARN_ON(!cetregs)) {\n\t\t/*\n\t\t * This shouldn't ever be NULL because shadow stack was\n\t\t * verified to be enabled above. This means\n\t\t * MSR_IA32_U_CET.CET_SHSTK_EN should be 1 and so\n\t\t * XFEATURE_CET_USER should not be in the init state.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\treturn membuf_write(&to, (unsigned long *)&cetregs->user_ssp,\n\t\t\t    sizeof(cetregs->user_ssp));\n}\n\nint ssp_set(struct task_struct *target, const struct user_regset *regset,\n\t    unsigned int pos, unsigned int count,\n\t    const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *xsave = &fpu->fpstate->regs.xsave;\n\tstruct cet_user_state *cetregs;\n\tunsigned long user_ssp;\n\tint r;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK) ||\n\t    !ssp_active(target, regset))\n\t\treturn -ENODEV;\n\n\tif (pos != 0 || count != sizeof(user_ssp))\n\t\treturn -EINVAL;\n\n\tr = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &user_ssp, 0, -1);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Some kernel instructions (IRET, etc) can cause exceptions in the case\n\t * of disallowed CET register values. Just prevent invalid values.\n\t */\n\tif (user_ssp >= TASK_SIZE_MAX || !IS_ALIGNED(user_ssp, 8))\n\t\treturn -EINVAL;\n\n\tfpu_force_restore(fpu);\n\n\tcetregs = get_xsave_addr(xsave, XFEATURE_CET_USER);\n\tif (WARN_ON(!cetregs)) {\n\t\t/*\n\t\t * This shouldn't ever be NULL because shadow stack was\n\t\t * verified to be enabled above. This means\n\t\t * MSR_IA32_U_CET.CET_SHSTK_EN should be 1 and so\n\t\t * XFEATURE_CET_USER should not be in the init state.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tcetregs->user_ssp = user_ssp;\n\treturn 0;\n}\n#endif /* CONFIG_X86_USER_SHADOW_STACK */\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\n/*\n * FPU tag word conversions.\n */\n\nstatic inline unsigned short twd_i387_to_fxsr(unsigned short twd)\n{\n\tunsigned int tmp; /* to avoid 16 bit prefixes in the code */\n\n\t/* Transform each pair of bits into 01 (valid) or 00 (empty) */\n\ttmp = ~twd;\n\ttmp = (tmp | (tmp>>1)) & 0x5555; /* 0V0V0V0V0V0V0V0V */\n\t/* and move the valid bits to the lower byte. */\n\ttmp = (tmp | (tmp >> 1)) & 0x3333; /* 00VV00VV00VV00VV */\n\ttmp = (tmp | (tmp >> 2)) & 0x0f0f; /* 0000VVVV0000VVVV */\n\ttmp = (tmp | (tmp >> 4)) & 0x00ff; /* 00000000VVVVVVVV */\n\n\treturn tmp;\n}\n\n#define FPREG_ADDR(f, n)\t((void *)&(f)->st_space + (n) * 16)\n#define FP_EXP_TAG_VALID\t0\n#define FP_EXP_TAG_ZERO\t\t1\n#define FP_EXP_TAG_SPECIAL\t2\n#define FP_EXP_TAG_EMPTY\t3\n\nstatic inline u32 twd_fxsr_to_i387(struct fxregs_state *fxsave)\n{\n\tstruct _fpxreg *st;\n\tu32 tos = (fxsave->swd >> 11) & 7;\n\tu32 twd = (unsigned long) fxsave->twd;\n\tu32 tag;\n\tu32 ret = 0xffff0000u;\n\tint i;\n\n\tfor (i = 0; i < 8; i++, twd >>= 1) {\n\t\tif (twd & 0x1) {\n\t\t\tst = FPREG_ADDR(fxsave, (i - tos) & 7);\n\n\t\t\tswitch (st->exponent & 0x7fff) {\n\t\t\tcase 0x7fff:\n\t\t\t\ttag = FP_EXP_TAG_SPECIAL;\n\t\t\t\tbreak;\n\t\t\tcase 0x0000:\n\t\t\t\tif (!st->significand[0] &&\n\t\t\t\t    !st->significand[1] &&\n\t\t\t\t    !st->significand[2] &&\n\t\t\t\t    !st->significand[3])\n\t\t\t\t\ttag = FP_EXP_TAG_ZERO;\n\t\t\t\telse\n\t\t\t\t\ttag = FP_EXP_TAG_SPECIAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tif (st->significand[3] & 0x8000)\n\t\t\t\t\ttag = FP_EXP_TAG_VALID;\n\t\t\t\telse\n\t\t\t\t\ttag = FP_EXP_TAG_SPECIAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\ttag = FP_EXP_TAG_EMPTY;\n\t\t}\n\t\tret |= tag << (2 * i);\n\t}\n\treturn ret;\n}\n\n/*\n * FXSR floating point environment conversions.\n */\n\nstatic void __convert_from_fxsr(struct user_i387_ia32_struct *env,\n\t\t\t\tstruct task_struct *tsk,\n\t\t\t\tstruct fxregs_state *fxsave)\n{\n\tstruct _fpreg *to = (struct _fpreg *) &env->st_space[0];\n\tstruct _fpxreg *from = (struct _fpxreg *) &fxsave->st_space[0];\n\tint i;\n\n\tenv->cwd = fxsave->cwd | 0xffff0000u;\n\tenv->swd = fxsave->swd | 0xffff0000u;\n\tenv->twd = twd_fxsr_to_i387(fxsave);\n\n#ifdef CONFIG_X86_64\n\tenv->fip = fxsave->rip;\n\tenv->foo = fxsave->rdp;\n\t/*\n\t * should be actually ds/cs at fpu exception time, but\n\t * that information is not available in 64bit mode.\n\t */\n\tenv->fcs = task_pt_regs(tsk)->cs;\n\tif (tsk == current) {\n\t\tsavesegment(ds, env->fos);\n\t} else {\n\t\tenv->fos = tsk->thread.ds;\n\t}\n\tenv->fos |= 0xffff0000;\n#else\n\tenv->fip = fxsave->fip;\n\tenv->fcs = (u16) fxsave->fcs | ((u32) fxsave->fop << 16);\n\tenv->foo = fxsave->foo;\n\tenv->fos = fxsave->fos;\n#endif\n\n\tfor (i = 0; i < 8; ++i)\n\t\tmemcpy(&to[i], &from[i], sizeof(to[0]));\n}\n\nvoid\nconvert_from_fxsr(struct user_i387_ia32_struct *env, struct task_struct *tsk)\n{\n\t__convert_from_fxsr(env, tsk, &tsk->thread.fpu.fpstate->regs.fxsave);\n}\n\nvoid convert_to_fxsr(struct fxregs_state *fxsave,\n\t\t     const struct user_i387_ia32_struct *env)\n\n{\n\tstruct _fpreg *from = (struct _fpreg *) &env->st_space[0];\n\tstruct _fpxreg *to = (struct _fpxreg *) &fxsave->st_space[0];\n\tint i;\n\n\tfxsave->cwd = env->cwd;\n\tfxsave->swd = env->swd;\n\tfxsave->twd = twd_i387_to_fxsr(env->twd);\n\tfxsave->fop = (u16) ((u32) env->fcs >> 16);\n#ifdef CONFIG_X86_64\n\tfxsave->rip = env->fip;\n\tfxsave->rdp = env->foo;\n\t/* cs and ds ignored */\n#else\n\tfxsave->fip = env->fip;\n\tfxsave->fcs = (env->fcs & 0xffff);\n\tfxsave->foo = env->foo;\n\tfxsave->fos = env->fos;\n#endif\n\n\tfor (i = 0; i < 8; ++i)\n\t\tmemcpy(&to[i], &from[i], sizeof(from[0]));\n}\n\nint fpregs_get(struct task_struct *target, const struct user_regset *regset,\n\t       struct membuf to)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct user_i387_ia32_struct env;\n\tstruct fxregs_state fxsave, *fx;\n\n\tsync_fpstate(fpu);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU))\n\t\treturn fpregs_soft_get(target, regset, to);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FXSR)) {\n\t\treturn membuf_write(&to, &fpu->fpstate->regs.fsave,\n\t\t\t\t    sizeof(struct fregs_state));\n\t}\n\n\tif (use_xsave()) {\n\t\tstruct membuf mb = { .p = &fxsave, .left = sizeof(fxsave) };\n\n\t\t/* Handle init state optimized xstate correctly */\n\t\tcopy_xstate_to_uabi_buf(mb, target, XSTATE_COPY_FP);\n\t\tfx = &fxsave;\n\t} else {\n\t\tfx = &fpu->fpstate->regs.fxsave;\n\t}\n\n\t__convert_from_fxsr(&env, target, fx);\n\treturn membuf_write(&to, &env, sizeof(env));\n}\n\nint fpregs_set(struct task_struct *target, const struct user_regset *regset,\n\t       unsigned int pos, unsigned int count,\n\t       const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct user_i387_ia32_struct env;\n\tint ret;\n\n\t/* No funny business with partial or oversized writes is permitted. */\n\tif (pos != 0 || count != sizeof(struct user_i387_ia32_struct))\n\t\treturn -EINVAL;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU))\n\t\treturn fpregs_soft_set(target, regset, pos, count, kbuf, ubuf);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &env, 0, -1);\n\tif (ret)\n\t\treturn ret;\n\n\tfpu_force_restore(fpu);\n\n\tif (cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\tconvert_to_fxsr(&fpu->fpstate->regs.fxsave, &env);\n\telse\n\t\tmemcpy(&fpu->fpstate->regs.fsave, &env, sizeof(env));\n\n\t/*\n\t * Update the header bit in the xsave header, indicating the\n\t * presence of FP.\n\t */\n\tif (cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\tfpu->fpstate->regs.xsave.header.xfeatures |= XFEATURE_MASK_FP;\n\n\treturn 0;\n}\n\n#endif\t/* CONFIG_X86_32 || CONFIG_IA32_EMULATION */\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From a9d9c33132d49329ada647e4514d210d15e31d81 Mon Sep 17 00:00:00 2001\nFrom: Rick Edgecombe <rick.p.edgecombe@intel.com>\nDate: Tue, 7 Jan 2025 15:30:56 -0800\nSubject: [PATCH] x86/fpu: Ensure shadow stack is active before \"getting\"\n registers\n\nThe x86 shadow stack support has its own set of registers. Those registers\nare XSAVE-managed, but they are \"supervisor state components\" which means\nthat userspace can not touch them with XSAVE/XRSTOR.  It also means that\nthey are not accessible from the existing ptrace ABI for XSAVE state.\nThus, there is a new ptrace get/set interface for it.\n\nThe regset code that ptrace uses provides an ->active() handler in\naddition to the get/set ones. For shadow stack this ->active() handler\nverifies that shadow stack is enabled via the ARCH_SHSTK_SHSTK bit in the\nthread struct. The ->active() handler is checked from some call sites of\nthe regset get/set handlers, but not the ptrace ones. This was not\nunderstood when shadow stack support was put in place.\n\nAs a result, both the set/get handlers can be called with\nXFEATURE_CET_USER in its init state, which would cause get_xsave_addr() to\nreturn NULL and trigger a WARN_ON(). The ssp_set() handler luckily has an\nssp_active() check to avoid surprising the kernel with shadow stack\nbehavior when the kernel is not ready for it (ARCH_SHSTK_SHSTK==0). That\ncheck just happened to avoid the warning.\n\nBut the ->get() side wasn't so lucky. It can be called with shadow stacks\ndisabled, triggering the warning in practice, as reported by Christina\nSchimpe:\n\nWARNING: CPU: 5 PID: 1773 at arch/x86/kernel/fpu/regset.c:198 ssp_get+0x89/0xa0\n[...]\nCall Trace:\n<TASK>\n? show_regs+0x6e/0x80\n? ssp_get+0x89/0xa0\n? __warn+0x91/0x150\n? ssp_get+0x89/0xa0\n? report_bug+0x19d/0x1b0\n? handle_bug+0x46/0x80\n? exc_invalid_op+0x1d/0x80\n? asm_exc_invalid_op+0x1f/0x30\n? __pfx_ssp_get+0x10/0x10\n? ssp_get+0x89/0xa0\n? ssp_get+0x52/0xa0\n__regset_get+0xad/0xf0\ncopy_regset_to_user+0x52/0xc0\nptrace_regset+0x119/0x140\nptrace_request+0x13c/0x850\n? wait_task_inactive+0x142/0x1d0\n? do_syscall_64+0x6d/0x90\narch_ptrace+0x102/0x300\n[...]\n\nEnsure that shadow stacks are active in a thread before looking them up\nin the XSAVE buffer. Since ARCH_SHSTK_SHSTK and user_ssp[SHSTK_EN] are\nset at the same time, the active check ensures that there will be\nsomething to find in the XSAVE buffer.\n\n[ dhansen: changelog/subject tweaks ]\n\nFixes: 2fab02b25ae7 (\"x86: Add PTRACE interface for shadow stack\")\nReported-by: Christina Schimpe <christina.schimpe@intel.com>\nSigned-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>\nSigned-off-by: Dave Hansen <dave.hansen@linux.intel.com>\nTested-by: Christina Schimpe <christina.schimpe@intel.com>\nCc:stable@vger.kernel.org\nLink: https://lore.kernel.org/all/20250107233056.235536-1-rick.p.edgecombe%40intel.com\n---\n arch/x86/kernel/fpu/regset.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c\nindex 6bc1eb2a21bd..887b0b8e21e3 100644\n--- a/arch/x86/kernel/fpu/regset.c\n+++ b/arch/x86/kernel/fpu/regset.c\n@@ -190,7 +190,8 @@ int ssp_get(struct task_struct *target, const struct user_regset *regset,\n \tstruct fpu *fpu = &target->thread.fpu;\n \tstruct cet_user_state *cetregs;\n \n-\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK))\n+\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK) ||\n+\t    !ssp_active(target, regset))\n \t\treturn -ENODEV;\n \n \tsync_fpstate(fpu);\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "arch/x86/kernel/fpu/regset.c": "// SPDX-License-Identifier: GPL-2.0\n/*\n * FPU register's regset abstraction, for ptrace, core dumps, etc.\n */\n#include <linux/sched/task_stack.h>\n#include <linux/vmalloc.h>\n\n#include <asm/fpu/api.h>\n#include <asm/fpu/signal.h>\n#include <asm/fpu/regset.h>\n#include <asm/prctl.h>\n\n#include \"context.h\"\n#include \"internal.h\"\n#include \"legacy.h\"\n#include \"xstate.h\"\n\n/*\n * The xstateregs_active() routine is the same as the regset_fpregs_active() routine,\n * as the \"regset->n\" for the xstate regset will be updated based on the feature\n * capabilities supported by the xsave.\n */\nint regset_fpregs_active(struct task_struct *target, const struct user_regset *regset)\n{\n\treturn regset->n;\n}\n\nint regset_xregset_fpregs_active(struct task_struct *target, const struct user_regset *regset)\n{\n\tif (boot_cpu_has(X86_FEATURE_FXSR))\n\t\treturn regset->n;\n\telse\n\t\treturn 0;\n}\n\n/*\n * The regset get() functions are invoked from:\n *\n *   - coredump to dump the current task's fpstate. If the current task\n *     owns the FPU then the memory state has to be synchronized and the\n *     FPU register state preserved. Otherwise fpstate is already in sync.\n *\n *   - ptrace to dump fpstate of a stopped task, in which case the registers\n *     have already been saved to fpstate on context switch.\n */\nstatic void sync_fpstate(struct fpu *fpu)\n{\n\tif (fpu == &current->thread.fpu)\n\t\tfpu_sync_fpstate(fpu);\n}\n\n/*\n * Invalidate cached FPU registers before modifying the stopped target\n * task's fpstate.\n *\n * This forces the target task on resume to restore the FPU registers from\n * modified fpstate. Otherwise the task might skip the restore and operate\n * with the cached FPU registers which discards the modifications.\n */\nstatic void fpu_force_restore(struct fpu *fpu)\n{\n\t/*\n\t * Only stopped child tasks can be used to modify the FPU\n\t * state in the fpstate buffer:\n\t */\n\tWARN_ON_FPU(fpu == &current->thread.fpu);\n\n\t__fpu_invalidate_fpregs_state(fpu);\n}\n\nint xfpregs_get(struct task_struct *target, const struct user_regset *regset,\n\t\tstruct membuf to)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\treturn -ENODEV;\n\n\tsync_fpstate(fpu);\n\n\tif (!use_xsave()) {\n\t\treturn membuf_write(&to, &fpu->fpstate->regs.fxsave,\n\t\t\t\t    sizeof(fpu->fpstate->regs.fxsave));\n\t}\n\n\tcopy_xstate_to_uabi_buf(to, target, XSTATE_COPY_FX);\n\treturn 0;\n}\n\nint xfpregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\tunsigned int pos, unsigned int count,\n\t\tconst void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct fxregs_state newstate;\n\tint ret;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\treturn -ENODEV;\n\n\t/* No funny business with partial or oversized writes is permitted. */\n\tif (pos != 0 || count != sizeof(newstate))\n\t\treturn -EINVAL;\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate, 0, -1);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Do not allow an invalid MXCSR value. */\n\tif (newstate.mxcsr & ~mxcsr_feature_mask)\n\t\treturn -EINVAL;\n\n\tfpu_force_restore(fpu);\n\n\t/* Copy the state  */\n\tmemcpy(&fpu->fpstate->regs.fxsave, &newstate, sizeof(newstate));\n\n\t/* Clear xmm8..15 for 32-bit callers */\n\tBUILD_BUG_ON(sizeof(fpu->__fpstate.regs.fxsave.xmm_space) != 16 * 16);\n\tif (in_ia32_syscall())\n\t\tmemset(&fpu->fpstate->regs.fxsave.xmm_space[8*4], 0, 8 * 16);\n\n\t/* Mark FP and SSE as in use when XSAVE is enabled */\n\tif (use_xsave())\n\t\tfpu->fpstate->regs.xsave.header.xfeatures |= XFEATURE_MASK_FPSSE;\n\n\treturn 0;\n}\n\nint xstateregs_get(struct task_struct *target, const struct user_regset *regset,\n\t\tstruct membuf to)\n{\n\tif (!cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\tsync_fpstate(&target->thread.fpu);\n\n\tcopy_xstate_to_uabi_buf(to, target, XSTATE_COPY_XSAVE);\n\treturn 0;\n}\n\nint xstateregs_set(struct task_struct *target, const struct user_regset *regset,\n\t\t  unsigned int pos, unsigned int count,\n\t\t  const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *tmpbuf = NULL;\n\tint ret;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\treturn -ENODEV;\n\n\t/*\n\t * A whole standard-format XSAVE buffer is needed:\n\t */\n\tif (pos != 0 || count != fpu_user_cfg.max_size)\n\t\treturn -EFAULT;\n\n\tif (!kbuf) {\n\t\ttmpbuf = vmalloc(count);\n\t\tif (!tmpbuf)\n\t\t\treturn -ENOMEM;\n\n\t\tif (copy_from_user(tmpbuf, ubuf, count)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfpu_force_restore(fpu);\n\tret = copy_uabi_from_kernel_to_xstate(fpu->fpstate, kbuf ?: tmpbuf, &target->thread.pkru);\n\nout:\n\tvfree(tmpbuf);\n\treturn ret;\n}\n\n#ifdef CONFIG_X86_USER_SHADOW_STACK\nint ssp_active(struct task_struct *target, const struct user_regset *regset)\n{\n\tif (target->thread.features & ARCH_SHSTK_SHSTK)\n\t\treturn regset->n;\n\n\treturn 0;\n}\n\nint ssp_get(struct task_struct *target, const struct user_regset *regset,\n\t    struct membuf to)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct cet_user_state *cetregs;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK))\n\t\treturn -ENODEV;\n\n\tsync_fpstate(fpu);\n\tcetregs = get_xsave_addr(&fpu->fpstate->regs.xsave, XFEATURE_CET_USER);\n\tif (WARN_ON(!cetregs)) {\n\t\t/*\n\t\t * This shouldn't ever be NULL because shadow stack was\n\t\t * verified to be enabled above. This means\n\t\t * MSR_IA32_U_CET.CET_SHSTK_EN should be 1 and so\n\t\t * XFEATURE_CET_USER should not be in the init state.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\treturn membuf_write(&to, (unsigned long *)&cetregs->user_ssp,\n\t\t\t    sizeof(cetregs->user_ssp));\n}\n\nint ssp_set(struct task_struct *target, const struct user_regset *regset,\n\t    unsigned int pos, unsigned int count,\n\t    const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct xregs_state *xsave = &fpu->fpstate->regs.xsave;\n\tstruct cet_user_state *cetregs;\n\tunsigned long user_ssp;\n\tint r;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_USER_SHSTK) ||\n\t    !ssp_active(target, regset))\n\t\treturn -ENODEV;\n\n\tif (pos != 0 || count != sizeof(user_ssp))\n\t\treturn -EINVAL;\n\n\tr = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &user_ssp, 0, -1);\n\tif (r)\n\t\treturn r;\n\n\t/*\n\t * Some kernel instructions (IRET, etc) can cause exceptions in the case\n\t * of disallowed CET register values. Just prevent invalid values.\n\t */\n\tif (user_ssp >= TASK_SIZE_MAX || !IS_ALIGNED(user_ssp, 8))\n\t\treturn -EINVAL;\n\n\tfpu_force_restore(fpu);\n\n\tcetregs = get_xsave_addr(xsave, XFEATURE_CET_USER);\n\tif (WARN_ON(!cetregs)) {\n\t\t/*\n\t\t * This shouldn't ever be NULL because shadow stack was\n\t\t * verified to be enabled above. This means\n\t\t * MSR_IA32_U_CET.CET_SHSTK_EN should be 1 and so\n\t\t * XFEATURE_CET_USER should not be in the init state.\n\t\t */\n\t\treturn -ENODEV;\n\t}\n\n\tcetregs->user_ssp = user_ssp;\n\treturn 0;\n}\n#endif /* CONFIG_X86_USER_SHADOW_STACK */\n\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\n/*\n * FPU tag word conversions.\n */\n\nstatic inline unsigned short twd_i387_to_fxsr(unsigned short twd)\n{\n\tunsigned int tmp; /* to avoid 16 bit prefixes in the code */\n\n\t/* Transform each pair of bits into 01 (valid) or 00 (empty) */\n\ttmp = ~twd;\n\ttmp = (tmp | (tmp>>1)) & 0x5555; /* 0V0V0V0V0V0V0V0V */\n\t/* and move the valid bits to the lower byte. */\n\ttmp = (tmp | (tmp >> 1)) & 0x3333; /* 00VV00VV00VV00VV */\n\ttmp = (tmp | (tmp >> 2)) & 0x0f0f; /* 0000VVVV0000VVVV */\n\ttmp = (tmp | (tmp >> 4)) & 0x00ff; /* 00000000VVVVVVVV */\n\n\treturn tmp;\n}\n\n#define FPREG_ADDR(f, n)\t((void *)&(f)->st_space + (n) * 16)\n#define FP_EXP_TAG_VALID\t0\n#define FP_EXP_TAG_ZERO\t\t1\n#define FP_EXP_TAG_SPECIAL\t2\n#define FP_EXP_TAG_EMPTY\t3\n\nstatic inline u32 twd_fxsr_to_i387(struct fxregs_state *fxsave)\n{\n\tstruct _fpxreg *st;\n\tu32 tos = (fxsave->swd >> 11) & 7;\n\tu32 twd = (unsigned long) fxsave->twd;\n\tu32 tag;\n\tu32 ret = 0xffff0000u;\n\tint i;\n\n\tfor (i = 0; i < 8; i++, twd >>= 1) {\n\t\tif (twd & 0x1) {\n\t\t\tst = FPREG_ADDR(fxsave, (i - tos) & 7);\n\n\t\t\tswitch (st->exponent & 0x7fff) {\n\t\t\tcase 0x7fff:\n\t\t\t\ttag = FP_EXP_TAG_SPECIAL;\n\t\t\t\tbreak;\n\t\t\tcase 0x0000:\n\t\t\t\tif (!st->significand[0] &&\n\t\t\t\t    !st->significand[1] &&\n\t\t\t\t    !st->significand[2] &&\n\t\t\t\t    !st->significand[3])\n\t\t\t\t\ttag = FP_EXP_TAG_ZERO;\n\t\t\t\telse\n\t\t\t\t\ttag = FP_EXP_TAG_SPECIAL;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tif (st->significand[3] & 0x8000)\n\t\t\t\t\ttag = FP_EXP_TAG_VALID;\n\t\t\t\telse\n\t\t\t\t\ttag = FP_EXP_TAG_SPECIAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\ttag = FP_EXP_TAG_EMPTY;\n\t\t}\n\t\tret |= tag << (2 * i);\n\t}\n\treturn ret;\n}\n\n/*\n * FXSR floating point environment conversions.\n */\n\nstatic void __convert_from_fxsr(struct user_i387_ia32_struct *env,\n\t\t\t\tstruct task_struct *tsk,\n\t\t\t\tstruct fxregs_state *fxsave)\n{\n\tstruct _fpreg *to = (struct _fpreg *) &env->st_space[0];\n\tstruct _fpxreg *from = (struct _fpxreg *) &fxsave->st_space[0];\n\tint i;\n\n\tenv->cwd = fxsave->cwd | 0xffff0000u;\n\tenv->swd = fxsave->swd | 0xffff0000u;\n\tenv->twd = twd_fxsr_to_i387(fxsave);\n\n#ifdef CONFIG_X86_64\n\tenv->fip = fxsave->rip;\n\tenv->foo = fxsave->rdp;\n\t/*\n\t * should be actually ds/cs at fpu exception time, but\n\t * that information is not available in 64bit mode.\n\t */\n\tenv->fcs = task_pt_regs(tsk)->cs;\n\tif (tsk == current) {\n\t\tsavesegment(ds, env->fos);\n\t} else {\n\t\tenv->fos = tsk->thread.ds;\n\t}\n\tenv->fos |= 0xffff0000;\n#else\n\tenv->fip = fxsave->fip;\n\tenv->fcs = (u16) fxsave->fcs | ((u32) fxsave->fop << 16);\n\tenv->foo = fxsave->foo;\n\tenv->fos = fxsave->fos;\n#endif\n\n\tfor (i = 0; i < 8; ++i)\n\t\tmemcpy(&to[i], &from[i], sizeof(to[0]));\n}\n\nvoid\nconvert_from_fxsr(struct user_i387_ia32_struct *env, struct task_struct *tsk)\n{\n\t__convert_from_fxsr(env, tsk, &tsk->thread.fpu.fpstate->regs.fxsave);\n}\n\nvoid convert_to_fxsr(struct fxregs_state *fxsave,\n\t\t     const struct user_i387_ia32_struct *env)\n\n{\n\tstruct _fpreg *from = (struct _fpreg *) &env->st_space[0];\n\tstruct _fpxreg *to = (struct _fpxreg *) &fxsave->st_space[0];\n\tint i;\n\n\tfxsave->cwd = env->cwd;\n\tfxsave->swd = env->swd;\n\tfxsave->twd = twd_i387_to_fxsr(env->twd);\n\tfxsave->fop = (u16) ((u32) env->fcs >> 16);\n#ifdef CONFIG_X86_64\n\tfxsave->rip = env->fip;\n\tfxsave->rdp = env->foo;\n\t/* cs and ds ignored */\n#else\n\tfxsave->fip = env->fip;\n\tfxsave->fcs = (env->fcs & 0xffff);\n\tfxsave->foo = env->foo;\n\tfxsave->fos = env->fos;\n#endif\n\n\tfor (i = 0; i < 8; ++i)\n\t\tmemcpy(&to[i], &from[i], sizeof(from[0]));\n}\n\nint fpregs_get(struct task_struct *target, const struct user_regset *regset,\n\t       struct membuf to)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct user_i387_ia32_struct env;\n\tstruct fxregs_state fxsave, *fx;\n\n\tsync_fpstate(fpu);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU))\n\t\treturn fpregs_soft_get(target, regset, to);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FXSR)) {\n\t\treturn membuf_write(&to, &fpu->fpstate->regs.fsave,\n\t\t\t\t    sizeof(struct fregs_state));\n\t}\n\n\tif (use_xsave()) {\n\t\tstruct membuf mb = { .p = &fxsave, .left = sizeof(fxsave) };\n\n\t\t/* Handle init state optimized xstate correctly */\n\t\tcopy_xstate_to_uabi_buf(mb, target, XSTATE_COPY_FP);\n\t\tfx = &fxsave;\n\t} else {\n\t\tfx = &fpu->fpstate->regs.fxsave;\n\t}\n\n\t__convert_from_fxsr(&env, target, fx);\n\treturn membuf_write(&to, &env, sizeof(env));\n}\n\nint fpregs_set(struct task_struct *target, const struct user_regset *regset,\n\t       unsigned int pos, unsigned int count,\n\t       const void *kbuf, const void __user *ubuf)\n{\n\tstruct fpu *fpu = &target->thread.fpu;\n\tstruct user_i387_ia32_struct env;\n\tint ret;\n\n\t/* No funny business with partial or oversized writes is permitted. */\n\tif (pos != 0 || count != sizeof(struct user_i387_ia32_struct))\n\t\treturn -EINVAL;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU))\n\t\treturn fpregs_soft_set(target, regset, pos, count, kbuf, ubuf);\n\n\tret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &env, 0, -1);\n\tif (ret)\n\t\treturn ret;\n\n\tfpu_force_restore(fpu);\n\n\tif (cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\tconvert_to_fxsr(&fpu->fpstate->regs.fxsave, &env);\n\telse\n\t\tmemcpy(&fpu->fpstate->regs.fsave, &env, sizeof(env));\n\n\t/*\n\t * Update the header bit in the xsave header, indicating the\n\t * presence of FP.\n\t */\n\tif (cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\tfpu->fpstate->regs.xsave.header.xfeatures |= XFEATURE_MASK_FP;\n\n\treturn 0;\n}\n\n#endif\t/* CONFIG_X86_32 || CONFIG_IA32_EMULATION */\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21633",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21633.json",
            "patch_attempts": [
                {
                    "upstream_commit": "c9a40292a44e78f71258b8522655bffaf5753bdb",
                    "upstream_commit_date": "2025-01-10 14:00:19 -0700",
                    "upstream_patch": "4b7cfa8b6c28a9fa22b86894166a1a34f6d630ba",
                    "total_versions_tested": 1,
                    "successful_patches": 1,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "aa7496d668c30ca7421b3bfdcd948ee861a13d17",
                            "downstream_commit": "2b30bffd9a77a346343906b5f79f54bdee229eb8",
                            "commit_date": "2025-01-17 13:40:52 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file io_uring/sqpoll.c\nHunk #1 succeeded at 275 (offset 7 lines).",
                            "downstream_patch_content": "commit aa7496d668c30ca7421b3bfdcd948ee861a13d17\nAuthor: Pavel Begunkov <asml.silence@gmail.com>\nDate:   Fri Jan 10 14:31:23 2025 +0000\n\n    io_uring/sqpoll: zero sqd->thread on tctx errors\n    \n    commit 4b7cfa8b6c28a9fa22b86894166a1a34f6d630ba upstream.\n    \n    Syzkeller reports:\n    \n    BUG: KASAN: slab-use-after-free in thread_group_cputime+0x409/0x700 kernel/sched/cputime.c:341\n    Read of size 8 at addr ffff88803578c510 by task syz.2.3223/27552\n     Call Trace:\n      <TASK>\n      ...\n      kasan_report+0x143/0x180 mm/kasan/report.c:602\n      thread_group_cputime+0x409/0x700 kernel/sched/cputime.c:341\n      thread_group_cputime_adjusted+0xa6/0x340 kernel/sched/cputime.c:639\n      getrusage+0x1000/0x1340 kernel/sys.c:1863\n      io_uring_show_fdinfo+0xdfe/0x1770 io_uring/fdinfo.c:197\n      seq_show+0x608/0x770 fs/proc/fd.c:68\n      ...\n    \n    That's due to sqd->task not being cleared properly in cases where\n    SQPOLL task tctx setup fails, which can essentially only happen with\n    fault injection to insert allocation errors.\n    \n    Cc: stable@vger.kernel.org\n    Fixes: 1251d2025c3e1 (\"io_uring/sqpoll: early exit thread if task_context wasn't allocated\")\n    Reported-by: syzbot+3d92cfcfa84070b0a470@syzkaller.appspotmail.com\n    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>\n    Link: https://lore.kernel.org/r/efc7ec7010784463b2e7466d7b5c02c2cb381635.1736519461.git.asml.silence@gmail.com\n    Signed-off-by: Jens Axboe <axboe@kernel.dk>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/io_uring/sqpoll.c b/io_uring/sqpoll.c\nindex 1cfcc735b8e3..5bc54c6df20f 100644\n--- a/io_uring/sqpoll.c\n+++ b/io_uring/sqpoll.c\n@@ -275,8 +275,12 @@ static int io_sq_thread(void *data)\n \tDEFINE_WAIT(wait);\n \n \t/* offload context creation failed, just exit */\n-\tif (!current->io_uring)\n+\tif (!current->io_uring) {\n+\t\tmutex_lock(&sqd->lock);\n+\t\tsqd->thread = NULL;\n+\t\tmutex_unlock(&sqd->lock);\n \t\tgoto err_out;\n+\t}\n \n \tsnprintf(buf, sizeof(buf), \"iou-sqp-%d\", sqd->task_pid);\n \tset_task_comm(current, buf);\n",
                            "downstream_file_content": {}
                        }
                    ],
                    "upstream_patch_content": "From 4b7cfa8b6c28a9fa22b86894166a1a34f6d630ba Mon Sep 17 00:00:00 2001\nFrom: Pavel Begunkov <asml.silence@gmail.com>\nDate: Fri, 10 Jan 2025 14:31:23 +0000\nSubject: [PATCH] io_uring/sqpoll: zero sqd->thread on tctx errors\n\nSyzkeller reports:\n\nBUG: KASAN: slab-use-after-free in thread_group_cputime+0x409/0x700 kernel/sched/cputime.c:341\nRead of size 8 at addr ffff88803578c510 by task syz.2.3223/27552\n Call Trace:\n  <TASK>\n  ...\n  kasan_report+0x143/0x180 mm/kasan/report.c:602\n  thread_group_cputime+0x409/0x700 kernel/sched/cputime.c:341\n  thread_group_cputime_adjusted+0xa6/0x340 kernel/sched/cputime.c:639\n  getrusage+0x1000/0x1340 kernel/sys.c:1863\n  io_uring_show_fdinfo+0xdfe/0x1770 io_uring/fdinfo.c:197\n  seq_show+0x608/0x770 fs/proc/fd.c:68\n  ...\n\nThat's due to sqd->task not being cleared properly in cases where\nSQPOLL task tctx setup fails, which can essentially only happen with\nfault injection to insert allocation errors.\n\nCc: stable@vger.kernel.org\nFixes: 1251d2025c3e1 (\"io_uring/sqpoll: early exit thread if task_context wasn't allocated\")\nReported-by: syzbot+3d92cfcfa84070b0a470@syzkaller.appspotmail.com\nSigned-off-by: Pavel Begunkov <asml.silence@gmail.com>\nLink: https://lore.kernel.org/r/efc7ec7010784463b2e7466d7b5c02c2cb381635.1736519461.git.asml.silence@gmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>\n---\n io_uring/sqpoll.c | 6 +++++-\n 1 file changed, 5 insertions(+), 1 deletion(-)\n\ndiff --git a/io_uring/sqpoll.c b/io_uring/sqpoll.c\nindex 9e5bd79fd2b5..8961a3c1e73c 100644\n--- a/io_uring/sqpoll.c\n+++ b/io_uring/sqpoll.c\n@@ -268,8 +268,12 @@ static int io_sq_thread(void *data)\n \tDEFINE_WAIT(wait);\n \n \t/* offload context creation failed, just exit */\n-\tif (!current->io_uring)\n+\tif (!current->io_uring) {\n+\t\tmutex_lock(&sqd->lock);\n+\t\tsqd->thread = NULL;\n+\t\tmutex_unlock(&sqd->lock);\n \t\tgoto err_out;\n+\t}\n \n \tsnprintf(buf, sizeof(buf), \"iou-sqp-%d\", sqd->task_pid);\n \tset_task_comm(current, buf);\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "io_uring/sqpoll.c": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Contains the core associated with submission side polling of the SQ\n * ring, offloading submissions from the application to a kernel thread.\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/audit.h>\n#include <linux/security.h>\n#include <linux/cpuset.h>\n#include <linux/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"napi.h\"\n#include \"sqpoll.h\"\n\n#define IORING_SQPOLL_CAP_ENTRIES_VALUE 8\n#define IORING_TW_CAP_ENTRIES_VALUE\t8\n\nenum {\n\tIO_SQ_THREAD_SHOULD_STOP = 0,\n\tIO_SQ_THREAD_SHOULD_PARK,\n};\n\nvoid io_sq_thread_unpark(struct io_sq_data *sqd)\n\t__releases(&sqd->lock)\n{\n\tWARN_ON_ONCE(sqd->thread == current);\n\n\t/*\n\t * Do the dance but not conditional clear_bit() because it'd race with\n\t * other threads incrementing park_pending and setting the bit.\n\t */\n\tclear_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\tif (atomic_dec_return(&sqd->park_pending))\n\t\tset_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\tmutex_unlock(&sqd->lock);\n}\n\nvoid io_sq_thread_park(struct io_sq_data *sqd)\n\t__acquires(&sqd->lock)\n{\n\tWARN_ON_ONCE(data_race(sqd->thread) == current);\n\n\tatomic_inc(&sqd->park_pending);\n\tset_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state);\n\tmutex_lock(&sqd->lock);\n\tif (sqd->thread)\n\t\twake_up_process(sqd->thread);\n}\n\nvoid io_sq_thread_stop(struct io_sq_data *sqd)\n{\n\tWARN_ON_ONCE(sqd->thread == current);\n\tWARN_ON_ONCE(test_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state));\n\n\tset_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n\tmutex_lock(&sqd->lock);\n\tif (sqd->thread)\n\t\twake_up_process(sqd->thread);\n\tmutex_unlock(&sqd->lock);\n\twait_for_completion(&sqd->exited);\n}\n\nvoid io_put_sq_data(struct io_sq_data *sqd)\n{\n\tif (refcount_dec_and_test(&sqd->refs)) {\n\t\tWARN_ON_ONCE(atomic_read(&sqd->park_pending));\n\n\t\tio_sq_thread_stop(sqd);\n\t\tkfree(sqd);\n\t}\n}\n\nstatic __cold void io_sqd_update_thread_idle(struct io_sq_data *sqd)\n{\n\tstruct io_ring_ctx *ctx;\n\tunsigned sq_thread_idle = 0;\n\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\tsq_thread_idle = max(sq_thread_idle, ctx->sq_thread_idle);\n\tsqd->sq_thread_idle = sq_thread_idle;\n}\n\nvoid io_sq_thread_finish(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (sqd) {\n\t\tio_sq_thread_park(sqd);\n\t\tlist_del_init(&ctx->sqd_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tio_put_sq_data(sqd);\n\t\tctx->sq_data = NULL;\n\t}\n}\n\nstatic struct io_sq_data *io_attach_sq_data(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx_attach;\n\tstruct io_sq_data *sqd;\n\tstruct fd f;\n\n\tf = fdget(p->wq_fd);\n\tif (!fd_file(f))\n\t\treturn ERR_PTR(-ENXIO);\n\tif (!io_is_uring_fops(fd_file(f))) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tctx_attach = fd_file(f)->private_data;\n\tsqd = ctx_attach->sq_data;\n\tif (!sqd) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tif (sqd->task_tgid != current->tgid) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EPERM);\n\t}\n\n\trefcount_inc(&sqd->refs);\n\tfdput(f);\n\treturn sqd;\n}\n\nstatic struct io_sq_data *io_get_sq_data(struct io_uring_params *p,\n\t\t\t\t\t bool *attached)\n{\n\tstruct io_sq_data *sqd;\n\n\t*attached = false;\n\tif (p->flags & IORING_SETUP_ATTACH_WQ) {\n\t\tsqd = io_attach_sq_data(p);\n\t\tif (!IS_ERR(sqd)) {\n\t\t\t*attached = true;\n\t\t\treturn sqd;\n\t\t}\n\t\t/* fall through for EPERM case, setup new sqd/task */\n\t\tif (PTR_ERR(sqd) != -EPERM)\n\t\t\treturn sqd;\n\t}\n\n\tsqd = kzalloc(sizeof(*sqd), GFP_KERNEL);\n\tif (!sqd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tatomic_set(&sqd->park_pending, 0);\n\trefcount_set(&sqd->refs, 1);\n\tINIT_LIST_HEAD(&sqd->ctx_list);\n\tmutex_init(&sqd->lock);\n\tinit_waitqueue_head(&sqd->wait);\n\tinit_completion(&sqd->exited);\n\treturn sqd;\n}\n\nstatic inline bool io_sqd_events_pending(struct io_sq_data *sqd)\n{\n\treturn READ_ONCE(sqd->state);\n}\n\nstatic int __io_sq_thread(struct io_ring_ctx *ctx, bool cap_entries)\n{\n\tunsigned int to_submit;\n\tint ret = 0;\n\n\tto_submit = io_sqring_entries(ctx);\n\t/* if we're handling multiple rings, cap submit size for fairness */\n\tif (cap_entries && to_submit > IORING_SQPOLL_CAP_ENTRIES_VALUE)\n\t\tto_submit = IORING_SQPOLL_CAP_ENTRIES_VALUE;\n\n\tif (to_submit || !wq_list_empty(&ctx->iopoll_list)) {\n\t\tconst struct cred *creds = NULL;\n\n\t\tif (ctx->sq_creds != current_cred())\n\t\t\tcreds = override_creds(ctx->sq_creds);\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tif (!wq_list_empty(&ctx->iopoll_list))\n\t\t\tio_do_iopoll(ctx, true);\n\n\t\t/*\n\t\t * Don't submit if refs are dying, good for io_uring_register(),\n\t\t * but also it is relied upon by io_ring_exit_work()\n\t\t */\n\t\tif (to_submit && likely(!percpu_ref_is_dying(&ctx->refs)) &&\n\t\t    !(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tif (to_submit && wq_has_sleeper(&ctx->sqo_sq_wait))\n\t\t\twake_up(&ctx->sqo_sq_wait);\n\t\tif (creds)\n\t\t\trevert_creds(creds);\n\t}\n\n\treturn ret;\n}\n\nstatic bool io_sqd_handle_event(struct io_sq_data *sqd)\n{\n\tbool did_sig = false;\n\tstruct ksignal ksig;\n\n\tif (test_bit(IO_SQ_THREAD_SHOULD_PARK, &sqd->state) ||\n\t    signal_pending(current)) {\n\t\tmutex_unlock(&sqd->lock);\n\t\tif (signal_pending(current))\n\t\t\tdid_sig = get_signal(&ksig);\n\t\tcond_resched();\n\t\tmutex_lock(&sqd->lock);\n\t\tsqd->sq_cpu = raw_smp_processor_id();\n\t}\n\treturn did_sig || test_bit(IO_SQ_THREAD_SHOULD_STOP, &sqd->state);\n}\n\n/*\n * Run task_work, processing the retry_list first. The retry_list holds\n * entries that we passed on in the previous run, if we had more task_work\n * than we were asked to process. Newly queued task_work isn't run until the\n * retry list has been fully processed.\n */\nstatic unsigned int io_sq_tw(struct llist_node **retry_list, int max_entries)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tunsigned int count = 0;\n\n\tif (*retry_list) {\n\t\t*retry_list = io_handle_tw_list(*retry_list, &count, max_entries);\n\t\tif (count >= max_entries)\n\t\t\tgoto out;\n\t\tmax_entries -= count;\n\t}\n\t*retry_list = tctx_task_work_run(tctx, max_entries, &count);\nout:\n\tif (task_work_pending(current))\n\t\ttask_work_run();\n\treturn count;\n}\n\nstatic bool io_sq_tw_pending(struct llist_node *retry_list)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\treturn retry_list || !llist_empty(&tctx->task_list);\n}\n\nstatic void io_sq_update_worktime(struct io_sq_data *sqd, struct rusage *start)\n{\n\tstruct rusage end;\n\n\tgetrusage(current, RUSAGE_SELF, &end);\n\tend.ru_stime.tv_sec -= start->ru_stime.tv_sec;\n\tend.ru_stime.tv_usec -= start->ru_stime.tv_usec;\n\n\tsqd->work_time += end.ru_stime.tv_usec + end.ru_stime.tv_sec * 1000000;\n}\n\nstatic int io_sq_thread(void *data)\n{\n\tstruct llist_node *retry_list = NULL;\n\tstruct io_sq_data *sqd = data;\n\tstruct io_ring_ctx *ctx;\n\tstruct rusage start;\n\tunsigned long timeout = 0;\n\tchar buf[TASK_COMM_LEN];\n\tDEFINE_WAIT(wait);\n\n\t/* offload context creation failed, just exit */\n\tif (!current->io_uring)\n\t\tgoto err_out;\n\n\tsnprintf(buf, sizeof(buf), \"iou-sqp-%d\", sqd->task_pid);\n\tset_task_comm(current, buf);\n\n\t/* reset to our pid after we've set task_comm, for fdinfo */\n\tsqd->task_pid = current->pid;\n\n\tif (sqd->sq_cpu != -1) {\n\t\tset_cpus_allowed_ptr(current, cpumask_of(sqd->sq_cpu));\n\t} else {\n\t\tset_cpus_allowed_ptr(current, cpu_online_mask);\n\t\tsqd->sq_cpu = raw_smp_processor_id();\n\t}\n\n\t/*\n\t * Force audit context to get setup, in case we do prep side async\n\t * operations that would trigger an audit call before any issue side\n\t * audit has been done.\n\t */\n\taudit_uring_entry(IORING_OP_NOP);\n\taudit_uring_exit(true, 0);\n\n\tmutex_lock(&sqd->lock);\n\twhile (1) {\n\t\tbool cap_entries, sqt_spin = false;\n\n\t\tif (io_sqd_events_pending(sqd) || signal_pending(current)) {\n\t\t\tif (io_sqd_handle_event(sqd))\n\t\t\t\tbreak;\n\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t}\n\n\t\tcap_entries = !list_is_singular(&sqd->ctx_list);\n\t\tgetrusage(current, RUSAGE_SELF, &start);\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\tint ret = __io_sq_thread(ctx, cap_entries);\n\n\t\t\tif (!sqt_spin && (ret > 0 || !wq_list_empty(&ctx->iopoll_list)))\n\t\t\t\tsqt_spin = true;\n\t\t}\n\t\tif (io_sq_tw(&retry_list, IORING_TW_CAP_ENTRIES_VALUE))\n\t\t\tsqt_spin = true;\n\n\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\tif (io_napi(ctx))\n\t\t\t\tio_napi_sqpoll_busy_poll(ctx);\n\n\t\tif (sqt_spin || !time_after(jiffies, timeout)) {\n\t\t\tif (sqt_spin) {\n\t\t\t\tio_sq_update_worktime(sqd, &start);\n\t\t\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t\t\t}\n\t\t\tif (unlikely(need_resched())) {\n\t\t\t\tmutex_unlock(&sqd->lock);\n\t\t\t\tcond_resched();\n\t\t\t\tmutex_lock(&sqd->lock);\n\t\t\t\tsqd->sq_cpu = raw_smp_processor_id();\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tprepare_to_wait(&sqd->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tif (!io_sqd_events_pending(sqd) && !io_sq_tw_pending(retry_list)) {\n\t\t\tbool needs_sched = true;\n\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list) {\n\t\t\t\tatomic_or(IORING_SQ_NEED_WAKEUP,\n\t\t\t\t\t\t&ctx->rings->sq_flags);\n\t\t\t\tif ((ctx->flags & IORING_SETUP_IOPOLL) &&\n\t\t\t\t    !wq_list_empty(&ctx->iopoll_list)) {\n\t\t\t\t\tneeds_sched = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * Ensure the store of the wakeup flag is not\n\t\t\t\t * reordered with the load of the SQ tail\n\t\t\t\t */\n\t\t\t\tsmp_mb__after_atomic();\n\n\t\t\t\tif (io_sqring_entries(ctx)) {\n\t\t\t\t\tneeds_sched = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (needs_sched) {\n\t\t\t\tmutex_unlock(&sqd->lock);\n\t\t\t\tschedule();\n\t\t\t\tmutex_lock(&sqd->lock);\n\t\t\t\tsqd->sq_cpu = raw_smp_processor_id();\n\t\t\t}\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tatomic_andnot(IORING_SQ_NEED_WAKEUP,\n\t\t\t\t\t\t&ctx->rings->sq_flags);\n\t\t}\n\n\t\tfinish_wait(&sqd->wait, &wait);\n\t\ttimeout = jiffies + sqd->sq_thread_idle;\n\t}\n\n\tif (retry_list)\n\t\tio_sq_tw(&retry_list, UINT_MAX);\n\n\tio_uring_cancel_generic(true, sqd);\n\tsqd->thread = NULL;\n\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\tatomic_or(IORING_SQ_NEED_WAKEUP, &ctx->rings->sq_flags);\n\tio_run_task_work();\n\tmutex_unlock(&sqd->lock);\nerr_out:\n\tcomplete(&sqd->exited);\n\tdo_exit(0);\n}\n\nvoid io_sqpoll_wait_sq(struct io_ring_ctx *ctx)\n{\n\tDEFINE_WAIT(wait);\n\n\tdo {\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tprepare_to_wait(&ctx->sqo_sq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tif (!io_sqring_full(ctx))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (!signal_pending(current));\n\n\tfinish_wait(&ctx->sqo_sq_wait, &wait);\n}\n\n__cold int io_sq_offload_create(struct io_ring_ctx *ctx,\n\t\t\t\tstruct io_uring_params *p)\n{\n\tstruct task_struct *task_to_put = NULL;\n\tint ret;\n\n\t/* Retain compatibility with failing for an invalid attach attempt */\n\tif ((ctx->flags & (IORING_SETUP_ATTACH_WQ | IORING_SETUP_SQPOLL)) ==\n\t\t\t\tIORING_SETUP_ATTACH_WQ) {\n\t\tstruct fd f;\n\n\t\tf = fdget(p->wq_fd);\n\t\tif (!fd_file(f))\n\t\t\treturn -ENXIO;\n\t\tif (!io_is_uring_fops(fd_file(f))) {\n\t\t\tfdput(f);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfdput(f);\n\t}\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tstruct task_struct *tsk;\n\t\tstruct io_sq_data *sqd;\n\t\tbool attached;\n\n\t\tret = security_uring_sqpoll();\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tsqd = io_get_sq_data(p, &attached);\n\t\tif (IS_ERR(sqd)) {\n\t\t\tret = PTR_ERR(sqd);\n\t\t\tgoto err;\n\t\t}\n\n\t\tctx->sq_creds = get_current_cred();\n\t\tctx->sq_data = sqd;\n\t\tctx->sq_thread_idle = msecs_to_jiffies(p->sq_thread_idle);\n\t\tif (!ctx->sq_thread_idle)\n\t\t\tctx->sq_thread_idle = HZ;\n\n\t\tio_sq_thread_park(sqd);\n\t\tlist_add(&ctx->sqd_list, &sqd->ctx_list);\n\t\tio_sqd_update_thread_idle(sqd);\n\t\t/* don't attach to a dying SQPOLL thread, would be racy */\n\t\tret = (attached && !sqd->thread) ? -ENXIO : 0;\n\t\tio_sq_thread_unpark(sqd);\n\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t\tif (attached)\n\t\t\treturn 0;\n\n\t\tif (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t\tcpumask_var_t allowed_mask;\n\t\t\tint cpu = p->sq_thread_cpu;\n\n\t\t\tret = -EINVAL;\n\t\t\tif (cpu >= nr_cpu_ids || !cpu_online(cpu))\n\t\t\t\tgoto err_sqpoll;\n\t\t\tret = -ENOMEM;\n\t\t\tif (!alloc_cpumask_var(&allowed_mask, GFP_KERNEL))\n\t\t\t\tgoto err_sqpoll;\n\t\t\tret = -EINVAL;\n\t\t\tcpuset_cpus_allowed(current, allowed_mask);\n\t\t\tif (!cpumask_test_cpu(cpu, allowed_mask)) {\n\t\t\t\tfree_cpumask_var(allowed_mask);\n\t\t\t\tgoto err_sqpoll;\n\t\t\t}\n\t\t\tfree_cpumask_var(allowed_mask);\n\t\t\tsqd->sq_cpu = cpu;\n\t\t} else {\n\t\t\tsqd->sq_cpu = -1;\n\t\t}\n\n\t\tsqd->task_pid = current->pid;\n\t\tsqd->task_tgid = current->tgid;\n\t\ttsk = create_io_thread(io_sq_thread, sqd, NUMA_NO_NODE);\n\t\tif (IS_ERR(tsk)) {\n\t\t\tret = PTR_ERR(tsk);\n\t\t\tgoto err_sqpoll;\n\t\t}\n\n\t\tsqd->thread = tsk;\n\t\ttask_to_put = get_task_struct(tsk);\n\t\tret = io_uring_alloc_task_context(tsk, ctx);\n\t\twake_up_new_task(tsk);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else if (p->flags & IORING_SETUP_SQ_AFF) {\n\t\t/* Can't have SQ_AFF without SQPOLL */\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (task_to_put)\n\t\tput_task_struct(task_to_put);\n\treturn 0;\nerr_sqpoll:\n\tcomplete(&ctx->sq_data->exited);\nerr:\n\tio_sq_thread_finish(ctx);\n\tif (task_to_put)\n\t\tput_task_struct(task_to_put);\n\treturn ret;\n}\n\n__cold int io_sqpoll_wq_cpu_affinity(struct io_ring_ctx *ctx,\n\t\t\t\t     cpumask_var_t mask)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\tint ret = -EINVAL;\n\n\tif (sqd) {\n\t\tio_sq_thread_park(sqd);\n\t\t/* Don't set affinity for a dying thread */\n\t\tif (sqd->thread)\n\t\t\tret = io_wq_cpu_affinity(sqd->thread->io_uring, mask);\n\t\tio_sq_thread_unpark(sqd);\n\t}\n\n\treturn ret;\n}\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21634",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21634.json",
            "patch_attempts": [
                {
                    "upstream_commit": "9b496a8bbed9cc292b0dfd796f38ec58b6d0375f",
                    "upstream_commit_date": "2025-01-08 15:54:39 -1000",
                    "upstream_patch": "3cb97a927fffe443e1e7e8eddbfebfdb062e86ed",
                    "total_versions_tested": 1,
                    "successful_patches": 1,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "11cb1d643a74665a4e14749414f48f82cbc15c64",
                            "downstream_commit": "e7960da6f2f438d907c17d463364dae6d242f775",
                            "commit_date": "2025-01-17 13:40:49 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file kernel/cgroup/cpuset.c\nHunk #1 succeeded at 3110 (offset -14 lines).\nHunk #2 succeeded at 3140 (offset -16 lines).",
                            "downstream_patch_content": "commit 11cb1d643a74665a4e14749414f48f82cbc15c64\nAuthor: Chen Ridong <chenridong@huawei.com>\nDate:   Mon Jan 6 08:19:04 2025 +0000\n\n    cgroup/cpuset: remove kernfs active break\n    \n    [ Upstream commit 3cb97a927fffe443e1e7e8eddbfebfdb062e86ed ]\n    \n    A warning was found:\n    \n    WARNING: CPU: 10 PID: 3486953 at fs/kernfs/file.c:828\n    CPU: 10 PID: 3486953 Comm: rmdir Kdump: loaded Tainted: G\n    RIP: 0010:kernfs_should_drain_open_files+0x1a1/0x1b0\n    RSP: 0018:ffff8881107ef9e0 EFLAGS: 00010202\n    RAX: 0000000080000002 RBX: ffff888154738c00 RCX: dffffc0000000000\n    RDX: 0000000000000007 RSI: 0000000000000004 RDI: ffff888154738c04\n    RBP: ffff888154738c04 R08: ffffffffaf27fa15 R09: ffffed102a8e7180\n    R10: ffff888154738c07 R11: 0000000000000000 R12: ffff888154738c08\n    R13: ffff888750f8c000 R14: ffff888750f8c0e8 R15: ffff888154738ca0\n    FS:  00007f84cd0be740(0000) GS:ffff8887ddc00000(0000) knlGS:0000000000000000\n    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n    CR2: 0000555f9fbe00c8 CR3: 0000000153eec001 CR4: 0000000000370ee0\n    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n    Call Trace:\n     kernfs_drain+0x15e/0x2f0\n     __kernfs_remove+0x165/0x300\n     kernfs_remove_by_name_ns+0x7b/0xc0\n     cgroup_rm_file+0x154/0x1c0\n     cgroup_addrm_files+0x1c2/0x1f0\n     css_clear_dir+0x77/0x110\n     kill_css+0x4c/0x1b0\n     cgroup_destroy_locked+0x194/0x380\n     cgroup_rmdir+0x2a/0x140\n    \n    It can be explained by:\n    rmdir                           echo 1 > cpuset.cpus\n                                    kernfs_fop_write_iter // active=0\n    cgroup_rm_file\n    kernfs_remove_by_name_ns        kernfs_get_active // active=1\n    __kernfs_remove                                   // active=0x80000002\n    kernfs_drain                    cpuset_write_resmask\n    wait_event\n    //waiting (active == 0x80000001)\n                                    kernfs_break_active_protection\n                                    // active = 0x80000001\n    // continue\n                                    kernfs_unbreak_active_protection\n                                    // active = 0x80000002\n    ...\n    kernfs_should_drain_open_files\n    // warning occurs\n                                    kernfs_put_active\n    \n    This warning is caused by 'kernfs_break_active_protection' when it is\n    writing to cpuset.cpus, and the cgroup is removed concurrently.\n    \n    The commit 3a5a6d0c2b03 (\"cpuset: don't nest cgroup_mutex inside\n    get_online_cpus()\") made cpuset_hotplug_workfn asynchronous, This change\n    involves calling flush_work(), which can create a multiple processes\n    circular locking dependency that involve cgroup_mutex, potentially leading\n    to a deadlock. To avoid deadlock. the commit 76bb5ab8f6e3 (\"cpuset: break\n    kernfs active protection in cpuset_write_resmask()\") added\n    'kernfs_break_active_protection' in the cpuset_write_resmask. This could\n    lead to this warning.\n    \n    After the commit 2125c0034c5d (\"cgroup/cpuset: Make cpuset hotplug\n    processing synchronous\"), the cpuset_write_resmask no longer needs to\n    wait the hotplug to finish, which means that concurrent hotplug and cpuset\n    operations are no longer possible. Therefore, the deadlock doesn't exist\n    anymore and it does not have to 'break active protection' now. To fix this\n    warning, just remove kernfs_break_active_protection operation in the\n    'cpuset_write_resmask'.\n    \n    Fixes: bdb2fd7fc56e (\"kernfs: Skip kernfs_drain_open_files() more aggressively\")\n    Fixes: 76bb5ab8f6e3 (\"cpuset: break kernfs active protection in cpuset_write_resmask()\")\n    Reported-by: Ji Fa <jifa@huawei.com>\n    Signed-off-by: Chen Ridong <chenridong@huawei.com>\n    Acked-by: Waiman Long <longman@redhat.com>\n    Signed-off-by: Tejun Heo <tj@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex c431c50512bd..24ece85fd3b1 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -3110,29 +3110,6 @@ ssize_t cpuset_write_resmask(struct kernfs_open_file *of,\n \tint retval = -ENODEV;\n \n \tbuf = strstrip(buf);\n-\n-\t/*\n-\t * CPU or memory hotunplug may leave @cs w/o any execution\n-\t * resources, in which case the hotplug code asynchronously updates\n-\t * configuration and transfers all tasks to the nearest ancestor\n-\t * which can execute.\n-\t *\n-\t * As writes to \"cpus\" or \"mems\" may restore @cs's execution\n-\t * resources, wait for the previously scheduled operations before\n-\t * proceeding, so that we don't end up keep removing tasks added\n-\t * after execution capability is restored.\n-\t *\n-\t * cpuset_handle_hotplug may call back into cgroup core asynchronously\n-\t * via cgroup_transfer_tasks() and waiting for it from a cgroupfs\n-\t * operation like this one can lead to a deadlock through kernfs\n-\t * active_ref protection.  Let's break the protection.  Losing the\n-\t * protection is okay as we check whether @cs is online after\n-\t * grabbing cpuset_mutex anyway.  This only happens on the legacy\n-\t * hierarchies.\n-\t */\n-\tcss_get(&cs->css);\n-\tkernfs_break_active_protection(of->kn);\n-\n \tcpus_read_lock();\n \tmutex_lock(&cpuset_mutex);\n \tif (!is_cpuset_online(cs))\n@@ -3163,8 +3140,6 @@ ssize_t cpuset_write_resmask(struct kernfs_open_file *of,\n out_unlock:\n \tmutex_unlock(&cpuset_mutex);\n \tcpus_read_unlock();\n-\tkernfs_unbreak_active_protection(of->kn);\n-\tcss_put(&cs->css);\n \tflush_workqueue(cpuset_migrate_mm_wq);\n \treturn retval ?: nbytes;\n }\n",
                            "downstream_file_content": {}
                        }
                    ],
                    "upstream_patch_content": "From 3cb97a927fffe443e1e7e8eddbfebfdb062e86ed Mon Sep 17 00:00:00 2001\nFrom: Chen Ridong <chenridong@huawei.com>\nDate: Mon, 6 Jan 2025 08:19:04 +0000\nSubject: [PATCH] cgroup/cpuset: remove kernfs active break\n\nA warning was found:\n\nWARNING: CPU: 10 PID: 3486953 at fs/kernfs/file.c:828\nCPU: 10 PID: 3486953 Comm: rmdir Kdump: loaded Tainted: G\nRIP: 0010:kernfs_should_drain_open_files+0x1a1/0x1b0\nRSP: 0018:ffff8881107ef9e0 EFLAGS: 00010202\nRAX: 0000000080000002 RBX: ffff888154738c00 RCX: dffffc0000000000\nRDX: 0000000000000007 RSI: 0000000000000004 RDI: ffff888154738c04\nRBP: ffff888154738c04 R08: ffffffffaf27fa15 R09: ffffed102a8e7180\nR10: ffff888154738c07 R11: 0000000000000000 R12: ffff888154738c08\nR13: ffff888750f8c000 R14: ffff888750f8c0e8 R15: ffff888154738ca0\nFS:  00007f84cd0be740(0000) GS:ffff8887ddc00000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 0000555f9fbe00c8 CR3: 0000000153eec001 CR4: 0000000000370ee0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n kernfs_drain+0x15e/0x2f0\n __kernfs_remove+0x165/0x300\n kernfs_remove_by_name_ns+0x7b/0xc0\n cgroup_rm_file+0x154/0x1c0\n cgroup_addrm_files+0x1c2/0x1f0\n css_clear_dir+0x77/0x110\n kill_css+0x4c/0x1b0\n cgroup_destroy_locked+0x194/0x380\n cgroup_rmdir+0x2a/0x140\n\nIt can be explained by:\nrmdir \t\t\t\techo 1 > cpuset.cpus\n\t\t\t\tkernfs_fop_write_iter // active=0\ncgroup_rm_file\nkernfs_remove_by_name_ns\tkernfs_get_active // active=1\n__kernfs_remove\t\t\t\t\t  // active=0x80000002\nkernfs_drain\t\t\tcpuset_write_resmask\nwait_event\n//waiting (active == 0x80000001)\n\t\t\t\tkernfs_break_active_protection\n\t\t\t\t// active = 0x80000001\n// continue\n\t\t\t\tkernfs_unbreak_active_protection\n\t\t\t\t// active = 0x80000002\n...\nkernfs_should_drain_open_files\n// warning occurs\n\t\t\t\tkernfs_put_active\n\nThis warning is caused by 'kernfs_break_active_protection' when it is\nwriting to cpuset.cpus, and the cgroup is removed concurrently.\n\nThe commit 3a5a6d0c2b03 (\"cpuset: don't nest cgroup_mutex inside\nget_online_cpus()\") made cpuset_hotplug_workfn asynchronous, This change\ninvolves calling flush_work(), which can create a multiple processes\ncircular locking dependency that involve cgroup_mutex, potentially leading\nto a deadlock. To avoid deadlock. the commit 76bb5ab8f6e3 (\"cpuset: break\nkernfs active protection in cpuset_write_resmask()\") added\n'kernfs_break_active_protection' in the cpuset_write_resmask. This could\nlead to this warning.\n\nAfter the commit 2125c0034c5d (\"cgroup/cpuset: Make cpuset hotplug\nprocessing synchronous\"), the cpuset_write_resmask no longer needs to\nwait the hotplug to finish, which means that concurrent hotplug and cpuset\noperations are no longer possible. Therefore, the deadlock doesn't exist\nanymore and it does not have to 'break active protection' now. To fix this\nwarning, just remove kernfs_break_active_protection operation in the\n'cpuset_write_resmask'.\n\nFixes: bdb2fd7fc56e (\"kernfs: Skip kernfs_drain_open_files() more aggressively\")\nFixes: 76bb5ab8f6e3 (\"cpuset: break kernfs active protection in cpuset_write_resmask()\")\nReported-by: Ji Fa <jifa@huawei.com>\nSigned-off-by: Chen Ridong <chenridong@huawei.com>\nAcked-by: Waiman Long <longman@redhat.com>\nSigned-off-by: Tejun Heo <tj@kernel.org>\n---\n kernel/cgroup/cpuset.c | 25 -------------------------\n 1 file changed, 25 deletions(-)\n\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 7ea559fb0cbf..0f910c828973 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -3124,29 +3124,6 @@ ssize_t cpuset_write_resmask(struct kernfs_open_file *of,\n \tint retval = -ENODEV;\n \n \tbuf = strstrip(buf);\n-\n-\t/*\n-\t * CPU or memory hotunplug may leave @cs w/o any execution\n-\t * resources, in which case the hotplug code asynchronously updates\n-\t * configuration and transfers all tasks to the nearest ancestor\n-\t * which can execute.\n-\t *\n-\t * As writes to \"cpus\" or \"mems\" may restore @cs's execution\n-\t * resources, wait for the previously scheduled operations before\n-\t * proceeding, so that we don't end up keep removing tasks added\n-\t * after execution capability is restored.\n-\t *\n-\t * cpuset_handle_hotplug may call back into cgroup core asynchronously\n-\t * via cgroup_transfer_tasks() and waiting for it from a cgroupfs\n-\t * operation like this one can lead to a deadlock through kernfs\n-\t * active_ref protection.  Let's break the protection.  Losing the\n-\t * protection is okay as we check whether @cs is online after\n-\t * grabbing cpuset_mutex anyway.  This only happens on the legacy\n-\t * hierarchies.\n-\t */\n-\tcss_get(&cs->css);\n-\tkernfs_break_active_protection(of->kn);\n-\n \tcpus_read_lock();\n \tmutex_lock(&cpuset_mutex);\n \tif (!is_cpuset_online(cs))\n@@ -3179,8 +3156,6 @@ ssize_t cpuset_write_resmask(struct kernfs_open_file *of,\n out_unlock:\n \tmutex_unlock(&cpuset_mutex);\n \tcpus_read_unlock();\n-\tkernfs_unbreak_active_protection(of->kn);\n-\tcss_put(&cs->css);\n \tflush_workqueue(cpuset_migrate_mm_wq);\n \treturn retval ?: nbytes;\n }\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "kernel/cgroup/cpuset.c": "/*\n *  kernel/cpuset.c\n *\n *  Processor and Memory placement constraints for sets of tasks.\n *\n *  Copyright (C) 2003 BULL SA.\n *  Copyright (C) 2004-2007 Silicon Graphics, Inc.\n *  Copyright (C) 2006 Google, Inc\n *\n *  Portions derived from Patrick Mochel's sysfs code.\n *  sysfs is Copyright (c) 2001-3 Patrick Mochel\n *\n *  2003-10-10 Written by Simon Derr.\n *  2003-10-22 Updates by Stephen Hemminger.\n *  2004 May-July Rework by Paul Jackson.\n *  2006 Rework by Paul Menage to use generic cgroups\n *  2008 Rework of the scheduler domains and CPU hotplug handling\n *       by Max Krasnyansky\n *\n *  This file is subject to the terms and conditions of the GNU General Public\n *  License.  See the file COPYING in the main directory of the Linux\n *  distribution for more details.\n */\n#include \"cgroup-internal.h\"\n#include \"cpuset-internal.h\"\n\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/mempolicy.h>\n#include <linux/mm.h>\n#include <linux/memory.h>\n#include <linux/export.h>\n#include <linux/rcupdate.h>\n#include <linux/sched.h>\n#include <linux/sched/deadline.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/security.h>\n#include <linux/oom.h>\n#include <linux/sched/isolation.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n\nDEFINE_STATIC_KEY_FALSE(cpusets_pre_enable_key);\nDEFINE_STATIC_KEY_FALSE(cpusets_enabled_key);\n\n/*\n * There could be abnormal cpuset configurations for cpu or memory\n * node binding, add this key to provide a quick low-cost judgment\n * of the situation.\n */\nDEFINE_STATIC_KEY_FALSE(cpusets_insane_config_key);\n\nstatic const char * const perr_strings[] = {\n\t[PERR_INVCPUS]   = \"Invalid cpu list in cpuset.cpus.exclusive\",\n\t[PERR_INVPARENT] = \"Parent is an invalid partition root\",\n\t[PERR_NOTPART]   = \"Parent is not a partition root\",\n\t[PERR_NOTEXCL]   = \"Cpu list in cpuset.cpus not exclusive\",\n\t[PERR_NOCPUS]    = \"Parent unable to distribute cpu downstream\",\n\t[PERR_HOTPLUG]   = \"No cpu available due to hotplug\",\n\t[PERR_CPUSEMPTY] = \"cpuset.cpus and cpuset.cpus.exclusive are empty\",\n\t[PERR_HKEEPING]  = \"partition config conflicts with housekeeping setup\",\n\t[PERR_ACCESS]    = \"Enable partition not permitted\",\n};\n\n/*\n * Exclusive CPUs distributed out to sub-partitions of top_cpuset\n */\nstatic cpumask_var_t\tsubpartitions_cpus;\n\n/*\n * Exclusive CPUs in isolated partitions\n */\nstatic cpumask_var_t\tisolated_cpus;\n\n/*\n * Housekeeping (HK_TYPE_DOMAIN) CPUs at boot\n */\nstatic cpumask_var_t\tboot_hk_cpus;\nstatic bool\t\thave_boot_isolcpus;\n\n/* List of remote partition root children */\nstatic struct list_head remote_children;\n\n/*\n * A flag to force sched domain rebuild at the end of an operation while\n * inhibiting it in the intermediate stages when set. Currently it is only\n * set in hotplug code.\n */\nstatic bool force_sd_rebuild;\n\n/*\n * Partition root states:\n *\n *   0 - member (not a partition root)\n *   1 - partition root\n *   2 - partition root without load balancing (isolated)\n *  -1 - invalid partition root\n *  -2 - invalid isolated partition root\n *\n *  There are 2 types of partitions - local or remote. Local partitions are\n *  those whose parents are partition root themselves. Setting of\n *  cpuset.cpus.exclusive are optional in setting up local partitions.\n *  Remote partitions are those whose parents are not partition roots. Passing\n *  down exclusive CPUs by setting cpuset.cpus.exclusive along its ancestor\n *  nodes are mandatory in creating a remote partition.\n *\n *  For simplicity, a local partition can be created under a local or remote\n *  partition but a remote partition cannot have any partition root in its\n *  ancestor chain except the cgroup root.\n */\n#define PRS_MEMBER\t\t0\n#define PRS_ROOT\t\t1\n#define PRS_ISOLATED\t\t2\n#define PRS_INVALID_ROOT\t-1\n#define PRS_INVALID_ISOLATED\t-2\n\nstatic inline bool is_prs_invalid(int prs_state)\n{\n\treturn prs_state < 0;\n}\n\n/*\n * Temporary cpumasks for working with partitions that are passed among\n * functions to avoid memory allocation in inner functions.\n */\nstruct tmpmasks {\n\tcpumask_var_t addmask, delmask;\t/* For partition root */\n\tcpumask_var_t new_cpus;\t\t/* For update_cpumasks_hier() */\n};\n\nvoid inc_dl_tasks_cs(struct task_struct *p)\n{\n\tstruct cpuset *cs = task_cs(p);\n\n\tcs->nr_deadline_tasks++;\n}\n\nvoid dec_dl_tasks_cs(struct task_struct *p)\n{\n\tstruct cpuset *cs = task_cs(p);\n\n\tcs->nr_deadline_tasks--;\n}\n\nstatic inline int is_partition_valid(const struct cpuset *cs)\n{\n\treturn cs->partition_root_state > 0;\n}\n\nstatic inline int is_partition_invalid(const struct cpuset *cs)\n{\n\treturn cs->partition_root_state < 0;\n}\n\n/*\n * Callers should hold callback_lock to modify partition_root_state.\n */\nstatic inline void make_partition_invalid(struct cpuset *cs)\n{\n\tif (cs->partition_root_state > 0)\n\t\tcs->partition_root_state = -cs->partition_root_state;\n}\n\n/*\n * Send notification event of whenever partition_root_state changes.\n */\nstatic inline void notify_partition_change(struct cpuset *cs, int old_prs)\n{\n\tif (old_prs == cs->partition_root_state)\n\t\treturn;\n\tcgroup_file_notify(&cs->partition_file);\n\n\t/* Reset prs_err if not invalid */\n\tif (is_partition_valid(cs))\n\t\tWRITE_ONCE(cs->prs_err, PERR_NONE);\n}\n\nstatic struct cpuset top_cpuset = {\n\t.flags = BIT(CS_ONLINE) | BIT(CS_CPU_EXCLUSIVE) |\n\t\t BIT(CS_MEM_EXCLUSIVE) | BIT(CS_SCHED_LOAD_BALANCE),\n\t.partition_root_state = PRS_ROOT,\n\t.relax_domain_level = -1,\n\t.remote_sibling = LIST_HEAD_INIT(top_cpuset.remote_sibling),\n};\n\n/*\n * There are two global locks guarding cpuset structures - cpuset_mutex and\n * callback_lock. We also require taking task_lock() when dereferencing a\n * task's cpuset pointer. See \"The task_lock() exception\", at the end of this\n * comment.  The cpuset code uses only cpuset_mutex. Other kernel subsystems\n * can use cpuset_lock()/cpuset_unlock() to prevent change to cpuset\n * structures. Note that cpuset_mutex needs to be a mutex as it is used in\n * paths that rely on priority inheritance (e.g. scheduler - on RT) for\n * correctness.\n *\n * A task must hold both locks to modify cpusets.  If a task holds\n * cpuset_mutex, it blocks others, ensuring that it is the only task able to\n * also acquire callback_lock and be able to modify cpusets.  It can perform\n * various checks on the cpuset structure first, knowing nothing will change.\n * It can also allocate memory while just holding cpuset_mutex.  While it is\n * performing these checks, various callback routines can briefly acquire\n * callback_lock to query cpusets.  Once it is ready to make the changes, it\n * takes callback_lock, blocking everyone else.\n *\n * Calls to the kernel memory allocator can not be made while holding\n * callback_lock, as that would risk double tripping on callback_lock\n * from one of the callbacks into the cpuset code from within\n * __alloc_pages().\n *\n * If a task is only holding callback_lock, then it has read-only\n * access to cpusets.\n *\n * Now, the task_struct fields mems_allowed and mempolicy may be changed\n * by other task, we use alloc_lock in the task_struct fields to protect\n * them.\n *\n * The cpuset_common_seq_show() handlers only hold callback_lock across\n * small pieces of code, such as when reading out possibly multi-word\n * cpumasks and nodemasks.\n *\n * Accessing a task's cpuset should be done in accordance with the\n * guidelines for accessing subsystem state in kernel/cgroup.c\n */\n\nstatic DEFINE_MUTEX(cpuset_mutex);\n\nvoid cpuset_lock(void)\n{\n\tmutex_lock(&cpuset_mutex);\n}\n\nvoid cpuset_unlock(void)\n{\n\tmutex_unlock(&cpuset_mutex);\n}\n\nstatic DEFINE_SPINLOCK(callback_lock);\n\nvoid cpuset_callback_lock_irq(void)\n{\n\tspin_lock_irq(&callback_lock);\n}\n\nvoid cpuset_callback_unlock_irq(void)\n{\n\tspin_unlock_irq(&callback_lock);\n}\n\nstatic struct workqueue_struct *cpuset_migrate_mm_wq;\n\nstatic DECLARE_WAIT_QUEUE_HEAD(cpuset_attach_wq);\n\nstatic inline void check_insane_mems_config(nodemask_t *nodes)\n{\n\tif (!cpusets_insane_config() &&\n\t\tmovable_only_nodes(nodes)) {\n\t\tstatic_branch_enable(&cpusets_insane_config_key);\n\t\tpr_info(\"Unsupported (movable nodes only) cpuset configuration detected (nmask=%*pbl)!\\n\"\n\t\t\t\"Cpuset allocations might fail even with a lot of memory available.\\n\",\n\t\t\tnodemask_pr_args(nodes));\n\t}\n}\n\n/*\n * decrease cs->attach_in_progress.\n * wake_up cpuset_attach_wq if cs->attach_in_progress==0.\n */\nstatic inline void dec_attach_in_progress_locked(struct cpuset *cs)\n{\n\tlockdep_assert_held(&cpuset_mutex);\n\n\tcs->attach_in_progress--;\n\tif (!cs->attach_in_progress)\n\t\twake_up(&cpuset_attach_wq);\n}\n\nstatic inline void dec_attach_in_progress(struct cpuset *cs)\n{\n\tmutex_lock(&cpuset_mutex);\n\tdec_attach_in_progress_locked(cs);\n\tmutex_unlock(&cpuset_mutex);\n}\n\n/*\n * Cgroup v2 behavior is used on the \"cpus\" and \"mems\" control files when\n * on default hierarchy or when the cpuset_v2_mode flag is set by mounting\n * the v1 cpuset cgroup filesystem with the \"cpuset_v2_mode\" mount option.\n * With v2 behavior, \"cpus\" and \"mems\" are always what the users have\n * requested and won't be changed by hotplug events. Only the effective\n * cpus or mems will be affected.\n */\nstatic inline bool is_in_v2_mode(void)\n{\n\treturn cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t      (cpuset_cgrp_subsys.root->flags & CGRP_ROOT_CPUSET_V2_MODE);\n}\n\n/**\n * partition_is_populated - check if partition has tasks\n * @cs: partition root to be checked\n * @excluded_child: a child cpuset to be excluded in task checking\n * Return: true if there are tasks, false otherwise\n *\n * It is assumed that @cs is a valid partition root. @excluded_child should\n * be non-NULL when this cpuset is going to become a partition itself.\n */\nstatic inline bool partition_is_populated(struct cpuset *cs,\n\t\t\t\t\t  struct cpuset *excluded_child)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *child;\n\n\tif (cs->css.cgroup->nr_populated_csets)\n\t\treturn true;\n\tif (!excluded_child && !cs->nr_subparts)\n\t\treturn cgroup_is_populated(cs->css.cgroup);\n\n\trcu_read_lock();\n\tcpuset_for_each_child(child, css, cs) {\n\t\tif (child == excluded_child)\n\t\t\tcontinue;\n\t\tif (is_partition_valid(child))\n\t\t\tcontinue;\n\t\tif (cgroup_is_populated(child->css.cgroup)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn false;\n}\n\n/*\n * Return in pmask the portion of a task's cpusets's cpus_allowed that\n * are online and are capable of running the task.  If none are found,\n * walk up the cpuset hierarchy until we find one that does have some\n * appropriate cpus.\n *\n * One way or another, we guarantee to return some non-empty subset\n * of cpu_online_mask.\n *\n * Call with callback_lock or cpuset_mutex held.\n */\nstatic void guarantee_online_cpus(struct task_struct *tsk,\n\t\t\t\t  struct cpumask *pmask)\n{\n\tconst struct cpumask *possible_mask = task_cpu_possible_mask(tsk);\n\tstruct cpuset *cs;\n\n\tif (WARN_ON(!cpumask_and(pmask, possible_mask, cpu_online_mask)))\n\t\tcpumask_copy(pmask, cpu_online_mask);\n\n\trcu_read_lock();\n\tcs = task_cs(tsk);\n\n\twhile (!cpumask_intersects(cs->effective_cpus, pmask))\n\t\tcs = parent_cs(cs);\n\n\tcpumask_and(pmask, pmask, cs->effective_cpus);\n\trcu_read_unlock();\n}\n\n/*\n * Return in *pmask the portion of a cpusets's mems_allowed that\n * are online, with memory.  If none are online with memory, walk\n * up the cpuset hierarchy until we find one that does have some\n * online mems.  The top cpuset always has some mems online.\n *\n * One way or another, we guarantee to return some non-empty subset\n * of node_states[N_MEMORY].\n *\n * Call with callback_lock or cpuset_mutex held.\n */\nstatic void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n{\n\twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n\t\tcs = parent_cs(cs);\n\tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n}\n\n/**\n * alloc_cpumasks - allocate three cpumasks for cpuset\n * @cs:  the cpuset that have cpumasks to be allocated.\n * @tmp: the tmpmasks structure pointer\n * Return: 0 if successful, -ENOMEM otherwise.\n *\n * Only one of the two input arguments should be non-NULL.\n */\nstatic inline int alloc_cpumasks(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tcpumask_var_t *pmask1, *pmask2, *pmask3, *pmask4;\n\n\tif (cs) {\n\t\tpmask1 = &cs->cpus_allowed;\n\t\tpmask2 = &cs->effective_cpus;\n\t\tpmask3 = &cs->effective_xcpus;\n\t\tpmask4 = &cs->exclusive_cpus;\n\t} else {\n\t\tpmask1 = &tmp->new_cpus;\n\t\tpmask2 = &tmp->addmask;\n\t\tpmask3 = &tmp->delmask;\n\t\tpmask4 = NULL;\n\t}\n\n\tif (!zalloc_cpumask_var(pmask1, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tif (!zalloc_cpumask_var(pmask2, GFP_KERNEL))\n\t\tgoto free_one;\n\n\tif (!zalloc_cpumask_var(pmask3, GFP_KERNEL))\n\t\tgoto free_two;\n\n\tif (pmask4 && !zalloc_cpumask_var(pmask4, GFP_KERNEL))\n\t\tgoto free_three;\n\n\n\treturn 0;\n\nfree_three:\n\tfree_cpumask_var(*pmask3);\nfree_two:\n\tfree_cpumask_var(*pmask2);\nfree_one:\n\tfree_cpumask_var(*pmask1);\n\treturn -ENOMEM;\n}\n\n/**\n * free_cpumasks - free cpumasks in a tmpmasks structure\n * @cs:  the cpuset that have cpumasks to be free.\n * @tmp: the tmpmasks structure pointer\n */\nstatic inline void free_cpumasks(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tif (cs) {\n\t\tfree_cpumask_var(cs->cpus_allowed);\n\t\tfree_cpumask_var(cs->effective_cpus);\n\t\tfree_cpumask_var(cs->effective_xcpus);\n\t\tfree_cpumask_var(cs->exclusive_cpus);\n\t}\n\tif (tmp) {\n\t\tfree_cpumask_var(tmp->new_cpus);\n\t\tfree_cpumask_var(tmp->addmask);\n\t\tfree_cpumask_var(tmp->delmask);\n\t}\n}\n\n/**\n * alloc_trial_cpuset - allocate a trial cpuset\n * @cs: the cpuset that the trial cpuset duplicates\n */\nstatic struct cpuset *alloc_trial_cpuset(struct cpuset *cs)\n{\n\tstruct cpuset *trial;\n\n\ttrial = kmemdup(cs, sizeof(*cs), GFP_KERNEL);\n\tif (!trial)\n\t\treturn NULL;\n\n\tif (alloc_cpumasks(trial, NULL)) {\n\t\tkfree(trial);\n\t\treturn NULL;\n\t}\n\n\tcpumask_copy(trial->cpus_allowed, cs->cpus_allowed);\n\tcpumask_copy(trial->effective_cpus, cs->effective_cpus);\n\tcpumask_copy(trial->effective_xcpus, cs->effective_xcpus);\n\tcpumask_copy(trial->exclusive_cpus, cs->exclusive_cpus);\n\treturn trial;\n}\n\n/**\n * free_cpuset - free the cpuset\n * @cs: the cpuset to be freed\n */\nstatic inline void free_cpuset(struct cpuset *cs)\n{\n\tfree_cpumasks(cs, NULL);\n\tkfree(cs);\n}\n\n/* Return user specified exclusive CPUs */\nstatic inline struct cpumask *user_xcpus(struct cpuset *cs)\n{\n\treturn cpumask_empty(cs->exclusive_cpus) ? cs->cpus_allowed\n\t\t\t\t\t\t : cs->exclusive_cpus;\n}\n\nstatic inline bool xcpus_empty(struct cpuset *cs)\n{\n\treturn cpumask_empty(cs->cpus_allowed) &&\n\t       cpumask_empty(cs->exclusive_cpus);\n}\n\n/*\n * cpusets_are_exclusive() - check if two cpusets are exclusive\n *\n * Return true if exclusive, false if not\n */\nstatic inline bool cpusets_are_exclusive(struct cpuset *cs1, struct cpuset *cs2)\n{\n\tstruct cpumask *xcpus1 = user_xcpus(cs1);\n\tstruct cpumask *xcpus2 = user_xcpus(cs2);\n\n\tif (cpumask_intersects(xcpus1, xcpus2))\n\t\treturn false;\n\treturn true;\n}\n\n/*\n * validate_change() - Used to validate that any proposed cpuset change\n *\t\t       follows the structural rules for cpusets.\n *\n * If we replaced the flag and mask values of the current cpuset\n * (cur) with those values in the trial cpuset (trial), would\n * our various subset and exclusive rules still be valid?  Presumes\n * cpuset_mutex held.\n *\n * 'cur' is the address of an actual, in-use cpuset.  Operations\n * such as list traversal that depend on the actual address of the\n * cpuset in the list must use cur below, not trial.\n *\n * 'trial' is the address of bulk structure copy of cur, with\n * perhaps one or more of the fields cpus_allowed, mems_allowed,\n * or flags changed to new, trial values.\n *\n * Return 0 if valid, -errno if not.\n */\n\nstatic int validate_change(struct cpuset *cur, struct cpuset *trial)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *c, *par;\n\tint ret = 0;\n\n\trcu_read_lock();\n\n\tif (!is_in_v2_mode())\n\t\tret = cpuset1_validate_change(cur, trial);\n\tif (ret)\n\t\tgoto out;\n\n\t/* Remaining checks don't apply to root cpuset */\n\tif (cur == &top_cpuset)\n\t\tgoto out;\n\n\tpar = parent_cs(cur);\n\n\t/*\n\t * Cpusets with tasks - existing or newly being attached - can't\n\t * be changed to have empty cpus_allowed or mems_allowed.\n\t */\n\tret = -ENOSPC;\n\tif ((cgroup_is_populated(cur->css.cgroup) || cur->attach_in_progress)) {\n\t\tif (!cpumask_empty(cur->cpus_allowed) &&\n\t\t    cpumask_empty(trial->cpus_allowed))\n\t\t\tgoto out;\n\t\tif (!nodes_empty(cur->mems_allowed) &&\n\t\t    nodes_empty(trial->mems_allowed))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * We can't shrink if we won't have enough room for SCHED_DEADLINE\n\t * tasks.\n\t */\n\tret = -EBUSY;\n\tif (is_cpu_exclusive(cur) &&\n\t    !cpuset_cpumask_can_shrink(cur->cpus_allowed,\n\t\t\t\t       trial->cpus_allowed))\n\t\tgoto out;\n\n\t/*\n\t * If either I or some sibling (!= me) is exclusive, we can't\n\t * overlap. exclusive_cpus cannot overlap with each other if set.\n\t */\n\tret = -EINVAL;\n\tcpuset_for_each_child(c, css, par) {\n\t\tbool txset, cxset;\t/* Are exclusive_cpus set? */\n\n\t\tif (c == cur)\n\t\t\tcontinue;\n\n\t\ttxset = !cpumask_empty(trial->exclusive_cpus);\n\t\tcxset = !cpumask_empty(c->exclusive_cpus);\n\t\tif (is_cpu_exclusive(trial) || is_cpu_exclusive(c) ||\n\t\t    (txset && cxset)) {\n\t\t\tif (!cpusets_are_exclusive(trial, c))\n\t\t\t\tgoto out;\n\t\t} else if (txset || cxset) {\n\t\t\tstruct cpumask *xcpus, *acpus;\n\n\t\t\t/*\n\t\t\t * When just one of the exclusive_cpus's is set,\n\t\t\t * cpus_allowed of the other cpuset, if set, cannot be\n\t\t\t * a subset of it or none of those CPUs will be\n\t\t\t * available if these exclusive CPUs are activated.\n\t\t\t */\n\t\t\tif (txset) {\n\t\t\t\txcpus = trial->exclusive_cpus;\n\t\t\t\tacpus = c->cpus_allowed;\n\t\t\t} else {\n\t\t\t\txcpus = c->exclusive_cpus;\n\t\t\t\tacpus = trial->cpus_allowed;\n\t\t\t}\n\t\t\tif (!cpumask_empty(acpus) && cpumask_subset(acpus, xcpus))\n\t\t\t\tgoto out;\n\t\t}\n\t\tif ((is_mem_exclusive(trial) || is_mem_exclusive(c)) &&\n\t\t    nodes_intersects(trial->mems_allowed, c->mems_allowed))\n\t\t\tgoto out;\n\t}\n\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\n#ifdef CONFIG_SMP\n/*\n * Helper routine for generate_sched_domains().\n * Do cpusets a, b have overlapping effective cpus_allowed masks?\n */\nstatic int cpusets_overlap(struct cpuset *a, struct cpuset *b)\n{\n\treturn cpumask_intersects(a->effective_cpus, b->effective_cpus);\n}\n\nstatic void\nupdate_domain_attr(struct sched_domain_attr *dattr, struct cpuset *c)\n{\n\tif (dattr->relax_domain_level < c->relax_domain_level)\n\t\tdattr->relax_domain_level = c->relax_domain_level;\n\treturn;\n}\n\nstatic void update_domain_attr_tree(struct sched_domain_attr *dattr,\n\t\t\t\t    struct cpuset *root_cs)\n{\n\tstruct cpuset *cp;\n\tstruct cgroup_subsys_state *pos_css;\n\n\trcu_read_lock();\n\tcpuset_for_each_descendant_pre(cp, pos_css, root_cs) {\n\t\t/* skip the whole subtree if @cp doesn't have any CPU */\n\t\tif (cpumask_empty(cp->cpus_allowed)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (is_sched_load_balance(cp))\n\t\t\tupdate_domain_attr(dattr, cp);\n\t}\n\trcu_read_unlock();\n}\n\n/* Must be called with cpuset_mutex held.  */\nstatic inline int nr_cpusets(void)\n{\n\t/* jump label reference count + the top-level cpuset */\n\treturn static_key_count(&cpusets_enabled_key.key) + 1;\n}\n\n/*\n * generate_sched_domains()\n *\n * This function builds a partial partition of the systems CPUs\n * A 'partial partition' is a set of non-overlapping subsets whose\n * union is a subset of that set.\n * The output of this function needs to be passed to kernel/sched/core.c\n * partition_sched_domains() routine, which will rebuild the scheduler's\n * load balancing domains (sched domains) as specified by that partial\n * partition.\n *\n * See \"What is sched_load_balance\" in Documentation/admin-guide/cgroup-v1/cpusets.rst\n * for a background explanation of this.\n *\n * Does not return errors, on the theory that the callers of this\n * routine would rather not worry about failures to rebuild sched\n * domains when operating in the severe memory shortage situations\n * that could cause allocation failures below.\n *\n * Must be called with cpuset_mutex held.\n *\n * The three key local variables below are:\n *    cp - cpuset pointer, used (together with pos_css) to perform a\n *\t   top-down scan of all cpusets. For our purposes, rebuilding\n *\t   the schedulers sched domains, we can ignore !is_sched_load_\n *\t   balance cpusets.\n *  csa  - (for CpuSet Array) Array of pointers to all the cpusets\n *\t   that need to be load balanced, for convenient iterative\n *\t   access by the subsequent code that finds the best partition,\n *\t   i.e the set of domains (subsets) of CPUs such that the\n *\t   cpus_allowed of every cpuset marked is_sched_load_balance\n *\t   is a subset of one of these domains, while there are as\n *\t   many such domains as possible, each as small as possible.\n * doms  - Conversion of 'csa' to an array of cpumasks, for passing to\n *\t   the kernel/sched/core.c routine partition_sched_domains() in a\n *\t   convenient format, that can be easily compared to the prior\n *\t   value to determine what partition elements (sched domains)\n *\t   were changed (added or removed.)\n *\n * Finding the best partition (set of domains):\n *\tThe double nested loops below over i, j scan over the load\n *\tbalanced cpusets (using the array of cpuset pointers in csa[])\n *\tlooking for pairs of cpusets that have overlapping cpus_allowed\n *\tand merging them using a union-find algorithm.\n *\n *\tThe union of the cpus_allowed masks from the set of all cpusets\n *\thaving the same root then form the one element of the partition\n *\t(one sched domain) to be passed to partition_sched_domains().\n *\n */\nstatic int generate_sched_domains(cpumask_var_t **domains,\n\t\t\tstruct sched_domain_attr **attributes)\n{\n\tstruct cpuset *cp;\t/* top-down scan of cpusets */\n\tstruct cpuset **csa;\t/* array of all cpuset ptrs */\n\tint csn;\t\t/* how many cpuset ptrs in csa so far */\n\tint i, j;\t\t/* indices for partition finding loops */\n\tcpumask_var_t *doms;\t/* resulting partition; i.e. sched domains */\n\tstruct sched_domain_attr *dattr;  /* attributes for custom domains */\n\tint ndoms = 0;\t\t/* number of sched domains in result */\n\tint nslot;\t\t/* next empty doms[] struct cpumask slot */\n\tstruct cgroup_subsys_state *pos_css;\n\tbool root_load_balance = is_sched_load_balance(&top_cpuset);\n\tbool cgrpv2 = cgroup_subsys_on_dfl(cpuset_cgrp_subsys);\n\tint nslot_update;\n\n\tdoms = NULL;\n\tdattr = NULL;\n\tcsa = NULL;\n\n\t/* Special case for the 99% of systems with one, full, sched domain */\n\tif (root_load_balance && cpumask_empty(subpartitions_cpus)) {\nsingle_root_domain:\n\t\tndoms = 1;\n\t\tdoms = alloc_sched_domains(ndoms);\n\t\tif (!doms)\n\t\t\tgoto done;\n\n\t\tdattr = kmalloc(sizeof(struct sched_domain_attr), GFP_KERNEL);\n\t\tif (dattr) {\n\t\t\t*dattr = SD_ATTR_INIT;\n\t\t\tupdate_domain_attr_tree(dattr, &top_cpuset);\n\t\t}\n\t\tcpumask_and(doms[0], top_cpuset.effective_cpus,\n\t\t\t    housekeeping_cpumask(HK_TYPE_DOMAIN));\n\n\t\tgoto done;\n\t}\n\n\tcsa = kmalloc_array(nr_cpusets(), sizeof(cp), GFP_KERNEL);\n\tif (!csa)\n\t\tgoto done;\n\tcsn = 0;\n\n\trcu_read_lock();\n\tif (root_load_balance)\n\t\tcsa[csn++] = &top_cpuset;\n\tcpuset_for_each_descendant_pre(cp, pos_css, &top_cpuset) {\n\t\tif (cp == &top_cpuset)\n\t\t\tcontinue;\n\n\t\tif (cgrpv2)\n\t\t\tgoto v2;\n\n\t\t/*\n\t\t * v1:\n\t\t * Continue traversing beyond @cp iff @cp has some CPUs and\n\t\t * isn't load balancing.  The former is obvious.  The\n\t\t * latter: All child cpusets contain a subset of the\n\t\t * parent's cpus, so just skip them, and then we call\n\t\t * update_domain_attr_tree() to calc relax_domain_level of\n\t\t * the corresponding sched domain.\n\t\t */\n\t\tif (!cpumask_empty(cp->cpus_allowed) &&\n\t\t    !(is_sched_load_balance(cp) &&\n\t\t      cpumask_intersects(cp->cpus_allowed,\n\t\t\t\t\t housekeeping_cpumask(HK_TYPE_DOMAIN))))\n\t\t\tcontinue;\n\n\t\tif (is_sched_load_balance(cp) &&\n\t\t    !cpumask_empty(cp->effective_cpus))\n\t\t\tcsa[csn++] = cp;\n\n\t\t/* skip @cp's subtree */\n\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\tcontinue;\n\nv2:\n\t\t/*\n\t\t * Only valid partition roots that are not isolated and with\n\t\t * non-empty effective_cpus will be saved into csn[].\n\t\t */\n\t\tif ((cp->partition_root_state == PRS_ROOT) &&\n\t\t    !cpumask_empty(cp->effective_cpus))\n\t\t\tcsa[csn++] = cp;\n\n\t\t/*\n\t\t * Skip @cp's subtree if not a partition root and has no\n\t\t * exclusive CPUs to be granted to child cpusets.\n\t\t */\n\t\tif (!is_partition_valid(cp) && cpumask_empty(cp->exclusive_cpus))\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * If there are only isolated partitions underneath the cgroup root,\n\t * we can optimize out unneeded sched domains scanning.\n\t */\n\tif (root_load_balance && (csn == 1))\n\t\tgoto single_root_domain;\n\n\tfor (i = 0; i < csn; i++)\n\t\tuf_node_init(&csa[i]->node);\n\n\t/* Merge overlapping cpusets */\n\tfor (i = 0; i < csn; i++) {\n\t\tfor (j = i + 1; j < csn; j++) {\n\t\t\tif (cpusets_overlap(csa[i], csa[j])) {\n\t\t\t\t/*\n\t\t\t\t * Cgroup v2 shouldn't pass down overlapping\n\t\t\t\t * partition root cpusets.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(cgrpv2);\n\t\t\t\tuf_union(&csa[i]->node, &csa[j]->node);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Count the total number of domains */\n\tfor (i = 0; i < csn; i++) {\n\t\tif (uf_find(&csa[i]->node) == &csa[i]->node)\n\t\t\tndoms++;\n\t}\n\n\t/*\n\t * Now we know how many domains to create.\n\t * Convert <csn, csa> to <ndoms, doms> and populate cpu masks.\n\t */\n\tdoms = alloc_sched_domains(ndoms);\n\tif (!doms)\n\t\tgoto done;\n\n\t/*\n\t * The rest of the code, including the scheduler, can deal with\n\t * dattr==NULL case. No need to abort if alloc fails.\n\t */\n\tdattr = kmalloc_array(ndoms, sizeof(struct sched_domain_attr),\n\t\t\t      GFP_KERNEL);\n\n\t/*\n\t * Cgroup v2 doesn't support domain attributes, just set all of them\n\t * to SD_ATTR_INIT. Also non-isolating partition root CPUs are a\n\t * subset of HK_TYPE_DOMAIN housekeeping CPUs.\n\t */\n\tif (cgrpv2) {\n\t\tfor (i = 0; i < ndoms; i++) {\n\t\t\t/*\n\t\t\t * The top cpuset may contain some boot time isolated\n\t\t\t * CPUs that need to be excluded from the sched domain.\n\t\t\t */\n\t\t\tif (csa[i] == &top_cpuset)\n\t\t\t\tcpumask_and(doms[i], csa[i]->effective_cpus,\n\t\t\t\t\t    housekeeping_cpumask(HK_TYPE_DOMAIN));\n\t\t\telse\n\t\t\t\tcpumask_copy(doms[i], csa[i]->effective_cpus);\n\t\t\tif (dattr)\n\t\t\t\tdattr[i] = SD_ATTR_INIT;\n\t\t}\n\t\tgoto done;\n\t}\n\n\tfor (nslot = 0, i = 0; i < csn; i++) {\n\t\tnslot_update = 0;\n\t\tfor (j = i; j < csn; j++) {\n\t\t\tif (uf_find(&csa[j]->node) == &csa[i]->node) {\n\t\t\t\tstruct cpumask *dp = doms[nslot];\n\n\t\t\t\tif (i == j) {\n\t\t\t\t\tnslot_update = 1;\n\t\t\t\t\tcpumask_clear(dp);\n\t\t\t\t\tif (dattr)\n\t\t\t\t\t\t*(dattr + nslot) = SD_ATTR_INIT;\n\t\t\t\t}\n\t\t\t\tcpumask_or(dp, dp, csa[j]->effective_cpus);\n\t\t\t\tcpumask_and(dp, dp, housekeeping_cpumask(HK_TYPE_DOMAIN));\n\t\t\t\tif (dattr)\n\t\t\t\t\tupdate_domain_attr_tree(dattr + nslot, csa[j]);\n\t\t\t}\n\t\t}\n\t\tif (nslot_update)\n\t\t\tnslot++;\n\t}\n\tBUG_ON(nslot != ndoms);\n\ndone:\n\tkfree(csa);\n\n\t/*\n\t * Fallback to the default domain if kmalloc() failed.\n\t * See comments in partition_sched_domains().\n\t */\n\tif (doms == NULL)\n\t\tndoms = 1;\n\n\t*domains    = doms;\n\t*attributes = dattr;\n\treturn ndoms;\n}\n\nstatic void dl_update_tasks_root_domain(struct cpuset *cs)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\n\tif (cs->nr_deadline_tasks == 0)\n\t\treturn;\n\n\tcss_task_iter_start(&cs->css, 0, &it);\n\n\twhile ((task = css_task_iter_next(&it)))\n\t\tdl_add_task_root_domain(task);\n\n\tcss_task_iter_end(&it);\n}\n\nstatic void dl_rebuild_rd_accounting(void)\n{\n\tstruct cpuset *cs = NULL;\n\tstruct cgroup_subsys_state *pos_css;\n\n\tlockdep_assert_held(&cpuset_mutex);\n\tlockdep_assert_cpus_held();\n\tlockdep_assert_held(&sched_domains_mutex);\n\n\trcu_read_lock();\n\n\t/*\n\t * Clear default root domain DL accounting, it will be computed again\n\t * if a task belongs to it.\n\t */\n\tdl_clear_root_domain(&def_root_domain);\n\n\tcpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\n\n\t\tif (cpumask_empty(cs->effective_cpus)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcss_get(&cs->css);\n\n\t\trcu_read_unlock();\n\n\t\tdl_update_tasks_root_domain(cs);\n\n\t\trcu_read_lock();\n\t\tcss_put(&cs->css);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void\npartition_and_rebuild_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t\t    struct sched_domain_attr *dattr_new)\n{\n\tmutex_lock(&sched_domains_mutex);\n\tpartition_sched_domains_locked(ndoms_new, doms_new, dattr_new);\n\tdl_rebuild_rd_accounting();\n\tmutex_unlock(&sched_domains_mutex);\n}\n\n/*\n * Rebuild scheduler domains.\n *\n * If the flag 'sched_load_balance' of any cpuset with non-empty\n * 'cpus' changes, or if the 'cpus' allowed changes in any cpuset\n * which has that flag enabled, or if any cpuset with a non-empty\n * 'cpus' is removed, then call this routine to rebuild the\n * scheduler's dynamic sched domains.\n *\n * Call with cpuset_mutex held.  Takes cpus_read_lock().\n */\nvoid rebuild_sched_domains_locked(void)\n{\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct sched_domain_attr *attr;\n\tcpumask_var_t *doms;\n\tstruct cpuset *cs;\n\tint ndoms;\n\n\tlockdep_assert_cpus_held();\n\tlockdep_assert_held(&cpuset_mutex);\n\n\t/*\n\t * If we have raced with CPU hotplug, return early to avoid\n\t * passing doms with offlined cpu to partition_sched_domains().\n\t * Anyways, cpuset_handle_hotplug() will rebuild sched domains.\n\t *\n\t * With no CPUs in any subpartitions, top_cpuset's effective CPUs\n\t * should be the same as the active CPUs, so checking only top_cpuset\n\t * is enough to detect racing CPU offlines.\n\t */\n\tif (cpumask_empty(subpartitions_cpus) &&\n\t    !cpumask_equal(top_cpuset.effective_cpus, cpu_active_mask))\n\t\treturn;\n\n\t/*\n\t * With subpartition CPUs, however, the effective CPUs of a partition\n\t * root should be only a subset of the active CPUs.  Since a CPU in any\n\t * partition root could be offlined, all must be checked.\n\t */\n\tif (!cpumask_empty(subpartitions_cpus)) {\n\t\trcu_read_lock();\n\t\tcpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\n\t\t\tif (!is_partition_valid(cs)) {\n\t\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!cpumask_subset(cs->effective_cpus,\n\t\t\t\t\t    cpu_active_mask)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t/* Generate domain masks and attrs */\n\tndoms = generate_sched_domains(&doms, &attr);\n\n\t/* Have scheduler rebuild the domains */\n\tpartition_and_rebuild_sched_domains(ndoms, doms, attr);\n}\n#else /* !CONFIG_SMP */\nvoid rebuild_sched_domains_locked(void)\n{\n}\n#endif /* CONFIG_SMP */\n\nstatic void rebuild_sched_domains_cpuslocked(void)\n{\n\tmutex_lock(&cpuset_mutex);\n\trebuild_sched_domains_locked();\n\tmutex_unlock(&cpuset_mutex);\n}\n\nvoid rebuild_sched_domains(void)\n{\n\tcpus_read_lock();\n\trebuild_sched_domains_cpuslocked();\n\tcpus_read_unlock();\n}\n\n/**\n * cpuset_update_tasks_cpumask - Update the cpumasks of tasks in the cpuset.\n * @cs: the cpuset in which each task's cpus_allowed mask needs to be changed\n * @new_cpus: the temp variable for the new effective_cpus mask\n *\n * Iterate through each task of @cs updating its cpus_allowed to the\n * effective cpuset's.  As this function is called with cpuset_mutex held,\n * cpuset membership stays stable. For top_cpuset, task_cpu_possible_mask()\n * is used instead of effective_cpus to make sure all offline CPUs are also\n * included as hotplug code won't update cpumasks for tasks in top_cpuset.\n */\nvoid cpuset_update_tasks_cpumask(struct cpuset *cs, struct cpumask *new_cpus)\n{\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\tbool top_cs = cs == &top_cpuset;\n\n\tcss_task_iter_start(&cs->css, 0, &it);\n\twhile ((task = css_task_iter_next(&it))) {\n\t\tconst struct cpumask *possible_mask = task_cpu_possible_mask(task);\n\n\t\tif (top_cs) {\n\t\t\t/*\n\t\t\t * Percpu kthreads in top_cpuset are ignored\n\t\t\t */\n\t\t\tif (kthread_is_per_cpu(task))\n\t\t\t\tcontinue;\n\t\t\tcpumask_andnot(new_cpus, possible_mask, subpartitions_cpus);\n\t\t} else {\n\t\t\tcpumask_and(new_cpus, possible_mask, cs->effective_cpus);\n\t\t}\n\t\tset_cpus_allowed_ptr(task, new_cpus);\n\t}\n\tcss_task_iter_end(&it);\n}\n\n/**\n * compute_effective_cpumask - Compute the effective cpumask of the cpuset\n * @new_cpus: the temp variable for the new effective_cpus mask\n * @cs: the cpuset the need to recompute the new effective_cpus mask\n * @parent: the parent cpuset\n *\n * The result is valid only if the given cpuset isn't a partition root.\n */\nstatic void compute_effective_cpumask(struct cpumask *new_cpus,\n\t\t\t\t      struct cpuset *cs, struct cpuset *parent)\n{\n\tcpumask_and(new_cpus, cs->cpus_allowed, parent->effective_cpus);\n}\n\n/*\n * Commands for update_parent_effective_cpumask\n */\nenum partition_cmd {\n\tpartcmd_enable,\t\t/* Enable partition root\t  */\n\tpartcmd_enablei,\t/* Enable isolated partition root */\n\tpartcmd_disable,\t/* Disable partition root\t  */\n\tpartcmd_update,\t\t/* Update parent's effective_cpus */\n\tpartcmd_invalidate,\t/* Make partition invalid\t  */\n};\n\nstatic void update_sibling_cpumasks(struct cpuset *parent, struct cpuset *cs,\n\t\t\t\t    struct tmpmasks *tmp);\n\n/*\n * Update partition exclusive flag\n *\n * Return: 0 if successful, an error code otherwise\n */\nstatic int update_partition_exclusive(struct cpuset *cs, int new_prs)\n{\n\tbool exclusive = (new_prs > PRS_MEMBER);\n\n\tif (exclusive && !is_cpu_exclusive(cs)) {\n\t\tif (cpuset_update_flag(CS_CPU_EXCLUSIVE, cs, 1))\n\t\t\treturn PERR_NOTEXCL;\n\t} else if (!exclusive && is_cpu_exclusive(cs)) {\n\t\t/* Turning off CS_CPU_EXCLUSIVE will not return error */\n\t\tcpuset_update_flag(CS_CPU_EXCLUSIVE, cs, 0);\n\t}\n\treturn 0;\n}\n\n/*\n * Update partition load balance flag and/or rebuild sched domain\n *\n * Changing load balance flag will automatically call\n * rebuild_sched_domains_locked().\n * This function is for cgroup v2 only.\n */\nstatic void update_partition_sd_lb(struct cpuset *cs, int old_prs)\n{\n\tint new_prs = cs->partition_root_state;\n\tbool rebuild_domains = (new_prs > 0) || (old_prs > 0);\n\tbool new_lb;\n\n\t/*\n\t * If cs is not a valid partition root, the load balance state\n\t * will follow its parent.\n\t */\n\tif (new_prs > 0) {\n\t\tnew_lb = (new_prs != PRS_ISOLATED);\n\t} else {\n\t\tnew_lb = is_sched_load_balance(parent_cs(cs));\n\t}\n\tif (new_lb != !!is_sched_load_balance(cs)) {\n\t\trebuild_domains = true;\n\t\tif (new_lb)\n\t\t\tset_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\t\telse\n\t\t\tclear_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\t}\n\n\tif (rebuild_domains && !force_sd_rebuild)\n\t\trebuild_sched_domains_locked();\n}\n\n/*\n * tasks_nocpu_error - Return true if tasks will have no effective_cpus\n */\nstatic bool tasks_nocpu_error(struct cpuset *parent, struct cpuset *cs,\n\t\t\t      struct cpumask *xcpus)\n{\n\t/*\n\t * A populated partition (cs or parent) can't have empty effective_cpus\n\t */\n\treturn (cpumask_subset(parent->effective_cpus, xcpus) &&\n\t\tpartition_is_populated(parent, cs)) ||\n\t       (!cpumask_intersects(xcpus, cpu_active_mask) &&\n\t\tpartition_is_populated(cs, NULL));\n}\n\nstatic void reset_partition_data(struct cpuset *cs)\n{\n\tstruct cpuset *parent = parent_cs(cs);\n\n\tif (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys))\n\t\treturn;\n\n\tlockdep_assert_held(&callback_lock);\n\n\tcs->nr_subparts = 0;\n\tif (cpumask_empty(cs->exclusive_cpus)) {\n\t\tcpumask_clear(cs->effective_xcpus);\n\t\tif (is_cpu_exclusive(cs))\n\t\t\tclear_bit(CS_CPU_EXCLUSIVE, &cs->flags);\n\t}\n\tif (!cpumask_and(cs->effective_cpus, parent->effective_cpus, cs->cpus_allowed))\n\t\tcpumask_copy(cs->effective_cpus, parent->effective_cpus);\n}\n\n/*\n * partition_xcpus_newstate - Exclusive CPUs state change\n * @old_prs: old partition_root_state\n * @new_prs: new partition_root_state\n * @xcpus: exclusive CPUs with state change\n */\nstatic void partition_xcpus_newstate(int old_prs, int new_prs, struct cpumask *xcpus)\n{\n\tWARN_ON_ONCE(old_prs == new_prs);\n\tif (new_prs == PRS_ISOLATED)\n\t\tcpumask_or(isolated_cpus, isolated_cpus, xcpus);\n\telse\n\t\tcpumask_andnot(isolated_cpus, isolated_cpus, xcpus);\n}\n\n/*\n * partition_xcpus_add - Add new exclusive CPUs to partition\n * @new_prs: new partition_root_state\n * @parent: parent cpuset\n * @xcpus: exclusive CPUs to be added\n * Return: true if isolated_cpus modified, false otherwise\n *\n * Remote partition if parent == NULL\n */\nstatic bool partition_xcpus_add(int new_prs, struct cpuset *parent,\n\t\t\t\tstruct cpumask *xcpus)\n{\n\tbool isolcpus_updated;\n\n\tWARN_ON_ONCE(new_prs < 0);\n\tlockdep_assert_held(&callback_lock);\n\tif (!parent)\n\t\tparent = &top_cpuset;\n\n\n\tif (parent == &top_cpuset)\n\t\tcpumask_or(subpartitions_cpus, subpartitions_cpus, xcpus);\n\n\tisolcpus_updated = (new_prs != parent->partition_root_state);\n\tif (isolcpus_updated)\n\t\tpartition_xcpus_newstate(parent->partition_root_state, new_prs,\n\t\t\t\t\t xcpus);\n\n\tcpumask_andnot(parent->effective_cpus, parent->effective_cpus, xcpus);\n\treturn isolcpus_updated;\n}\n\n/*\n * partition_xcpus_del - Remove exclusive CPUs from partition\n * @old_prs: old partition_root_state\n * @parent: parent cpuset\n * @xcpus: exclusive CPUs to be removed\n * Return: true if isolated_cpus modified, false otherwise\n *\n * Remote partition if parent == NULL\n */\nstatic bool partition_xcpus_del(int old_prs, struct cpuset *parent,\n\t\t\t\tstruct cpumask *xcpus)\n{\n\tbool isolcpus_updated;\n\n\tWARN_ON_ONCE(old_prs < 0);\n\tlockdep_assert_held(&callback_lock);\n\tif (!parent)\n\t\tparent = &top_cpuset;\n\n\tif (parent == &top_cpuset)\n\t\tcpumask_andnot(subpartitions_cpus, subpartitions_cpus, xcpus);\n\n\tisolcpus_updated = (old_prs != parent->partition_root_state);\n\tif (isolcpus_updated)\n\t\tpartition_xcpus_newstate(old_prs, parent->partition_root_state,\n\t\t\t\t\t xcpus);\n\n\tcpumask_and(xcpus, xcpus, cpu_active_mask);\n\tcpumask_or(parent->effective_cpus, parent->effective_cpus, xcpus);\n\treturn isolcpus_updated;\n}\n\nstatic void update_unbound_workqueue_cpumask(bool isolcpus_updated)\n{\n\tint ret;\n\n\tlockdep_assert_cpus_held();\n\n\tif (!isolcpus_updated)\n\t\treturn;\n\n\tret = workqueue_unbound_exclude_cpumask(isolated_cpus);\n\tWARN_ON_ONCE(ret < 0);\n}\n\n/**\n * cpuset_cpu_is_isolated - Check if the given CPU is isolated\n * @cpu: the CPU number to be checked\n * Return: true if CPU is used in an isolated partition, false otherwise\n */\nbool cpuset_cpu_is_isolated(int cpu)\n{\n\treturn cpumask_test_cpu(cpu, isolated_cpus);\n}\nEXPORT_SYMBOL_GPL(cpuset_cpu_is_isolated);\n\n/*\n * compute_effective_exclusive_cpumask - compute effective exclusive CPUs\n * @cs: cpuset\n * @xcpus: effective exclusive CPUs value to be set\n * Return: true if xcpus is not empty, false otherwise.\n *\n * Starting with exclusive_cpus (cpus_allowed if exclusive_cpus is not set),\n * it must be a subset of parent's effective_xcpus.\n */\nstatic bool compute_effective_exclusive_cpumask(struct cpuset *cs,\n\t\t\t\t\t\tstruct cpumask *xcpus)\n{\n\tstruct cpuset *parent = parent_cs(cs);\n\n\tif (!xcpus)\n\t\txcpus = cs->effective_xcpus;\n\n\treturn cpumask_and(xcpus, user_xcpus(cs), parent->effective_xcpus);\n}\n\nstatic inline bool is_remote_partition(struct cpuset *cs)\n{\n\treturn !list_empty(&cs->remote_sibling);\n}\n\nstatic inline bool is_local_partition(struct cpuset *cs)\n{\n\treturn is_partition_valid(cs) && !is_remote_partition(cs);\n}\n\n/*\n * remote_partition_enable - Enable current cpuset as a remote partition root\n * @cs: the cpuset to update\n * @new_prs: new partition_root_state\n * @tmp: temparary masks\n * Return: 0 if successful, errcode if error\n *\n * Enable the current cpuset to become a remote partition root taking CPUs\n * directly from the top cpuset. cpuset_mutex must be held by the caller.\n */\nstatic int remote_partition_enable(struct cpuset *cs, int new_prs,\n\t\t\t\t   struct tmpmasks *tmp)\n{\n\tbool isolcpus_updated;\n\n\t/*\n\t * The user must have sysadmin privilege.\n\t */\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn PERR_ACCESS;\n\n\t/*\n\t * The requested exclusive_cpus must not be allocated to other\n\t * partitions and it can't use up all the root's effective_cpus.\n\t *\n\t * Note that if there is any local partition root above it or\n\t * remote partition root underneath it, its exclusive_cpus must\n\t * have overlapped with subpartitions_cpus.\n\t */\n\tcompute_effective_exclusive_cpumask(cs, tmp->new_cpus);\n\tif (cpumask_empty(tmp->new_cpus) ||\n\t    cpumask_intersects(tmp->new_cpus, subpartitions_cpus) ||\n\t    cpumask_subset(top_cpuset.effective_cpus, tmp->new_cpus))\n\t\treturn PERR_INVCPUS;\n\n\tspin_lock_irq(&callback_lock);\n\tisolcpus_updated = partition_xcpus_add(new_prs, NULL, tmp->new_cpus);\n\tlist_add(&cs->remote_sibling, &remote_children);\n\tspin_unlock_irq(&callback_lock);\n\tupdate_unbound_workqueue_cpumask(isolcpus_updated);\n\n\t/*\n\t * Proprogate changes in top_cpuset's effective_cpus down the hierarchy.\n\t */\n\tcpuset_update_tasks_cpumask(&top_cpuset, tmp->new_cpus);\n\tupdate_sibling_cpumasks(&top_cpuset, NULL, tmp);\n\treturn 0;\n}\n\n/*\n * remote_partition_disable - Remove current cpuset from remote partition list\n * @cs: the cpuset to update\n * @tmp: temparary masks\n *\n * The effective_cpus is also updated.\n *\n * cpuset_mutex must be held by the caller.\n */\nstatic void remote_partition_disable(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tbool isolcpus_updated;\n\n\tcompute_effective_exclusive_cpumask(cs, tmp->new_cpus);\n\tWARN_ON_ONCE(!is_remote_partition(cs));\n\tWARN_ON_ONCE(!cpumask_subset(tmp->new_cpus, subpartitions_cpus));\n\n\tspin_lock_irq(&callback_lock);\n\tlist_del_init(&cs->remote_sibling);\n\tisolcpus_updated = partition_xcpus_del(cs->partition_root_state,\n\t\t\t\t\t       NULL, tmp->new_cpus);\n\tcs->partition_root_state = -cs->partition_root_state;\n\tif (!cs->prs_err)\n\t\tcs->prs_err = PERR_INVCPUS;\n\treset_partition_data(cs);\n\tspin_unlock_irq(&callback_lock);\n\tupdate_unbound_workqueue_cpumask(isolcpus_updated);\n\n\t/*\n\t * Proprogate changes in top_cpuset's effective_cpus down the hierarchy.\n\t */\n\tcpuset_update_tasks_cpumask(&top_cpuset, tmp->new_cpus);\n\tupdate_sibling_cpumasks(&top_cpuset, NULL, tmp);\n}\n\n/*\n * remote_cpus_update - cpus_exclusive change of remote partition\n * @cs: the cpuset to be updated\n * @newmask: the new effective_xcpus mask\n * @tmp: temparary masks\n *\n * top_cpuset and subpartitions_cpus will be updated or partition can be\n * invalidated.\n */\nstatic void remote_cpus_update(struct cpuset *cs, struct cpumask *newmask,\n\t\t\t       struct tmpmasks *tmp)\n{\n\tbool adding, deleting;\n\tint prs = cs->partition_root_state;\n\tint isolcpus_updated = 0;\n\n\tif (WARN_ON_ONCE(!is_remote_partition(cs)))\n\t\treturn;\n\n\tWARN_ON_ONCE(!cpumask_subset(cs->effective_xcpus, subpartitions_cpus));\n\n\tif (cpumask_empty(newmask))\n\t\tgoto invalidate;\n\n\tadding   = cpumask_andnot(tmp->addmask, newmask, cs->effective_xcpus);\n\tdeleting = cpumask_andnot(tmp->delmask, cs->effective_xcpus, newmask);\n\n\t/*\n\t * Additions of remote CPUs is only allowed if those CPUs are\n\t * not allocated to other partitions and there are effective_cpus\n\t * left in the top cpuset.\n\t */\n\tif (adding && (!capable(CAP_SYS_ADMIN) ||\n\t\t       cpumask_intersects(tmp->addmask, subpartitions_cpus) ||\n\t\t       cpumask_subset(top_cpuset.effective_cpus, tmp->addmask)))\n\t\tgoto invalidate;\n\n\tspin_lock_irq(&callback_lock);\n\tif (adding)\n\t\tisolcpus_updated += partition_xcpus_add(prs, NULL, tmp->addmask);\n\tif (deleting)\n\t\tisolcpus_updated += partition_xcpus_del(prs, NULL, tmp->delmask);\n\tspin_unlock_irq(&callback_lock);\n\tupdate_unbound_workqueue_cpumask(isolcpus_updated);\n\n\t/*\n\t * Proprogate changes in top_cpuset's effective_cpus down the hierarchy.\n\t */\n\tcpuset_update_tasks_cpumask(&top_cpuset, tmp->new_cpus);\n\tupdate_sibling_cpumasks(&top_cpuset, NULL, tmp);\n\treturn;\n\ninvalidate:\n\tremote_partition_disable(cs, tmp);\n}\n\n/*\n * remote_partition_check - check if a child remote partition needs update\n * @cs: the cpuset to be updated\n * @newmask: the new effective_xcpus mask\n * @delmask: temporary mask for deletion (not in tmp)\n * @tmp: temparary masks\n *\n * This should be called before the given cs has updated its cpus_allowed\n * and/or effective_xcpus.\n */\nstatic void remote_partition_check(struct cpuset *cs, struct cpumask *newmask,\n\t\t\t\t   struct cpumask *delmask, struct tmpmasks *tmp)\n{\n\tstruct cpuset *child, *next;\n\tint disable_cnt = 0;\n\n\t/*\n\t * Compute the effective exclusive CPUs that will be deleted.\n\t */\n\tif (!cpumask_andnot(delmask, cs->effective_xcpus, newmask) ||\n\t    !cpumask_intersects(delmask, subpartitions_cpus))\n\t\treturn;\t/* No deletion of exclusive CPUs in partitions */\n\n\t/*\n\t * Searching the remote children list to look for those that will\n\t * be impacted by the deletion of exclusive CPUs.\n\t *\n\t * Since a cpuset must be removed from the remote children list\n\t * before it can go offline and holding cpuset_mutex will prevent\n\t * any change in cpuset status. RCU read lock isn't needed.\n\t */\n\tlockdep_assert_held(&cpuset_mutex);\n\tlist_for_each_entry_safe(child, next, &remote_children, remote_sibling)\n\t\tif (cpumask_intersects(child->effective_cpus, delmask)) {\n\t\t\tremote_partition_disable(child, tmp);\n\t\t\tdisable_cnt++;\n\t\t}\n\tif (disable_cnt && !force_sd_rebuild)\n\t\trebuild_sched_domains_locked();\n}\n\n/*\n * prstate_housekeeping_conflict - check for partition & housekeeping conflicts\n * @prstate: partition root state to be checked\n * @new_cpus: cpu mask\n * Return: true if there is conflict, false otherwise\n *\n * CPUs outside of boot_hk_cpus, if defined, can only be used in an\n * isolated partition.\n */\nstatic bool prstate_housekeeping_conflict(int prstate, struct cpumask *new_cpus)\n{\n\tif (!have_boot_isolcpus)\n\t\treturn false;\n\n\tif ((prstate != PRS_ISOLATED) && !cpumask_subset(new_cpus, boot_hk_cpus))\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * update_parent_effective_cpumask - update effective_cpus mask of parent cpuset\n * @cs:      The cpuset that requests change in partition root state\n * @cmd:     Partition root state change command\n * @newmask: Optional new cpumask for partcmd_update\n * @tmp:     Temporary addmask and delmask\n * Return:   0 or a partition root state error code\n *\n * For partcmd_enable*, the cpuset is being transformed from a non-partition\n * root to a partition root. The effective_xcpus (cpus_allowed if\n * effective_xcpus not set) mask of the given cpuset will be taken away from\n * parent's effective_cpus. The function will return 0 if all the CPUs listed\n * in effective_xcpus can be granted or an error code will be returned.\n *\n * For partcmd_disable, the cpuset is being transformed from a partition\n * root back to a non-partition root. Any CPUs in effective_xcpus will be\n * given back to parent's effective_cpus. 0 will always be returned.\n *\n * For partcmd_update, if the optional newmask is specified, the cpu list is\n * to be changed from effective_xcpus to newmask. Otherwise, effective_xcpus is\n * assumed to remain the same. The cpuset should either be a valid or invalid\n * partition root. The partition root state may change from valid to invalid\n * or vice versa. An error code will be returned if transitioning from\n * invalid to valid violates the exclusivity rule.\n *\n * For partcmd_invalidate, the current partition will be made invalid.\n *\n * The partcmd_enable* and partcmd_disable commands are used by\n * update_prstate(). An error code may be returned and the caller will check\n * for error.\n *\n * The partcmd_update command is used by update_cpumasks_hier() with newmask\n * NULL and update_cpumask() with newmask set. The partcmd_invalidate is used\n * by update_cpumask() with NULL newmask. In both cases, the callers won't\n * check for error and so partition_root_state and prs_error will be updated\n * directly.\n */\nstatic int update_parent_effective_cpumask(struct cpuset *cs, int cmd,\n\t\t\t\t\t   struct cpumask *newmask,\n\t\t\t\t\t   struct tmpmasks *tmp)\n{\n\tstruct cpuset *parent = parent_cs(cs);\n\tint adding;\t/* Adding cpus to parent's effective_cpus\t*/\n\tint deleting;\t/* Deleting cpus from parent's effective_cpus\t*/\n\tint old_prs, new_prs;\n\tint part_error = PERR_NONE;\t/* Partition error? */\n\tint subparts_delta = 0;\n\tstruct cpumask *xcpus;\t\t/* cs effective_xcpus */\n\tint isolcpus_updated = 0;\n\tbool nocpu;\n\n\tlockdep_assert_held(&cpuset_mutex);\n\n\t/*\n\t * new_prs will only be changed for the partcmd_update and\n\t * partcmd_invalidate commands.\n\t */\n\tadding = deleting = false;\n\told_prs = new_prs = cs->partition_root_state;\n\txcpus = user_xcpus(cs);\n\n\tif (cmd == partcmd_invalidate) {\n\t\tif (is_prs_invalid(old_prs))\n\t\t\treturn 0;\n\n\t\t/*\n\t\t * Make the current partition invalid.\n\t\t */\n\t\tif (is_partition_valid(parent))\n\t\t\tadding = cpumask_and(tmp->addmask,\n\t\t\t\t\t     xcpus, parent->effective_xcpus);\n\t\tif (old_prs > 0) {\n\t\t\tnew_prs = -old_prs;\n\t\t\tsubparts_delta--;\n\t\t}\n\t\tgoto write_error;\n\t}\n\n\t/*\n\t * The parent must be a partition root.\n\t * The new cpumask, if present, or the current cpus_allowed must\n\t * not be empty.\n\t */\n\tif (!is_partition_valid(parent)) {\n\t\treturn is_partition_invalid(parent)\n\t\t       ? PERR_INVPARENT : PERR_NOTPART;\n\t}\n\tif (!newmask && xcpus_empty(cs))\n\t\treturn PERR_CPUSEMPTY;\n\n\tnocpu = tasks_nocpu_error(parent, cs, xcpus);\n\n\tif ((cmd == partcmd_enable) || (cmd == partcmd_enablei)) {\n\t\t/*\n\t\t * Enabling partition root is not allowed if its\n\t\t * effective_xcpus is empty or doesn't overlap with\n\t\t * parent's effective_xcpus.\n\t\t */\n\t\tif (cpumask_empty(xcpus) ||\n\t\t    !cpumask_intersects(xcpus, parent->effective_xcpus))\n\t\t\treturn PERR_INVCPUS;\n\n\t\tif (prstate_housekeeping_conflict(new_prs, xcpus))\n\t\t\treturn PERR_HKEEPING;\n\n\t\t/*\n\t\t * A parent can be left with no CPU as long as there is no\n\t\t * task directly associated with the parent partition.\n\t\t */\n\t\tif (nocpu)\n\t\t\treturn PERR_NOCPUS;\n\n\t\tcpumask_copy(tmp->delmask, xcpus);\n\t\tdeleting = true;\n\t\tsubparts_delta++;\n\t\tnew_prs = (cmd == partcmd_enable) ? PRS_ROOT : PRS_ISOLATED;\n\t} else if (cmd == partcmd_disable) {\n\t\t/*\n\t\t * May need to add cpus to parent's effective_cpus for\n\t\t * valid partition root.\n\t\t */\n\t\tadding = !is_prs_invalid(old_prs) &&\n\t\t\t  cpumask_and(tmp->addmask, xcpus, parent->effective_xcpus);\n\t\tif (adding)\n\t\t\tsubparts_delta--;\n\t\tnew_prs = PRS_MEMBER;\n\t} else if (newmask) {\n\t\t/*\n\t\t * Empty cpumask is not allowed\n\t\t */\n\t\tif (cpumask_empty(newmask)) {\n\t\t\tpart_error = PERR_CPUSEMPTY;\n\t\t\tgoto write_error;\n\t\t}\n\t\t/* Check newmask again, whether cpus are available for parent/cs */\n\t\tnocpu |= tasks_nocpu_error(parent, cs, newmask);\n\n\t\t/*\n\t\t * partcmd_update with newmask:\n\t\t *\n\t\t * Compute add/delete mask to/from effective_cpus\n\t\t *\n\t\t * For valid partition:\n\t\t *   addmask = exclusive_cpus & ~newmask\n\t\t *\t\t\t      & parent->effective_xcpus\n\t\t *   delmask = newmask & ~exclusive_cpus\n\t\t *\t\t       & parent->effective_xcpus\n\t\t *\n\t\t * For invalid partition:\n\t\t *   delmask = newmask & parent->effective_xcpus\n\t\t */\n\t\tif (is_prs_invalid(old_prs)) {\n\t\t\tadding = false;\n\t\t\tdeleting = cpumask_and(tmp->delmask,\n\t\t\t\t\tnewmask, parent->effective_xcpus);\n\t\t} else {\n\t\t\tcpumask_andnot(tmp->addmask, xcpus, newmask);\n\t\t\tadding = cpumask_and(tmp->addmask, tmp->addmask,\n\t\t\t\t\t     parent->effective_xcpus);\n\n\t\t\tcpumask_andnot(tmp->delmask, newmask, xcpus);\n\t\t\tdeleting = cpumask_and(tmp->delmask, tmp->delmask,\n\t\t\t\t\t       parent->effective_xcpus);\n\t\t}\n\t\t/*\n\t\t * Make partition invalid if parent's effective_cpus could\n\t\t * become empty and there are tasks in the parent.\n\t\t */\n\t\tif (nocpu && (!adding ||\n\t\t    !cpumask_intersects(tmp->addmask, cpu_active_mask))) {\n\t\t\tpart_error = PERR_NOCPUS;\n\t\t\tdeleting = false;\n\t\t\tadding = cpumask_and(tmp->addmask,\n\t\t\t\t\t     xcpus, parent->effective_xcpus);\n\t\t}\n\t} else {\n\t\t/*\n\t\t * partcmd_update w/o newmask\n\t\t *\n\t\t * delmask = effective_xcpus & parent->effective_cpus\n\t\t *\n\t\t * This can be called from:\n\t\t * 1) update_cpumasks_hier()\n\t\t * 2) cpuset_hotplug_update_tasks()\n\t\t *\n\t\t * Check to see if it can be transitioned from valid to\n\t\t * invalid partition or vice versa.\n\t\t *\n\t\t * A partition error happens when parent has tasks and all\n\t\t * its effective CPUs will have to be distributed out.\n\t\t */\n\t\tWARN_ON_ONCE(!is_partition_valid(parent));\n\t\tif (nocpu) {\n\t\t\tpart_error = PERR_NOCPUS;\n\t\t\tif (is_partition_valid(cs))\n\t\t\t\tadding = cpumask_and(tmp->addmask,\n\t\t\t\t\t\txcpus, parent->effective_xcpus);\n\t\t} else if (is_partition_invalid(cs) &&\n\t\t\t   cpumask_subset(xcpus, parent->effective_xcpus)) {\n\t\t\tstruct cgroup_subsys_state *css;\n\t\t\tstruct cpuset *child;\n\t\t\tbool exclusive = true;\n\n\t\t\t/*\n\t\t\t * Convert invalid partition to valid has to\n\t\t\t * pass the cpu exclusivity test.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tcpuset_for_each_child(child, css, parent) {\n\t\t\t\tif (child == cs)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!cpusets_are_exclusive(cs, child)) {\n\t\t\t\t\texclusive = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\trcu_read_unlock();\n\t\t\tif (exclusive)\n\t\t\t\tdeleting = cpumask_and(tmp->delmask,\n\t\t\t\t\t\txcpus, parent->effective_cpus);\n\t\t\telse\n\t\t\t\tpart_error = PERR_NOTEXCL;\n\t\t}\n\t}\n\nwrite_error:\n\tif (part_error)\n\t\tWRITE_ONCE(cs->prs_err, part_error);\n\n\tif (cmd == partcmd_update) {\n\t\t/*\n\t\t * Check for possible transition between valid and invalid\n\t\t * partition root.\n\t\t */\n\t\tswitch (cs->partition_root_state) {\n\t\tcase PRS_ROOT:\n\t\tcase PRS_ISOLATED:\n\t\t\tif (part_error) {\n\t\t\t\tnew_prs = -old_prs;\n\t\t\t\tsubparts_delta--;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase PRS_INVALID_ROOT:\n\t\tcase PRS_INVALID_ISOLATED:\n\t\t\tif (!part_error) {\n\t\t\t\tnew_prs = -old_prs;\n\t\t\t\tsubparts_delta++;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!adding && !deleting && (new_prs == old_prs))\n\t\treturn 0;\n\n\t/*\n\t * Transitioning between invalid to valid or vice versa may require\n\t * changing CS_CPU_EXCLUSIVE. In the case of partcmd_update,\n\t * validate_change() has already been successfully called and\n\t * CPU lists in cs haven't been updated yet. So defer it to later.\n\t */\n\tif ((old_prs != new_prs) && (cmd != partcmd_update))  {\n\t\tint err = update_partition_exclusive(cs, new_prs);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t/*\n\t * Change the parent's effective_cpus & effective_xcpus (top cpuset\n\t * only).\n\t *\n\t * Newly added CPUs will be removed from effective_cpus and\n\t * newly deleted ones will be added back to effective_cpus.\n\t */\n\tspin_lock_irq(&callback_lock);\n\tif (old_prs != new_prs) {\n\t\tcs->partition_root_state = new_prs;\n\t\tif (new_prs <= 0)\n\t\t\tcs->nr_subparts = 0;\n\t}\n\t/*\n\t * Adding to parent's effective_cpus means deletion CPUs from cs\n\t * and vice versa.\n\t */\n\tif (adding)\n\t\tisolcpus_updated += partition_xcpus_del(old_prs, parent,\n\t\t\t\t\t\t\ttmp->addmask);\n\tif (deleting)\n\t\tisolcpus_updated += partition_xcpus_add(new_prs, parent,\n\t\t\t\t\t\t\ttmp->delmask);\n\n\tif (is_partition_valid(parent)) {\n\t\tparent->nr_subparts += subparts_delta;\n\t\tWARN_ON_ONCE(parent->nr_subparts < 0);\n\t}\n\tspin_unlock_irq(&callback_lock);\n\tupdate_unbound_workqueue_cpumask(isolcpus_updated);\n\n\tif ((old_prs != new_prs) && (cmd == partcmd_update))\n\t\tupdate_partition_exclusive(cs, new_prs);\n\n\tif (adding || deleting) {\n\t\tcpuset_update_tasks_cpumask(parent, tmp->addmask);\n\t\tupdate_sibling_cpumasks(parent, cs, tmp);\n\t}\n\n\t/*\n\t * For partcmd_update without newmask, it is being called from\n\t * cpuset_handle_hotplug(). Update the load balance flag and\n\t * scheduling domain accordingly.\n\t */\n\tif ((cmd == partcmd_update) && !newmask)\n\t\tupdate_partition_sd_lb(cs, old_prs);\n\n\tnotify_partition_change(cs, old_prs);\n\treturn 0;\n}\n\n/**\n * compute_partition_effective_cpumask - compute effective_cpus for partition\n * @cs: partition root cpuset\n * @new_ecpus: previously computed effective_cpus to be updated\n *\n * Compute the effective_cpus of a partition root by scanning effective_xcpus\n * of child partition roots and excluding their effective_xcpus.\n *\n * This has the side effect of invalidating valid child partition roots,\n * if necessary. Since it is called from either cpuset_hotplug_update_tasks()\n * or update_cpumasks_hier() where parent and children are modified\n * successively, we don't need to call update_parent_effective_cpumask()\n * and the child's effective_cpus will be updated in later iterations.\n *\n * Note that rcu_read_lock() is assumed to be held.\n */\nstatic void compute_partition_effective_cpumask(struct cpuset *cs,\n\t\t\t\t\t\tstruct cpumask *new_ecpus)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *child;\n\tbool populated = partition_is_populated(cs, NULL);\n\n\t/*\n\t * Check child partition roots to see if they should be\n\t * invalidated when\n\t *  1) child effective_xcpus not a subset of new\n\t *     excluisve_cpus\n\t *  2) All the effective_cpus will be used up and cp\n\t *     has tasks\n\t */\n\tcompute_effective_exclusive_cpumask(cs, new_ecpus);\n\tcpumask_and(new_ecpus, new_ecpus, cpu_active_mask);\n\n\trcu_read_lock();\n\tcpuset_for_each_child(child, css, cs) {\n\t\tif (!is_partition_valid(child))\n\t\t\tcontinue;\n\n\t\tchild->prs_err = 0;\n\t\tif (!cpumask_subset(child->effective_xcpus,\n\t\t\t\t    cs->effective_xcpus))\n\t\t\tchild->prs_err = PERR_INVCPUS;\n\t\telse if (populated &&\n\t\t\t cpumask_subset(new_ecpus, child->effective_xcpus))\n\t\t\tchild->prs_err = PERR_NOCPUS;\n\n\t\tif (child->prs_err) {\n\t\t\tint old_prs = child->partition_root_state;\n\n\t\t\t/*\n\t\t\t * Invalidate child partition\n\t\t\t */\n\t\t\tspin_lock_irq(&callback_lock);\n\t\t\tmake_partition_invalid(child);\n\t\t\tcs->nr_subparts--;\n\t\t\tchild->nr_subparts = 0;\n\t\t\tspin_unlock_irq(&callback_lock);\n\t\t\tnotify_partition_change(child, old_prs);\n\t\t\tcontinue;\n\t\t}\n\t\tcpumask_andnot(new_ecpus, new_ecpus,\n\t\t\t       child->effective_xcpus);\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * update_cpumasks_hier() flags\n */\n#define HIER_CHECKALL\t\t0x01\t/* Check all cpusets with no skipping */\n#define HIER_NO_SD_REBUILD\t0x02\t/* Don't rebuild sched domains */\n\n/*\n * update_cpumasks_hier - Update effective cpumasks and tasks in the subtree\n * @cs:  the cpuset to consider\n * @tmp: temp variables for calculating effective_cpus & partition setup\n * @force: don't skip any descendant cpusets if set\n *\n * When configured cpumask is changed, the effective cpumasks of this cpuset\n * and all its descendants need to be updated.\n *\n * On legacy hierarchy, effective_cpus will be the same with cpu_allowed.\n *\n * Called with cpuset_mutex held\n */\nstatic void update_cpumasks_hier(struct cpuset *cs, struct tmpmasks *tmp,\n\t\t\t\t int flags)\n{\n\tstruct cpuset *cp;\n\tstruct cgroup_subsys_state *pos_css;\n\tbool need_rebuild_sched_domains = false;\n\tint old_prs, new_prs;\n\n\trcu_read_lock();\n\tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n\t\tstruct cpuset *parent = parent_cs(cp);\n\t\tbool remote = is_remote_partition(cp);\n\t\tbool update_parent = false;\n\n\t\t/*\n\t\t * Skip descendent remote partition that acquires CPUs\n\t\t * directly from top cpuset unless it is cs.\n\t\t */\n\t\tif (remote && (cp != cs)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * Update effective_xcpus if exclusive_cpus set.\n\t\t * The case when exclusive_cpus isn't set is handled later.\n\t\t */\n\t\tif (!cpumask_empty(cp->exclusive_cpus) && (cp != cs)) {\n\t\t\tspin_lock_irq(&callback_lock);\n\t\t\tcompute_effective_exclusive_cpumask(cp, NULL);\n\t\t\tspin_unlock_irq(&callback_lock);\n\t\t}\n\n\t\told_prs = new_prs = cp->partition_root_state;\n\t\tif (remote || (is_partition_valid(parent) &&\n\t\t\t       is_partition_valid(cp)))\n\t\t\tcompute_partition_effective_cpumask(cp, tmp->new_cpus);\n\t\telse\n\t\t\tcompute_effective_cpumask(tmp->new_cpus, cp, parent);\n\n\t\t/*\n\t\t * A partition with no effective_cpus is allowed as long as\n\t\t * there is no task associated with it. Call\n\t\t * update_parent_effective_cpumask() to check it.\n\t\t */\n\t\tif (is_partition_valid(cp) && cpumask_empty(tmp->new_cpus)) {\n\t\t\tupdate_parent = true;\n\t\t\tgoto update_parent_effective;\n\t\t}\n\n\t\t/*\n\t\t * If it becomes empty, inherit the effective mask of the\n\t\t * parent, which is guaranteed to have some CPUs unless\n\t\t * it is a partition root that has explicitly distributed\n\t\t * out all its CPUs.\n\t\t */\n\t\tif (is_in_v2_mode() && !remote && cpumask_empty(tmp->new_cpus))\n\t\t\tcpumask_copy(tmp->new_cpus, parent->effective_cpus);\n\n\t\tif (remote)\n\t\t\tgoto get_css;\n\n\t\t/*\n\t\t * Skip the whole subtree if\n\t\t * 1) the cpumask remains the same,\n\t\t * 2) has no partition root state,\n\t\t * 3) HIER_CHECKALL flag not set, and\n\t\t * 4) for v2 load balance state same as its parent.\n\t\t */\n\t\tif (!cp->partition_root_state && !(flags & HIER_CHECKALL) &&\n\t\t    cpumask_equal(tmp->new_cpus, cp->effective_cpus) &&\n\t\t    (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t\t    (is_sched_load_balance(parent) == is_sched_load_balance(cp)))) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\nupdate_parent_effective:\n\t\t/*\n\t\t * update_parent_effective_cpumask() should have been called\n\t\t * for cs already in update_cpumask(). We should also call\n\t\t * cpuset_update_tasks_cpumask() again for tasks in the parent\n\t\t * cpuset if the parent's effective_cpus changes.\n\t\t */\n\t\tif ((cp != cs) && old_prs) {\n\t\t\tswitch (parent->partition_root_state) {\n\t\t\tcase PRS_ROOT:\n\t\t\tcase PRS_ISOLATED:\n\t\t\t\tupdate_parent = true;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\t/*\n\t\t\t\t * When parent is not a partition root or is\n\t\t\t\t * invalid, child partition roots become\n\t\t\t\t * invalid too.\n\t\t\t\t */\n\t\t\t\tif (is_partition_valid(cp))\n\t\t\t\t\tnew_prs = -cp->partition_root_state;\n\t\t\t\tWRITE_ONCE(cp->prs_err,\n\t\t\t\t\t   is_partition_invalid(parent)\n\t\t\t\t\t   ? PERR_INVPARENT : PERR_NOTPART);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\nget_css:\n\t\tif (!css_tryget_online(&cp->css))\n\t\t\tcontinue;\n\t\trcu_read_unlock();\n\n\t\tif (update_parent) {\n\t\t\tupdate_parent_effective_cpumask(cp, partcmd_update, NULL, tmp);\n\t\t\t/*\n\t\t\t * The cpuset partition_root_state may become\n\t\t\t * invalid. Capture it.\n\t\t\t */\n\t\t\tnew_prs = cp->partition_root_state;\n\t\t}\n\n\t\tspin_lock_irq(&callback_lock);\n\t\tcpumask_copy(cp->effective_cpus, tmp->new_cpus);\n\t\tcp->partition_root_state = new_prs;\n\t\t/*\n\t\t * Make sure effective_xcpus is properly set for a valid\n\t\t * partition root.\n\t\t */\n\t\tif ((new_prs > 0) && cpumask_empty(cp->exclusive_cpus))\n\t\t\tcpumask_and(cp->effective_xcpus,\n\t\t\t\t    cp->cpus_allowed, parent->effective_xcpus);\n\t\telse if (new_prs < 0)\n\t\t\treset_partition_data(cp);\n\t\tspin_unlock_irq(&callback_lock);\n\n\t\tnotify_partition_change(cp, old_prs);\n\n\t\tWARN_ON(!is_in_v2_mode() &&\n\t\t\t!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));\n\n\t\tcpuset_update_tasks_cpumask(cp, cp->effective_cpus);\n\n\t\t/*\n\t\t * On default hierarchy, inherit the CS_SCHED_LOAD_BALANCE\n\t\t * from parent if current cpuset isn't a valid partition root\n\t\t * and their load balance states differ.\n\t\t */\n\t\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t\t    !is_partition_valid(cp) &&\n\t\t    (is_sched_load_balance(parent) != is_sched_load_balance(cp))) {\n\t\t\tif (is_sched_load_balance(parent))\n\t\t\t\tset_bit(CS_SCHED_LOAD_BALANCE, &cp->flags);\n\t\t\telse\n\t\t\t\tclear_bit(CS_SCHED_LOAD_BALANCE, &cp->flags);\n\t\t}\n\n\t\t/*\n\t\t * On legacy hierarchy, if the effective cpumask of any non-\n\t\t * empty cpuset is changed, we need to rebuild sched domains.\n\t\t * On default hierarchy, the cpuset needs to be a partition\n\t\t * root as well.\n\t\t */\n\t\tif (!cpumask_empty(cp->cpus_allowed) &&\n\t\t    is_sched_load_balance(cp) &&\n\t\t   (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t\t    is_partition_valid(cp)))\n\t\t\tneed_rebuild_sched_domains = true;\n\n\t\trcu_read_lock();\n\t\tcss_put(&cp->css);\n\t}\n\trcu_read_unlock();\n\n\tif (need_rebuild_sched_domains && !(flags & HIER_NO_SD_REBUILD) &&\n\t    !force_sd_rebuild)\n\t\trebuild_sched_domains_locked();\n}\n\n/**\n * update_sibling_cpumasks - Update siblings cpumasks\n * @parent:  Parent cpuset\n * @cs:      Current cpuset\n * @tmp:     Temp variables\n */\nstatic void update_sibling_cpumasks(struct cpuset *parent, struct cpuset *cs,\n\t\t\t\t    struct tmpmasks *tmp)\n{\n\tstruct cpuset *sibling;\n\tstruct cgroup_subsys_state *pos_css;\n\n\tlockdep_assert_held(&cpuset_mutex);\n\n\t/*\n\t * Check all its siblings and call update_cpumasks_hier()\n\t * if their effective_cpus will need to be changed.\n\t *\n\t * It is possible a change in parent's effective_cpus\n\t * due to a change in a child partition's effective_xcpus will impact\n\t * its siblings even if they do not inherit parent's effective_cpus\n\t * directly.\n\t *\n\t * The update_cpumasks_hier() function may sleep. So we have to\n\t * release the RCU read lock before calling it. HIER_NO_SD_REBUILD\n\t * flag is used to suppress rebuild of sched domains as the callers\n\t * will take care of that.\n\t */\n\trcu_read_lock();\n\tcpuset_for_each_child(sibling, pos_css, parent) {\n\t\tif (sibling == cs)\n\t\t\tcontinue;\n\t\tif (!is_partition_valid(sibling)) {\n\t\t\tcompute_effective_cpumask(tmp->new_cpus, sibling,\n\t\t\t\t\t\t  parent);\n\t\t\tif (cpumask_equal(tmp->new_cpus, sibling->effective_cpus))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (!css_tryget_online(&sibling->css))\n\t\t\tcontinue;\n\n\t\trcu_read_unlock();\n\t\tupdate_cpumasks_hier(sibling, tmp, HIER_NO_SD_REBUILD);\n\t\trcu_read_lock();\n\t\tcss_put(&sibling->css);\n\t}\n\trcu_read_unlock();\n}\n\n/**\n * update_cpumask - update the cpus_allowed mask of a cpuset and all tasks in it\n * @cs: the cpuset to consider\n * @trialcs: trial cpuset\n * @buf: buffer of cpu numbers written to this cpuset\n */\nstatic int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,\n\t\t\t  const char *buf)\n{\n\tint retval;\n\tstruct tmpmasks tmp;\n\tstruct cpuset *parent = parent_cs(cs);\n\tbool invalidate = false;\n\tint hier_flags = 0;\n\tint old_prs = cs->partition_root_state;\n\n\t/* top_cpuset.cpus_allowed tracks cpu_online_mask; it's read-only */\n\tif (cs == &top_cpuset)\n\t\treturn -EACCES;\n\n\t/*\n\t * An empty cpus_allowed is ok only if the cpuset has no tasks.\n\t * Since cpulist_parse() fails on an empty mask, we special case\n\t * that parsing.  The validate_change() call ensures that cpusets\n\t * with tasks have cpus.\n\t */\n\tif (!*buf) {\n\t\tcpumask_clear(trialcs->cpus_allowed);\n\t\tif (cpumask_empty(trialcs->exclusive_cpus))\n\t\t\tcpumask_clear(trialcs->effective_xcpus);\n\t} else {\n\t\tretval = cpulist_parse(buf, trialcs->cpus_allowed);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\n\t\tif (!cpumask_subset(trialcs->cpus_allowed,\n\t\t\t\t    top_cpuset.cpus_allowed))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * When exclusive_cpus isn't explicitly set, it is constrainted\n\t\t * by cpus_allowed and parent's effective_xcpus. Otherwise,\n\t\t * trialcs->effective_xcpus is used as a temporary cpumask\n\t\t * for checking validity of the partition root.\n\t\t */\n\t\tif (!cpumask_empty(trialcs->exclusive_cpus) || is_partition_valid(cs))\n\t\t\tcompute_effective_exclusive_cpumask(trialcs, NULL);\n\t}\n\n\t/* Nothing to do if the cpus didn't change */\n\tif (cpumask_equal(cs->cpus_allowed, trialcs->cpus_allowed))\n\t\treturn 0;\n\n\tif (alloc_cpumasks(NULL, &tmp))\n\t\treturn -ENOMEM;\n\n\tif (old_prs) {\n\t\tif (is_partition_valid(cs) &&\n\t\t    cpumask_empty(trialcs->effective_xcpus)) {\n\t\t\tinvalidate = true;\n\t\t\tcs->prs_err = PERR_INVCPUS;\n\t\t} else if (prstate_housekeeping_conflict(old_prs, trialcs->effective_xcpus)) {\n\t\t\tinvalidate = true;\n\t\t\tcs->prs_err = PERR_HKEEPING;\n\t\t} else if (tasks_nocpu_error(parent, cs, trialcs->effective_xcpus)) {\n\t\t\tinvalidate = true;\n\t\t\tcs->prs_err = PERR_NOCPUS;\n\t\t}\n\t}\n\n\t/*\n\t * Check all the descendants in update_cpumasks_hier() if\n\t * effective_xcpus is to be changed.\n\t */\n\tif (!cpumask_equal(cs->effective_xcpus, trialcs->effective_xcpus))\n\t\thier_flags = HIER_CHECKALL;\n\n\tretval = validate_change(cs, trialcs);\n\n\tif ((retval == -EINVAL) && cgroup_subsys_on_dfl(cpuset_cgrp_subsys)) {\n\t\tstruct cgroup_subsys_state *css;\n\t\tstruct cpuset *cp;\n\n\t\t/*\n\t\t * The -EINVAL error code indicates that partition sibling\n\t\t * CPU exclusivity rule has been violated. We still allow\n\t\t * the cpumask change to proceed while invalidating the\n\t\t * partition. However, any conflicting sibling partitions\n\t\t * have to be marked as invalid too.\n\t\t */\n\t\tinvalidate = true;\n\t\trcu_read_lock();\n\t\tcpuset_for_each_child(cp, css, parent) {\n\t\t\tstruct cpumask *xcpus = user_xcpus(trialcs);\n\n\t\t\tif (is_partition_valid(cp) &&\n\t\t\t    cpumask_intersects(xcpus, cp->effective_xcpus)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tupdate_parent_effective_cpumask(cp, partcmd_invalidate, NULL, &tmp);\n\t\t\t\trcu_read_lock();\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\tretval = 0;\n\t}\n\n\tif (retval < 0)\n\t\tgoto out_free;\n\n\tif (is_partition_valid(cs) ||\n\t   (is_partition_invalid(cs) && !invalidate)) {\n\t\tstruct cpumask *xcpus = trialcs->effective_xcpus;\n\n\t\tif (cpumask_empty(xcpus) && is_partition_invalid(cs))\n\t\t\txcpus = trialcs->cpus_allowed;\n\n\t\t/*\n\t\t * Call remote_cpus_update() to handle valid remote partition\n\t\t */\n\t\tif (is_remote_partition(cs))\n\t\t\tremote_cpus_update(cs, xcpus, &tmp);\n\t\telse if (invalidate)\n\t\t\tupdate_parent_effective_cpumask(cs, partcmd_invalidate,\n\t\t\t\t\t\t\tNULL, &tmp);\n\t\telse\n\t\t\tupdate_parent_effective_cpumask(cs, partcmd_update,\n\t\t\t\t\t\t\txcpus, &tmp);\n\t} else if (!cpumask_empty(cs->exclusive_cpus)) {\n\t\t/*\n\t\t * Use trialcs->effective_cpus as a temp cpumask\n\t\t */\n\t\tremote_partition_check(cs, trialcs->effective_xcpus,\n\t\t\t\t       trialcs->effective_cpus, &tmp);\n\t}\n\n\tspin_lock_irq(&callback_lock);\n\tcpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);\n\tcpumask_copy(cs->effective_xcpus, trialcs->effective_xcpus);\n\tif ((old_prs > 0) && !is_partition_valid(cs))\n\t\treset_partition_data(cs);\n\tspin_unlock_irq(&callback_lock);\n\n\t/* effective_cpus/effective_xcpus will be updated here */\n\tupdate_cpumasks_hier(cs, &tmp, hier_flags);\n\n\t/* Update CS_SCHED_LOAD_BALANCE and/or sched_domains, if necessary */\n\tif (cs->partition_root_state)\n\t\tupdate_partition_sd_lb(cs, old_prs);\nout_free:\n\tfree_cpumasks(NULL, &tmp);\n\treturn retval;\n}\n\n/**\n * update_exclusive_cpumask - update the exclusive_cpus mask of a cpuset\n * @cs: the cpuset to consider\n * @trialcs: trial cpuset\n * @buf: buffer of cpu numbers written to this cpuset\n *\n * The tasks' cpumask will be updated if cs is a valid partition root.\n */\nstatic int update_exclusive_cpumask(struct cpuset *cs, struct cpuset *trialcs,\n\t\t\t\t    const char *buf)\n{\n\tint retval;\n\tstruct tmpmasks tmp;\n\tstruct cpuset *parent = parent_cs(cs);\n\tbool invalidate = false;\n\tint hier_flags = 0;\n\tint old_prs = cs->partition_root_state;\n\n\tif (!*buf) {\n\t\tcpumask_clear(trialcs->exclusive_cpus);\n\t\tcpumask_clear(trialcs->effective_xcpus);\n\t} else {\n\t\tretval = cpulist_parse(buf, trialcs->exclusive_cpus);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t}\n\n\t/* Nothing to do if the CPUs didn't change */\n\tif (cpumask_equal(cs->exclusive_cpus, trialcs->exclusive_cpus))\n\t\treturn 0;\n\n\tif (*buf)\n\t\tcompute_effective_exclusive_cpumask(trialcs, NULL);\n\n\t/*\n\t * Check all the descendants in update_cpumasks_hier() if\n\t * effective_xcpus is to be changed.\n\t */\n\tif (!cpumask_equal(cs->effective_xcpus, trialcs->effective_xcpus))\n\t\thier_flags = HIER_CHECKALL;\n\n\tretval = validate_change(cs, trialcs);\n\tif (retval)\n\t\treturn retval;\n\n\tif (alloc_cpumasks(NULL, &tmp))\n\t\treturn -ENOMEM;\n\n\tif (old_prs) {\n\t\tif (cpumask_empty(trialcs->effective_xcpus)) {\n\t\t\tinvalidate = true;\n\t\t\tcs->prs_err = PERR_INVCPUS;\n\t\t} else if (prstate_housekeeping_conflict(old_prs, trialcs->effective_xcpus)) {\n\t\t\tinvalidate = true;\n\t\t\tcs->prs_err = PERR_HKEEPING;\n\t\t} else if (tasks_nocpu_error(parent, cs, trialcs->effective_xcpus)) {\n\t\t\tinvalidate = true;\n\t\t\tcs->prs_err = PERR_NOCPUS;\n\t\t}\n\n\t\tif (is_remote_partition(cs)) {\n\t\t\tif (invalidate)\n\t\t\t\tremote_partition_disable(cs, &tmp);\n\t\t\telse\n\t\t\t\tremote_cpus_update(cs, trialcs->effective_xcpus,\n\t\t\t\t\t\t   &tmp);\n\t\t} else if (invalidate) {\n\t\t\tupdate_parent_effective_cpumask(cs, partcmd_invalidate,\n\t\t\t\t\t\t\tNULL, &tmp);\n\t\t} else {\n\t\t\tupdate_parent_effective_cpumask(cs, partcmd_update,\n\t\t\t\t\t\ttrialcs->effective_xcpus, &tmp);\n\t\t}\n\t} else if (!cpumask_empty(trialcs->exclusive_cpus)) {\n\t\t/*\n\t\t * Use trialcs->effective_cpus as a temp cpumask\n\t\t */\n\t\tremote_partition_check(cs, trialcs->effective_xcpus,\n\t\t\t\t       trialcs->effective_cpus, &tmp);\n\t}\n\tspin_lock_irq(&callback_lock);\n\tcpumask_copy(cs->exclusive_cpus, trialcs->exclusive_cpus);\n\tcpumask_copy(cs->effective_xcpus, trialcs->effective_xcpus);\n\tif ((old_prs > 0) && !is_partition_valid(cs))\n\t\treset_partition_data(cs);\n\tspin_unlock_irq(&callback_lock);\n\n\t/*\n\t * Call update_cpumasks_hier() to update effective_cpus/effective_xcpus\n\t * of the subtree when it is a valid partition root or effective_xcpus\n\t * is updated.\n\t */\n\tif (is_partition_valid(cs) || hier_flags)\n\t\tupdate_cpumasks_hier(cs, &tmp, hier_flags);\n\n\t/* Update CS_SCHED_LOAD_BALANCE and/or sched_domains, if necessary */\n\tif (cs->partition_root_state)\n\t\tupdate_partition_sd_lb(cs, old_prs);\n\n\tfree_cpumasks(NULL, &tmp);\n\treturn 0;\n}\n\n/*\n * Migrate memory region from one set of nodes to another.  This is\n * performed asynchronously as it can be called from process migration path\n * holding locks involved in process management.  All mm migrations are\n * performed in the queued order and can be waited for by flushing\n * cpuset_migrate_mm_wq.\n */\n\nstruct cpuset_migrate_mm_work {\n\tstruct work_struct\twork;\n\tstruct mm_struct\t*mm;\n\tnodemask_t\t\tfrom;\n\tnodemask_t\t\tto;\n};\n\nstatic void cpuset_migrate_mm_workfn(struct work_struct *work)\n{\n\tstruct cpuset_migrate_mm_work *mwork =\n\t\tcontainer_of(work, struct cpuset_migrate_mm_work, work);\n\n\t/* on a wq worker, no need to worry about %current's mems_allowed */\n\tdo_migrate_pages(mwork->mm, &mwork->from, &mwork->to, MPOL_MF_MOVE_ALL);\n\tmmput(mwork->mm);\n\tkfree(mwork);\n}\n\nstatic void cpuset_migrate_mm(struct mm_struct *mm, const nodemask_t *from,\n\t\t\t\t\t\t\tconst nodemask_t *to)\n{\n\tstruct cpuset_migrate_mm_work *mwork;\n\n\tif (nodes_equal(*from, *to)) {\n\t\tmmput(mm);\n\t\treturn;\n\t}\n\n\tmwork = kzalloc(sizeof(*mwork), GFP_KERNEL);\n\tif (mwork) {\n\t\tmwork->mm = mm;\n\t\tmwork->from = *from;\n\t\tmwork->to = *to;\n\t\tINIT_WORK(&mwork->work, cpuset_migrate_mm_workfn);\n\t\tqueue_work(cpuset_migrate_mm_wq, &mwork->work);\n\t} else {\n\t\tmmput(mm);\n\t}\n}\n\nstatic void cpuset_post_attach(void)\n{\n\tflush_workqueue(cpuset_migrate_mm_wq);\n}\n\n/*\n * cpuset_change_task_nodemask - change task's mems_allowed and mempolicy\n * @tsk: the task to change\n * @newmems: new nodes that the task will be set\n *\n * We use the mems_allowed_seq seqlock to safely update both tsk->mems_allowed\n * and rebind an eventual tasks' mempolicy. If the task is allocating in\n * parallel, it might temporarily see an empty intersection, which results in\n * a seqlock check and retry before OOM or allocation failure.\n */\nstatic void cpuset_change_task_nodemask(struct task_struct *tsk,\n\t\t\t\t\tnodemask_t *newmems)\n{\n\ttask_lock(tsk);\n\n\tlocal_irq_disable();\n\twrite_seqcount_begin(&tsk->mems_allowed_seq);\n\n\tnodes_or(tsk->mems_allowed, tsk->mems_allowed, *newmems);\n\tmpol_rebind_task(tsk, newmems);\n\ttsk->mems_allowed = *newmems;\n\n\twrite_seqcount_end(&tsk->mems_allowed_seq);\n\tlocal_irq_enable();\n\n\ttask_unlock(tsk);\n}\n\nstatic void *cpuset_being_rebound;\n\n/**\n * cpuset_update_tasks_nodemask - Update the nodemasks of tasks in the cpuset.\n * @cs: the cpuset in which each task's mems_allowed mask needs to be changed\n *\n * Iterate through each task of @cs updating its mems_allowed to the\n * effective cpuset's.  As this function is called with cpuset_mutex held,\n * cpuset membership stays stable.\n */\nvoid cpuset_update_tasks_nodemask(struct cpuset *cs)\n{\n\tstatic nodemask_t newmems;\t/* protected by cpuset_mutex */\n\tstruct css_task_iter it;\n\tstruct task_struct *task;\n\n\tcpuset_being_rebound = cs;\t\t/* causes mpol_dup() rebind */\n\n\tguarantee_online_mems(cs, &newmems);\n\n\t/*\n\t * The mpol_rebind_mm() call takes mmap_lock, which we couldn't\n\t * take while holding tasklist_lock.  Forks can happen - the\n\t * mpol_dup() cpuset_being_rebound check will catch such forks,\n\t * and rebind their vma mempolicies too.  Because we still hold\n\t * the global cpuset_mutex, we know that no other rebind effort\n\t * will be contending for the global variable cpuset_being_rebound.\n\t * It's ok if we rebind the same mm twice; mpol_rebind_mm()\n\t * is idempotent.  Also migrate pages in each mm to new nodes.\n\t */\n\tcss_task_iter_start(&cs->css, 0, &it);\n\twhile ((task = css_task_iter_next(&it))) {\n\t\tstruct mm_struct *mm;\n\t\tbool migrate;\n\n\t\tcpuset_change_task_nodemask(task, &newmems);\n\n\t\tmm = get_task_mm(task);\n\t\tif (!mm)\n\t\t\tcontinue;\n\n\t\tmigrate = is_memory_migrate(cs);\n\n\t\tmpol_rebind_mm(mm, &cs->mems_allowed);\n\t\tif (migrate)\n\t\t\tcpuset_migrate_mm(mm, &cs->old_mems_allowed, &newmems);\n\t\telse\n\t\t\tmmput(mm);\n\t}\n\tcss_task_iter_end(&it);\n\n\t/*\n\t * All the tasks' nodemasks have been updated, update\n\t * cs->old_mems_allowed.\n\t */\n\tcs->old_mems_allowed = newmems;\n\n\t/* We're done rebinding vmas to this cpuset's new mems_allowed. */\n\tcpuset_being_rebound = NULL;\n}\n\n/*\n * update_nodemasks_hier - Update effective nodemasks and tasks in the subtree\n * @cs: the cpuset to consider\n * @new_mems: a temp variable for calculating new effective_mems\n *\n * When configured nodemask is changed, the effective nodemasks of this cpuset\n * and all its descendants need to be updated.\n *\n * On legacy hierarchy, effective_mems will be the same with mems_allowed.\n *\n * Called with cpuset_mutex held\n */\nstatic void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n{\n\tstruct cpuset *cp;\n\tstruct cgroup_subsys_state *pos_css;\n\n\trcu_read_lock();\n\tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n\t\tstruct cpuset *parent = parent_cs(cp);\n\n\t\tnodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n\n\t\t/*\n\t\t * If it becomes empty, inherit the effective mask of the\n\t\t * parent, which is guaranteed to have some MEMs.\n\t\t */\n\t\tif (is_in_v2_mode() && nodes_empty(*new_mems))\n\t\t\t*new_mems = parent->effective_mems;\n\n\t\t/* Skip the whole subtree if the nodemask remains the same. */\n\t\tif (nodes_equal(*new_mems, cp->effective_mems)) {\n\t\t\tpos_css = css_rightmost_descendant(pos_css);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!css_tryget_online(&cp->css))\n\t\t\tcontinue;\n\t\trcu_read_unlock();\n\n\t\tspin_lock_irq(&callback_lock);\n\t\tcp->effective_mems = *new_mems;\n\t\tspin_unlock_irq(&callback_lock);\n\n\t\tWARN_ON(!is_in_v2_mode() &&\n\t\t\t!nodes_equal(cp->mems_allowed, cp->effective_mems));\n\n\t\tcpuset_update_tasks_nodemask(cp);\n\n\t\trcu_read_lock();\n\t\tcss_put(&cp->css);\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * Handle user request to change the 'mems' memory placement\n * of a cpuset.  Needs to validate the request, update the\n * cpusets mems_allowed, and for each task in the cpuset,\n * update mems_allowed and rebind task's mempolicy and any vma\n * mempolicies and if the cpuset is marked 'memory_migrate',\n * migrate the tasks pages to the new memory.\n *\n * Call with cpuset_mutex held. May take callback_lock during call.\n * Will take tasklist_lock, scan tasklist for tasks in cpuset cs,\n * lock each such tasks mm->mmap_lock, scan its vma's and rebind\n * their mempolicies to the cpusets new mems_allowed.\n */\nstatic int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n\t\t\t   const char *buf)\n{\n\tint retval;\n\n\t/*\n\t * top_cpuset.mems_allowed tracks node_stats[N_MEMORY];\n\t * it's read-only\n\t */\n\tif (cs == &top_cpuset) {\n\t\tretval = -EACCES;\n\t\tgoto done;\n\t}\n\n\t/*\n\t * An empty mems_allowed is ok iff there are no tasks in the cpuset.\n\t * Since nodelist_parse() fails on an empty mask, we special case\n\t * that parsing.  The validate_change() call ensures that cpusets\n\t * with tasks have memory.\n\t */\n\tif (!*buf) {\n\t\tnodes_clear(trialcs->mems_allowed);\n\t} else {\n\t\tretval = nodelist_parse(buf, trialcs->mems_allowed);\n\t\tif (retval < 0)\n\t\t\tgoto done;\n\n\t\tif (!nodes_subset(trialcs->mems_allowed,\n\t\t\t\t  top_cpuset.mems_allowed)) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (nodes_equal(cs->mems_allowed, trialcs->mems_allowed)) {\n\t\tretval = 0;\t\t/* Too easy - nothing to do */\n\t\tgoto done;\n\t}\n\tretval = validate_change(cs, trialcs);\n\tif (retval < 0)\n\t\tgoto done;\n\n\tcheck_insane_mems_config(&trialcs->mems_allowed);\n\n\tspin_lock_irq(&callback_lock);\n\tcs->mems_allowed = trialcs->mems_allowed;\n\tspin_unlock_irq(&callback_lock);\n\n\t/* use trialcs->mems_allowed as a temp variable */\n\tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\ndone:\n\treturn retval;\n}\n\nbool current_cpuset_is_being_rebound(void)\n{\n\tbool ret;\n\n\trcu_read_lock();\n\tret = task_cs(current) == cpuset_being_rebound;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n/*\n * cpuset_update_flag - read a 0 or a 1 in a file and update associated flag\n * bit:\t\tthe bit to update (see cpuset_flagbits_t)\n * cs:\t\tthe cpuset to update\n * turning_on: \twhether the flag is being set or cleared\n *\n * Call with cpuset_mutex held.\n */\n\nint cpuset_update_flag(cpuset_flagbits_t bit, struct cpuset *cs,\n\t\t       int turning_on)\n{\n\tstruct cpuset *trialcs;\n\tint balance_flag_changed;\n\tint spread_flag_changed;\n\tint err;\n\n\ttrialcs = alloc_trial_cpuset(cs);\n\tif (!trialcs)\n\t\treturn -ENOMEM;\n\n\tif (turning_on)\n\t\tset_bit(bit, &trialcs->flags);\n\telse\n\t\tclear_bit(bit, &trialcs->flags);\n\n\terr = validate_change(cs, trialcs);\n\tif (err < 0)\n\t\tgoto out;\n\n\tbalance_flag_changed = (is_sched_load_balance(cs) !=\n\t\t\t\tis_sched_load_balance(trialcs));\n\n\tspread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))\n\t\t\t|| (is_spread_page(cs) != is_spread_page(trialcs)));\n\n\tspin_lock_irq(&callback_lock);\n\tcs->flags = trialcs->flags;\n\tspin_unlock_irq(&callback_lock);\n\n\tif (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed &&\n\t    !force_sd_rebuild)\n\t\trebuild_sched_domains_locked();\n\n\tif (spread_flag_changed)\n\t\tcpuset1_update_tasks_flags(cs);\nout:\n\tfree_cpuset(trialcs);\n\treturn err;\n}\n\n/**\n * update_prstate - update partition_root_state\n * @cs: the cpuset to update\n * @new_prs: new partition root state\n * Return: 0 if successful, != 0 if error\n *\n * Call with cpuset_mutex held.\n */\nstatic int update_prstate(struct cpuset *cs, int new_prs)\n{\n\tint err = PERR_NONE, old_prs = cs->partition_root_state;\n\tstruct cpuset *parent = parent_cs(cs);\n\tstruct tmpmasks tmpmask;\n\tbool new_xcpus_state = false;\n\n\tif (old_prs == new_prs)\n\t\treturn 0;\n\n\t/*\n\t * Treat a previously invalid partition root as if it is a \"member\".\n\t */\n\tif (new_prs && is_prs_invalid(old_prs))\n\t\told_prs = PRS_MEMBER;\n\n\tif (alloc_cpumasks(NULL, &tmpmask))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Setup effective_xcpus if not properly set yet, it will be cleared\n\t * later if partition becomes invalid.\n\t */\n\tif ((new_prs > 0) && cpumask_empty(cs->exclusive_cpus)) {\n\t\tspin_lock_irq(&callback_lock);\n\t\tcpumask_and(cs->effective_xcpus,\n\t\t\t    cs->cpus_allowed, parent->effective_xcpus);\n\t\tspin_unlock_irq(&callback_lock);\n\t}\n\n\terr = update_partition_exclusive(cs, new_prs);\n\tif (err)\n\t\tgoto out;\n\n\tif (!old_prs) {\n\t\t/*\n\t\t * cpus_allowed and exclusive_cpus cannot be both empty.\n\t\t */\n\t\tif (xcpus_empty(cs)) {\n\t\t\terr = PERR_CPUSEMPTY;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * If parent is valid partition, enable local partiion.\n\t\t * Otherwise, enable a remote partition.\n\t\t */\n\t\tif (is_partition_valid(parent)) {\n\t\t\tenum partition_cmd cmd = (new_prs == PRS_ROOT)\n\t\t\t\t\t       ? partcmd_enable : partcmd_enablei;\n\n\t\t\terr = update_parent_effective_cpumask(cs, cmd, NULL, &tmpmask);\n\t\t} else {\n\t\t\terr = remote_partition_enable(cs, new_prs, &tmpmask);\n\t\t}\n\t} else if (old_prs && new_prs) {\n\t\t/*\n\t\t * A change in load balance state only, no change in cpumasks.\n\t\t */\n\t\tnew_xcpus_state = true;\n\t} else {\n\t\t/*\n\t\t * Switching back to member is always allowed even if it\n\t\t * disables child partitions.\n\t\t */\n\t\tif (is_remote_partition(cs))\n\t\t\tremote_partition_disable(cs, &tmpmask);\n\t\telse\n\t\t\tupdate_parent_effective_cpumask(cs, partcmd_disable,\n\t\t\t\t\t\t\tNULL, &tmpmask);\n\n\t\t/*\n\t\t * Invalidation of child partitions will be done in\n\t\t * update_cpumasks_hier().\n\t\t */\n\t}\nout:\n\t/*\n\t * Make partition invalid & disable CS_CPU_EXCLUSIVE if an error\n\t * happens.\n\t */\n\tif (err) {\n\t\tnew_prs = -new_prs;\n\t\tupdate_partition_exclusive(cs, new_prs);\n\t}\n\n\tspin_lock_irq(&callback_lock);\n\tcs->partition_root_state = new_prs;\n\tWRITE_ONCE(cs->prs_err, err);\n\tif (!is_partition_valid(cs))\n\t\treset_partition_data(cs);\n\telse if (new_xcpus_state)\n\t\tpartition_xcpus_newstate(old_prs, new_prs, cs->effective_xcpus);\n\tspin_unlock_irq(&callback_lock);\n\tupdate_unbound_workqueue_cpumask(new_xcpus_state);\n\n\t/* Force update if switching back to member */\n\tupdate_cpumasks_hier(cs, &tmpmask, !new_prs ? HIER_CHECKALL : 0);\n\n\t/* Update sched domains and load balance flag */\n\tupdate_partition_sd_lb(cs, old_prs);\n\n\tnotify_partition_change(cs, old_prs);\n\tfree_cpumasks(NULL, &tmpmask);\n\treturn 0;\n}\n\nstatic struct cpuset *cpuset_attach_old_cs;\n\n/*\n * Check to see if a cpuset can accept a new task\n * For v1, cpus_allowed and mems_allowed can't be empty.\n * For v2, effective_cpus can't be empty.\n * Note that in v1, effective_cpus = cpus_allowed.\n */\nstatic int cpuset_can_attach_check(struct cpuset *cs)\n{\n\tif (cpumask_empty(cs->effective_cpus) ||\n\t   (!is_in_v2_mode() && nodes_empty(cs->mems_allowed)))\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\nstatic void reset_migrate_dl_data(struct cpuset *cs)\n{\n\tcs->nr_migrate_dl_tasks = 0;\n\tcs->sum_migrate_dl_bw = 0;\n}\n\n/* Called by cgroups to determine if a cpuset is usable; cpuset_mutex held */\nstatic int cpuset_can_attach(struct cgroup_taskset *tset)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *cs, *oldcs;\n\tstruct task_struct *task;\n\tbool cpus_updated, mems_updated;\n\tint ret;\n\n\t/* used later by cpuset_attach() */\n\tcpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset, &css));\n\toldcs = cpuset_attach_old_cs;\n\tcs = css_cs(css);\n\n\tmutex_lock(&cpuset_mutex);\n\n\t/* Check to see if task is allowed in the cpuset */\n\tret = cpuset_can_attach_check(cs);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tcpus_updated = !cpumask_equal(cs->effective_cpus, oldcs->effective_cpus);\n\tmems_updated = !nodes_equal(cs->effective_mems, oldcs->effective_mems);\n\n\tcgroup_taskset_for_each(task, css, tset) {\n\t\tret = task_can_attach(task);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Skip rights over task check in v2 when nothing changes,\n\t\t * migration permission derives from hierarchy ownership in\n\t\t * cgroup_procs_write_permission()).\n\t\t */\n\t\tif (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) ||\n\t\t    (cpus_updated || mems_updated)) {\n\t\t\tret = security_task_setscheduler(task);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (dl_task(task)) {\n\t\t\tcs->nr_migrate_dl_tasks++;\n\t\t\tcs->sum_migrate_dl_bw += task->dl.dl_bw;\n\t\t}\n\t}\n\n\tif (!cs->nr_migrate_dl_tasks)\n\t\tgoto out_success;\n\n\tif (!cpumask_intersects(oldcs->effective_cpus, cs->effective_cpus)) {\n\t\tint cpu = cpumask_any_and(cpu_active_mask, cs->effective_cpus);\n\n\t\tif (unlikely(cpu >= nr_cpu_ids)) {\n\t\t\treset_migrate_dl_data(cs);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tret = dl_bw_alloc(cpu, cs->sum_migrate_dl_bw);\n\t\tif (ret) {\n\t\t\treset_migrate_dl_data(cs);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\nout_success:\n\t/*\n\t * Mark attach is in progress.  This makes validate_change() fail\n\t * changes which zero cpus/mems_allowed.\n\t */\n\tcs->attach_in_progress++;\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\treturn ret;\n}\n\nstatic void cpuset_cancel_attach(struct cgroup_taskset *tset)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *cs;\n\n\tcgroup_taskset_first(tset, &css);\n\tcs = css_cs(css);\n\n\tmutex_lock(&cpuset_mutex);\n\tdec_attach_in_progress_locked(cs);\n\n\tif (cs->nr_migrate_dl_tasks) {\n\t\tint cpu = cpumask_any(cs->effective_cpus);\n\n\t\tdl_bw_free(cpu, cs->sum_migrate_dl_bw);\n\t\treset_migrate_dl_data(cs);\n\t}\n\n\tmutex_unlock(&cpuset_mutex);\n}\n\n/*\n * Protected by cpuset_mutex. cpus_attach is used only by cpuset_attach_task()\n * but we can't allocate it dynamically there.  Define it global and\n * allocate from cpuset_init().\n */\nstatic cpumask_var_t cpus_attach;\nstatic nodemask_t cpuset_attach_nodemask_to;\n\nstatic void cpuset_attach_task(struct cpuset *cs, struct task_struct *task)\n{\n\tlockdep_assert_held(&cpuset_mutex);\n\n\tif (cs != &top_cpuset)\n\t\tguarantee_online_cpus(task, cpus_attach);\n\telse\n\t\tcpumask_andnot(cpus_attach, task_cpu_possible_mask(task),\n\t\t\t       subpartitions_cpus);\n\t/*\n\t * can_attach beforehand should guarantee that this doesn't\n\t * fail.  TODO: have a better way to handle failure here\n\t */\n\tWARN_ON_ONCE(set_cpus_allowed_ptr(task, cpus_attach));\n\n\tcpuset_change_task_nodemask(task, &cpuset_attach_nodemask_to);\n\tcpuset1_update_task_spread_flags(cs, task);\n}\n\nstatic void cpuset_attach(struct cgroup_taskset *tset)\n{\n\tstruct task_struct *task;\n\tstruct task_struct *leader;\n\tstruct cgroup_subsys_state *css;\n\tstruct cpuset *cs;\n\tstruct cpuset *oldcs = cpuset_attach_old_cs;\n\tbool cpus_updated, mems_updated;\n\n\tcgroup_taskset_first(tset, &css);\n\tcs = css_cs(css);\n\n\tlockdep_assert_cpus_held();\t/* see cgroup_attach_lock() */\n\tmutex_lock(&cpuset_mutex);\n\tcpus_updated = !cpumask_equal(cs->effective_cpus,\n\t\t\t\t      oldcs->effective_cpus);\n\tmems_updated = !nodes_equal(cs->effective_mems, oldcs->effective_mems);\n\n\t/*\n\t * In the default hierarchy, enabling cpuset in the child cgroups\n\t * will trigger a number of cpuset_attach() calls with no change\n\t * in effective cpus and mems. In that case, we can optimize out\n\t * by skipping the task iteration and update.\n\t */\n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t    !cpus_updated && !mems_updated) {\n\t\tcpuset_attach_nodemask_to = cs->effective_mems;\n\t\tgoto out;\n\t}\n\n\tguarantee_online_mems(cs, &cpuset_attach_nodemask_to);\n\n\tcgroup_taskset_for_each(task, css, tset)\n\t\tcpuset_attach_task(cs, task);\n\n\t/*\n\t * Change mm for all threadgroup leaders. This is expensive and may\n\t * sleep and should be moved outside migration path proper. Skip it\n\t * if there is no change in effective_mems and CS_MEMORY_MIGRATE is\n\t * not set.\n\t */\n\tcpuset_attach_nodemask_to = cs->effective_mems;\n\tif (!is_memory_migrate(cs) && !mems_updated)\n\t\tgoto out;\n\n\tcgroup_taskset_for_each_leader(leader, css, tset) {\n\t\tstruct mm_struct *mm = get_task_mm(leader);\n\n\t\tif (mm) {\n\t\t\tmpol_rebind_mm(mm, &cpuset_attach_nodemask_to);\n\n\t\t\t/*\n\t\t\t * old_mems_allowed is the same with mems_allowed\n\t\t\t * here, except if this task is being moved\n\t\t\t * automatically due to hotplug.  In that case\n\t\t\t * @mems_allowed has been updated and is empty, so\n\t\t\t * @old_mems_allowed is the right nodesets that we\n\t\t\t * migrate mm from.\n\t\t\t */\n\t\t\tif (is_memory_migrate(cs))\n\t\t\t\tcpuset_migrate_mm(mm, &oldcs->old_mems_allowed,\n\t\t\t\t\t\t  &cpuset_attach_nodemask_to);\n\t\t\telse\n\t\t\t\tmmput(mm);\n\t\t}\n\t}\n\nout:\n\tcs->old_mems_allowed = cpuset_attach_nodemask_to;\n\n\tif (cs->nr_migrate_dl_tasks) {\n\t\tcs->nr_deadline_tasks += cs->nr_migrate_dl_tasks;\n\t\toldcs->nr_deadline_tasks -= cs->nr_migrate_dl_tasks;\n\t\treset_migrate_dl_data(cs);\n\t}\n\n\tdec_attach_in_progress_locked(cs);\n\n\tmutex_unlock(&cpuset_mutex);\n}\n\n/*\n * Common handling for a write to a \"cpus\" or \"mems\" file.\n */\nssize_t cpuset_write_resmask(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct cpuset *cs = css_cs(of_css(of));\n\tstruct cpuset *trialcs;\n\tint retval = -ENODEV;\n\n\tbuf = strstrip(buf);\n\n\t/*\n\t * CPU or memory hotunplug may leave @cs w/o any execution\n\t * resources, in which case the hotplug code asynchronously updates\n\t * configuration and transfers all tasks to the nearest ancestor\n\t * which can execute.\n\t *\n\t * As writes to \"cpus\" or \"mems\" may restore @cs's execution\n\t * resources, wait for the previously scheduled operations before\n\t * proceeding, so that we don't end up keep removing tasks added\n\t * after execution capability is restored.\n\t *\n\t * cpuset_handle_hotplug may call back into cgroup core asynchronously\n\t * via cgroup_transfer_tasks() and waiting for it from a cgroupfs\n\t * operation like this one can lead to a deadlock through kernfs\n\t * active_ref protection.  Let's break the protection.  Losing the\n\t * protection is okay as we check whether @cs is online after\n\t * grabbing cpuset_mutex anyway.  This only happens on the legacy\n\t * hierarchies.\n\t */\n\tcss_get(&cs->css);\n\tkernfs_break_active_protection(of->kn);\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\tif (!is_cpuset_online(cs))\n\t\tgoto out_unlock;\n\n\ttrialcs = alloc_trial_cpuset(cs);\n\tif (!trialcs) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (of_cft(of)->private) {\n\tcase FILE_CPULIST:\n\t\tretval = update_cpumask(cs, trialcs, buf);\n\t\tbreak;\n\tcase FILE_EXCLUSIVE_CPULIST:\n\t\tretval = update_exclusive_cpumask(cs, trialcs, buf);\n\t\tbreak;\n\tcase FILE_MEMLIST:\n\t\tretval = update_nodemask(cs, trialcs, buf);\n\t\tbreak;\n\tdefault:\n\t\tretval = -EINVAL;\n\t\tbreak;\n\t}\n\n\tfree_cpuset(trialcs);\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\tkernfs_unbreak_active_protection(of->kn);\n\tcss_put(&cs->css);\n\tflush_workqueue(cpuset_migrate_mm_wq);\n\treturn retval ?: nbytes;\n}\n\n/*\n * These ascii lists should be read in a single call, by using a user\n * buffer large enough to hold the entire map.  If read in smaller\n * chunks, there is no guarantee of atomicity.  Since the display format\n * used, list of ranges of sequential numbers, is variable length,\n * and since these maps can change value dynamically, one could read\n * gibberish by doing partial reads while a list was changing.\n */\nint cpuset_common_seq_show(struct seq_file *sf, void *v)\n{\n\tstruct cpuset *cs = css_cs(seq_css(sf));\n\tcpuset_filetype_t type = seq_cft(sf)->private;\n\tint ret = 0;\n\n\tspin_lock_irq(&callback_lock);\n\n\tswitch (type) {\n\tcase FILE_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->cpus_allowed));\n\t\tbreak;\n\tcase FILE_MEMLIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", nodemask_pr_args(&cs->mems_allowed));\n\t\tbreak;\n\tcase FILE_EFFECTIVE_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->effective_cpus));\n\t\tbreak;\n\tcase FILE_EFFECTIVE_MEMLIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", nodemask_pr_args(&cs->effective_mems));\n\t\tbreak;\n\tcase FILE_EXCLUSIVE_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->exclusive_cpus));\n\t\tbreak;\n\tcase FILE_EFFECTIVE_XCPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(cs->effective_xcpus));\n\t\tbreak;\n\tcase FILE_SUBPARTS_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(subpartitions_cpus));\n\t\tbreak;\n\tcase FILE_ISOLATED_CPULIST:\n\t\tseq_printf(sf, \"%*pbl\\n\", cpumask_pr_args(isolated_cpus));\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tspin_unlock_irq(&callback_lock);\n\treturn ret;\n}\n\nstatic int sched_partition_show(struct seq_file *seq, void *v)\n{\n\tstruct cpuset *cs = css_cs(seq_css(seq));\n\tconst char *err, *type = NULL;\n\n\tswitch (cs->partition_root_state) {\n\tcase PRS_ROOT:\n\t\tseq_puts(seq, \"root\\n\");\n\t\tbreak;\n\tcase PRS_ISOLATED:\n\t\tseq_puts(seq, \"isolated\\n\");\n\t\tbreak;\n\tcase PRS_MEMBER:\n\t\tseq_puts(seq, \"member\\n\");\n\t\tbreak;\n\tcase PRS_INVALID_ROOT:\n\t\ttype = \"root\";\n\t\tfallthrough;\n\tcase PRS_INVALID_ISOLATED:\n\t\tif (!type)\n\t\t\ttype = \"isolated\";\n\t\terr = perr_strings[READ_ONCE(cs->prs_err)];\n\t\tif (err)\n\t\t\tseq_printf(seq, \"%s invalid (%s)\\n\", type, err);\n\t\telse\n\t\t\tseq_printf(seq, \"%s invalid\\n\", type);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic ssize_t sched_partition_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t     size_t nbytes, loff_t off)\n{\n\tstruct cpuset *cs = css_cs(of_css(of));\n\tint val;\n\tint retval = -ENODEV;\n\n\tbuf = strstrip(buf);\n\n\tif (!strcmp(buf, \"root\"))\n\t\tval = PRS_ROOT;\n\telse if (!strcmp(buf, \"member\"))\n\t\tval = PRS_MEMBER;\n\telse if (!strcmp(buf, \"isolated\"))\n\t\tval = PRS_ISOLATED;\n\telse\n\t\treturn -EINVAL;\n\n\tcss_get(&cs->css);\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\tif (!is_cpuset_online(cs))\n\t\tgoto out_unlock;\n\n\tretval = update_prstate(cs, val);\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\tcss_put(&cs->css);\n\treturn retval ?: nbytes;\n}\n\n/*\n * This is currently a minimal set for the default hierarchy. It can be\n * expanded later on by migrating more features and control files from v1.\n */\nstatic struct cftype dfl_files[] = {\n\t{\n\t\t.name = \"cpus\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * NR_CPUS),\n\t\t.private = FILE_CPULIST,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t},\n\n\t{\n\t\t.name = \"mems\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * MAX_NUMNODES),\n\t\t.private = FILE_MEMLIST,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t},\n\n\t{\n\t\t.name = \"cpus.effective\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_CPULIST,\n\t},\n\n\t{\n\t\t.name = \"mems.effective\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_MEMLIST,\n\t},\n\n\t{\n\t\t.name = \"cpus.partition\",\n\t\t.seq_show = sched_partition_show,\n\t\t.write = sched_partition_write,\n\t\t.private = FILE_PARTITION_ROOT,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.file_offset = offsetof(struct cpuset, partition_file),\n\t},\n\n\t{\n\t\t.name = \"cpus.exclusive\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.write = cpuset_write_resmask,\n\t\t.max_write_len = (100U + 6 * NR_CPUS),\n\t\t.private = FILE_EXCLUSIVE_CPULIST,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t},\n\n\t{\n\t\t.name = \"cpus.exclusive.effective\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_EFFECTIVE_XCPULIST,\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t},\n\n\t{\n\t\t.name = \"cpus.subpartitions\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_SUBPARTS_CPULIST,\n\t\t.flags = CFTYPE_ONLY_ON_ROOT | CFTYPE_DEBUG,\n\t},\n\n\t{\n\t\t.name = \"cpus.isolated\",\n\t\t.seq_show = cpuset_common_seq_show,\n\t\t.private = FILE_ISOLATED_CPULIST,\n\t\t.flags = CFTYPE_ONLY_ON_ROOT,\n\t},\n\n\t{ }\t/* terminate */\n};\n\n\n/**\n * cpuset_css_alloc - Allocate a cpuset css\n * @parent_css: Parent css of the control group that the new cpuset will be\n *              part of\n * Return: cpuset css on success, -ENOMEM on failure.\n *\n * Allocate and initialize a new cpuset css, for non-NULL @parent_css, return\n * top cpuset css otherwise.\n */\nstatic struct cgroup_subsys_state *\ncpuset_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct cpuset *cs;\n\n\tif (!parent_css)\n\t\treturn &top_cpuset.css;\n\n\tcs = kzalloc(sizeof(*cs), GFP_KERNEL);\n\tif (!cs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (alloc_cpumasks(cs, NULL)) {\n\t\tkfree(cs);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t__set_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\tfmeter_init(&cs->fmeter);\n\tcs->relax_domain_level = -1;\n\tINIT_LIST_HEAD(&cs->remote_sibling);\n\n\t/* Set CS_MEMORY_MIGRATE for default hierarchy */\n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys))\n\t\t__set_bit(CS_MEMORY_MIGRATE, &cs->flags);\n\n\treturn &cs->css;\n}\n\nstatic int cpuset_css_online(struct cgroup_subsys_state *css)\n{\n\tstruct cpuset *cs = css_cs(css);\n\tstruct cpuset *parent = parent_cs(cs);\n\tstruct cpuset *tmp_cs;\n\tstruct cgroup_subsys_state *pos_css;\n\n\tif (!parent)\n\t\treturn 0;\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\n\tset_bit(CS_ONLINE, &cs->flags);\n\tif (is_spread_page(parent))\n\t\tset_bit(CS_SPREAD_PAGE, &cs->flags);\n\tif (is_spread_slab(parent))\n\t\tset_bit(CS_SPREAD_SLAB, &cs->flags);\n\t/*\n\t * For v2, clear CS_SCHED_LOAD_BALANCE if parent is isolated\n\t */\n\tif (cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t    !is_sched_load_balance(parent))\n\t\tclear_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\n\n\tcpuset_inc();\n\n\tspin_lock_irq(&callback_lock);\n\tif (is_in_v2_mode()) {\n\t\tcpumask_copy(cs->effective_cpus, parent->effective_cpus);\n\t\tcs->effective_mems = parent->effective_mems;\n\t}\n\tspin_unlock_irq(&callback_lock);\n\n\tif (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))\n\t\tgoto out_unlock;\n\n\t/*\n\t * Clone @parent's configuration if CGRP_CPUSET_CLONE_CHILDREN is\n\t * set.  This flag handling is implemented in cgroup core for\n\t * historical reasons - the flag may be specified during mount.\n\t *\n\t * Currently, if any sibling cpusets have exclusive cpus or mem, we\n\t * refuse to clone the configuration - thereby refusing the task to\n\t * be entered, and as a result refusing the sys_unshare() or\n\t * clone() which initiated it.  If this becomes a problem for some\n\t * users who wish to allow that scenario, then this could be\n\t * changed to grant parent->cpus_allowed-sibling_cpus_exclusive\n\t * (and likewise for mems) to the new cgroup.\n\t */\n\trcu_read_lock();\n\tcpuset_for_each_child(tmp_cs, pos_css, parent) {\n\t\tif (is_mem_exclusive(tmp_cs) || is_cpu_exclusive(tmp_cs)) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tspin_lock_irq(&callback_lock);\n\tcs->mems_allowed = parent->mems_allowed;\n\tcs->effective_mems = parent->mems_allowed;\n\tcpumask_copy(cs->cpus_allowed, parent->cpus_allowed);\n\tcpumask_copy(cs->effective_cpus, parent->cpus_allowed);\n\tspin_unlock_irq(&callback_lock);\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n\treturn 0;\n}\n\n/*\n * If the cpuset being removed has its flag 'sched_load_balance'\n * enabled, then simulate turning sched_load_balance off, which\n * will call rebuild_sched_domains_locked(). That is not needed\n * in the default hierarchy where only changes in partition\n * will cause repartitioning.\n *\n * If the cpuset has the 'sched.partition' flag enabled, simulate\n * turning 'sched.partition\" off.\n */\n\nstatic void cpuset_css_offline(struct cgroup_subsys_state *css)\n{\n\tstruct cpuset *cs = css_cs(css);\n\n\tcpus_read_lock();\n\tmutex_lock(&cpuset_mutex);\n\n\tif (is_partition_valid(cs))\n\t\tupdate_prstate(cs, 0);\n\n\tif (!cgroup_subsys_on_dfl(cpuset_cgrp_subsys) &&\n\t    is_sched_load_balance(cs))\n\t\tcpuset_update_flag(CS_SCHED_LOAD_BALANCE, cs, 0);\n\n\tcpuset_dec();\n\tclear_bit(CS_ONLINE, &cs->flags);\n\n\tmutex_unlock(&cpuset_mutex);\n\tcpus_read_unlock();\n}\n\nstatic void cpuset_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct cpuset *cs = css_cs(css);\n\n\tfree_cpuset(cs);\n}\n\nstatic void cpuset_bind(struct cgroup_subsys_state *root_css)\n{\n\tmutex_lock(&cpuset_mutex);\n\tspin_lock_irq(&callback_lock);\n\n\tif (is_in_v2_mode()) {\n\t\tcpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);\n\t\tcpumask_copy(top_cpuset.effective_xcpus, cpu_possible_mask);\n\t\ttop_cpuset.mems_allowed = node_possible_map;\n\t} else {\n\t\tcpumask_copy(top_cpuset.cpus_allowed,\n\t\t\t     top_cpuset.effective_cpus);\n\t\ttop_cpuset.mems_allowed = top_cpuset.effective_mems;\n\t}\n\n\tspin_unlock_irq(&callback_lock);\n\tmutex_unlock(&cpuset_mutex);\n}\n\n/*\n * In case the child is cloned into a cpuset different from its parent,\n * additional checks are done to see if the move is allowed.\n */\nstatic int cpuset_can_fork(struct task_struct *task, struct css_set *cset)\n{\n\tstruct cpuset *cs = css_cs(cset->subsys[cpuset_cgrp_id]);\n\tbool same_cs;\n\tint ret;\n\n\trcu_read_lock();\n\tsame_cs = (cs == task_cs(current));\n\trcu_read_unlock();\n\n\tif (same_cs)\n\t\treturn 0;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\tmutex_lock(&cpuset_mutex);\n\n\t/* Check to see if task is allowed in the cpuset */\n\tret = cpuset_can_attach_check(cs);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = task_can_attach(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = security_task_setscheduler(task);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t/*\n\t * Mark attach is in progress.  This makes validate_change() fail\n\t * changes which zero cpus/mems_allowed.\n\t */\n\tcs->attach_in_progress++;\nout_unlock:\n\tmutex_unlock(&cpuset_mutex);\n\treturn ret;\n}\n\nstatic void cpuset_cancel_fork(struct task_struct *task, struct css_set *cset)\n{\n\tstruct cpuset *cs = css_cs(cset->subsys[cpuset_cgrp_id]);\n\tbool same_cs;\n\n\trcu_read_lock();\n\tsame_cs = (cs == task_cs(current));\n\trcu_read_unlock();\n\n\tif (same_cs)\n\t\treturn;\n\n\tdec_attach_in_progress(cs);\n}\n\n/*\n * Make sure the new task conform to the current state of its parent,\n * which could have been changed by cpuset just after it inherits the\n * state from the parent and before it sits on the cgroup's task list.\n */\nstatic void cpuset_fork(struct task_struct *task)\n{\n\tstruct cpuset *cs;\n\tbool same_cs;\n\n\trcu_read_lock();\n\tcs = task_cs(task);\n\tsame_cs = (cs == task_cs(current));\n\trcu_read_unlock();\n\n\tif (same_cs) {\n\t\tif (cs == &top_cpuset)\n\t\t\treturn;\n\n\t\tset_cpus_allowed_ptr(task, current->cpus_ptr);\n\t\ttask->mems_allowed = current->mems_allowed;\n\t\treturn;\n\t}\n\n\t/* CLONE_INTO_CGROUP */\n\tmutex_lock(&cpuset_mutex);\n\tguarantee_online_mems(cs, &cpuset_attach_nodemask_to);\n\tcpuset_attach_task(cs, task);\n\n\tdec_attach_in_progress_locked(cs);\n\tmutex_unlock(&cpuset_mutex);\n}\n\nstruct cgroup_subsys cpuset_cgrp_subsys = {\n\t.css_alloc\t= cpuset_css_alloc,\n\t.css_online\t= cpuset_css_online,\n\t.css_offline\t= cpuset_css_offline,\n\t.css_free\t= cpuset_css_free,\n\t.can_attach\t= cpuset_can_attach,\n\t.cancel_attach\t= cpuset_cancel_attach,\n\t.attach\t\t= cpuset_attach,\n\t.post_attach\t= cpuset_post_attach,\n\t.bind\t\t= cpuset_bind,\n\t.can_fork\t= cpuset_can_fork,\n\t.cancel_fork\t= cpuset_cancel_fork,\n\t.fork\t\t= cpuset_fork,\n#ifdef CONFIG_CPUSETS_V1\n\t.legacy_cftypes\t= cpuset1_files,\n#endif\n\t.dfl_cftypes\t= dfl_files,\n\t.early_init\t= true,\n\t.threaded\t= true,\n};\n\n/**\n * cpuset_init - initialize cpusets at system boot\n *\n * Description: Initialize top_cpuset\n **/\n\nint __init cpuset_init(void)\n{\n\tBUG_ON(!alloc_cpumask_var(&top_cpuset.cpus_allowed, GFP_KERNEL));\n\tBUG_ON(!alloc_cpumask_var(&top_cpuset.effective_cpus, GFP_KERNEL));\n\tBUG_ON(!alloc_cpumask_var(&top_cpuset.effective_xcpus, GFP_KERNEL));\n\tBUG_ON(!alloc_cpumask_var(&top_cpuset.exclusive_cpus, GFP_KERNEL));\n\tBUG_ON(!zalloc_cpumask_var(&subpartitions_cpus, GFP_KERNEL));\n\tBUG_ON(!zalloc_cpumask_var(&isolated_cpus, GFP_KERNEL));\n\n\tcpumask_setall(top_cpuset.cpus_allowed);\n\tnodes_setall(top_cpuset.mems_allowed);\n\tcpumask_setall(top_cpuset.effective_cpus);\n\tcpumask_setall(top_cpuset.effective_xcpus);\n\tcpumask_setall(top_cpuset.exclusive_cpus);\n\tnodes_setall(top_cpuset.effective_mems);\n\n\tfmeter_init(&top_cpuset.fmeter);\n\tINIT_LIST_HEAD(&remote_children);\n\n\tBUG_ON(!alloc_cpumask_var(&cpus_attach, GFP_KERNEL));\n\n\thave_boot_isolcpus = housekeeping_enabled(HK_TYPE_DOMAIN);\n\tif (have_boot_isolcpus) {\n\t\tBUG_ON(!alloc_cpumask_var(&boot_hk_cpus, GFP_KERNEL));\n\t\tcpumask_copy(boot_hk_cpus, housekeeping_cpumask(HK_TYPE_DOMAIN));\n\t\tcpumask_andnot(isolated_cpus, cpu_possible_mask, boot_hk_cpus);\n\t}\n\n\treturn 0;\n}\n\nstatic void\nhotplug_update_tasks(struct cpuset *cs,\n\t\t     struct cpumask *new_cpus, nodemask_t *new_mems,\n\t\t     bool cpus_updated, bool mems_updated)\n{\n\t/* A partition root is allowed to have empty effective cpus */\n\tif (cpumask_empty(new_cpus) && !is_partition_valid(cs))\n\t\tcpumask_copy(new_cpus, parent_cs(cs)->effective_cpus);\n\tif (nodes_empty(*new_mems))\n\t\t*new_mems = parent_cs(cs)->effective_mems;\n\n\tspin_lock_irq(&callback_lock);\n\tcpumask_copy(cs->effective_cpus, new_cpus);\n\tcs->effective_mems = *new_mems;\n\tspin_unlock_irq(&callback_lock);\n\n\tif (cpus_updated)\n\t\tcpuset_update_tasks_cpumask(cs, new_cpus);\n\tif (mems_updated)\n\t\tcpuset_update_tasks_nodemask(cs);\n}\n\nvoid cpuset_force_rebuild(void)\n{\n\tforce_sd_rebuild = true;\n}\n\n/**\n * cpuset_hotplug_update_tasks - update tasks in a cpuset for hotunplug\n * @cs: cpuset in interest\n * @tmp: the tmpmasks structure pointer\n *\n * Compare @cs's cpu and mem masks against top_cpuset and if some have gone\n * offline, update @cs accordingly.  If @cs ends up with no CPU or memory,\n * all its tasks are moved to the nearest ancestor with both resources.\n */\nstatic void cpuset_hotplug_update_tasks(struct cpuset *cs, struct tmpmasks *tmp)\n{\n\tstatic cpumask_t new_cpus;\n\tstatic nodemask_t new_mems;\n\tbool cpus_updated;\n\tbool mems_updated;\n\tbool remote;\n\tint partcmd = -1;\n\tstruct cpuset *parent;\nretry:\n\twait_event(cpuset_attach_wq, cs->attach_in_progress == 0);\n\n\tmutex_lock(&cpuset_mutex);\n\n\t/*\n\t * We have raced with task attaching. We wait until attaching\n\t * is finished, so we won't attach a task to an empty cpuset.\n\t */\n\tif (cs->attach_in_progress) {\n\t\tmutex_unlock(&cpuset_mutex);\n\t\tgoto retry;\n\t}\n\n\tparent = parent_cs(cs);\n\tcompute_effective_cpumask(&new_cpus, cs, parent);\n\tnodes_and(new_mems, cs->mems_allowed, parent->effective_mems);\n\n\tif (!tmp || !cs->partition_root_state)\n\t\tgoto update_tasks;\n\n\t/*\n\t * Compute effective_cpus for valid partition root, may invalidate\n\t * child partition roots if necessary.\n\t */\n\tremote = is_remote_partition(cs);\n\tif (remote || (is_partition_valid(cs) && is_partition_valid(parent)))\n\t\tcompute_partition_effective_cpumask(cs, &new_cpus);\n\n\tif (remote && cpumask_empty(&new_cpus) &&\n\t    partition_is_populated(cs, NULL)) {\n\t\tremote_partition_disable(cs, tmp);\n\t\tcompute_effective_cpumask(&new_cpus, cs, parent);\n\t\tremote = false;\n\t\tcpuset_force_rebuild();\n\t}\n\n\t/*\n\t * Force the partition to become invalid if either one of\n\t * the following conditions hold:\n\t * 1) empty effective cpus but not valid empty partition.\n\t * 2) parent is invalid or doesn't grant any cpus to child\n\t *    partitions.\n\t */\n\tif (is_local_partition(cs) && (!is_partition_valid(parent) ||\n\t\t\t\ttasks_nocpu_error(parent, cs, &new_cpus)))\n\t\tpartcmd = partcmd_invalidate;\n\t/*\n\t * On the other hand, an invalid partition root may be transitioned\n\t * back to a regular one.\n\t */\n\telse if (is_partition_valid(parent) && is_partition_invalid(cs))\n\t\tpartcmd = partcmd_update;\n\n\tif (partcmd >= 0) {\n\t\tupdate_parent_effective_cpumask(cs, partcmd, NULL, tmp);\n\t\tif ((partcmd == partcmd_invalidate) || is_partition_valid(cs)) {\n\t\t\tcompute_partition_effective_cpumask(cs, &new_cpus);\n\t\t\tcpuset_force_rebuild();\n\t\t}\n\t}\n\nupdate_tasks:\n\tcpus_updated = !cpumask_equal(&new_cpus, cs->effective_cpus);\n\tmems_updated = !nodes_equal(new_mems, cs->effective_mems);\n\tif (!cpus_updated && !mems_updated)\n\t\tgoto unlock;\t/* Hotplug doesn't affect this cpuset */\n\n\tif (mems_updated)\n\t\tcheck_insane_mems_config(&new_mems);\n\n\tif (is_in_v2_mode())\n\t\thotplug_update_tasks(cs, &new_cpus, &new_mems,\n\t\t\t\t     cpus_updated, mems_updated);\n\telse\n\t\tcpuset1_hotplug_update_tasks(cs, &new_cpus, &new_mems,\n\t\t\t\t\t    cpus_updated, mems_updated);\n\nunlock:\n\tmutex_unlock(&cpuset_mutex);\n}\n\n/**\n * cpuset_handle_hotplug - handle CPU/memory hot{,un}plug for a cpuset\n *\n * This function is called after either CPU or memory configuration has\n * changed and updates cpuset accordingly.  The top_cpuset is always\n * synchronized to cpu_active_mask and N_MEMORY, which is necessary in\n * order to make cpusets transparent (of no affect) on systems that are\n * actively using CPU hotplug but making no active use of cpusets.\n *\n * Non-root cpusets are only affected by offlining.  If any CPUs or memory\n * nodes have been taken down, cpuset_hotplug_update_tasks() is invoked on\n * all descendants.\n *\n * Note that CPU offlining during suspend is ignored.  We don't modify\n * cpusets across suspend/resume cycles at all.\n *\n * CPU / memory hotplug is handled synchronously.\n */\nstatic void cpuset_handle_hotplug(void)\n{\n\tstatic cpumask_t new_cpus;\n\tstatic nodemask_t new_mems;\n\tbool cpus_updated, mems_updated;\n\tbool on_dfl = is_in_v2_mode();\n\tstruct tmpmasks tmp, *ptmp = NULL;\n\n\tif (on_dfl && !alloc_cpumasks(NULL, &tmp))\n\t\tptmp = &tmp;\n\n\tlockdep_assert_cpus_held();\n\tmutex_lock(&cpuset_mutex);\n\n\t/* fetch the available cpus/mems and find out which changed how */\n\tcpumask_copy(&new_cpus, cpu_active_mask);\n\tnew_mems = node_states[N_MEMORY];\n\n\t/*\n\t * If subpartitions_cpus is populated, it is likely that the check\n\t * below will produce a false positive on cpus_updated when the cpu\n\t * list isn't changed. It is extra work, but it is better to be safe.\n\t */\n\tcpus_updated = !cpumask_equal(top_cpuset.effective_cpus, &new_cpus) ||\n\t\t       !cpumask_empty(subpartitions_cpus);\n\tmems_updated = !nodes_equal(top_cpuset.effective_mems, new_mems);\n\n\t/* For v1, synchronize cpus_allowed to cpu_active_mask */\n\tif (cpus_updated) {\n\t\tcpuset_force_rebuild();\n\t\tspin_lock_irq(&callback_lock);\n\t\tif (!on_dfl)\n\t\t\tcpumask_copy(top_cpuset.cpus_allowed, &new_cpus);\n\t\t/*\n\t\t * Make sure that CPUs allocated to child partitions\n\t\t * do not show up in effective_cpus. If no CPU is left,\n\t\t * we clear the subpartitions_cpus & let the child partitions\n\t\t * fight for the CPUs again.\n\t\t */\n\t\tif (!cpumask_empty(subpartitions_cpus)) {\n\t\t\tif (cpumask_subset(&new_cpus, subpartitions_cpus)) {\n\t\t\t\ttop_cpuset.nr_subparts = 0;\n\t\t\t\tcpumask_clear(subpartitions_cpus);\n\t\t\t} else {\n\t\t\t\tcpumask_andnot(&new_cpus, &new_cpus,\n\t\t\t\t\t       subpartitions_cpus);\n\t\t\t}\n\t\t}\n\t\tcpumask_copy(top_cpuset.effective_cpus, &new_cpus);\n\t\tspin_unlock_irq(&callback_lock);\n\t\t/* we don't mess with cpumasks of tasks in top_cpuset */\n\t}\n\n\t/* synchronize mems_allowed to N_MEMORY */\n\tif (mems_updated) {\n\t\tspin_lock_irq(&callback_lock);\n\t\tif (!on_dfl)\n\t\t\ttop_cpuset.mems_allowed = new_mems;\n\t\ttop_cpuset.effective_mems = new_mems;\n\t\tspin_unlock_irq(&callback_lock);\n\t\tcpuset_update_tasks_nodemask(&top_cpuset);\n\t}\n\n\tmutex_unlock(&cpuset_mutex);\n\n\t/* if cpus or mems changed, we need to propagate to descendants */\n\tif (cpus_updated || mems_updated) {\n\t\tstruct cpuset *cs;\n\t\tstruct cgroup_subsys_state *pos_css;\n\n\t\trcu_read_lock();\n\t\tcpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\n\t\t\tif (cs == &top_cpuset || !css_tryget_online(&cs->css))\n\t\t\t\tcontinue;\n\t\t\trcu_read_unlock();\n\n\t\t\tcpuset_hotplug_update_tasks(cs, ptmp);\n\n\t\t\trcu_read_lock();\n\t\t\tcss_put(&cs->css);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t/* rebuild sched domains if cpus_allowed has changed */\n\tif (force_sd_rebuild) {\n\t\tforce_sd_rebuild = false;\n\t\trebuild_sched_domains_cpuslocked();\n\t}\n\n\tfree_cpumasks(NULL, ptmp);\n}\n\nvoid cpuset_update_active_cpus(void)\n{\n\t/*\n\t * We're inside cpu hotplug critical region which usually nests\n\t * inside cgroup synchronization.  Bounce actual hotplug processing\n\t * to a work item to avoid reverse locking order.\n\t */\n\tcpuset_handle_hotplug();\n}\n\n/*\n * Keep top_cpuset.mems_allowed tracking node_states[N_MEMORY].\n * Call this routine anytime after node_states[N_MEMORY] changes.\n * See cpuset_update_active_cpus() for CPU hotplug handling.\n */\nstatic int cpuset_track_online_nodes(struct notifier_block *self,\n\t\t\t\tunsigned long action, void *arg)\n{\n\tcpuset_handle_hotplug();\n\treturn NOTIFY_OK;\n}\n\n/**\n * cpuset_init_smp - initialize cpus_allowed\n *\n * Description: Finish top cpuset after cpu, node maps are initialized\n */\nvoid __init cpuset_init_smp(void)\n{\n\t/*\n\t * cpus_allowd/mems_allowed set to v2 values in the initial\n\t * cpuset_bind() call will be reset to v1 values in another\n\t * cpuset_bind() call when v1 cpuset is mounted.\n\t */\n\ttop_cpuset.old_mems_allowed = top_cpuset.mems_allowed;\n\n\tcpumask_copy(top_cpuset.effective_cpus, cpu_active_mask);\n\ttop_cpuset.effective_mems = node_states[N_MEMORY];\n\n\thotplug_memory_notifier(cpuset_track_online_nodes, CPUSET_CALLBACK_PRI);\n\n\tcpuset_migrate_mm_wq = alloc_ordered_workqueue(\"cpuset_migrate_mm\", 0);\n\tBUG_ON(!cpuset_migrate_mm_wq);\n}\n\n/**\n * cpuset_cpus_allowed - return cpus_allowed mask from a tasks cpuset.\n * @tsk: pointer to task_struct from which to obtain cpuset->cpus_allowed.\n * @pmask: pointer to struct cpumask variable to receive cpus_allowed set.\n *\n * Description: Returns the cpumask_var_t cpus_allowed of the cpuset\n * attached to the specified @tsk.  Guaranteed to return some non-empty\n * subset of cpu_online_mask, even if this means going outside the\n * tasks cpuset, except when the task is in the top cpuset.\n **/\n\nvoid cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)\n{\n\tunsigned long flags;\n\tstruct cpuset *cs;\n\n\tspin_lock_irqsave(&callback_lock, flags);\n\trcu_read_lock();\n\n\tcs = task_cs(tsk);\n\tif (cs != &top_cpuset)\n\t\tguarantee_online_cpus(tsk, pmask);\n\t/*\n\t * Tasks in the top cpuset won't get update to their cpumasks\n\t * when a hotplug online/offline event happens. So we include all\n\t * offline cpus in the allowed cpu list.\n\t */\n\tif ((cs == &top_cpuset) || cpumask_empty(pmask)) {\n\t\tconst struct cpumask *possible_mask = task_cpu_possible_mask(tsk);\n\n\t\t/*\n\t\t * We first exclude cpus allocated to partitions. If there is no\n\t\t * allowable online cpu left, we fall back to all possible cpus.\n\t\t */\n\t\tcpumask_andnot(pmask, possible_mask, subpartitions_cpus);\n\t\tif (!cpumask_intersects(pmask, cpu_online_mask))\n\t\t\tcpumask_copy(pmask, possible_mask);\n\t}\n\n\trcu_read_unlock();\n\tspin_unlock_irqrestore(&callback_lock, flags);\n}\n\n/**\n * cpuset_cpus_allowed_fallback - final fallback before complete catastrophe.\n * @tsk: pointer to task_struct with which the scheduler is struggling\n *\n * Description: In the case that the scheduler cannot find an allowed cpu in\n * tsk->cpus_allowed, we fall back to task_cs(tsk)->cpus_allowed. In legacy\n * mode however, this value is the same as task_cs(tsk)->effective_cpus,\n * which will not contain a sane cpumask during cases such as cpu hotplugging.\n * This is the absolute last resort for the scheduler and it is only used if\n * _every_ other avenue has been traveled.\n *\n * Returns true if the affinity of @tsk was changed, false otherwise.\n **/\n\nbool cpuset_cpus_allowed_fallback(struct task_struct *tsk)\n{\n\tconst struct cpumask *possible_mask = task_cpu_possible_mask(tsk);\n\tconst struct cpumask *cs_mask;\n\tbool changed = false;\n\n\trcu_read_lock();\n\tcs_mask = task_cs(tsk)->cpus_allowed;\n\tif (is_in_v2_mode() && cpumask_subset(cs_mask, possible_mask)) {\n\t\tdo_set_cpus_allowed(tsk, cs_mask);\n\t\tchanged = true;\n\t}\n\trcu_read_unlock();\n\n\t/*\n\t * We own tsk->cpus_allowed, nobody can change it under us.\n\t *\n\t * But we used cs && cs->cpus_allowed lockless and thus can\n\t * race with cgroup_attach_task() or update_cpumask() and get\n\t * the wrong tsk->cpus_allowed. However, both cases imply the\n\t * subsequent cpuset_change_cpumask()->set_cpus_allowed_ptr()\n\t * which takes task_rq_lock().\n\t *\n\t * If we are called after it dropped the lock we must see all\n\t * changes in tsk_cs()->cpus_allowed. Otherwise we can temporary\n\t * set any mask even if it is not right from task_cs() pov,\n\t * the pending set_cpus_allowed_ptr() will fix things.\n\t *\n\t * select_fallback_rq() will fix things ups and set cpu_possible_mask\n\t * if required.\n\t */\n\treturn changed;\n}\n\nvoid __init cpuset_init_current_mems_allowed(void)\n{\n\tnodes_setall(current->mems_allowed);\n}\n\n/**\n * cpuset_mems_allowed - return mems_allowed mask from a tasks cpuset.\n * @tsk: pointer to task_struct from which to obtain cpuset->mems_allowed.\n *\n * Description: Returns the nodemask_t mems_allowed of the cpuset\n * attached to the specified @tsk.  Guaranteed to return some non-empty\n * subset of node_states[N_MEMORY], even if this means going outside the\n * tasks cpuset.\n **/\n\nnodemask_t cpuset_mems_allowed(struct task_struct *tsk)\n{\n\tnodemask_t mask;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&callback_lock, flags);\n\trcu_read_lock();\n\tguarantee_online_mems(task_cs(tsk), &mask);\n\trcu_read_unlock();\n\tspin_unlock_irqrestore(&callback_lock, flags);\n\n\treturn mask;\n}\n\n/**\n * cpuset_nodemask_valid_mems_allowed - check nodemask vs. current mems_allowed\n * @nodemask: the nodemask to be checked\n *\n * Are any of the nodes in the nodemask allowed in current->mems_allowed?\n */\nint cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)\n{\n\treturn nodes_intersects(*nodemask, current->mems_allowed);\n}\n\n/*\n * nearest_hardwall_ancestor() - Returns the nearest mem_exclusive or\n * mem_hardwall ancestor to the specified cpuset.  Call holding\n * callback_lock.  If no ancestor is mem_exclusive or mem_hardwall\n * (an unusual configuration), then returns the root cpuset.\n */\nstatic struct cpuset *nearest_hardwall_ancestor(struct cpuset *cs)\n{\n\twhile (!(is_mem_exclusive(cs) || is_mem_hardwall(cs)) && parent_cs(cs))\n\t\tcs = parent_cs(cs);\n\treturn cs;\n}\n\n/*\n * cpuset_node_allowed - Can we allocate on a memory node?\n * @node: is this an allowed node?\n * @gfp_mask: memory allocation flags\n *\n * If we're in interrupt, yes, we can always allocate.  If @node is set in\n * current's mems_allowed, yes.  If it's not a __GFP_HARDWALL request and this\n * node is set in the nearest hardwalled cpuset ancestor to current's cpuset,\n * yes.  If current has access to memory reserves as an oom victim, yes.\n * Otherwise, no.\n *\n * GFP_USER allocations are marked with the __GFP_HARDWALL bit,\n * and do not allow allocations outside the current tasks cpuset\n * unless the task has been OOM killed.\n * GFP_KERNEL allocations are not so marked, so can escape to the\n * nearest enclosing hardwalled ancestor cpuset.\n *\n * Scanning up parent cpusets requires callback_lock.  The\n * __alloc_pages() routine only calls here with __GFP_HARDWALL bit\n * _not_ set if it's a GFP_KERNEL allocation, and all nodes in the\n * current tasks mems_allowed came up empty on the first pass over\n * the zonelist.  So only GFP_KERNEL allocations, if all nodes in the\n * cpuset are short of memory, might require taking the callback_lock.\n *\n * The first call here from mm/page_alloc:get_page_from_freelist()\n * has __GFP_HARDWALL set in gfp_mask, enforcing hardwall cpusets,\n * so no allocation on a node outside the cpuset is allowed (unless\n * in interrupt, of course).\n *\n * The second pass through get_page_from_freelist() doesn't even call\n * here for GFP_ATOMIC calls.  For those calls, the __alloc_pages()\n * variable 'wait' is not set, and the bit ALLOC_CPUSET is not set\n * in alloc_flags.  That logic and the checks below have the combined\n * affect that:\n *\tin_interrupt - any node ok (current task context irrelevant)\n *\tGFP_ATOMIC   - any node ok\n *\ttsk_is_oom_victim   - any node ok\n *\tGFP_KERNEL   - any node in enclosing hardwalled cpuset ok\n *\tGFP_USER     - only nodes in current tasks mems allowed ok.\n */\nbool cpuset_node_allowed(int node, gfp_t gfp_mask)\n{\n\tstruct cpuset *cs;\t\t/* current cpuset ancestors */\n\tbool allowed;\t\t\t/* is allocation in zone z allowed? */\n\tunsigned long flags;\n\n\tif (in_interrupt())\n\t\treturn true;\n\tif (node_isset(node, current->mems_allowed))\n\t\treturn true;\n\t/*\n\t * Allow tasks that have access to memory reserves because they have\n\t * been OOM killed to get memory anywhere.\n\t */\n\tif (unlikely(tsk_is_oom_victim(current)))\n\t\treturn true;\n\tif (gfp_mask & __GFP_HARDWALL)\t/* If hardwall request, stop here */\n\t\treturn false;\n\n\tif (current->flags & PF_EXITING) /* Let dying task have memory */\n\t\treturn true;\n\n\t/* Not hardwall and node outside mems_allowed: scan up cpusets */\n\tspin_lock_irqsave(&callback_lock, flags);\n\n\trcu_read_lock();\n\tcs = nearest_hardwall_ancestor(task_cs(current));\n\tallowed = node_isset(node, cs->mems_allowed);\n\trcu_read_unlock();\n\n\tspin_unlock_irqrestore(&callback_lock, flags);\n\treturn allowed;\n}\n\n/**\n * cpuset_spread_node() - On which node to begin search for a page\n * @rotor: round robin rotor\n *\n * If a task is marked PF_SPREAD_PAGE or PF_SPREAD_SLAB (as for\n * tasks in a cpuset with is_spread_page or is_spread_slab set),\n * and if the memory allocation used cpuset_mem_spread_node()\n * to determine on which node to start looking, as it will for\n * certain page cache or slab cache pages such as used for file\n * system buffers and inode caches, then instead of starting on the\n * local node to look for a free page, rather spread the starting\n * node around the tasks mems_allowed nodes.\n *\n * We don't have to worry about the returned node being offline\n * because \"it can't happen\", and even if it did, it would be ok.\n *\n * The routines calling guarantee_online_mems() are careful to\n * only set nodes in task->mems_allowed that are online.  So it\n * should not be possible for the following code to return an\n * offline node.  But if it did, that would be ok, as this routine\n * is not returning the node where the allocation must be, only\n * the node where the search should start.  The zonelist passed to\n * __alloc_pages() will include all nodes.  If the slab allocator\n * is passed an offline node, it will fall back to the local node.\n * See kmem_cache_alloc_node().\n */\nstatic int cpuset_spread_node(int *rotor)\n{\n\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n}\n\n/**\n * cpuset_mem_spread_node() - On which node to begin search for a file page\n */\nint cpuset_mem_spread_node(void)\n{\n\tif (current->cpuset_mem_spread_rotor == NUMA_NO_NODE)\n\t\tcurrent->cpuset_mem_spread_rotor =\n\t\t\tnode_random(&current->mems_allowed);\n\n\treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n}\n\n/**\n * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n * @tsk1: pointer to task_struct of some task.\n * @tsk2: pointer to task_struct of some other task.\n *\n * Description: Return true if @tsk1's mems_allowed intersects the\n * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n * one of the task's memory usage might impact the memory available\n * to the other.\n **/\n\nint cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n\t\t\t\t   const struct task_struct *tsk2)\n{\n\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n}\n\n/**\n * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n *\n * Description: Prints current's name, cpuset name, and cached copy of its\n * mems_allowed to the kernel log.\n */\nvoid cpuset_print_current_mems_allowed(void)\n{\n\tstruct cgroup *cgrp;\n\n\trcu_read_lock();\n\n\tcgrp = task_cs(current)->css.cgroup;\n\tpr_cont(\",cpuset=\");\n\tpr_cont_cgroup_name(cgrp);\n\tpr_cont(\",mems_allowed=%*pbl\",\n\t\tnodemask_pr_args(&current->mems_allowed));\n\n\trcu_read_unlock();\n}\n\n#ifdef CONFIG_PROC_PID_CPUSET\n/*\n * proc_cpuset_show()\n *  - Print tasks cpuset path into seq_file.\n *  - Used for /proc/<pid>/cpuset.\n *  - No need to task_lock(tsk) on this tsk->cpuset reference, as it\n *    doesn't really matter if tsk->cpuset changes after we read it,\n *    and we take cpuset_mutex, keeping cpuset_attach() from changing it\n *    anyway.\n */\nint proc_cpuset_show(struct seq_file *m, struct pid_namespace *ns,\n\t\t     struct pid *pid, struct task_struct *tsk)\n{\n\tchar *buf;\n\tstruct cgroup_subsys_state *css;\n\tint retval;\n\n\tretval = -ENOMEM;\n\tbuf = kmalloc(PATH_MAX, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\trcu_read_lock();\n\tspin_lock_irq(&css_set_lock);\n\tcss = task_css(tsk, cpuset_cgrp_id);\n\tretval = cgroup_path_ns_locked(css->cgroup, buf, PATH_MAX,\n\t\t\t\t       current->nsproxy->cgroup_ns);\n\tspin_unlock_irq(&css_set_lock);\n\trcu_read_unlock();\n\n\tif (retval == -E2BIG)\n\t\tretval = -ENAMETOOLONG;\n\tif (retval < 0)\n\t\tgoto out_free;\n\tseq_puts(m, buf);\n\tseq_putc(m, '\\n');\n\tretval = 0;\nout_free:\n\tkfree(buf);\nout:\n\treturn retval;\n}\n#endif /* CONFIG_PROC_PID_CPUSET */\n\n/* Display task mems_allowed in /proc/<pid>/status file. */\nvoid cpuset_task_status_allowed(struct seq_file *m, struct task_struct *task)\n{\n\tseq_printf(m, \"Mems_allowed:\\t%*pb\\n\",\n\t\t   nodemask_pr_args(&task->mems_allowed));\n\tseq_printf(m, \"Mems_allowed_list:\\t%*pbl\\n\",\n\t\t   nodemask_pr_args(&task->mems_allowed));\n}\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21635",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21635.json",
            "patch_attempts": [
                {
                    "upstream_commit": "6259d2484d0ceff42245d1f09cc8cb6ee72d847a",
                    "upstream_commit_date": "2025-01-09 08:53:35 -0800",
                    "upstream_patch": "7f5611cbc4871c7fb1ad36c2e5a9edad63dca95c",
                    "total_versions_tested": 1,
                    "successful_patches": 1,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "de8d6de0ee27be4b2b1e5b06f04aeacbabbba492",
                            "downstream_commit": "bcf8c60074e81ed2ac2d35130917175a3949c917",
                            "commit_date": "2025-01-17 13:40:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/rds/tcp.c",
                            "downstream_patch_content": "commit de8d6de0ee27be4b2b1e5b06f04aeacbabbba492\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:37 2025 +0100\n\n    rds: sysctl: rds_tcp_{rcv,snd}buf: avoid using current->nsproxy\n    \n    commit 7f5611cbc4871c7fb1ad36c2e5a9edad63dca95c upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The per-netns structure can be obtained from the table->data using\n    container_of(), then the 'net' one can be retrieved from the listen\n    socket (if available).\n    \n    Fixes: c6a58ffed536 (\"RDS: TCP: Add sysctl tunables for sndbuf/rcvbuf on rds-tcp socket\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-9-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/rds/tcp.c b/net/rds/tcp.c\nindex 351ac1747224..0581c53e6517 100644\n--- a/net/rds/tcp.c\n+++ b/net/rds/tcp.c\n@@ -61,8 +61,10 @@ static atomic_t rds_tcp_unloading = ATOMIC_INIT(0);\n \n static struct kmem_cache *rds_tcp_conn_slab;\n \n-static int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n-\t\t\t\t void *buffer, size_t *lenp, loff_t *fpos);\n+static int rds_tcp_sndbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos);\n+static int rds_tcp_rcvbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos);\n \n static int rds_tcp_min_sndbuf = SOCK_MIN_SNDBUF;\n static int rds_tcp_min_rcvbuf = SOCK_MIN_RCVBUF;\n@@ -74,7 +76,7 @@ static struct ctl_table rds_tcp_sysctl_table[] = {\n \t\t/* data is per-net pointer */\n \t\t.maxlen         = sizeof(int),\n \t\t.mode           = 0644,\n-\t\t.proc_handler   = rds_tcp_skbuf_handler,\n+\t\t.proc_handler   = rds_tcp_sndbuf_handler,\n \t\t.extra1\t\t= &rds_tcp_min_sndbuf,\n \t},\n #define\tRDS_TCP_RCVBUF\t1\n@@ -83,7 +85,7 @@ static struct ctl_table rds_tcp_sysctl_table[] = {\n \t\t/* data is per-net pointer */\n \t\t.maxlen         = sizeof(int),\n \t\t.mode           = 0644,\n-\t\t.proc_handler   = rds_tcp_skbuf_handler,\n+\t\t.proc_handler   = rds_tcp_rcvbuf_handler,\n \t\t.extra1\t\t= &rds_tcp_min_rcvbuf,\n \t},\n };\n@@ -682,10 +684,10 @@ static void rds_tcp_sysctl_reset(struct net *net)\n \tspin_unlock_irq(&rds_tcp_conn_lock);\n }\n \n-static int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n+static int rds_tcp_skbuf_handler(struct rds_tcp_net *rtn,\n+\t\t\t\t const struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *fpos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n \tint err;\n \n \terr = proc_dointvec_minmax(ctl, write, buffer, lenp, fpos);\n@@ -694,11 +696,34 @@ static int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n \t\t\t*(int *)(ctl->extra1));\n \t\treturn err;\n \t}\n-\tif (write)\n+\n+\tif (write && rtn->rds_tcp_listen_sock && rtn->rds_tcp_listen_sock->sk) {\n+\t\tstruct net *net = sock_net(rtn->rds_tcp_listen_sock->sk);\n+\n \t\trds_tcp_sysctl_reset(net);\n+\t}\n+\n \treturn 0;\n }\n \n+static int rds_tcp_sndbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos)\n+{\n+\tstruct rds_tcp_net *rtn = container_of(ctl->data, struct rds_tcp_net,\n+\t\t\t\t\t       sndbuf_size);\n+\n+\treturn rds_tcp_skbuf_handler(rtn, ctl, write, buffer, lenp, fpos);\n+}\n+\n+static int rds_tcp_rcvbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos)\n+{\n+\tstruct rds_tcp_net *rtn = container_of(ctl->data, struct rds_tcp_net,\n+\t\t\t\t\t       rcvbuf_size);\n+\n+\treturn rds_tcp_skbuf_handler(rtn, ctl, write, buffer, lenp, fpos);\n+}\n+\n static void rds_tcp_exit(void)\n {\n \trds_tcp_set_unloading();\n",
                            "downstream_file_content": {}
                        }
                    ],
                    "upstream_patch_content": "From 7f5611cbc4871c7fb1ad36c2e5a9edad63dca95c Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:37 +0100\nSubject: [PATCH] rds: sysctl: rds_tcp_{rcv,snd}buf: avoid using\n current->nsproxy\n\nAs mentioned in a previous commit of this series, using the 'net'\nstructure via 'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe per-netns structure can be obtained from the table->data using\ncontainer_of(), then the 'net' one can be retrieved from the listen\nsocket (if available).\n\nFixes: c6a58ffed536 (\"RDS: TCP: Add sysctl tunables for sndbuf/rcvbuf on rds-tcp socket\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-9-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/rds/tcp.c | 39 ++++++++++++++++++++++++++++++++-------\n 1 file changed, 32 insertions(+), 7 deletions(-)\n\ndiff --git a/net/rds/tcp.c b/net/rds/tcp.c\nindex 351ac1747224..0581c53e6517 100644\n--- a/net/rds/tcp.c\n+++ b/net/rds/tcp.c\n@@ -61,8 +61,10 @@ static atomic_t rds_tcp_unloading = ATOMIC_INIT(0);\n \n static struct kmem_cache *rds_tcp_conn_slab;\n \n-static int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n-\t\t\t\t void *buffer, size_t *lenp, loff_t *fpos);\n+static int rds_tcp_sndbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos);\n+static int rds_tcp_rcvbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos);\n \n static int rds_tcp_min_sndbuf = SOCK_MIN_SNDBUF;\n static int rds_tcp_min_rcvbuf = SOCK_MIN_RCVBUF;\n@@ -74,7 +76,7 @@ static struct ctl_table rds_tcp_sysctl_table[] = {\n \t\t/* data is per-net pointer */\n \t\t.maxlen         = sizeof(int),\n \t\t.mode           = 0644,\n-\t\t.proc_handler   = rds_tcp_skbuf_handler,\n+\t\t.proc_handler   = rds_tcp_sndbuf_handler,\n \t\t.extra1\t\t= &rds_tcp_min_sndbuf,\n \t},\n #define\tRDS_TCP_RCVBUF\t1\n@@ -83,7 +85,7 @@ static struct ctl_table rds_tcp_sysctl_table[] = {\n \t\t/* data is per-net pointer */\n \t\t.maxlen         = sizeof(int),\n \t\t.mode           = 0644,\n-\t\t.proc_handler   = rds_tcp_skbuf_handler,\n+\t\t.proc_handler   = rds_tcp_rcvbuf_handler,\n \t\t.extra1\t\t= &rds_tcp_min_rcvbuf,\n \t},\n };\n@@ -682,10 +684,10 @@ static void rds_tcp_sysctl_reset(struct net *net)\n \tspin_unlock_irq(&rds_tcp_conn_lock);\n }\n \n-static int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n+static int rds_tcp_skbuf_handler(struct rds_tcp_net *rtn,\n+\t\t\t\t const struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *fpos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n \tint err;\n \n \terr = proc_dointvec_minmax(ctl, write, buffer, lenp, fpos);\n@@ -694,11 +696,34 @@ static int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n \t\t\t*(int *)(ctl->extra1));\n \t\treturn err;\n \t}\n-\tif (write)\n+\n+\tif (write && rtn->rds_tcp_listen_sock && rtn->rds_tcp_listen_sock->sk) {\n+\t\tstruct net *net = sock_net(rtn->rds_tcp_listen_sock->sk);\n+\n \t\trds_tcp_sysctl_reset(net);\n+\t}\n+\n \treturn 0;\n }\n \n+static int rds_tcp_sndbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos)\n+{\n+\tstruct rds_tcp_net *rtn = container_of(ctl->data, struct rds_tcp_net,\n+\t\t\t\t\t       sndbuf_size);\n+\n+\treturn rds_tcp_skbuf_handler(rtn, ctl, write, buffer, lenp, fpos);\n+}\n+\n+static int rds_tcp_rcvbuf_handler(const struct ctl_table *ctl, int write,\n+\t\t\t\t  void *buffer, size_t *lenp, loff_t *fpos)\n+{\n+\tstruct rds_tcp_net *rtn = container_of(ctl->data, struct rds_tcp_net,\n+\t\t\t\t\t       rcvbuf_size);\n+\n+\treturn rds_tcp_skbuf_handler(rtn, ctl, write, buffer, lenp, fpos);\n+}\n+\n static void rds_tcp_exit(void)\n {\n \trds_tcp_set_unloading();\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/rds/tcp.c": "/*\n * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the\n * OpenIB.org BSD license below:\n *\n *     Redistribution and use in source and binary forms, with or\n *     without modification, are permitted provided that the following\n *     conditions are met:\n *\n *      - Redistributions of source code must retain the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer.\n *\n *      - Redistributions in binary form must reproduce the above\n *        copyright notice, this list of conditions and the following\n *        disclaimer in the documentation and/or other materials\n *        provided with the distribution.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n */\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/in.h>\n#include <linux/module.h>\n#include <net/tcp.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/addrconf.h>\n\n#include \"rds.h\"\n#include \"tcp.h\"\n\n/* only for info exporting */\nstatic DEFINE_SPINLOCK(rds_tcp_tc_list_lock);\nstatic LIST_HEAD(rds_tcp_tc_list);\n\n/* rds_tcp_tc_count counts only IPv4 connections.\n * rds6_tcp_tc_count counts both IPv4 and IPv6 connections.\n */\nstatic unsigned int rds_tcp_tc_count;\n#if IS_ENABLED(CONFIG_IPV6)\nstatic unsigned int rds6_tcp_tc_count;\n#endif\n\n/* Track rds_tcp_connection structs so they can be cleaned up */\nstatic DEFINE_SPINLOCK(rds_tcp_conn_lock);\nstatic LIST_HEAD(rds_tcp_conn_list);\nstatic atomic_t rds_tcp_unloading = ATOMIC_INIT(0);\n\nstatic struct kmem_cache *rds_tcp_conn_slab;\n\nstatic int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *fpos);\n\nstatic int rds_tcp_min_sndbuf = SOCK_MIN_SNDBUF;\nstatic int rds_tcp_min_rcvbuf = SOCK_MIN_RCVBUF;\n\nstatic struct ctl_table rds_tcp_sysctl_table[] = {\n#define\tRDS_TCP_SNDBUF\t0\n\t{\n\t\t.procname       = \"rds_tcp_sndbuf\",\n\t\t/* data is per-net pointer */\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = rds_tcp_skbuf_handler,\n\t\t.extra1\t\t= &rds_tcp_min_sndbuf,\n\t},\n#define\tRDS_TCP_RCVBUF\t1\n\t{\n\t\t.procname       = \"rds_tcp_rcvbuf\",\n\t\t/* data is per-net pointer */\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = rds_tcp_skbuf_handler,\n\t\t.extra1\t\t= &rds_tcp_min_rcvbuf,\n\t},\n};\n\nu32 rds_tcp_write_seq(struct rds_tcp_connection *tc)\n{\n\t/* seq# of the last byte of data in tcp send buffer */\n\treturn tcp_sk(tc->t_sock->sk)->write_seq;\n}\n\nu32 rds_tcp_snd_una(struct rds_tcp_connection *tc)\n{\n\treturn tcp_sk(tc->t_sock->sk)->snd_una;\n}\n\nvoid rds_tcp_restore_callbacks(struct socket *sock,\n\t\t\t       struct rds_tcp_connection *tc)\n{\n\trdsdebug(\"restoring sock %p callbacks from tc %p\\n\", sock, tc);\n\twrite_lock_bh(&sock->sk->sk_callback_lock);\n\n\t/* done under the callback_lock to serialize with write_space */\n\tspin_lock(&rds_tcp_tc_list_lock);\n\tlist_del_init(&tc->t_list_item);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds6_tcp_tc_count--;\n#endif\n\tif (!tc->t_cpath->cp_conn->c_isv6)\n\t\trds_tcp_tc_count--;\n\tspin_unlock(&rds_tcp_tc_list_lock);\n\n\ttc->t_sock = NULL;\n\n\tsock->sk->sk_write_space = tc->t_orig_write_space;\n\tsock->sk->sk_data_ready = tc->t_orig_data_ready;\n\tsock->sk->sk_state_change = tc->t_orig_state_change;\n\tsock->sk->sk_user_data = NULL;\n\n\twrite_unlock_bh(&sock->sk->sk_callback_lock);\n}\n\n/*\n * rds_tcp_reset_callbacks() switches the to the new sock and\n * returns the existing tc->t_sock.\n *\n * The only functions that set tc->t_sock are rds_tcp_set_callbacks\n * and rds_tcp_reset_callbacks.  Send and receive trust that\n * it is set.  The absence of RDS_CONN_UP bit protects those paths\n * from being called while it isn't set.\n */\nvoid rds_tcp_reset_callbacks(struct socket *sock,\n\t\t\t     struct rds_conn_path *cp)\n{\n\tstruct rds_tcp_connection *tc = cp->cp_transport_data;\n\tstruct socket *osock = tc->t_sock;\n\n\tif (!osock)\n\t\tgoto newsock;\n\n\t/* Need to resolve a duelling SYN between peers.\n\t * We have an outstanding SYN to this peer, which may\n\t * potentially have transitioned to the RDS_CONN_UP state,\n\t * so we must quiesce any send threads before resetting\n\t * cp_transport_data. We quiesce these threads by setting\n\t * cp_state to something other than RDS_CONN_UP, and then\n\t * waiting for any existing threads in rds_send_xmit to\n\t * complete release_in_xmit(). (Subsequent threads entering\n\t * rds_send_xmit() will bail on !rds_conn_up().\n\t *\n\t * However an incoming syn-ack at this point would end up\n\t * marking the conn as RDS_CONN_UP, and would again permit\n\t * rds_send_xmi() threads through, so ideally we would\n\t * synchronize on RDS_CONN_UP after lock_sock(), but cannot\n\t * do that: waiting on !RDS_IN_XMIT after lock_sock() may\n\t * end up deadlocking with tcp_sendmsg(), and the RDS_IN_XMIT\n\t * would not get set. As a result, we set c_state to\n\t * RDS_CONN_RESETTTING, to ensure that rds_tcp_state_change\n\t * cannot mark rds_conn_path_up() in the window before lock_sock()\n\t */\n\tatomic_set(&cp->cp_state, RDS_CONN_RESETTING);\n\twait_event(cp->cp_waitq, !test_bit(RDS_IN_XMIT, &cp->cp_flags));\n\t/* reset receive side state for rds_tcp_data_recv() for osock  */\n\tcancel_delayed_work_sync(&cp->cp_send_w);\n\tcancel_delayed_work_sync(&cp->cp_recv_w);\n\tlock_sock(osock->sk);\n\tif (tc->t_tinc) {\n\t\trds_inc_put(&tc->t_tinc->ti_inc);\n\t\ttc->t_tinc = NULL;\n\t}\n\ttc->t_tinc_hdr_rem = sizeof(struct rds_header);\n\ttc->t_tinc_data_rem = 0;\n\trds_tcp_restore_callbacks(osock, tc);\n\trelease_sock(osock->sk);\n\tsock_release(osock);\nnewsock:\n\trds_send_path_reset(cp);\n\tlock_sock(sock->sk);\n\trds_tcp_set_callbacks(sock, cp);\n\trelease_sock(sock->sk);\n}\n\n/* Add tc to rds_tcp_tc_list and set tc->t_sock. See comments\n * above rds_tcp_reset_callbacks for notes about synchronization\n * with data path\n */\nvoid rds_tcp_set_callbacks(struct socket *sock, struct rds_conn_path *cp)\n{\n\tstruct rds_tcp_connection *tc = cp->cp_transport_data;\n\n\trdsdebug(\"setting sock %p callbacks to tc %p\\n\", sock, tc);\n\twrite_lock_bh(&sock->sk->sk_callback_lock);\n\n\t/* done under the callback_lock to serialize with write_space */\n\tspin_lock(&rds_tcp_tc_list_lock);\n\tlist_add_tail(&tc->t_list_item, &rds_tcp_tc_list);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds6_tcp_tc_count++;\n#endif\n\tif (!tc->t_cpath->cp_conn->c_isv6)\n\t\trds_tcp_tc_count++;\n\tspin_unlock(&rds_tcp_tc_list_lock);\n\n\t/* accepted sockets need our listen data ready undone */\n\tif (sock->sk->sk_data_ready == rds_tcp_listen_data_ready)\n\t\tsock->sk->sk_data_ready = sock->sk->sk_user_data;\n\n\ttc->t_sock = sock;\n\ttc->t_cpath = cp;\n\ttc->t_orig_data_ready = sock->sk->sk_data_ready;\n\ttc->t_orig_write_space = sock->sk->sk_write_space;\n\ttc->t_orig_state_change = sock->sk->sk_state_change;\n\n\tsock->sk->sk_user_data = cp;\n\tsock->sk->sk_data_ready = rds_tcp_data_ready;\n\tsock->sk->sk_write_space = rds_tcp_write_space;\n\tsock->sk->sk_state_change = rds_tcp_state_change;\n\n\twrite_unlock_bh(&sock->sk->sk_callback_lock);\n}\n\n/* Handle RDS_INFO_TCP_SOCKETS socket option.  It only returns IPv4\n * connections for backward compatibility.\n */\nstatic void rds_tcp_tc_info(struct socket *rds_sock, unsigned int len,\n\t\t\t    struct rds_info_iterator *iter,\n\t\t\t    struct rds_info_lengths *lens)\n{\n\tstruct rds_info_tcp_socket tsinfo;\n\tstruct rds_tcp_connection *tc;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rds_tcp_tc_list_lock, flags);\n\n\tif (len / sizeof(tsinfo) < rds_tcp_tc_count)\n\t\tgoto out;\n\n\tlist_for_each_entry(tc, &rds_tcp_tc_list, t_list_item) {\n\t\tstruct inet_sock *inet = inet_sk(tc->t_sock->sk);\n\n\t\tif (tc->t_cpath->cp_conn->c_isv6)\n\t\t\tcontinue;\n\n\t\ttsinfo.local_addr = inet->inet_saddr;\n\t\ttsinfo.local_port = inet->inet_sport;\n\t\ttsinfo.peer_addr = inet->inet_daddr;\n\t\ttsinfo.peer_port = inet->inet_dport;\n\n\t\ttsinfo.hdr_rem = tc->t_tinc_hdr_rem;\n\t\ttsinfo.data_rem = tc->t_tinc_data_rem;\n\t\ttsinfo.last_sent_nxt = tc->t_last_sent_nxt;\n\t\ttsinfo.last_expected_una = tc->t_last_expected_una;\n\t\ttsinfo.last_seen_una = tc->t_last_seen_una;\n\t\ttsinfo.tos = tc->t_cpath->cp_conn->c_tos;\n\n\t\trds_info_copy(iter, &tsinfo, sizeof(tsinfo));\n\t}\n\nout:\n\tlens->nr = rds_tcp_tc_count;\n\tlens->each = sizeof(tsinfo);\n\n\tspin_unlock_irqrestore(&rds_tcp_tc_list_lock, flags);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n/* Handle RDS6_INFO_TCP_SOCKETS socket option. It returns both IPv4 and\n * IPv6 connections. IPv4 connection address is returned in an IPv4 mapped\n * address.\n */\nstatic void rds6_tcp_tc_info(struct socket *sock, unsigned int len,\n\t\t\t     struct rds_info_iterator *iter,\n\t\t\t     struct rds_info_lengths *lens)\n{\n\tstruct rds6_info_tcp_socket tsinfo6;\n\tstruct rds_tcp_connection *tc;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rds_tcp_tc_list_lock, flags);\n\n\tif (len / sizeof(tsinfo6) < rds6_tcp_tc_count)\n\t\tgoto out;\n\n\tlist_for_each_entry(tc, &rds_tcp_tc_list, t_list_item) {\n\t\tstruct sock *sk = tc->t_sock->sk;\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\ttsinfo6.local_addr = sk->sk_v6_rcv_saddr;\n\t\ttsinfo6.local_port = inet->inet_sport;\n\t\ttsinfo6.peer_addr = sk->sk_v6_daddr;\n\t\ttsinfo6.peer_port = inet->inet_dport;\n\n\t\ttsinfo6.hdr_rem = tc->t_tinc_hdr_rem;\n\t\ttsinfo6.data_rem = tc->t_tinc_data_rem;\n\t\ttsinfo6.last_sent_nxt = tc->t_last_sent_nxt;\n\t\ttsinfo6.last_expected_una = tc->t_last_expected_una;\n\t\ttsinfo6.last_seen_una = tc->t_last_seen_una;\n\n\t\trds_info_copy(iter, &tsinfo6, sizeof(tsinfo6));\n\t}\n\nout:\n\tlens->nr = rds6_tcp_tc_count;\n\tlens->each = sizeof(tsinfo6);\n\n\tspin_unlock_irqrestore(&rds_tcp_tc_list_lock, flags);\n}\n#endif\n\nint rds_tcp_laddr_check(struct net *net, const struct in6_addr *addr,\n\t\t\t__u32 scope_id)\n{\n\tstruct net_device *dev = NULL;\n#if IS_ENABLED(CONFIG_IPV6)\n\tint ret;\n#endif\n\n\tif (ipv6_addr_v4mapped(addr)) {\n\t\tif (inet_addr_type(net, addr->s6_addr32[3]) == RTN_LOCAL)\n\t\t\treturn 0;\n\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\t/* If the scope_id is specified, check only those addresses\n\t * hosted on the specified interface.\n\t */\n\tif (scope_id != 0) {\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_index_rcu(net, scope_id);\n\t\t/* scope_id is not valid... */\n\t\tif (!dev) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EADDRNOTAVAIL;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\tret = ipv6_chk_addr(net, addr, dev, 0);\n\tif (ret)\n\t\treturn 0;\n#endif\n\treturn -EADDRNOTAVAIL;\n}\n\nstatic void rds_tcp_conn_free(void *arg)\n{\n\tstruct rds_tcp_connection *tc = arg;\n\tunsigned long flags;\n\n\trdsdebug(\"freeing tc %p\\n\", tc);\n\n\tspin_lock_irqsave(&rds_tcp_conn_lock, flags);\n\tif (!tc->t_tcp_node_detached)\n\t\tlist_del(&tc->t_tcp_node);\n\tspin_unlock_irqrestore(&rds_tcp_conn_lock, flags);\n\n\tkmem_cache_free(rds_tcp_conn_slab, tc);\n}\n\nstatic int rds_tcp_conn_alloc(struct rds_connection *conn, gfp_t gfp)\n{\n\tstruct rds_tcp_connection *tc;\n\tint i, j;\n\tint ret = 0;\n\n\tfor (i = 0; i < RDS_MPATH_WORKERS; i++) {\n\t\ttc = kmem_cache_alloc(rds_tcp_conn_slab, gfp);\n\t\tif (!tc) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t\tmutex_init(&tc->t_conn_path_lock);\n\t\ttc->t_sock = NULL;\n\t\ttc->t_tinc = NULL;\n\t\ttc->t_tinc_hdr_rem = sizeof(struct rds_header);\n\t\ttc->t_tinc_data_rem = 0;\n\n\t\tconn->c_path[i].cp_transport_data = tc;\n\t\ttc->t_cpath = &conn->c_path[i];\n\t\ttc->t_tcp_node_detached = true;\n\n\t\trdsdebug(\"rds_conn_path [%d] tc %p\\n\", i,\n\t\t\t conn->c_path[i].cp_transport_data);\n\t}\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tfor (i = 0; i < RDS_MPATH_WORKERS; i++) {\n\t\ttc = conn->c_path[i].cp_transport_data;\n\t\ttc->t_tcp_node_detached = false;\n\t\tlist_add_tail(&tc->t_tcp_node, &rds_tcp_conn_list);\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\nfail:\n\tif (ret) {\n\t\tfor (j = 0; j < i; j++)\n\t\t\trds_tcp_conn_free(conn->c_path[j].cp_transport_data);\n\t}\n\treturn ret;\n}\n\nstatic bool list_has_conn(struct list_head *list, struct rds_connection *conn)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\n\tlist_for_each_entry_safe(tc, _tc, list, t_tcp_node) {\n\t\tif (tc->t_cpath->cp_conn == conn)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void rds_tcp_set_unloading(void)\n{\n\tatomic_set(&rds_tcp_unloading, 1);\n}\n\nstatic bool rds_tcp_is_unloading(struct rds_connection *conn)\n{\n\treturn atomic_read(&rds_tcp_unloading) != 0;\n}\n\nstatic void rds_tcp_destroy_conns(void)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\n\t/* avoid calling conn_destroy with irqs off */\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn))\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}\n\nstatic void rds_tcp_exit(void);\n\nstatic u8 rds_tcp_get_tos_map(u8 tos)\n{\n\t/* all user tos mapped to default 0 for TCP transport */\n\treturn 0;\n}\n\nstruct rds_transport rds_tcp_transport = {\n\t.laddr_check\t\t= rds_tcp_laddr_check,\n\t.xmit_path_prepare\t= rds_tcp_xmit_path_prepare,\n\t.xmit_path_complete\t= rds_tcp_xmit_path_complete,\n\t.xmit\t\t\t= rds_tcp_xmit,\n\t.recv_path\t\t= rds_tcp_recv_path,\n\t.conn_alloc\t\t= rds_tcp_conn_alloc,\n\t.conn_free\t\t= rds_tcp_conn_free,\n\t.conn_path_connect\t= rds_tcp_conn_path_connect,\n\t.conn_path_shutdown\t= rds_tcp_conn_path_shutdown,\n\t.inc_copy_to_user\t= rds_tcp_inc_copy_to_user,\n\t.inc_free\t\t= rds_tcp_inc_free,\n\t.stats_info_copy\t= rds_tcp_stats_info_copy,\n\t.exit\t\t\t= rds_tcp_exit,\n\t.get_tos_map\t\t= rds_tcp_get_tos_map,\n\t.t_owner\t\t= THIS_MODULE,\n\t.t_name\t\t\t= \"tcp\",\n\t.t_type\t\t\t= RDS_TRANS_TCP,\n\t.t_prefer_loopback\t= 1,\n\t.t_mp_capable\t\t= 1,\n\t.t_unloading\t\t= rds_tcp_is_unloading,\n};\n\nstatic unsigned int rds_tcp_netid;\n\n/* per-network namespace private data for this module */\nstruct rds_tcp_net {\n\tstruct socket *rds_tcp_listen_sock;\n\tstruct work_struct rds_tcp_accept_w;\n\tstruct ctl_table_header *rds_tcp_sysctl;\n\tstruct ctl_table *ctl_table;\n\tint sndbuf_size;\n\tint rcvbuf_size;\n};\n\n/* All module specific customizations to the RDS-TCP socket should be done in\n * rds_tcp_tune() and applied after socket creation.\n */\nbool rds_tcp_tune(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct rds_tcp_net *rtn;\n\n\ttcp_sock_set_nodelay(sock->sk);\n\tlock_sock(sk);\n\t/* TCP timer functions might access net namespace even after\n\t * a process which created this net namespace terminated.\n\t */\n\tif (!sk->sk_net_refcnt) {\n\t\tif (!maybe_get_net(net)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn false;\n\t\t}\n\t\t/* Update ns_tracker to current stack trace and refcounted tracker */\n\t\t__netns_tracker_free(net, &sk->ns_tracker, false);\n\n\t\tsk->sk_net_refcnt = 1;\n\t\tnetns_tracker_alloc(net, &sk->ns_tracker, GFP_KERNEL);\n\t\tsock_inuse_add(net, 1);\n\t}\n\trtn = net_generic(net, rds_tcp_netid);\n\tif (rtn->sndbuf_size > 0) {\n\t\tsk->sk_sndbuf = rtn->sndbuf_size;\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t}\n\tif (rtn->rcvbuf_size > 0) {\n\t\tsk->sk_rcvbuf = rtn->rcvbuf_size;\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t}\n\trelease_sock(sk);\n\treturn true;\n}\n\nstatic void rds_tcp_accept_worker(struct work_struct *work)\n{\n\tstruct rds_tcp_net *rtn = container_of(work,\n\t\t\t\t\t       struct rds_tcp_net,\n\t\t\t\t\t       rds_tcp_accept_w);\n\n\twhile (rds_tcp_accept_one(rtn->rds_tcp_listen_sock) == 0)\n\t\tcond_resched();\n}\n\nvoid rds_tcp_accept_work(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\n\tqueue_work(rds_wq, &rtn->rds_tcp_accept_w);\n}\n\nstatic __net_init int rds_tcp_init_net(struct net *net)\n{\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct ctl_table *tbl;\n\tint err = 0;\n\n\tmemset(rtn, 0, sizeof(*rtn));\n\n\t/* {snd, rcv}buf_size default to 0, which implies we let the\n\t * stack pick the value, and permit auto-tuning of buffer size.\n\t */\n\tif (net == &init_net) {\n\t\ttbl = rds_tcp_sysctl_table;\n\t} else {\n\t\ttbl = kmemdup(rds_tcp_sysctl_table,\n\t\t\t      sizeof(rds_tcp_sysctl_table), GFP_KERNEL);\n\t\tif (!tbl) {\n\t\t\tpr_warn(\"could not set allocate sysctl table\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\trtn->ctl_table = tbl;\n\t}\n\ttbl[RDS_TCP_SNDBUF].data = &rtn->sndbuf_size;\n\ttbl[RDS_TCP_RCVBUF].data = &rtn->rcvbuf_size;\n\trtn->rds_tcp_sysctl = register_net_sysctl_sz(net, \"net/rds/tcp\", tbl,\n\t\t\t\t\t\t     ARRAY_SIZE(rds_tcp_sysctl_table));\n\tif (!rtn->rds_tcp_sysctl) {\n\t\tpr_warn(\"could not register sysctl\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n#if IS_ENABLED(CONFIG_IPV6)\n\trtn->rds_tcp_listen_sock = rds_tcp_listen_init(net, true);\n#else\n\trtn->rds_tcp_listen_sock = rds_tcp_listen_init(net, false);\n#endif\n\tif (!rtn->rds_tcp_listen_sock) {\n\t\tpr_warn(\"could not set up IPv6 listen sock\\n\");\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t/* Try IPv4 as some systems disable IPv6 */\n\t\trtn->rds_tcp_listen_sock = rds_tcp_listen_init(net, false);\n\t\tif (!rtn->rds_tcp_listen_sock) {\n#endif\n\t\t\tunregister_net_sysctl_table(rtn->rds_tcp_sysctl);\n\t\t\trtn->rds_tcp_sysctl = NULL;\n\t\t\terr = -EAFNOSUPPORT;\n\t\t\tgoto fail;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t}\n#endif\n\t}\n\tINIT_WORK(&rtn->rds_tcp_accept_w, rds_tcp_accept_worker);\n\treturn 0;\n\nfail:\n\tif (net != &init_net)\n\t\tkfree(tbl);\n\treturn err;\n}\n\nstatic void rds_tcp_kill_sock(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\tLIST_HEAD(tmp_list);\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\trtn->rds_tcp_listen_sock = NULL;\n\trds_tcp_listen_stop(lsock, &rtn->rds_tcp_accept_w);\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net)\n\t\t\tcontinue;\n\t\tif (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {\n\t\t\tlist_move_tail(&tc->t_tcp_node, &tmp_list);\n\t\t} else {\n\t\t\tlist_del(&tc->t_tcp_node);\n\t\t\ttc->t_tcp_node_detached = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node)\n\t\trds_conn_destroy(tc->t_cpath->cp_conn);\n}\n\nstatic void __net_exit rds_tcp_exit_net(struct net *net)\n{\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\n\trds_tcp_kill_sock(net);\n\n\tif (rtn->rds_tcp_sysctl)\n\t\tunregister_net_sysctl_table(rtn->rds_tcp_sysctl);\n\n\tif (net != &init_net)\n\t\tkfree(rtn->ctl_table);\n}\n\nstatic struct pernet_operations rds_tcp_net_ops = {\n\t.init = rds_tcp_init_net,\n\t.exit = rds_tcp_exit_net,\n\t.id = &rds_tcp_netid,\n\t.size = sizeof(struct rds_tcp_net),\n};\n\nvoid *rds_tcp_listen_sock_def_readable(struct net *net)\n{\n\tstruct rds_tcp_net *rtn = net_generic(net, rds_tcp_netid);\n\tstruct socket *lsock = rtn->rds_tcp_listen_sock;\n\n\tif (!lsock)\n\t\treturn NULL;\n\n\treturn lsock->sk->sk_user_data;\n}\n\n/* when sysctl is used to modify some kernel socket parameters,this\n * function  resets the RDS connections in that netns  so that we can\n * restart with new parameters.  The assumption is that such reset\n * events are few and far-between.\n */\nstatic void rds_tcp_sysctl_reset(struct net *net)\n{\n\tstruct rds_tcp_connection *tc, *_tc;\n\n\tspin_lock_irq(&rds_tcp_conn_lock);\n\tlist_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {\n\t\tstruct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);\n\n\t\tif (net != c_net || !tc->t_sock)\n\t\t\tcontinue;\n\n\t\t/* reconnect with new parameters */\n\t\trds_conn_path_drop(tc->t_cpath, false);\n\t}\n\tspin_unlock_irq(&rds_tcp_conn_lock);\n}\n\nstatic int rds_tcp_skbuf_handler(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *fpos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tint err;\n\n\terr = proc_dointvec_minmax(ctl, write, buffer, lenp, fpos);\n\tif (err < 0) {\n\t\tpr_warn(\"Invalid input. Must be >= %d\\n\",\n\t\t\t*(int *)(ctl->extra1));\n\t\treturn err;\n\t}\n\tif (write)\n\t\trds_tcp_sysctl_reset(net);\n\treturn 0;\n}\n\nstatic void rds_tcp_exit(void)\n{\n\trds_tcp_set_unloading();\n\tsynchronize_rcu();\n\trds_info_deregister_func(RDS_INFO_TCP_SOCKETS, rds_tcp_tc_info);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds_info_deregister_func(RDS6_INFO_TCP_SOCKETS, rds6_tcp_tc_info);\n#endif\n\tunregister_pernet_device(&rds_tcp_net_ops);\n\trds_tcp_destroy_conns();\n\trds_trans_unregister(&rds_tcp_transport);\n\trds_tcp_recv_exit();\n\tkmem_cache_destroy(rds_tcp_conn_slab);\n}\nmodule_exit(rds_tcp_exit);\n\nstatic int __init rds_tcp_init(void)\n{\n\tint ret;\n\n\trds_tcp_conn_slab = KMEM_CACHE(rds_tcp_connection, 0);\n\tif (!rds_tcp_conn_slab) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = rds_tcp_recv_init();\n\tif (ret)\n\t\tgoto out_slab;\n\n\tret = register_pernet_device(&rds_tcp_net_ops);\n\tif (ret)\n\t\tgoto out_recv;\n\n\trds_trans_register(&rds_tcp_transport);\n\n\trds_info_register_func(RDS_INFO_TCP_SOCKETS, rds_tcp_tc_info);\n#if IS_ENABLED(CONFIG_IPV6)\n\trds_info_register_func(RDS6_INFO_TCP_SOCKETS, rds6_tcp_tc_info);\n#endif\n\n\tgoto out;\nout_recv:\n\trds_tcp_recv_exit();\nout_slab:\n\tkmem_cache_destroy(rds_tcp_conn_slab);\nout:\n\treturn ret;\n}\nmodule_init(rds_tcp_init);\n\nMODULE_AUTHOR(\"Oracle Corporation <rds-devel@oss.oracle.com>\");\nMODULE_DESCRIPTION(\"RDS: TCP transport\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21636",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21636.json",
            "patch_attempts": [
                {
                    "upstream_commit": "c10377bbc1972d858eaf0ab366a311b39f8ef1b6",
                    "upstream_commit_date": "2025-01-09 08:53:35 -0800",
                    "upstream_patch": "6259d2484d0ceff42245d1f09cc8cb6ee72d847a",
                    "total_versions_tested": 4,
                    "successful_patches": 4,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "44ee8635922b6eb940faddb961a8347c6857d722",
                            "downstream_commit": "e919197fb8616331f5dc81e4c3cc3d12769cb725",
                            "commit_date": "2025-01-17 13:34:42 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 562 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 44ee8635922b6eb940faddb961a8347c6857d722\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:36 2025 +0100\n\n    sctp: sysctl: plpmtud_probe_interval: avoid using current->nsproxy\n    \n    commit 6259d2484d0ceff42245d1f09cc8cb6ee72d847a upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.probe_interval' is\n    used.\n    \n    Fixes: d1e462a7a5f3 (\"sctp: add probe_interval in sysctl and sock/asoc/transport\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-8-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex ebe81539d30a..916dc2e81e42 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -562,7 +562,8 @@ static int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n static int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n \t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.probe_interval);\n \tstruct ctl_table tbl;\n \tint ret, new_value;\n \n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "284a221f8fa503628432c7bb5108277c688c6ffa",
                            "downstream_commit": "55627918febdf9d71107a1e68d1528dc591c9a15",
                            "commit_date": "2025-01-17 13:36:18 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 573 with fuzz 1 (offset 4 lines).",
                            "downstream_patch_content": "commit 284a221f8fa503628432c7bb5108277c688c6ffa\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:36 2025 +0100\n\n    sctp: sysctl: plpmtud_probe_interval: avoid using current->nsproxy\n    \n    commit 6259d2484d0ceff42245d1f09cc8cb6ee72d847a upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.probe_interval' is\n    used.\n    \n    Fixes: d1e462a7a5f3 (\"sctp: add probe_interval in sysctl and sock/asoc/transport\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-8-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 04494f61fb48..fd73be940f46 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -573,7 +573,8 @@ static int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n static int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n \t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.probe_interval);\n \tstruct ctl_table tbl;\n \tint ret, new_value;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table,\n\t\t\t\t\t\t\t ARRAY_SIZE(sctp_net_table));\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "bcf8c60074e81ed2ac2d35130917175a3949c917",
                            "downstream_commit": "5b77d73f3be5102720fb685b9e6900e3500e1096",
                            "commit_date": "2025-01-17 13:40:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c",
                            "downstream_patch_content": "commit bcf8c60074e81ed2ac2d35130917175a3949c917\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:36 2025 +0100\n\n    sctp: sysctl: plpmtud_probe_interval: avoid using current->nsproxy\n    \n    commit 6259d2484d0ceff42245d1f09cc8cb6ee72d847a upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.probe_interval' is\n    used.\n    \n    Fixes: d1e462a7a5f3 (\"sctp: add probe_interval in sysctl and sock/asoc/transport\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-8-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 18fa4f44e8ec..8e1e97be4df7 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -569,7 +569,8 @@ static int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n \t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.probe_interval);\n \tstruct ctl_table tbl;\n \tint ret, new_value;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n};\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tsize_t table_size = ARRAY_SIZE(sctp_net_table);\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < table_size; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table, table_size);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tconst struct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "1dc5da6c4178f3e4b95c631418f72de9f86c0449",
                            "downstream_commit": "0a0966312ac3eedd7f5f2a766ed4702df39a9a65",
                            "commit_date": "2025-01-23 17:15:51 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 562 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 1dc5da6c4178f3e4b95c631418f72de9f86c0449\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:36 2025 +0100\n\n    sctp: sysctl: plpmtud_probe_interval: avoid using current->nsproxy\n    \n    commit 6259d2484d0ceff42245d1f09cc8cb6ee72d847a upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.probe_interval' is\n    used.\n    \n    Fixes: d1e462a7a5f3 (\"sctp: add probe_interval in sysctl and sock/asoc/transport\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-8-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex ebe81539d30a..916dc2e81e42 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -562,7 +562,8 @@ static int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n static int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n \t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.probe_interval);\n \tstruct ctl_table tbl;\n \tint ret, new_value;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From 6259d2484d0ceff42245d1f09cc8cb6ee72d847a Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:36 +0100\nSubject: [PATCH] sctp: sysctl: plpmtud_probe_interval: avoid using\n current->nsproxy\n\nAs mentioned in a previous commit of this series, using the 'net'\nstructure via 'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe 'net' structure can be obtained from the table->data using\ncontainer_of().\n\nNote that table->data could also be used directly, as this is the only\nmember needed from the 'net' structure, but that would increase the size\nof this fix, to use '*data' everywhere 'net->sctp.probe_interval' is\nused.\n\nFixes: d1e462a7a5f3 (\"sctp: add probe_interval in sysctl and sock/asoc/transport\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-8-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/sctp/sysctl.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 18fa4f44e8ec..8e1e97be4df7 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -569,7 +569,8 @@ static int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n \t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.probe_interval);\n \tstruct ctl_table tbl;\n \tint ret, new_value;\n \n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21637",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21637.json",
            "patch_attempts": [
                {
                    "upstream_commit": "15649fd5415eda664ef35780c2013adeb5d9c695",
                    "upstream_commit_date": "2025-01-09 08:53:35 -0800",
                    "upstream_patch": "c10377bbc1972d858eaf0ab366a311b39f8ef1b6",
                    "total_versions_tested": 4,
                    "successful_patches": 4,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "e919197fb8616331f5dc81e4c3cc3d12769cb725",
                            "downstream_commit": "1b67030d39f2b00f94ac1f0af11ba6657589e4d3",
                            "commit_date": "2025-01-17 13:34:42 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 521 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit e919197fb8616331f5dc81e4c3cc3d12769cb725\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:35 2025 +0100\n\n    sctp: sysctl: udp_port: avoid using current->nsproxy\n    \n    commit c10377bbc1972d858eaf0ab366a311b39f8ef1b6 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: 046c052b475e (\"sctp: enable udp tunneling socks\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-7-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 6dae62136155..ebe81539d30a 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -521,7 +521,7 @@ static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n static int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n \tunsigned int min = *(unsigned int *)ctl->extra1;\n \tunsigned int max = *(unsigned int *)ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "55627918febdf9d71107a1e68d1528dc591c9a15",
                            "downstream_commit": "7ec30c54f339c640aa7e49d7e9f7bbed6bd42bf6",
                            "commit_date": "2025-01-17 13:36:18 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 532 with fuzz 1 (offset 4 lines).",
                            "downstream_patch_content": "commit 55627918febdf9d71107a1e68d1528dc591c9a15\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:35 2025 +0100\n\n    sctp: sysctl: udp_port: avoid using current->nsproxy\n    \n    commit c10377bbc1972d858eaf0ab366a311b39f8ef1b6 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: 046c052b475e (\"sctp: enable udp tunneling socks\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-7-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 3ed15ea96ec3..04494f61fb48 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -532,7 +532,7 @@ static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n static int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n \tunsigned int min = *(unsigned int *)ctl->extra1;\n \tunsigned int max = *(unsigned int *)ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table,\n\t\t\t\t\t\t\t ARRAY_SIZE(sctp_net_table));\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "5b77d73f3be5102720fb685b9e6900e3500e1096",
                            "downstream_commit": "c184bc621e3cef03ac9ba81a50dda2dae6a21d36",
                            "commit_date": "2025-01-17 13:40:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c",
                            "downstream_patch_content": "commit 5b77d73f3be5102720fb685b9e6900e3500e1096\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:35 2025 +0100\n\n    sctp: sysctl: udp_port: avoid using current->nsproxy\n    \n    commit c10377bbc1972d858eaf0ab366a311b39f8ef1b6 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: 046c052b475e (\"sctp: enable udp tunneling socks\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-7-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 9d29611621fe..18fa4f44e8ec 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -528,7 +528,7 @@ static int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n \tunsigned int min = *(unsigned int *)ctl->extra1;\n \tunsigned int max = *(unsigned int *)ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n};\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tsize_t table_size = ARRAY_SIZE(sctp_net_table);\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < table_size; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table, table_size);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tconst struct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "0a0966312ac3eedd7f5f2a766ed4702df39a9a65",
                            "downstream_commit": "bd2a2939423566c654545fa3e96a656662a0af9e",
                            "commit_date": "2025-01-23 17:15:51 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 521 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 0a0966312ac3eedd7f5f2a766ed4702df39a9a65\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:35 2025 +0100\n\n    sctp: sysctl: udp_port: avoid using current->nsproxy\n    \n    commit c10377bbc1972d858eaf0ab366a311b39f8ef1b6 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: 046c052b475e (\"sctp: enable udp tunneling socks\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-7-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 6dae62136155..ebe81539d30a 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -521,7 +521,7 @@ static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n static int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n \tunsigned int min = *(unsigned int *)ctl->extra1;\n \tunsigned int max = *(unsigned int *)ctl->extra2;\n \tstruct ctl_table tbl;\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From c10377bbc1972d858eaf0ab366a311b39f8ef1b6 Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:35 +0100\nSubject: [PATCH] sctp: sysctl: udp_port: avoid using current->nsproxy\n\nAs mentioned in a previous commit of this series, using the 'net'\nstructure via 'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe 'net' structure can be obtained from the table->data using\ncontainer_of().\n\nNote that table->data could also be used directly, but that would\nincrease the size of this fix, while 'sctp.ctl_sock' still needs to be\nretrieved from 'net' structure.\n\nFixes: 046c052b475e (\"sctp: enable udp tunneling socks\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-7-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/sctp/sysctl.c | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 9d29611621fe..18fa4f44e8ec 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -528,7 +528,7 @@ static int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.udp_port);\n \tunsigned int min = *(unsigned int *)ctl->extra1;\n \tunsigned int max = *(unsigned int *)ctl->extra2;\n \tstruct ctl_table tbl;\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21638",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21638.json",
            "patch_attempts": [
                {
                    "upstream_commit": "9fc17b76fc70763780aa78b38fcf4742384044a5",
                    "upstream_commit_date": "2025-01-09 08:53:35 -0800",
                    "upstream_patch": "15649fd5415eda664ef35780c2013adeb5d9c695",
                    "total_versions_tested": 6,
                    "successful_patches": 6,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "1b67030d39f2b00f94ac1f0af11ba6657589e4d3",
                            "downstream_commit": "4059507e34aa5fe0fa9fd5b2b5f0c8b26ab2d482",
                            "commit_date": "2025-01-17 13:34:41 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 492 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 1b67030d39f2b00f94ac1f0af11ba6657589e4d3\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:34 2025 +0100\n\n    sctp: sysctl: auth_enable: avoid using current->nsproxy\n    \n    commit 15649fd5415eda664ef35780c2013adeb5d9c695 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 0dd5da971689..6dae62136155 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -492,7 +492,7 @@ static int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n \t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "7ec30c54f339c640aa7e49d7e9f7bbed6bd42bf6",
                            "downstream_commit": "dc9d0e3cfd16f66fbf0862857c6b391c8613ca9f",
                            "commit_date": "2025-01-17 13:36:18 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 503 with fuzz 1 (offset 4 lines).",
                            "downstream_patch_content": "commit 7ec30c54f339c640aa7e49d7e9f7bbed6bd42bf6\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:34 2025 +0100\n\n    sctp: sysctl: auth_enable: avoid using current->nsproxy\n    \n    commit 15649fd5415eda664ef35780c2013adeb5d9c695 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 2df0b5fa22f8..3ed15ea96ec3 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -503,7 +503,7 @@ static int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n \t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table,\n\t\t\t\t\t\t\t ARRAY_SIZE(sctp_net_table));\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "c184bc621e3cef03ac9ba81a50dda2dae6a21d36",
                            "downstream_commit": "c87f1f6ade56c711f8736901e330685b453e420e",
                            "commit_date": "2025-01-17 13:40:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c",
                            "downstream_patch_content": "commit c184bc621e3cef03ac9ba81a50dda2dae6a21d36\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:34 2025 +0100\n\n    sctp: sysctl: auth_enable: avoid using current->nsproxy\n    \n    commit 15649fd5415eda664ef35780c2013adeb5d9c695 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex a5285815264d..9d29611621fe 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -499,7 +499,7 @@ static int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n \t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n};\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tsize_t table_size = ARRAY_SIZE(sctp_net_table);\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < table_size; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table, table_size);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tconst struct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "bd2a2939423566c654545fa3e96a656662a0af9e",
                            "downstream_commit": "0f78f09466744589e420935e646ae78212a38290",
                            "commit_date": "2025-01-23 17:15:51 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 492 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit bd2a2939423566c654545fa3e96a656662a0af9e\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:34 2025 +0100\n\n    sctp: sysctl: auth_enable: avoid using current->nsproxy\n    \n    commit 15649fd5415eda664ef35780c2013adeb5d9c695 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 0dd5da971689..6dae62136155 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -492,7 +492,7 @@ static int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n \t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "dc583e7e5f8515ca489c0df28e4362a70eade382",
                            "downstream_commit": "03ca51faba2b017bf6c90e139434c4117d0afcdc",
                            "commit_date": "2025-02-01 18:22:20 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 462 with fuzz 1 (offset -37 lines).",
                            "downstream_patch_content": "commit dc583e7e5f8515ca489c0df28e4362a70eade382\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:34 2025 +0100\n\n    sctp: sysctl: auth_enable: avoid using current->nsproxy\n    \n    commit 15649fd5415eda664ef35780c2013adeb5d9c695 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex a2113f4c5415..8be80096fbb6 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -462,7 +462,7 @@ static int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n \t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "cf387cdebfaebae228dfba162f94c567a67610c3",
                            "downstream_commit": "5599b212d2f4466e1832a94e9932684aaa364587",
                            "commit_date": "2025-04-10 14:29:35 +0200",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 442 with fuzz 2 (offset -57 lines).",
                            "downstream_patch_content": "commit cf387cdebfaebae228dfba162f94c567a67610c3\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Tue Mar 11 15:54:27 2025 -0300\n\n    sctp: sysctl: auth_enable: avoid using current->nsproxy\n    \n    commit 15649fd5415eda664ef35780c2013adeb5d9c695 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, but that would\n    increase the size of this fix, while 'sctp.ctl_sock' still needs to be\n    retrieved from 'net' structure.\n    \n    Fixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Magali Lemes <magali.lemes@canonical.com>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 4116b3cd83c2..f6fe63f60acd 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -442,7 +442,7 @@ static int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n \t\t\t     void __user *buffer, size_t *lenp,\n \t\t\t     loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t\t   loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic struct ctl_table sctp_net_table[] = {\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t{\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t\t   loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From 15649fd5415eda664ef35780c2013adeb5d9c695 Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:34 +0100\nSubject: [PATCH] sctp: sysctl: auth_enable: avoid using current->nsproxy\n\nAs mentioned in a previous commit of this series, using the 'net'\nstructure via 'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe 'net' structure can be obtained from the table->data using\ncontainer_of().\n\nNote that table->data could also be used directly, but that would\nincrease the size of this fix, while 'sctp.ctl_sock' still needs to be\nretrieved from 'net' structure.\n\nFixes: b14878ccb7fa (\"net: sctp: cache auth_enable per endpoint\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-6-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/sctp/sysctl.c | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex a5285815264d..9d29611621fe 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -499,7 +499,7 @@ static int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n static int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n \t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net, sctp.auth_enable);\n \tstruct ctl_table tbl;\n \tint new_value, ret;\n \n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net,\n\t\t\t\t       sctp.sctp_hmac_alg);\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21640",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21640.json",
            "patch_attempts": [
                {
                    "upstream_commit": "92cf7a51bdae24a32c592adcdd59a773ae149289",
                    "upstream_commit_date": "2025-01-09 08:53:34 -0800",
                    "upstream_patch": "ea62dd1383913b5999f3d16ae99d411f41b528d4",
                    "total_versions_tested": 6,
                    "successful_patches": 6,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "3cd0659deb9c03535fd61839e91d4d4d3e51ac71",
                            "downstream_commit": "c0dde4a52b8438e5266027e76dc9cbbe806d301b",
                            "commit_date": "2025-01-17 13:34:41 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 380 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 3cd0659deb9c03535fd61839e91d4d4d3e51ac71\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:32 2025 +0100\n\n    sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n    \n    commit ea62dd1383913b5999f3d16ae99d411f41b528d4 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\n    used.\n    \n    Fixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 43ebf090029d..01fe23faf20d 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -380,7 +380,8 @@ static struct ctl_table sctp_net_table[] = {\n static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "ad673e514b2793b8d5902f6ba6ab7e890dea23d5",
                            "downstream_commit": "c0e394fd6b887e84da17e38aaa6c1c104f9c86c2",
                            "commit_date": "2025-01-17 13:36:17 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 391 with fuzz 1 (offset 4 lines).",
                            "downstream_patch_content": "commit ad673e514b2793b8d5902f6ba6ab7e890dea23d5\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:32 2025 +0100\n\n    sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n    \n    commit ea62dd1383913b5999f3d16ae99d411f41b528d4 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\n    used.\n    \n    Fixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex f65d6f92afcb..680ee80055f7 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -391,7 +391,8 @@ static struct ctl_table sctp_net_table[] = {\n static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table,\n\t\t\t\t\t\t\t ARRAY_SIZE(sctp_net_table));\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "f0bb3935470684306e4e04793a20ac4c4b08de0b",
                            "downstream_commit": "4c74fbdc5ab95b13945be01e6065940b68222db7",
                            "commit_date": "2025-01-17 13:40:47 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c",
                            "downstream_patch_content": "commit f0bb3935470684306e4e04793a20ac4c4b08de0b\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:32 2025 +0100\n\n    sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n    \n    commit ea62dd1383913b5999f3d16ae99d411f41b528d4 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\n    used.\n    \n    Fixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex e5a5af343c4c..9848d19630a4 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -387,7 +387,8 @@ static struct ctl_table sctp_net_table[] = {\n static int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\t{\n\t\t.procname\t= \"l3mdev_accept\",\n\t\t.data\t\t= &init_net.sctp.l3mdev_accept,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n#endif\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n};\n\nstatic int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(const struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(const struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(const struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(const struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tsize_t table_size = ARRAY_SIZE(sctp_net_table);\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < table_size; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl_sz(net, \"net/sctp\",\n\t\t\t\t\t\t\t table, table_size);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tconst struct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "86ddf8118123cb58a0fb8724cad6979c4069065b",
                            "downstream_commit": "e52a55ec2d1f7816a466a0b3e7bbdc7e473cb815",
                            "commit_date": "2025-01-23 17:15:51 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 380 with fuzz 1 (offset -7 lines).",
                            "downstream_patch_content": "commit 86ddf8118123cb58a0fb8724cad6979c4069065b\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:32 2025 +0100\n\n    sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n    \n    commit ea62dd1383913b5999f3d16ae99d411f41b528d4 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\n    used.\n    \n    Fixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 43ebf090029d..01fe23faf20d 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -380,7 +380,8 @@ static struct ctl_table sctp_net_table[] = {\n static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "03ca51faba2b017bf6c90e139434c4117d0afcdc",
                            "downstream_commit": "2c1a42fbd98e882df4ef4b4c630563cd6160891e",
                            "commit_date": "2025-02-01 18:22:20 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 350 with fuzz 1 (offset -37 lines).",
                            "downstream_patch_content": "commit 03ca51faba2b017bf6c90e139434c4117d0afcdc\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:32 2025 +0100\n\n    sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n    \n    commit ea62dd1383913b5999f3d16ae99d411f41b528d4 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\n    used.\n    \n    Fixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex e4af050aec1b..a2113f4c5415 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -350,7 +350,8 @@ static struct ctl_table sctp_net_table[] = {\n static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "5599b212d2f4466e1832a94e9932684aaa364587",
                            "downstream_commit": "19573dcddb8819fd68d6cd1f916c1c99c3fa4ff4",
                            "commit_date": "2025-04-10 14:29:35 +0200",
                            "result": "success",
                            "patch_apply_output": "patching file net/sctp/sysctl.c\nHunk #1 succeeded at 441 with fuzz 2 (offset 54 lines).",
                            "downstream_patch_content": "commit 5599b212d2f4466e1832a94e9932684aaa364587\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Tue Mar 11 15:54:26 2025 -0300\n\n    sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n    \n    commit ea62dd1383913b5999f3d16ae99d411f41b528d4 upstream.\n    \n    As mentioned in a previous commit of this series, using the 'net'\n    structure via 'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'net' structure can be obtained from the table->data using\n    container_of().\n    \n    Note that table->data could also be used directly, as this is the only\n    member needed from the 'net' structure, but that would increase the size\n    of this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\n    used.\n    \n    Fixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Magali Lemes <magali.lemes@canonical.com>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex 4ecd3857204d..4116b3cd83c2 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -326,7 +326,8 @@ static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n \t\t\t\tvoid __user *buffer, size_t *lenp,\n \t\t\t\tloff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n",
                            "downstream_file_content": {
                                "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t\t   loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic struct ctl_table sctp_net_table[] = {\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t{\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_min);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid __user *buffer, size_t *lenp,\n\t\t\t\tloff_t *ppos)\n{\n\tstruct net *net = container_of(ctl->data, struct net, sctp.rto_max);\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t\t   loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From ea62dd1383913b5999f3d16ae99d411f41b528d4 Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:32 +0100\nSubject: [PATCH] sctp: sysctl: cookie_hmac_alg: avoid using current->nsproxy\n\nAs mentioned in a previous commit of this series, using the 'net'\nstructure via 'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe 'net' structure can be obtained from the table->data using\ncontainer_of().\n\nNote that table->data could also be used directly, as this is the only\nmember needed from the 'net' structure, but that would increase the size\nof this fix, to use '*data' everywhere 'net->sctp.sctp_hmac_alg' is\nused.\n\nFixes: 3c68198e7511 (\"sctp: Make hmac algorithm selection for cookie generation dynamic\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-4-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/sctp/sysctl.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c\nindex e5a5af343c4c..9848d19630a4 100644\n--- a/net/sctp/sysctl.c\n+++ b/net/sctp/sysctl.c\n@@ -387,7 +387,8 @@ static struct ctl_table sctp_net_table[] = {\n static int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,\n \t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tstruct net *net = current->nsproxy->net_ns;\n+\tstruct net *net = container_of(ctl->data, struct net,\n+\t\t\t\t       sctp.sctp_hmac_alg);\n \tstruct ctl_table tbl;\n \tbool changed = false;\n \tchar *none = \"none\";\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/sctp/sysctl.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* SCTP kernel implementation\n * (C) Copyright IBM Corp. 2002, 2004\n * Copyright (c) 2002 Intel Corp.\n *\n * This file is part of the SCTP kernel implementation\n *\n * Sysctl related interfaces for SCTP.\n *\n * Please send any bug reports or fixes you make to the\n * email address(es):\n *    lksctp developers <linux-sctp@vger.kernel.org>\n *\n * Written or modified by:\n *    Mingqin Liu           <liuming@us.ibm.com>\n *    Jon Grimm             <jgrimm@us.ibm.com>\n *    Ardelle Fan           <ardelle.fan@intel.com>\n *    Ryan Layer            <rmlayer@us.ibm.com>\n *    Sridhar Samudrala     <sri@us.ibm.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <net/sctp/structs.h>\n#include <net/sctp/sctp.h>\n#include <linux/sysctl.h>\n\nstatic int timer_max = 86400000; /* ms in one day */\nstatic int sack_timer_min = 1;\nstatic int sack_timer_max = 500;\nstatic int addr_scope_max = SCTP_SCOPE_POLICY_MAX;\nstatic int rwnd_scale_max = 16;\nstatic int rto_alpha_min = 0;\nstatic int rto_beta_min = 0;\nstatic int rto_alpha_max = 1000;\nstatic int rto_beta_max = 1000;\nstatic int pf_expose_max = SCTP_PF_EXPOSE_MAX;\nstatic int ps_retrans_max = SCTP_PS_RETRANS_MAX;\nstatic int udp_port_max = 65535;\n\nstatic unsigned long max_autoclose_min = 0;\nstatic unsigned long max_autoclose_max =\n\t(MAX_SCHEDULE_TIMEOUT / HZ > UINT_MAX)\n\t? UINT_MAX : MAX_SCHEDULE_TIMEOUT / HZ;\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\tsize_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write, void *buffer,\n\t\t\t\t size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos);\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos);\n\nstatic struct ctl_table sctp_table[] = {\n\t{\n\t\t.procname\t= \"sctp_mem\",\n\t\t.data\t\t= &sysctl_sctp_mem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_mem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax\n\t},\n\t{\n\t\t.procname\t= \"sctp_rmem\",\n\t\t.data\t\t= &sysctl_sctp_rmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_rmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"sctp_wmem\",\n\t\t.data\t\t= &sysctl_sctp_wmem,\n\t\t.maxlen\t\t= sizeof(sysctl_sctp_wmem),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\n\t{ /* sentinel */ }\n};\n\n/* The following index defines are used in sctp_sysctl_net_register().\n * If you add new items to the sctp_net_table, please ensure that\n * the index values of these defines hold the same meaning indicated by\n * their macro names when they appear in sctp_net_table.\n */\n#define SCTP_RTO_MIN_IDX       0\n#define SCTP_RTO_MAX_IDX       1\n#define SCTP_PF_RETRANS_IDX    2\n#define SCTP_PS_RETRANS_IDX    3\n\nstatic struct ctl_table sctp_net_table[] = {\n\t[SCTP_RTO_MIN_IDX] = {\n\t\t.procname\t= \"rto_min\",\n\t\t.data\t\t= &init_net.sctp.rto_min,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_min,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &init_net.sctp.rto_max\n\t},\n\t[SCTP_RTO_MAX_IDX] =  {\n\t\t.procname\t= \"rto_max\",\n\t\t.data\t\t= &init_net.sctp.rto_max,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_rto_max,\n\t\t.extra1         = &init_net.sctp.rto_min,\n\t\t.extra2         = &timer_max\n\t},\n\t[SCTP_PF_RETRANS_IDX] = {\n\t\t.procname\t= \"pf_retrans\",\n\t\t.data\t\t= &init_net.sctp.pf_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &init_net.sctp.ps_retrans,\n\t},\n\t[SCTP_PS_RETRANS_IDX] = {\n\t\t.procname\t= \"ps_retrans\",\n\t\t.data\t\t= &init_net.sctp.ps_retrans,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &init_net.sctp.pf_retrans,\n\t\t.extra2\t\t= &ps_retrans_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_initial\",\n\t\t.data\t\t= &init_net.sctp.rto_initial,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"rto_alpha_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_alpha,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_alpha_min,\n\t\t.extra2\t\t= &rto_alpha_max,\n\t},\n\t{\n\t\t.procname\t= \"rto_beta_exp_divisor\",\n\t\t.data\t\t= &init_net.sctp.rto_beta,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_alpha_beta,\n\t\t.extra1\t\t= &rto_beta_min,\n\t\t.extra2\t\t= &rto_beta_max,\n\t},\n\t{\n\t\t.procname\t= \"max_burst\",\n\t\t.data\t\t= &init_net.sctp.max_burst,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"cookie_preserve_enable\",\n\t\t.data\t\t= &init_net.sctp.cookie_preserve_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"cookie_hmac_alg\",\n\t\t.data\t\t= &init_net.sctp.sctp_hmac_alg,\n\t\t.maxlen\t\t= 8,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_hmac_alg,\n\t},\n\t{\n\t\t.procname\t= \"valid_cookie_life\",\n\t\t.data\t\t= &init_net.sctp.valid_cookie_life,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"sack_timeout\",\n\t\t.data\t\t= &init_net.sctp.sack_timeout,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = &sack_timer_min,\n\t\t.extra2         = &sack_timer_max,\n\t},\n\t{\n\t\t.procname\t= \"hb_interval\",\n\t\t.data\t\t= &init_net.sctp.hb_interval,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1         = SYSCTL_ONE,\n\t\t.extra2         = &timer_max\n\t},\n\t{\n\t\t.procname\t= \"association_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_association,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"path_max_retrans\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_path,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"max_init_retransmits\",\n\t\t.data\t\t= &init_net.sctp.max_retrans_init,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= SYSCTL_INT_MAX,\n\t},\n\t{\n\t\t.procname\t= \"sndbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.sndbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"rcvbuf_policy\",\n\t\t.data\t\t= &init_net.sctp.rcvbuf_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"default_auto_asconf\",\n\t\t.data\t\t= &init_net.sctp.default_auto_asconf,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"addip_noauth_enable\",\n\t\t.data\t\t= &init_net.sctp.addip_noauth,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"prsctp_enable\",\n\t\t.data\t\t= &init_net.sctp.prsctp_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"reconf_enable\",\n\t\t.data\t\t= &init_net.sctp.reconf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"auth_enable\",\n\t\t.data\t\t= &init_net.sctp.auth_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_auth,\n\t},\n\t{\n\t\t.procname\t= \"intl_enable\",\n\t\t.data\t\t= &init_net.sctp.intl_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"ecn_enable\",\n\t\t.data\t\t= &init_net.sctp.ecn_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"plpmtud_probe_interval\",\n\t\t.data\t\t= &init_net.sctp.probe_interval,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_probe_interval,\n\t},\n\t{\n\t\t.procname\t= \"udp_port\",\n\t\t.data\t\t= &init_net.sctp.udp_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_sctp_do_udp_port,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"encap_port\",\n\t\t.data\t\t= &init_net.sctp.encap_port,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &udp_port_max,\n\t},\n\t{\n\t\t.procname\t= \"addr_scope_policy\",\n\t\t.data\t\t= &init_net.sctp.scope_policy,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &addr_scope_max,\n\t},\n\t{\n\t\t.procname\t= \"rwnd_update_shift\",\n\t\t.data\t\t= &init_net.sctp.rwnd_upd_shift,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ONE,\n\t\t.extra2\t\t= &rwnd_scale_max,\n\t},\n\t{\n\t\t.procname\t= \"max_autoclose\",\n\t\t.data\t\t= &init_net.sctp.max_autoclose,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= &proc_doulongvec_minmax,\n\t\t.extra1\t\t= &max_autoclose_min,\n\t\t.extra2\t\t= &max_autoclose_max,\n\t},\n\t{\n\t\t.procname\t= \"pf_enable\",\n\t\t.data\t\t= &init_net.sctp.pf_enable,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"pf_expose\",\n\t\t.data\t\t= &init_net.sctp.pf_expose,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= &pf_expose_max,\n\t},\n\n\t{ /* sentinel */ }\n};\n\nstatic int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tbool changed = false;\n\tchar *none = \"none\";\n\tchar tmp[8] = {0};\n\tint ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\n\tif (write) {\n\t\ttbl.data = tmp;\n\t\ttbl.maxlen = sizeof(tmp);\n\t} else {\n\t\ttbl.data = net->sctp.sctp_hmac_alg ? : none;\n\t\ttbl.maxlen = strlen(tbl.data);\n\t}\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n#ifdef CONFIG_CRYPTO_MD5\n\t\tif (!strncmp(tmp, \"md5\", 3)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"md5\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n#ifdef CONFIG_CRYPTO_SHA1\n\t\tif (!strncmp(tmp, \"sha1\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = \"sha1\";\n\t\t\tchanged = true;\n\t\t}\n#endif\n\t\tif (!strncmp(tmp, \"none\", 4)) {\n\t\t\tnet->sctp.sctp_hmac_alg = NULL;\n\t\t\tchanged = true;\n\t\t}\n\t\tif (!changed)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_min;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_min = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,\n\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *) ctl->extra1;\n\tunsigned int max = *(unsigned int *) ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.rto_max;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.rto_max = new_value;\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_alpha_beta(struct ctl_table *ctl, int write,\n\t\t\t\t   void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (write)\n\t\tpr_warn_once(\"Changing rto_alpha or rto_beta may lead to \"\n\t\t\t     \"suboptimal rtt/srtt estimations!\\n\");\n\n\treturn proc_dointvec_minmax(ctl, write, buffer, lenp, ppos);\n}\n\nstatic int proc_sctp_do_auth(struct ctl_table *ctl, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint new_value, ret;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.auth_enable;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tnet->sctp.auth_enable = new_value;\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->ep->auth_enable = new_value;\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_udp_port(struct ctl_table *ctl, int write,\n\t\t\t\t void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tunsigned int min = *(unsigned int *)ctl->extra1;\n\tunsigned int max = *(unsigned int *)ctl->extra2;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.udp_port;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tstruct sock *sk = net->sctp.ctl_sock;\n\n\t\tif (new_value > max || new_value < min)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.udp_port = new_value;\n\t\tsctp_udp_sock_stop(net);\n\t\tif (new_value) {\n\t\t\tret = sctp_udp_sock_start(net);\n\t\t\tif (ret)\n\t\t\t\tnet->sctp.udp_port = 0;\n\t\t}\n\n\t\t/* Update the value in the control socket */\n\t\tlock_sock(sk);\n\t\tsctp_sk(sk)->udp_port = htons(net->sctp.udp_port);\n\t\trelease_sock(sk);\n\t}\n\n\treturn ret;\n}\n\nstatic int proc_sctp_do_probe_interval(struct ctl_table *ctl, int write,\n\t\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct ctl_table tbl;\n\tint ret, new_value;\n\n\tmemset(&tbl, 0, sizeof(struct ctl_table));\n\ttbl.maxlen = sizeof(unsigned int);\n\n\tif (write)\n\t\ttbl.data = &new_value;\n\telse\n\t\ttbl.data = &net->sctp.probe_interval;\n\n\tret = proc_dointvec(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0) {\n\t\tif (new_value && new_value < SCTP_PROBE_TIMER_MIN)\n\t\t\treturn -EINVAL;\n\n\t\tnet->sctp.probe_interval = new_value;\n\t}\n\n\treturn ret;\n}\n\nint sctp_sysctl_net_register(struct net *net)\n{\n\tstruct ctl_table *table;\n\tint i;\n\n\ttable = kmemdup(sctp_net_table, sizeof(sctp_net_table), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; table[i].data; i++)\n\t\ttable[i].data += (char *)(&net->sctp) - (char *)&init_net.sctp;\n\n\ttable[SCTP_RTO_MIN_IDX].extra2 = &net->sctp.rto_max;\n\ttable[SCTP_RTO_MAX_IDX].extra1 = &net->sctp.rto_min;\n\ttable[SCTP_PF_RETRANS_IDX].extra2 = &net->sctp.ps_retrans;\n\ttable[SCTP_PS_RETRANS_IDX].extra1 = &net->sctp.pf_retrans;\n\n\tnet->sctp.sysctl_header = register_net_sysctl(net, \"net/sctp\", table);\n\tif (net->sctp.sysctl_header == NULL) {\n\t\tkfree(table);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid sctp_sysctl_net_unregister(struct net *net)\n{\n\tstruct ctl_table *table;\n\n\ttable = net->sctp.sysctl_header->ctl_table_arg;\n\tunregister_net_sysctl_table(net->sctp.sysctl_header);\n\tkfree(table);\n}\n\nstatic struct ctl_table_header *sctp_sysctl_header;\n\n/* Sysctl registration.  */\nvoid sctp_sysctl_register(void)\n{\n\tsctp_sysctl_header = register_net_sysctl(&init_net, \"net/sctp\", sctp_table);\n}\n\n/* Sysctl deregistration.  */\nvoid sctp_sysctl_unregister(void)\n{\n\tunregister_net_sysctl_table(sctp_sysctl_header);\n}\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21641",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21641.json",
            "patch_attempts": [
                {
                    "upstream_commit": "d38e26e36206ae3d544d496513212ae931d1da0a",
                    "upstream_commit_date": "2025-01-09 08:53:34 -0800",
                    "upstream_patch": "92cf7a51bdae24a32c592adcdd59a773ae149289",
                    "total_versions_tested": 1,
                    "successful_patches": 1,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "4c74fbdc5ab95b13945be01e6065940b68222db7",
                            "downstream_commit": "6035702381c35a8f16757332381e58b348a9eaf9",
                            "commit_date": "2025-01-17 13:40:46 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/mptcp/ctrl.c",
                            "downstream_patch_content": "commit 4c74fbdc5ab95b13945be01e6065940b68222db7\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:31 2025 +0100\n\n    mptcp: sysctl: blackhole timeout: avoid using current->nsproxy\n    \n    commit 92cf7a51bdae24a32c592adcdd59a773ae149289 upstream.\n    \n    As mentioned in the previous commit, using the 'net' structure via\n    'current' is not recommended for different reasons:\n    \n    - Inconsistency: getting info from the reader's/writer's netns vs only\n      from the opener's netns.\n    \n    - current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n      (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n      syzbot [1] using acct(2).\n    \n    The 'pernet' structure can be obtained from the table->data using\n    container_of().\n    \n    Fixes: 27069e7cb3d1 (\"mptcp: disable active MPTCP in case of blackhole\")\n    Cc: stable@vger.kernel.org\n    Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Reviewed-by: Mat Martineau <martineau@kernel.org>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-3-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/mptcp/ctrl.c b/net/mptcp/ctrl.c\nindex 81c30aa02196..b0dd008e2114 100644\n--- a/net/mptcp/ctrl.c\n+++ b/net/mptcp/ctrl.c\n@@ -160,7 +160,9 @@ static int proc_blackhole_detect_timeout(const struct ctl_table *table,\n \t\t\t\t\t int write, void *buffer, size_t *lenp,\n \t\t\t\t\t loff_t *ppos)\n {\n-\tstruct mptcp_pernet *pernet = mptcp_get_pernet(current->nsproxy->net_ns);\n+\tstruct mptcp_pernet *pernet = container_of(table->data,\n+\t\t\t\t\t\t   struct mptcp_pernet,\n+\t\t\t\t\t\t   blackhole_timeout);\n \tint ret;\n \n \tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n",
                            "downstream_file_content": {}
                        }
                    ],
                    "upstream_patch_content": "From 92cf7a51bdae24a32c592adcdd59a773ae149289 Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:31 +0100\nSubject: [PATCH] mptcp: sysctl: blackhole timeout: avoid using\n current->nsproxy\n\nAs mentioned in the previous commit, using the 'net' structure via\n'current' is not recommended for different reasons:\n\n- Inconsistency: getting info from the reader's/writer's netns vs only\n  from the opener's netns.\n\n- current->nsproxy can be NULL in some cases, resulting in an 'Oops'\n  (null-ptr-deref), e.g. when the current task is exiting, as spotted by\n  syzbot [1] using acct(2).\n\nThe 'pernet' structure can be obtained from the table->data using\ncontainer_of().\n\nFixes: 27069e7cb3d1 (\"mptcp: disable active MPTCP in case of blackhole\")\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nReviewed-by: Mat Martineau <martineau@kernel.org>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-3-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/mptcp/ctrl.c | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a/net/mptcp/ctrl.c b/net/mptcp/ctrl.c\nindex 81c30aa02196..b0dd008e2114 100644\n--- a/net/mptcp/ctrl.c\n+++ b/net/mptcp/ctrl.c\n@@ -160,7 +160,9 @@ static int proc_blackhole_detect_timeout(const struct ctl_table *table,\n \t\t\t\t\t int write, void *buffer, size_t *lenp,\n \t\t\t\t\t loff_t *ppos)\n {\n-\tstruct mptcp_pernet *pernet = mptcp_get_pernet(current->nsproxy->net_ns);\n+\tstruct mptcp_pernet *pernet = container_of(table->data,\n+\t\t\t\t\t\t   struct mptcp_pernet,\n+\t\t\t\t\t\t   blackhole_timeout);\n \tint ret;\n \n \tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/mptcp/ctrl.c": "// SPDX-License-Identifier: GPL-2.0\n/* Multipath TCP\n *\n * Copyright (c) 2019, Tessares SA.\n */\n\n#ifdef CONFIG_SYSCTL\n#include <linux/sysctl.h>\n#endif\n\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n#include \"protocol.h\"\n#include \"mib.h\"\n\n#define MPTCP_SYSCTL_PATH \"net/mptcp\"\n\nstatic int mptcp_pernet_id;\n\n#ifdef CONFIG_SYSCTL\nstatic int mptcp_pm_type_max = __MPTCP_PM_TYPE_MAX;\n#endif\n\nstruct mptcp_pernet {\n#ifdef CONFIG_SYSCTL\n\tstruct ctl_table_header *ctl_table_hdr;\n#endif\n\n\tunsigned int add_addr_timeout;\n\tunsigned int blackhole_timeout;\n\tunsigned int close_timeout;\n\tunsigned int stale_loss_cnt;\n\tatomic_t active_disable_times;\n\tunsigned long active_disable_stamp;\n\tu8 mptcp_enabled;\n\tu8 checksum_enabled;\n\tu8 allow_join_initial_addr_port;\n\tu8 pm_type;\n\tchar scheduler[MPTCP_SCHED_NAME_MAX];\n};\n\nstatic struct mptcp_pernet *mptcp_get_pernet(const struct net *net)\n{\n\treturn net_generic(net, mptcp_pernet_id);\n}\n\nint mptcp_is_enabled(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->mptcp_enabled;\n}\n\nunsigned int mptcp_get_add_addr_timeout(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->add_addr_timeout;\n}\n\nint mptcp_is_checksum_enabled(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->checksum_enabled;\n}\n\nint mptcp_allow_join_id0(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->allow_join_initial_addr_port;\n}\n\nunsigned int mptcp_stale_loss_cnt(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->stale_loss_cnt;\n}\n\nunsigned int mptcp_close_timeout(const struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\treturn TCP_TIMEWAIT_LEN;\n\treturn mptcp_get_pernet(sock_net(sk))->close_timeout;\n}\n\nint mptcp_get_pm_type(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->pm_type;\n}\n\nconst char *mptcp_get_scheduler(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->scheduler;\n}\n\nstatic void mptcp_pernet_set_defaults(struct mptcp_pernet *pernet)\n{\n\tpernet->mptcp_enabled = 1;\n\tpernet->add_addr_timeout = TCP_RTO_MAX;\n\tpernet->blackhole_timeout = 3600;\n\tatomic_set(&pernet->active_disable_times, 0);\n\tpernet->close_timeout = TCP_TIMEWAIT_LEN;\n\tpernet->checksum_enabled = 0;\n\tpernet->allow_join_initial_addr_port = 1;\n\tpernet->stale_loss_cnt = 4;\n\tpernet->pm_type = MPTCP_PM_TYPE_KERNEL;\n\tstrscpy(pernet->scheduler, \"default\", sizeof(pernet->scheduler));\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int mptcp_set_scheduler(char *scheduler, const char *name)\n{\n\tstruct mptcp_sched_ops *sched;\n\tint ret = 0;\n\n\trcu_read_lock();\n\tsched = mptcp_sched_find(name);\n\tif (sched)\n\t\tstrscpy(scheduler, name, MPTCP_SCHED_NAME_MAX);\n\telse\n\t\tret = -ENOENT;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int proc_scheduler(const struct ctl_table *ctl, int write,\n\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tchar (*scheduler)[MPTCP_SCHED_NAME_MAX] = ctl->data;\n\tchar val[MPTCP_SCHED_NAME_MAX];\n\tstruct ctl_table tbl = {\n\t\t.data = val,\n\t\t.maxlen = MPTCP_SCHED_NAME_MAX,\n\t};\n\tint ret;\n\n\tstrscpy(val, *scheduler, MPTCP_SCHED_NAME_MAX);\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0)\n\t\tret = mptcp_set_scheduler(*scheduler, val);\n\n\treturn ret;\n}\n\nstatic int proc_available_schedulers(const struct ctl_table *ctl,\n\t\t\t\t     int write, void *buffer,\n\t\t\t\t     size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table tbl = { .maxlen = MPTCP_SCHED_BUF_MAX, };\n\tint ret;\n\n\ttbl.data = kmalloc(tbl.maxlen, GFP_USER);\n\tif (!tbl.data)\n\t\treturn -ENOMEM;\n\n\tmptcp_get_available_schedulers(tbl.data, MPTCP_SCHED_BUF_MAX);\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tkfree(tbl.data);\n\n\treturn ret;\n}\n\nstatic int proc_blackhole_detect_timeout(const struct ctl_table *table,\n\t\t\t\t\t int write, void *buffer, size_t *lenp,\n\t\t\t\t\t loff_t *ppos)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(current->nsproxy->net_ns);\n\tint ret;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (write && ret == 0)\n\t\tatomic_set(&pernet->active_disable_times, 0);\n\n\treturn ret;\n}\n\nstatic struct ctl_table mptcp_sysctl_table[] = {\n\t{\n\t\t.procname = \"enabled\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t/* users with CAP_NET_ADMIN or root (not and) can change this\n\t\t * value, same as other sysctl or the 'net' tree.\n\t\t */\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"add_addr_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname = \"checksum_enabled\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"allow_join_initial_addr_port\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"stale_loss_cnt\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_douintvec_minmax,\n\t},\n\t{\n\t\t.procname = \"pm_type\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = &mptcp_pm_type_max\n\t},\n\t{\n\t\t.procname = \"scheduler\",\n\t\t.maxlen\t= MPTCP_SCHED_NAME_MAX,\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_scheduler,\n\t},\n\t{\n\t\t.procname = \"available_schedulers\",\n\t\t.maxlen\t= MPTCP_SCHED_BUF_MAX,\n\t\t.mode = 0444,\n\t\t.proc_handler = proc_available_schedulers,\n\t},\n\t{\n\t\t.procname = \"close_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname = \"blackhole_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_blackhole_detect_timeout,\n\t\t.extra1 = SYSCTL_ZERO,\n\t},\n};\n\nstatic int mptcp_pernet_new_table(struct net *net, struct mptcp_pernet *pernet)\n{\n\tstruct ctl_table_header *hdr;\n\tstruct ctl_table *table;\n\n\ttable = mptcp_sysctl_table;\n\tif (!net_eq(net, &init_net)) {\n\t\ttable = kmemdup(table, sizeof(mptcp_sysctl_table), GFP_KERNEL);\n\t\tif (!table)\n\t\t\tgoto err_alloc;\n\t}\n\n\ttable[0].data = &pernet->mptcp_enabled;\n\ttable[1].data = &pernet->add_addr_timeout;\n\ttable[2].data = &pernet->checksum_enabled;\n\ttable[3].data = &pernet->allow_join_initial_addr_port;\n\ttable[4].data = &pernet->stale_loss_cnt;\n\ttable[5].data = &pernet->pm_type;\n\ttable[6].data = &pernet->scheduler;\n\t/* table[7] is for available_schedulers which is read-only info */\n\ttable[8].data = &pernet->close_timeout;\n\ttable[9].data = &pernet->blackhole_timeout;\n\n\thdr = register_net_sysctl_sz(net, MPTCP_SYSCTL_PATH, table,\n\t\t\t\t     ARRAY_SIZE(mptcp_sysctl_table));\n\tif (!hdr)\n\t\tgoto err_reg;\n\n\tpernet->ctl_table_hdr = hdr;\n\n\treturn 0;\n\nerr_reg:\n\tif (!net_eq(net, &init_net))\n\t\tkfree(table);\nerr_alloc:\n\treturn -ENOMEM;\n}\n\nstatic void mptcp_pernet_del_table(struct mptcp_pernet *pernet)\n{\n\tconst struct ctl_table *table = pernet->ctl_table_hdr->ctl_table_arg;\n\n\tunregister_net_sysctl_table(pernet->ctl_table_hdr);\n\n\tkfree(table);\n}\n\n#else\n\nstatic int mptcp_pernet_new_table(struct net *net, struct mptcp_pernet *pernet)\n{\n\treturn 0;\n}\n\nstatic void mptcp_pernet_del_table(struct mptcp_pernet *pernet) {}\n\n#endif /* CONFIG_SYSCTL */\n\n/* The following code block is to deal with middle box issues with MPTCP,\n * similar to what is done with TFO.\n * The proposed solution is to disable active MPTCP globally when SYN+MPC are\n * dropped, while SYN without MPC aren't. In this case, active side MPTCP is\n * disabled globally for 1hr at first. Then if it happens again, it is disabled\n * for 2h, then 4h, 8h, ...\n * The timeout is reset back to 1hr when a successful active MPTCP connection is\n * fully established.\n */\n\n/* Disable active MPTCP and record current jiffies and active_disable_times */\nvoid mptcp_active_disable(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct mptcp_pernet *pernet;\n\n\tpernet = mptcp_get_pernet(net);\n\n\tif (!READ_ONCE(pernet->blackhole_timeout))\n\t\treturn;\n\n\t/* Paired with READ_ONCE() in mptcp_active_should_disable() */\n\tWRITE_ONCE(pernet->active_disable_stamp, jiffies);\n\n\t/* Paired with smp_rmb() in mptcp_active_should_disable().\n\t * We want pernet->active_disable_stamp to be updated first.\n\t */\n\tsmp_mb__before_atomic();\n\tatomic_inc(&pernet->active_disable_times);\n\n\tMPTCP_INC_STATS(net, MPTCP_MIB_BLACKHOLE);\n}\n\n/* Calculate timeout for MPTCP active disable\n * Return true if we are still in the active MPTCP disable period\n * Return false if timeout already expired and we should use active MPTCP\n */\nbool mptcp_active_should_disable(struct sock *ssk)\n{\n\tstruct net *net = sock_net(ssk);\n\tunsigned int blackhole_timeout;\n\tstruct mptcp_pernet *pernet;\n\tunsigned long timeout;\n\tint disable_times;\n\tint multiplier;\n\n\tpernet = mptcp_get_pernet(net);\n\tblackhole_timeout = READ_ONCE(pernet->blackhole_timeout);\n\n\tif (!blackhole_timeout)\n\t\treturn false;\n\n\tdisable_times = atomic_read(&pernet->active_disable_times);\n\tif (!disable_times)\n\t\treturn false;\n\n\t/* Paired with smp_mb__before_atomic() in mptcp_active_disable() */\n\tsmp_rmb();\n\n\t/* Limit timeout to max: 2^6 * initial timeout */\n\tmultiplier = 1 << min(disable_times - 1, 6);\n\n\t/* Paired with the WRITE_ONCE() in mptcp_active_disable(). */\n\ttimeout = READ_ONCE(pernet->active_disable_stamp) +\n\t\t  multiplier * blackhole_timeout * HZ;\n\n\treturn time_before(jiffies, timeout);\n}\n\n/* Enable active MPTCP and reset active_disable_times if needed */\nvoid mptcp_active_enable(struct sock *sk)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(sock_net(sk));\n\n\tif (atomic_read(&pernet->active_disable_times)) {\n\t\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\t\tif (dst && dst->dev && (dst->dev->flags & IFF_LOOPBACK))\n\t\t\tatomic_set(&pernet->active_disable_times, 0);\n\t}\n}\n\n/* Check the number of retransmissions, and fallback to TCP if needed */\nvoid mptcp_active_detect_blackhole(struct sock *ssk, bool expired)\n{\n\tstruct mptcp_subflow_context *subflow;\n\tu32 timeouts;\n\n\tif (!sk_is_mptcp(ssk))\n\t\treturn;\n\n\ttimeouts = inet_csk(ssk)->icsk_retransmits;\n\tsubflow = mptcp_subflow_ctx(ssk);\n\n\tif (subflow->request_mptcp && ssk->sk_state == TCP_SYN_SENT) {\n\t\tif (timeouts == 2 || (timeouts < 2 && expired)) {\n\t\t\tMPTCP_INC_STATS(sock_net(ssk), MPTCP_MIB_MPCAPABLEACTIVEDROP);\n\t\t\tsubflow->mpc_drop = 1;\n\t\t\tmptcp_subflow_early_fallback(mptcp_sk(subflow->conn), subflow);\n\t\t} else {\n\t\t\tsubflow->mpc_drop = 0;\n\t\t}\n\t}\n}\n\nstatic int __net_init mptcp_net_init(struct net *net)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\n\tmptcp_pernet_set_defaults(pernet);\n\n\treturn mptcp_pernet_new_table(net, pernet);\n}\n\n/* Note: the callback will only be called per extra netns */\nstatic void __net_exit mptcp_net_exit(struct net *net)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\n\tmptcp_pernet_del_table(pernet);\n}\n\nstatic struct pernet_operations mptcp_pernet_ops = {\n\t.init = mptcp_net_init,\n\t.exit = mptcp_net_exit,\n\t.id = &mptcp_pernet_id,\n\t.size = sizeof(struct mptcp_pernet),\n};\n\nvoid __init mptcp_init(void)\n{\n\tmptcp_join_cookie_init();\n\tmptcp_proto_init();\n\n\tif (register_pernet_subsys(&mptcp_pernet_ops) < 0)\n\t\tpanic(\"Failed to register MPTCP pernet subsystem.\\n\");\n}\n\n#if IS_ENABLED(CONFIG_MPTCP_IPV6)\nint __init mptcpv6_init(void)\n{\n\tint err;\n\n\terr = mptcp_proto_v6_init();\n\n\treturn err;\n}\n#endif\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21642",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21642.json",
            "patch_attempts": [
                {
                    "upstream_commit": "771ec78dc8b48d562e6015bb535ed3cd37043d78",
                    "upstream_commit_date": "2025-01-09 08:53:34 -0800",
                    "upstream_patch": "d38e26e36206ae3d544d496513212ae931d1da0a",
                    "total_versions_tested": 2,
                    "successful_patches": 2,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "c0e394fd6b887e84da17e38aaa6c1c104f9c86c2",
                            "downstream_commit": "a57ce97c1978c65d102581282e040b1fb4af82ae",
                            "commit_date": "2025-01-17 13:36:17 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/mptcp/ctrl.c\nHunk #1 succeeded at 87 (offset -15 lines).\nHunk #2 succeeded at 106 with fuzz 1 (offset -15 lines).\nHunk #3 succeeded at 114 (offset -15 lines).",
                            "downstream_patch_content": "commit c0e394fd6b887e84da17e38aaa6c1c104f9c86c2\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:30 2025 +0100\n\n    mptcp: sysctl: sched: avoid using current->nsproxy\n    \n    commit d38e26e36206ae3d544d496513212ae931d1da0a upstream.\n    \n    Using the 'net' structure via 'current' is not recommended for different\n    reasons.\n    \n    First, if the goal is to use it to read or write per-netns data, this is\n    inconsistent with how the \"generic\" sysctl entries are doing: directly\n    by only using pointers set to the table entry, e.g. table->data. Linked\n    to that, the per-netns data should always be obtained from the table\n    linked to the netns it had been created for, which may not coincide with\n    the reader's or writer's netns.\n    \n    Another reason is that access to current->nsproxy->netns can oops if\n    attempted when current->nsproxy had been dropped when the current task\n    is exiting. This is what syzbot found, when using acct(2):\n    \n      Oops: general protection fault, probably for non-canonical address 0xdffffc0000000005: 0000 [#1] PREEMPT SMP KASAN PTI\n      KASAN: null-ptr-deref in range [0x0000000000000028-0x000000000000002f]\n      CPU: 1 UID: 0 PID: 5924 Comm: syz-executor Not tainted 6.13.0-rc5-syzkaller-00004-gccb98ccef0e5 #0\n      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 09/13/2024\n      RIP: 0010:proc_scheduler+0xc6/0x3c0 net/mptcp/ctrl.c:125\n      Code: 03 42 80 3c 38 00 0f 85 fe 02 00 00 4d 8b a4 24 08 09 00 00 48 b8 00 00 00 00 00 fc ff df 49 8d 7c 24 28 48 89 fa 48 c1 ea 03 <80> 3c 02 00 0f 85 cc 02 00 00 4d 8b 7c 24 28 48 8d 84 24 c8 00 00\n      RSP: 0018:ffffc900034774e8 EFLAGS: 00010206\n    \n      RAX: dffffc0000000000 RBX: 1ffff9200068ee9e RCX: ffffc90003477620\n      RDX: 0000000000000005 RSI: ffffffff8b08f91e RDI: 0000000000000028\n      RBP: 0000000000000001 R08: ffffc90003477710 R09: 0000000000000040\n      R10: 0000000000000040 R11: 00000000726f7475 R12: 0000000000000000\n      R13: ffffc90003477620 R14: ffffc90003477710 R15: dffffc0000000000\n      FS:  0000000000000000(0000) GS:ffff8880b8700000(0000) knlGS:0000000000000000\n      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n      CR2: 00007fee3cd452d8 CR3: 000000007d116000 CR4: 00000000003526f0\n      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n      Call Trace:\n       <TASK>\n       proc_sys_call_handler+0x403/0x5d0 fs/proc/proc_sysctl.c:601\n       __kernel_write_iter+0x318/0xa80 fs/read_write.c:612\n       __kernel_write+0xf6/0x140 fs/read_write.c:632\n       do_acct_process+0xcb0/0x14a0 kernel/acct.c:539\n       acct_pin_kill+0x2d/0x100 kernel/acct.c:192\n       pin_kill+0x194/0x7c0 fs/fs_pin.c:44\n       mnt_pin_kill+0x61/0x1e0 fs/fs_pin.c:81\n       cleanup_mnt+0x3ac/0x450 fs/namespace.c:1366\n       task_work_run+0x14e/0x250 kernel/task_work.c:239\n       exit_task_work include/linux/task_work.h:43 [inline]\n       do_exit+0xad8/0x2d70 kernel/exit.c:938\n       do_group_exit+0xd3/0x2a0 kernel/exit.c:1087\n       get_signal+0x2576/0x2610 kernel/signal.c:3017\n       arch_do_signal_or_restart+0x90/0x7e0 arch/x86/kernel/signal.c:337\n       exit_to_user_mode_loop kernel/entry/common.c:111 [inline]\n       exit_to_user_mode_prepare include/linux/entry-common.h:329 [inline]\n       __syscall_exit_to_user_mode_work kernel/entry/common.c:207 [inline]\n       syscall_exit_to_user_mode+0x150/0x2a0 kernel/entry/common.c:218\n       do_syscall_64+0xda/0x250 arch/x86/entry/common.c:89\n       entry_SYSCALL_64_after_hwframe+0x77/0x7f\n      RIP: 0033:0x7fee3cb87a6a\n      Code: Unable to access opcode bytes at 0x7fee3cb87a40.\n      RSP: 002b:00007fffcccac688 EFLAGS: 00000202 ORIG_RAX: 0000000000000037\n      RAX: 0000000000000000 RBX: 00007fffcccac710 RCX: 00007fee3cb87a6a\n      RDX: 0000000000000041 RSI: 0000000000000000 RDI: 0000000000000003\n      RBP: 0000000000000003 R08: 00007fffcccac6ac R09: 00007fffcccacac7\n      R10: 00007fffcccac710 R11: 0000000000000202 R12: 00007fee3cd49500\n      R13: 00007fffcccac6ac R14: 0000000000000000 R15: 00007fee3cd4b000\n       </TASK>\n      Modules linked in:\n      ---[ end trace 0000000000000000 ]---\n      RIP: 0010:proc_scheduler+0xc6/0x3c0 net/mptcp/ctrl.c:125\n      Code: 03 42 80 3c 38 00 0f 85 fe 02 00 00 4d 8b a4 24 08 09 00 00 48 b8 00 00 00 00 00 fc ff df 49 8d 7c 24 28 48 89 fa 48 c1 ea 03 <80> 3c 02 00 0f 85 cc 02 00 00 4d 8b 7c 24 28 48 8d 84 24 c8 00 00\n      RSP: 0018:ffffc900034774e8 EFLAGS: 00010206\n      RAX: dffffc0000000000 RBX: 1ffff9200068ee9e RCX: ffffc90003477620\n      RDX: 0000000000000005 RSI: ffffffff8b08f91e RDI: 0000000000000028\n      RBP: 0000000000000001 R08: ffffc90003477710 R09: 0000000000000040\n      R10: 0000000000000040 R11: 00000000726f7475 R12: 0000000000000000\n      R13: ffffc90003477620 R14: ffffc90003477710 R15: dffffc0000000000\n      FS:  0000000000000000(0000) GS:ffff8880b8700000(0000) knlGS:0000000000000000\n      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n      CR2: 00007fee3cd452d8 CR3: 000000007d116000 CR4: 00000000003526f0\n      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n      ----------------\n      Code disassembly (best guess), 1 bytes skipped:\n         0: 42 80 3c 38 00          cmpb   $0x0,(%rax,%r15,1)\n         5: 0f 85 fe 02 00 00       jne    0x309\n         b: 4d 8b a4 24 08 09 00    mov    0x908(%r12),%r12\n        12: 00\n        13: 48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax\n        1a: fc ff df\n        1d: 49 8d 7c 24 28          lea    0x28(%r12),%rdi\n        22: 48 89 fa                mov    %rdi,%rdx\n        25: 48 c1 ea 03             shr    $0x3,%rdx\n      * 29: 80 3c 02 00             cmpb   $0x0,(%rdx,%rax,1) <-- trapping instruction\n        2d: 0f 85 cc 02 00 00       jne    0x2ff\n        33: 4d 8b 7c 24 28          mov    0x28(%r12),%r15\n        38: 48                      rex.W\n        39: 8d                      .byte 0x8d\n        3a: 84 24 c8                test   %ah,(%rax,%rcx,8)\n    \n    Here with 'net.mptcp.scheduler', the 'net' structure is not really\n    needed, because the table->data already has a pointer to the current\n    scheduler, the only thing needed from the per-netns data.\n    Simply use 'data', instead of getting (most of the time) the same thing,\n    but from a longer and indirect way.\n    \n    Fixes: 6963c508fd7a (\"mptcp: only allow set existing scheduler for net.mptcp.scheduler\")\n    Cc: stable@vger.kernel.org\n    Reported-by: syzbot+e364f774c6f57f2c86d1@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Reviewed-by: Mat Martineau <martineau@kernel.org>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-2-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/mptcp/ctrl.c b/net/mptcp/ctrl.c\nindex de75df904a00..34ae0ef4f9f3 100644\n--- a/net/mptcp/ctrl.c\n+++ b/net/mptcp/ctrl.c\n@@ -87,16 +87,15 @@ static void mptcp_pernet_set_defaults(struct mptcp_pernet *pernet)\n }\n \n #ifdef CONFIG_SYSCTL\n-static int mptcp_set_scheduler(const struct net *net, const char *name)\n+static int mptcp_set_scheduler(char *scheduler, const char *name)\n {\n-\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n \tstruct mptcp_sched_ops *sched;\n \tint ret = 0;\n \n \trcu_read_lock();\n \tsched = mptcp_sched_find(name);\n \tif (sched)\n-\t\tstrscpy(pernet->scheduler, name, MPTCP_SCHED_NAME_MAX);\n+\t\tstrscpy(scheduler, name, MPTCP_SCHED_NAME_MAX);\n \telse\n \t\tret = -ENOENT;\n \trcu_read_unlock();\n@@ -107,7 +106,7 @@ static int mptcp_set_scheduler(const struct net *net, const char *name)\n static int proc_scheduler(struct ctl_table *ctl, int write,\n \t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tconst struct net *net = current->nsproxy->net_ns;\n+\tchar (*scheduler)[MPTCP_SCHED_NAME_MAX] = ctl->data;\n \tchar val[MPTCP_SCHED_NAME_MAX];\n \tstruct ctl_table tbl = {\n \t\t.data = val,\n@@ -115,11 +114,11 @@ static int proc_scheduler(struct ctl_table *ctl, int write,\n \t};\n \tint ret;\n \n-\tstrscpy(val, mptcp_get_scheduler(net), MPTCP_SCHED_NAME_MAX);\n+\tstrscpy(val, *scheduler, MPTCP_SCHED_NAME_MAX);\n \n \tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n \tif (write && ret == 0)\n-\t\tret = mptcp_set_scheduler(net, val);\n+\t\tret = mptcp_set_scheduler(*scheduler, val);\n \n \treturn ret;\n }\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "6035702381c35a8f16757332381e58b348a9eaf9",
                            "downstream_commit": "8d242069660aefdc7175b7fef8e20c2d3fde7868",
                            "commit_date": "2025-01-17 13:40:46 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file net/mptcp/ctrl.c",
                            "downstream_patch_content": "commit 6035702381c35a8f16757332381e58b348a9eaf9\nAuthor: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nDate:   Wed Jan 8 16:34:30 2025 +0100\n\n    mptcp: sysctl: sched: avoid using current->nsproxy\n    \n    commit d38e26e36206ae3d544d496513212ae931d1da0a upstream.\n    \n    Using the 'net' structure via 'current' is not recommended for different\n    reasons.\n    \n    First, if the goal is to use it to read or write per-netns data, this is\n    inconsistent with how the \"generic\" sysctl entries are doing: directly\n    by only using pointers set to the table entry, e.g. table->data. Linked\n    to that, the per-netns data should always be obtained from the table\n    linked to the netns it had been created for, which may not coincide with\n    the reader's or writer's netns.\n    \n    Another reason is that access to current->nsproxy->netns can oops if\n    attempted when current->nsproxy had been dropped when the current task\n    is exiting. This is what syzbot found, when using acct(2):\n    \n      Oops: general protection fault, probably for non-canonical address 0xdffffc0000000005: 0000 [#1] PREEMPT SMP KASAN PTI\n      KASAN: null-ptr-deref in range [0x0000000000000028-0x000000000000002f]\n      CPU: 1 UID: 0 PID: 5924 Comm: syz-executor Not tainted 6.13.0-rc5-syzkaller-00004-gccb98ccef0e5 #0\n      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 09/13/2024\n      RIP: 0010:proc_scheduler+0xc6/0x3c0 net/mptcp/ctrl.c:125\n      Code: 03 42 80 3c 38 00 0f 85 fe 02 00 00 4d 8b a4 24 08 09 00 00 48 b8 00 00 00 00 00 fc ff df 49 8d 7c 24 28 48 89 fa 48 c1 ea 03 <80> 3c 02 00 0f 85 cc 02 00 00 4d 8b 7c 24 28 48 8d 84 24 c8 00 00\n      RSP: 0018:ffffc900034774e8 EFLAGS: 00010206\n    \n      RAX: dffffc0000000000 RBX: 1ffff9200068ee9e RCX: ffffc90003477620\n      RDX: 0000000000000005 RSI: ffffffff8b08f91e RDI: 0000000000000028\n      RBP: 0000000000000001 R08: ffffc90003477710 R09: 0000000000000040\n      R10: 0000000000000040 R11: 00000000726f7475 R12: 0000000000000000\n      R13: ffffc90003477620 R14: ffffc90003477710 R15: dffffc0000000000\n      FS:  0000000000000000(0000) GS:ffff8880b8700000(0000) knlGS:0000000000000000\n      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n      CR2: 00007fee3cd452d8 CR3: 000000007d116000 CR4: 00000000003526f0\n      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n      Call Trace:\n       <TASK>\n       proc_sys_call_handler+0x403/0x5d0 fs/proc/proc_sysctl.c:601\n       __kernel_write_iter+0x318/0xa80 fs/read_write.c:612\n       __kernel_write+0xf6/0x140 fs/read_write.c:632\n       do_acct_process+0xcb0/0x14a0 kernel/acct.c:539\n       acct_pin_kill+0x2d/0x100 kernel/acct.c:192\n       pin_kill+0x194/0x7c0 fs/fs_pin.c:44\n       mnt_pin_kill+0x61/0x1e0 fs/fs_pin.c:81\n       cleanup_mnt+0x3ac/0x450 fs/namespace.c:1366\n       task_work_run+0x14e/0x250 kernel/task_work.c:239\n       exit_task_work include/linux/task_work.h:43 [inline]\n       do_exit+0xad8/0x2d70 kernel/exit.c:938\n       do_group_exit+0xd3/0x2a0 kernel/exit.c:1087\n       get_signal+0x2576/0x2610 kernel/signal.c:3017\n       arch_do_signal_or_restart+0x90/0x7e0 arch/x86/kernel/signal.c:337\n       exit_to_user_mode_loop kernel/entry/common.c:111 [inline]\n       exit_to_user_mode_prepare include/linux/entry-common.h:329 [inline]\n       __syscall_exit_to_user_mode_work kernel/entry/common.c:207 [inline]\n       syscall_exit_to_user_mode+0x150/0x2a0 kernel/entry/common.c:218\n       do_syscall_64+0xda/0x250 arch/x86/entry/common.c:89\n       entry_SYSCALL_64_after_hwframe+0x77/0x7f\n      RIP: 0033:0x7fee3cb87a6a\n      Code: Unable to access opcode bytes at 0x7fee3cb87a40.\n      RSP: 002b:00007fffcccac688 EFLAGS: 00000202 ORIG_RAX: 0000000000000037\n      RAX: 0000000000000000 RBX: 00007fffcccac710 RCX: 00007fee3cb87a6a\n      RDX: 0000000000000041 RSI: 0000000000000000 RDI: 0000000000000003\n      RBP: 0000000000000003 R08: 00007fffcccac6ac R09: 00007fffcccacac7\n      R10: 00007fffcccac710 R11: 0000000000000202 R12: 00007fee3cd49500\n      R13: 00007fffcccac6ac R14: 0000000000000000 R15: 00007fee3cd4b000\n       </TASK>\n      Modules linked in:\n      ---[ end trace 0000000000000000 ]---\n      RIP: 0010:proc_scheduler+0xc6/0x3c0 net/mptcp/ctrl.c:125\n      Code: 03 42 80 3c 38 00 0f 85 fe 02 00 00 4d 8b a4 24 08 09 00 00 48 b8 00 00 00 00 00 fc ff df 49 8d 7c 24 28 48 89 fa 48 c1 ea 03 <80> 3c 02 00 0f 85 cc 02 00 00 4d 8b 7c 24 28 48 8d 84 24 c8 00 00\n      RSP: 0018:ffffc900034774e8 EFLAGS: 00010206\n      RAX: dffffc0000000000 RBX: 1ffff9200068ee9e RCX: ffffc90003477620\n      RDX: 0000000000000005 RSI: ffffffff8b08f91e RDI: 0000000000000028\n      RBP: 0000000000000001 R08: ffffc90003477710 R09: 0000000000000040\n      R10: 0000000000000040 R11: 00000000726f7475 R12: 0000000000000000\n      R13: ffffc90003477620 R14: ffffc90003477710 R15: dffffc0000000000\n      FS:  0000000000000000(0000) GS:ffff8880b8700000(0000) knlGS:0000000000000000\n      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n      CR2: 00007fee3cd452d8 CR3: 000000007d116000 CR4: 00000000003526f0\n      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n      ----------------\n      Code disassembly (best guess), 1 bytes skipped:\n         0: 42 80 3c 38 00          cmpb   $0x0,(%rax,%r15,1)\n         5: 0f 85 fe 02 00 00       jne    0x309\n         b: 4d 8b a4 24 08 09 00    mov    0x908(%r12),%r12\n        12: 00\n        13: 48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax\n        1a: fc ff df\n        1d: 49 8d 7c 24 28          lea    0x28(%r12),%rdi\n        22: 48 89 fa                mov    %rdi,%rdx\n        25: 48 c1 ea 03             shr    $0x3,%rdx\n      * 29: 80 3c 02 00             cmpb   $0x0,(%rdx,%rax,1) <-- trapping instruction\n        2d: 0f 85 cc 02 00 00       jne    0x2ff\n        33: 4d 8b 7c 24 28          mov    0x28(%r12),%r15\n        38: 48                      rex.W\n        39: 8d                      .byte 0x8d\n        3a: 84 24 c8                test   %ah,(%rax,%rcx,8)\n    \n    Here with 'net.mptcp.scheduler', the 'net' structure is not really\n    needed, because the table->data already has a pointer to the current\n    scheduler, the only thing needed from the per-netns data.\n    Simply use 'data', instead of getting (most of the time) the same thing,\n    but from a longer and indirect way.\n    \n    Fixes: 6963c508fd7a (\"mptcp: only allow set existing scheduler for net.mptcp.scheduler\")\n    Cc: stable@vger.kernel.org\n    Reported-by: syzbot+e364f774c6f57f2c86d1@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com\n    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>\n    Reviewed-by: Mat Martineau <martineau@kernel.org>\n    Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\n    Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-2-5df34b2083e8@kernel.org\n    Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\n\ndiff --git a/net/mptcp/ctrl.c b/net/mptcp/ctrl.c\nindex d9b57fab2a13..81c30aa02196 100644\n--- a/net/mptcp/ctrl.c\n+++ b/net/mptcp/ctrl.c\n@@ -102,16 +102,15 @@ static void mptcp_pernet_set_defaults(struct mptcp_pernet *pernet)\n }\n \n #ifdef CONFIG_SYSCTL\n-static int mptcp_set_scheduler(const struct net *net, const char *name)\n+static int mptcp_set_scheduler(char *scheduler, const char *name)\n {\n-\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n \tstruct mptcp_sched_ops *sched;\n \tint ret = 0;\n \n \trcu_read_lock();\n \tsched = mptcp_sched_find(name);\n \tif (sched)\n-\t\tstrscpy(pernet->scheduler, name, MPTCP_SCHED_NAME_MAX);\n+\t\tstrscpy(scheduler, name, MPTCP_SCHED_NAME_MAX);\n \telse\n \t\tret = -ENOENT;\n \trcu_read_unlock();\n@@ -122,7 +121,7 @@ static int mptcp_set_scheduler(const struct net *net, const char *name)\n static int proc_scheduler(const struct ctl_table *ctl, int write,\n \t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tconst struct net *net = current->nsproxy->net_ns;\n+\tchar (*scheduler)[MPTCP_SCHED_NAME_MAX] = ctl->data;\n \tchar val[MPTCP_SCHED_NAME_MAX];\n \tstruct ctl_table tbl = {\n \t\t.data = val,\n@@ -130,11 +129,11 @@ static int proc_scheduler(const struct ctl_table *ctl, int write,\n \t};\n \tint ret;\n \n-\tstrscpy(val, mptcp_get_scheduler(net), MPTCP_SCHED_NAME_MAX);\n+\tstrscpy(val, *scheduler, MPTCP_SCHED_NAME_MAX);\n \n \tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n \tif (write && ret == 0)\n-\t\tret = mptcp_set_scheduler(net, val);\n+\t\tret = mptcp_set_scheduler(*scheduler, val);\n \n \treturn ret;\n }\n",
                            "downstream_file_content": {
                                "net/mptcp/ctrl.c": "// SPDX-License-Identifier: GPL-2.0\n/* Multipath TCP\n *\n * Copyright (c) 2019, Tessares SA.\n */\n\n#ifdef CONFIG_SYSCTL\n#include <linux/sysctl.h>\n#endif\n\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n#include \"protocol.h\"\n#include \"mib.h\"\n\n#define MPTCP_SYSCTL_PATH \"net/mptcp\"\n\nstatic int mptcp_pernet_id;\n\n#ifdef CONFIG_SYSCTL\nstatic int mptcp_pm_type_max = __MPTCP_PM_TYPE_MAX;\n#endif\n\nstruct mptcp_pernet {\n#ifdef CONFIG_SYSCTL\n\tstruct ctl_table_header *ctl_table_hdr;\n#endif\n\n\tunsigned int add_addr_timeout;\n\tunsigned int blackhole_timeout;\n\tunsigned int close_timeout;\n\tunsigned int stale_loss_cnt;\n\tatomic_t active_disable_times;\n\tunsigned long active_disable_stamp;\n\tu8 mptcp_enabled;\n\tu8 checksum_enabled;\n\tu8 allow_join_initial_addr_port;\n\tu8 pm_type;\n\tchar scheduler[MPTCP_SCHED_NAME_MAX];\n};\n\nstatic struct mptcp_pernet *mptcp_get_pernet(const struct net *net)\n{\n\treturn net_generic(net, mptcp_pernet_id);\n}\n\nint mptcp_is_enabled(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->mptcp_enabled;\n}\n\nunsigned int mptcp_get_add_addr_timeout(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->add_addr_timeout;\n}\n\nint mptcp_is_checksum_enabled(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->checksum_enabled;\n}\n\nint mptcp_allow_join_id0(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->allow_join_initial_addr_port;\n}\n\nunsigned int mptcp_stale_loss_cnt(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->stale_loss_cnt;\n}\n\nunsigned int mptcp_close_timeout(const struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\treturn TCP_TIMEWAIT_LEN;\n\treturn mptcp_get_pernet(sock_net(sk))->close_timeout;\n}\n\nint mptcp_get_pm_type(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->pm_type;\n}\n\nconst char *mptcp_get_scheduler(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->scheduler;\n}\n\nstatic void mptcp_pernet_set_defaults(struct mptcp_pernet *pernet)\n{\n\tpernet->mptcp_enabled = 1;\n\tpernet->add_addr_timeout = TCP_RTO_MAX;\n\tpernet->blackhole_timeout = 3600;\n\tatomic_set(&pernet->active_disable_times, 0);\n\tpernet->close_timeout = TCP_TIMEWAIT_LEN;\n\tpernet->checksum_enabled = 0;\n\tpernet->allow_join_initial_addr_port = 1;\n\tpernet->stale_loss_cnt = 4;\n\tpernet->pm_type = MPTCP_PM_TYPE_KERNEL;\n\tstrscpy(pernet->scheduler, \"default\", sizeof(pernet->scheduler));\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int mptcp_set_scheduler(const struct net *net, const char *name)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\tstruct mptcp_sched_ops *sched;\n\tint ret = 0;\n\n\trcu_read_lock();\n\tsched = mptcp_sched_find(name);\n\tif (sched)\n\t\tstrscpy(pernet->scheduler, name, MPTCP_SCHED_NAME_MAX);\n\telse\n\t\tret = -ENOENT;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int proc_scheduler(const struct ctl_table *ctl, int write,\n\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tconst struct net *net = current->nsproxy->net_ns;\n\tchar val[MPTCP_SCHED_NAME_MAX];\n\tstruct ctl_table tbl = {\n\t\t.data = val,\n\t\t.maxlen = MPTCP_SCHED_NAME_MAX,\n\t};\n\tint ret;\n\n\tstrscpy(val, mptcp_get_scheduler(net), MPTCP_SCHED_NAME_MAX);\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0)\n\t\tret = mptcp_set_scheduler(net, val);\n\n\treturn ret;\n}\n\nstatic int proc_available_schedulers(const struct ctl_table *ctl,\n\t\t\t\t     int write, void *buffer,\n\t\t\t\t     size_t *lenp, loff_t *ppos)\n{\n\tstruct ctl_table tbl = { .maxlen = MPTCP_SCHED_BUF_MAX, };\n\tint ret;\n\n\ttbl.data = kmalloc(tbl.maxlen, GFP_USER);\n\tif (!tbl.data)\n\t\treturn -ENOMEM;\n\n\tmptcp_get_available_schedulers(tbl.data, MPTCP_SCHED_BUF_MAX);\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tkfree(tbl.data);\n\n\treturn ret;\n}\n\nstatic int proc_blackhole_detect_timeout(const struct ctl_table *table,\n\t\t\t\t\t int write, void *buffer, size_t *lenp,\n\t\t\t\t\t loff_t *ppos)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(current->nsproxy->net_ns);\n\tint ret;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (write && ret == 0)\n\t\tatomic_set(&pernet->active_disable_times, 0);\n\n\treturn ret;\n}\n\nstatic struct ctl_table mptcp_sysctl_table[] = {\n\t{\n\t\t.procname = \"enabled\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t/* users with CAP_NET_ADMIN or root (not and) can change this\n\t\t * value, same as other sysctl or the 'net' tree.\n\t\t */\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"add_addr_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname = \"checksum_enabled\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"allow_join_initial_addr_port\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"stale_loss_cnt\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_douintvec_minmax,\n\t},\n\t{\n\t\t.procname = \"pm_type\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = &mptcp_pm_type_max\n\t},\n\t{\n\t\t.procname = \"scheduler\",\n\t\t.maxlen\t= MPTCP_SCHED_NAME_MAX,\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_scheduler,\n\t},\n\t{\n\t\t.procname = \"available_schedulers\",\n\t\t.maxlen\t= MPTCP_SCHED_BUF_MAX,\n\t\t.mode = 0444,\n\t\t.proc_handler = proc_available_schedulers,\n\t},\n\t{\n\t\t.procname = \"close_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname = \"blackhole_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_blackhole_detect_timeout,\n\t\t.extra1 = SYSCTL_ZERO,\n\t},\n};\n\nstatic int mptcp_pernet_new_table(struct net *net, struct mptcp_pernet *pernet)\n{\n\tstruct ctl_table_header *hdr;\n\tstruct ctl_table *table;\n\n\ttable = mptcp_sysctl_table;\n\tif (!net_eq(net, &init_net)) {\n\t\ttable = kmemdup(table, sizeof(mptcp_sysctl_table), GFP_KERNEL);\n\t\tif (!table)\n\t\t\tgoto err_alloc;\n\t}\n\n\ttable[0].data = &pernet->mptcp_enabled;\n\ttable[1].data = &pernet->add_addr_timeout;\n\ttable[2].data = &pernet->checksum_enabled;\n\ttable[3].data = &pernet->allow_join_initial_addr_port;\n\ttable[4].data = &pernet->stale_loss_cnt;\n\ttable[5].data = &pernet->pm_type;\n\ttable[6].data = &pernet->scheduler;\n\t/* table[7] is for available_schedulers which is read-only info */\n\ttable[8].data = &pernet->close_timeout;\n\ttable[9].data = &pernet->blackhole_timeout;\n\n\thdr = register_net_sysctl_sz(net, MPTCP_SYSCTL_PATH, table,\n\t\t\t\t     ARRAY_SIZE(mptcp_sysctl_table));\n\tif (!hdr)\n\t\tgoto err_reg;\n\n\tpernet->ctl_table_hdr = hdr;\n\n\treturn 0;\n\nerr_reg:\n\tif (!net_eq(net, &init_net))\n\t\tkfree(table);\nerr_alloc:\n\treturn -ENOMEM;\n}\n\nstatic void mptcp_pernet_del_table(struct mptcp_pernet *pernet)\n{\n\tconst struct ctl_table *table = pernet->ctl_table_hdr->ctl_table_arg;\n\n\tunregister_net_sysctl_table(pernet->ctl_table_hdr);\n\n\tkfree(table);\n}\n\n#else\n\nstatic int mptcp_pernet_new_table(struct net *net, struct mptcp_pernet *pernet)\n{\n\treturn 0;\n}\n\nstatic void mptcp_pernet_del_table(struct mptcp_pernet *pernet) {}\n\n#endif /* CONFIG_SYSCTL */\n\n/* The following code block is to deal with middle box issues with MPTCP,\n * similar to what is done with TFO.\n * The proposed solution is to disable active MPTCP globally when SYN+MPC are\n * dropped, while SYN without MPC aren't. In this case, active side MPTCP is\n * disabled globally for 1hr at first. Then if it happens again, it is disabled\n * for 2h, then 4h, 8h, ...\n * The timeout is reset back to 1hr when a successful active MPTCP connection is\n * fully established.\n */\n\n/* Disable active MPTCP and record current jiffies and active_disable_times */\nvoid mptcp_active_disable(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct mptcp_pernet *pernet;\n\n\tpernet = mptcp_get_pernet(net);\n\n\tif (!READ_ONCE(pernet->blackhole_timeout))\n\t\treturn;\n\n\t/* Paired with READ_ONCE() in mptcp_active_should_disable() */\n\tWRITE_ONCE(pernet->active_disable_stamp, jiffies);\n\n\t/* Paired with smp_rmb() in mptcp_active_should_disable().\n\t * We want pernet->active_disable_stamp to be updated first.\n\t */\n\tsmp_mb__before_atomic();\n\tatomic_inc(&pernet->active_disable_times);\n\n\tMPTCP_INC_STATS(net, MPTCP_MIB_BLACKHOLE);\n}\n\n/* Calculate timeout for MPTCP active disable\n * Return true if we are still in the active MPTCP disable period\n * Return false if timeout already expired and we should use active MPTCP\n */\nbool mptcp_active_should_disable(struct sock *ssk)\n{\n\tstruct net *net = sock_net(ssk);\n\tunsigned int blackhole_timeout;\n\tstruct mptcp_pernet *pernet;\n\tunsigned long timeout;\n\tint disable_times;\n\tint multiplier;\n\n\tpernet = mptcp_get_pernet(net);\n\tblackhole_timeout = READ_ONCE(pernet->blackhole_timeout);\n\n\tif (!blackhole_timeout)\n\t\treturn false;\n\n\tdisable_times = atomic_read(&pernet->active_disable_times);\n\tif (!disable_times)\n\t\treturn false;\n\n\t/* Paired with smp_mb__before_atomic() in mptcp_active_disable() */\n\tsmp_rmb();\n\n\t/* Limit timeout to max: 2^6 * initial timeout */\n\tmultiplier = 1 << min(disable_times - 1, 6);\n\n\t/* Paired with the WRITE_ONCE() in mptcp_active_disable(). */\n\ttimeout = READ_ONCE(pernet->active_disable_stamp) +\n\t\t  multiplier * blackhole_timeout * HZ;\n\n\treturn time_before(jiffies, timeout);\n}\n\n/* Enable active MPTCP and reset active_disable_times if needed */\nvoid mptcp_active_enable(struct sock *sk)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(sock_net(sk));\n\n\tif (atomic_read(&pernet->active_disable_times)) {\n\t\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\t\tif (dst && dst->dev && (dst->dev->flags & IFF_LOOPBACK))\n\t\t\tatomic_set(&pernet->active_disable_times, 0);\n\t}\n}\n\n/* Check the number of retransmissions, and fallback to TCP if needed */\nvoid mptcp_active_detect_blackhole(struct sock *ssk, bool expired)\n{\n\tstruct mptcp_subflow_context *subflow;\n\tu32 timeouts;\n\n\tif (!sk_is_mptcp(ssk))\n\t\treturn;\n\n\ttimeouts = inet_csk(ssk)->icsk_retransmits;\n\tsubflow = mptcp_subflow_ctx(ssk);\n\n\tif (subflow->request_mptcp && ssk->sk_state == TCP_SYN_SENT) {\n\t\tif (timeouts == 2 || (timeouts < 2 && expired)) {\n\t\t\tMPTCP_INC_STATS(sock_net(ssk), MPTCP_MIB_MPCAPABLEACTIVEDROP);\n\t\t\tsubflow->mpc_drop = 1;\n\t\t\tmptcp_subflow_early_fallback(mptcp_sk(subflow->conn), subflow);\n\t\t} else {\n\t\t\tsubflow->mpc_drop = 0;\n\t\t}\n\t}\n}\n\nstatic int __net_init mptcp_net_init(struct net *net)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\n\tmptcp_pernet_set_defaults(pernet);\n\n\treturn mptcp_pernet_new_table(net, pernet);\n}\n\n/* Note: the callback will only be called per extra netns */\nstatic void __net_exit mptcp_net_exit(struct net *net)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\n\tmptcp_pernet_del_table(pernet);\n}\n\nstatic struct pernet_operations mptcp_pernet_ops = {\n\t.init = mptcp_net_init,\n\t.exit = mptcp_net_exit,\n\t.id = &mptcp_pernet_id,\n\t.size = sizeof(struct mptcp_pernet),\n};\n\nvoid __init mptcp_init(void)\n{\n\tmptcp_join_cookie_init();\n\tmptcp_proto_init();\n\n\tif (register_pernet_subsys(&mptcp_pernet_ops) < 0)\n\t\tpanic(\"Failed to register MPTCP pernet subsystem.\\n\");\n}\n\n#if IS_ENABLED(CONFIG_MPTCP_IPV6)\nint __init mptcpv6_init(void)\n{\n\tint err;\n\n\terr = mptcp_proto_v6_init();\n\n\treturn err;\n}\n#endif\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From d38e26e36206ae3d544d496513212ae931d1da0a Mon Sep 17 00:00:00 2001\nFrom: \"Matthieu Baerts (NGI0)\" <matttbe@kernel.org>\nDate: Wed, 8 Jan 2025 16:34:30 +0100\nSubject: [PATCH] mptcp: sysctl: sched: avoid using current->nsproxy\n\nUsing the 'net' structure via 'current' is not recommended for different\nreasons.\n\nFirst, if the goal is to use it to read or write per-netns data, this is\ninconsistent with how the \"generic\" sysctl entries are doing: directly\nby only using pointers set to the table entry, e.g. table->data. Linked\nto that, the per-netns data should always be obtained from the table\nlinked to the netns it had been created for, which may not coincide with\nthe reader's or writer's netns.\n\nAnother reason is that access to current->nsproxy->netns can oops if\nattempted when current->nsproxy had been dropped when the current task\nis exiting. This is what syzbot found, when using acct(2):\n\n  Oops: general protection fault, probably for non-canonical address 0xdffffc0000000005: 0000 [#1] PREEMPT SMP KASAN PTI\n  KASAN: null-ptr-deref in range [0x0000000000000028-0x000000000000002f]\n  CPU: 1 UID: 0 PID: 5924 Comm: syz-executor Not tainted 6.13.0-rc5-syzkaller-00004-gccb98ccef0e5 #0\n  Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 09/13/2024\n  RIP: 0010:proc_scheduler+0xc6/0x3c0 net/mptcp/ctrl.c:125\n  Code: 03 42 80 3c 38 00 0f 85 fe 02 00 00 4d 8b a4 24 08 09 00 00 48 b8 00 00 00 00 00 fc ff df 49 8d 7c 24 28 48 89 fa 48 c1 ea 03 <80> 3c 02 00 0f 85 cc 02 00 00 4d 8b 7c 24 28 48 8d 84 24 c8 00 00\n  RSP: 0018:ffffc900034774e8 EFLAGS: 00010206\n\n  RAX: dffffc0000000000 RBX: 1ffff9200068ee9e RCX: ffffc90003477620\n  RDX: 0000000000000005 RSI: ffffffff8b08f91e RDI: 0000000000000028\n  RBP: 0000000000000001 R08: ffffc90003477710 R09: 0000000000000040\n  R10: 0000000000000040 R11: 00000000726f7475 R12: 0000000000000000\n  R13: ffffc90003477620 R14: ffffc90003477710 R15: dffffc0000000000\n  FS:  0000000000000000(0000) GS:ffff8880b8700000(0000) knlGS:0000000000000000\n  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  CR2: 00007fee3cd452d8 CR3: 000000007d116000 CR4: 00000000003526f0\n  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n  Call Trace:\n   <TASK>\n   proc_sys_call_handler+0x403/0x5d0 fs/proc/proc_sysctl.c:601\n   __kernel_write_iter+0x318/0xa80 fs/read_write.c:612\n   __kernel_write+0xf6/0x140 fs/read_write.c:632\n   do_acct_process+0xcb0/0x14a0 kernel/acct.c:539\n   acct_pin_kill+0x2d/0x100 kernel/acct.c:192\n   pin_kill+0x194/0x7c0 fs/fs_pin.c:44\n   mnt_pin_kill+0x61/0x1e0 fs/fs_pin.c:81\n   cleanup_mnt+0x3ac/0x450 fs/namespace.c:1366\n   task_work_run+0x14e/0x250 kernel/task_work.c:239\n   exit_task_work include/linux/task_work.h:43 [inline]\n   do_exit+0xad8/0x2d70 kernel/exit.c:938\n   do_group_exit+0xd3/0x2a0 kernel/exit.c:1087\n   get_signal+0x2576/0x2610 kernel/signal.c:3017\n   arch_do_signal_or_restart+0x90/0x7e0 arch/x86/kernel/signal.c:337\n   exit_to_user_mode_loop kernel/entry/common.c:111 [inline]\n   exit_to_user_mode_prepare include/linux/entry-common.h:329 [inline]\n   __syscall_exit_to_user_mode_work kernel/entry/common.c:207 [inline]\n   syscall_exit_to_user_mode+0x150/0x2a0 kernel/entry/common.c:218\n   do_syscall_64+0xda/0x250 arch/x86/entry/common.c:89\n   entry_SYSCALL_64_after_hwframe+0x77/0x7f\n  RIP: 0033:0x7fee3cb87a6a\n  Code: Unable to access opcode bytes at 0x7fee3cb87a40.\n  RSP: 002b:00007fffcccac688 EFLAGS: 00000202 ORIG_RAX: 0000000000000037\n  RAX: 0000000000000000 RBX: 00007fffcccac710 RCX: 00007fee3cb87a6a\n  RDX: 0000000000000041 RSI: 0000000000000000 RDI: 0000000000000003\n  RBP: 0000000000000003 R08: 00007fffcccac6ac R09: 00007fffcccacac7\n  R10: 00007fffcccac710 R11: 0000000000000202 R12: 00007fee3cd49500\n  R13: 00007fffcccac6ac R14: 0000000000000000 R15: 00007fee3cd4b000\n   </TASK>\n  Modules linked in:\n  ---[ end trace 0000000000000000 ]---\n  RIP: 0010:proc_scheduler+0xc6/0x3c0 net/mptcp/ctrl.c:125\n  Code: 03 42 80 3c 38 00 0f 85 fe 02 00 00 4d 8b a4 24 08 09 00 00 48 b8 00 00 00 00 00 fc ff df 49 8d 7c 24 28 48 89 fa 48 c1 ea 03 <80> 3c 02 00 0f 85 cc 02 00 00 4d 8b 7c 24 28 48 8d 84 24 c8 00 00\n  RSP: 0018:ffffc900034774e8 EFLAGS: 00010206\n  RAX: dffffc0000000000 RBX: 1ffff9200068ee9e RCX: ffffc90003477620\n  RDX: 0000000000000005 RSI: ffffffff8b08f91e RDI: 0000000000000028\n  RBP: 0000000000000001 R08: ffffc90003477710 R09: 0000000000000040\n  R10: 0000000000000040 R11: 00000000726f7475 R12: 0000000000000000\n  R13: ffffc90003477620 R14: ffffc90003477710 R15: dffffc0000000000\n  FS:  0000000000000000(0000) GS:ffff8880b8700000(0000) knlGS:0000000000000000\n  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  CR2: 00007fee3cd452d8 CR3: 000000007d116000 CR4: 00000000003526f0\n  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n  ----------------\n  Code disassembly (best guess), 1 bytes skipped:\n     0:\t42 80 3c 38 00       \tcmpb   $0x0,(%rax,%r15,1)\n     5:\t0f 85 fe 02 00 00    \tjne    0x309\n     b:\t4d 8b a4 24 08 09 00 \tmov    0x908(%r12),%r12\n    12:\t00\n    13:\t48 b8 00 00 00 00 00 \tmovabs $0xdffffc0000000000,%rax\n    1a:\tfc ff df\n    1d:\t49 8d 7c 24 28       \tlea    0x28(%r12),%rdi\n    22:\t48 89 fa             \tmov    %rdi,%rdx\n    25:\t48 c1 ea 03          \tshr    $0x3,%rdx\n  * 29:\t80 3c 02 00          \tcmpb   $0x0,(%rdx,%rax,1) <-- trapping instruction\n    2d:\t0f 85 cc 02 00 00    \tjne    0x2ff\n    33:\t4d 8b 7c 24 28       \tmov    0x28(%r12),%r15\n    38:\t48                   \trex.W\n    39:\t8d                   \t.byte 0x8d\n    3a:\t84 24 c8             \ttest   %ah,(%rax,%rcx,8)\n\nHere with 'net.mptcp.scheduler', the 'net' structure is not really\nneeded, because the table->data already has a pointer to the current\nscheduler, the only thing needed from the per-netns data.\nSimply use 'data', instead of getting (most of the time) the same thing,\nbut from a longer and indirect way.\n\nFixes: 6963c508fd7a (\"mptcp: only allow set existing scheduler for net.mptcp.scheduler\")\nCc: stable@vger.kernel.org\nReported-by: syzbot+e364f774c6f57f2c86d1@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com\nSuggested-by: Al Viro <viro@zeniv.linux.org.uk>\nReviewed-by: Mat Martineau <martineau@kernel.org>\nSigned-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>\nLink: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-2-5df34b2083e8@kernel.org\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\n---\n net/mptcp/ctrl.c | 11 +++++------\n 1 file changed, 5 insertions(+), 6 deletions(-)\n\ndiff --git a/net/mptcp/ctrl.c b/net/mptcp/ctrl.c\nindex d9b57fab2a13..81c30aa02196 100644\n--- a/net/mptcp/ctrl.c\n+++ b/net/mptcp/ctrl.c\n@@ -102,16 +102,15 @@ static void mptcp_pernet_set_defaults(struct mptcp_pernet *pernet)\n }\n \n #ifdef CONFIG_SYSCTL\n-static int mptcp_set_scheduler(const struct net *net, const char *name)\n+static int mptcp_set_scheduler(char *scheduler, const char *name)\n {\n-\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n \tstruct mptcp_sched_ops *sched;\n \tint ret = 0;\n \n \trcu_read_lock();\n \tsched = mptcp_sched_find(name);\n \tif (sched)\n-\t\tstrscpy(pernet->scheduler, name, MPTCP_SCHED_NAME_MAX);\n+\t\tstrscpy(scheduler, name, MPTCP_SCHED_NAME_MAX);\n \telse\n \t\tret = -ENOENT;\n \trcu_read_unlock();\n@@ -122,7 +121,7 @@ static int mptcp_set_scheduler(const struct net *net, const char *name)\n static int proc_scheduler(const struct ctl_table *ctl, int write,\n \t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n {\n-\tconst struct net *net = current->nsproxy->net_ns;\n+\tchar (*scheduler)[MPTCP_SCHED_NAME_MAX] = ctl->data;\n \tchar val[MPTCP_SCHED_NAME_MAX];\n \tstruct ctl_table tbl = {\n \t\t.data = val,\n@@ -130,11 +129,11 @@ static int proc_scheduler(const struct ctl_table *ctl, int write,\n \t};\n \tint ret;\n \n-\tstrscpy(val, mptcp_get_scheduler(net), MPTCP_SCHED_NAME_MAX);\n+\tstrscpy(val, *scheduler, MPTCP_SCHED_NAME_MAX);\n \n \tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n \tif (write && ret == 0)\n-\t\tret = mptcp_set_scheduler(net, val);\n+\t\tret = mptcp_set_scheduler(*scheduler, val);\n \n \treturn ret;\n }\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "net/mptcp/ctrl.c": "// SPDX-License-Identifier: GPL-2.0\n/* Multipath TCP\n *\n * Copyright (c) 2019, Tessares SA.\n */\n\n#ifdef CONFIG_SYSCTL\n#include <linux/sysctl.h>\n#endif\n\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n#include \"protocol.h\"\n\n#define MPTCP_SYSCTL_PATH \"net/mptcp\"\n\nstatic int mptcp_pernet_id;\n\n#ifdef CONFIG_SYSCTL\nstatic int mptcp_pm_type_max = __MPTCP_PM_TYPE_MAX;\n#endif\n\nstruct mptcp_pernet {\n#ifdef CONFIG_SYSCTL\n\tstruct ctl_table_header *ctl_table_hdr;\n#endif\n\n\tunsigned int add_addr_timeout;\n\tunsigned int stale_loss_cnt;\n\tu8 mptcp_enabled;\n\tu8 checksum_enabled;\n\tu8 allow_join_initial_addr_port;\n\tu8 pm_type;\n\tchar scheduler[MPTCP_SCHED_NAME_MAX];\n};\n\nstatic struct mptcp_pernet *mptcp_get_pernet(const struct net *net)\n{\n\treturn net_generic(net, mptcp_pernet_id);\n}\n\nint mptcp_is_enabled(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->mptcp_enabled;\n}\n\nunsigned int mptcp_get_add_addr_timeout(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->add_addr_timeout;\n}\n\nint mptcp_is_checksum_enabled(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->checksum_enabled;\n}\n\nint mptcp_allow_join_id0(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->allow_join_initial_addr_port;\n}\n\nunsigned int mptcp_stale_loss_cnt(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->stale_loss_cnt;\n}\n\nint mptcp_get_pm_type(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->pm_type;\n}\n\nconst char *mptcp_get_scheduler(const struct net *net)\n{\n\treturn mptcp_get_pernet(net)->scheduler;\n}\n\nstatic void mptcp_pernet_set_defaults(struct mptcp_pernet *pernet)\n{\n\tpernet->mptcp_enabled = 1;\n\tpernet->add_addr_timeout = TCP_RTO_MAX;\n\tpernet->checksum_enabled = 0;\n\tpernet->allow_join_initial_addr_port = 1;\n\tpernet->stale_loss_cnt = 4;\n\tpernet->pm_type = MPTCP_PM_TYPE_KERNEL;\n\tstrcpy(pernet->scheduler, \"default\");\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int mptcp_set_scheduler(const struct net *net, const char *name)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\tstruct mptcp_sched_ops *sched;\n\tint ret = 0;\n\n\trcu_read_lock();\n\tsched = mptcp_sched_find(name);\n\tif (sched)\n\t\tstrscpy(pernet->scheduler, name, MPTCP_SCHED_NAME_MAX);\n\telse\n\t\tret = -ENOENT;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int proc_scheduler(struct ctl_table *ctl, int write,\n\t\t\t  void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tconst struct net *net = current->nsproxy->net_ns;\n\tchar val[MPTCP_SCHED_NAME_MAX];\n\tstruct ctl_table tbl = {\n\t\t.data = val,\n\t\t.maxlen = MPTCP_SCHED_NAME_MAX,\n\t};\n\tint ret;\n\n\tstrscpy(val, mptcp_get_scheduler(net), MPTCP_SCHED_NAME_MAX);\n\n\tret = proc_dostring(&tbl, write, buffer, lenp, ppos);\n\tif (write && ret == 0)\n\t\tret = mptcp_set_scheduler(net, val);\n\n\treturn ret;\n}\n\nstatic struct ctl_table mptcp_sysctl_table[] = {\n\t{\n\t\t.procname = \"enabled\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t/* users with CAP_NET_ADMIN or root (not and) can change this\n\t\t * value, same as other sysctl or the 'net' tree.\n\t\t */\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"add_addr_timeout\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dointvec_jiffies,\n\t},\n\t{\n\t\t.procname = \"checksum_enabled\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"allow_join_initial_addr_port\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = SYSCTL_ONE\n\t},\n\t{\n\t\t.procname = \"stale_loss_cnt\",\n\t\t.maxlen = sizeof(unsigned int),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_douintvec_minmax,\n\t},\n\t{\n\t\t.procname = \"pm_type\",\n\t\t.maxlen = sizeof(u8),\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_dou8vec_minmax,\n\t\t.extra1       = SYSCTL_ZERO,\n\t\t.extra2       = &mptcp_pm_type_max\n\t},\n\t{\n\t\t.procname = \"scheduler\",\n\t\t.maxlen\t= MPTCP_SCHED_NAME_MAX,\n\t\t.mode = 0644,\n\t\t.proc_handler = proc_scheduler,\n\t},\n\t{}\n};\n\nstatic int mptcp_pernet_new_table(struct net *net, struct mptcp_pernet *pernet)\n{\n\tstruct ctl_table_header *hdr;\n\tstruct ctl_table *table;\n\n\ttable = mptcp_sysctl_table;\n\tif (!net_eq(net, &init_net)) {\n\t\ttable = kmemdup(table, sizeof(mptcp_sysctl_table), GFP_KERNEL);\n\t\tif (!table)\n\t\t\tgoto err_alloc;\n\t}\n\n\ttable[0].data = &pernet->mptcp_enabled;\n\ttable[1].data = &pernet->add_addr_timeout;\n\ttable[2].data = &pernet->checksum_enabled;\n\ttable[3].data = &pernet->allow_join_initial_addr_port;\n\ttable[4].data = &pernet->stale_loss_cnt;\n\ttable[5].data = &pernet->pm_type;\n\ttable[6].data = &pernet->scheduler;\n\n\thdr = register_net_sysctl_sz(net, MPTCP_SYSCTL_PATH, table,\n\t\t\t\t     ARRAY_SIZE(mptcp_sysctl_table));\n\tif (!hdr)\n\t\tgoto err_reg;\n\n\tpernet->ctl_table_hdr = hdr;\n\n\treturn 0;\n\nerr_reg:\n\tif (!net_eq(net, &init_net))\n\t\tkfree(table);\nerr_alloc:\n\treturn -ENOMEM;\n}\n\nstatic void mptcp_pernet_del_table(struct mptcp_pernet *pernet)\n{\n\tstruct ctl_table *table = pernet->ctl_table_hdr->ctl_table_arg;\n\n\tunregister_net_sysctl_table(pernet->ctl_table_hdr);\n\n\tkfree(table);\n}\n\n#else\n\nstatic int mptcp_pernet_new_table(struct net *net, struct mptcp_pernet *pernet)\n{\n\treturn 0;\n}\n\nstatic void mptcp_pernet_del_table(struct mptcp_pernet *pernet) {}\n\n#endif /* CONFIG_SYSCTL */\n\nstatic int __net_init mptcp_net_init(struct net *net)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\n\tmptcp_pernet_set_defaults(pernet);\n\n\treturn mptcp_pernet_new_table(net, pernet);\n}\n\n/* Note: the callback will only be called per extra netns */\nstatic void __net_exit mptcp_net_exit(struct net *net)\n{\n\tstruct mptcp_pernet *pernet = mptcp_get_pernet(net);\n\n\tmptcp_pernet_del_table(pernet);\n}\n\nstatic struct pernet_operations mptcp_pernet_ops = {\n\t.init = mptcp_net_init,\n\t.exit = mptcp_net_exit,\n\t.id = &mptcp_pernet_id,\n\t.size = sizeof(struct mptcp_pernet),\n};\n\nvoid __init mptcp_init(void)\n{\n\tmptcp_join_cookie_init();\n\tmptcp_proto_init();\n\n\tif (register_pernet_subsys(&mptcp_pernet_ops) < 0)\n\t\tpanic(\"Failed to register MPTCP pernet subsystem.\\n\");\n}\n\n#if IS_ENABLED(CONFIG_MPTCP_IPV6)\nint __init mptcpv6_init(void)\n{\n\tint err;\n\n\terr = mptcp_proto_v6_init();\n\n\treturn err;\n}\n#endif\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21643",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21643.json",
            "patch_attempts": [
                {
                    "upstream_commit": "482d520d86e889cf40d8173388a498375079cab6",
                    "upstream_commit_date": "2025-01-09 17:18:51 +0100",
                    "upstream_patch": "3f6bc9e3ab9b127171d39f9ac6eca1abb693b731",
                    "total_versions_tested": 1,
                    "successful_patches": 1,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "9f3a265836844eda30bf34c2584b8011fd4f0f49",
                            "downstream_commit": "09b94ddc58c6640cbbc7775a61a5387b8be71488",
                            "commit_date": "2025-01-17 13:40:45 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file fs/netfs/direct_write.c",
                            "downstream_patch_content": "commit 9f3a265836844eda30bf34c2584b8011fd4f0f49\nAuthor: David Howells <dhowells@redhat.com>\nDate:   Tue Jan 7 18:39:27 2025 +0000\n\n    netfs: Fix kernel async DIO\n    \n    [ Upstream commit 3f6bc9e3ab9b127171d39f9ac6eca1abb693b731 ]\n    \n    Netfslib needs to be able to handle kernel-initiated asynchronous DIO that\n    is supplied with a bio_vec[] array.  Currently, because of the async flag,\n    this gets passed to netfs_extract_user_iter() which throws a warning and\n    fails because it only handles IOVEC and UBUF iterators.  This can be\n    triggered through a combination of cifs and a loopback blockdev with\n    something like:\n    \n            mount //my/cifs/share /foo\n            dd if=/dev/zero of=/foo/m0 bs=4K count=1K\n            losetup --sector-size 4096 --direct-io=on /dev/loop2046 /foo/m0\n            echo hello >/dev/loop2046\n    \n    This causes the following to appear in syslog:\n    \n            WARNING: CPU: 2 PID: 109 at fs/netfs/iterator.c:50 netfs_extract_user_iter+0x170/0x250 [netfs]\n    \n    and the write to fail.\n    \n    Fix this by removing the check in netfs_unbuffered_write_iter_locked() that\n    causes async kernel DIO writes to be handled as userspace writes.  Note\n    that this change relies on the kernel caller maintaining the existence of\n    the bio_vec array (or kvec[] or folio_queue) until the op is complete.\n    \n    Fixes: 153a9961b551 (\"netfs: Implement unbuffered/DIO write support\")\n    Reported-by: Nicolas Baranger <nicolas.baranger@3xo.fr>\n    Closes: https://lore.kernel.org/r/fedd8a40d54b2969097ffa4507979858@3xo.fr/\n    Signed-off-by: David Howells <dhowells@redhat.com>\n    Link: https://lore.kernel.org/r/608725.1736275167@warthog.procyon.org.uk\n    Tested-by: Nicolas Baranger <nicolas.baranger@3xo.fr>\n    Acked-by: Paulo Alcantara (Red Hat) <pc@manguebit.com>\n    cc: Steve French <smfrench@gmail.com>\n    cc: Jeff Layton <jlayton@kernel.org>\n    cc: netfs@lists.linux.dev\n    cc: linux-cifs@vger.kernel.org\n    cc: linux-fsdevel@vger.kernel.org\n    Signed-off-by: Christian Brauner <brauner@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/fs/netfs/direct_write.c b/fs/netfs/direct_write.c\nindex 88f2adfab75e..26cf9c94deeb 100644\n--- a/fs/netfs/direct_write.c\n+++ b/fs/netfs/direct_write.c\n@@ -67,7 +67,7 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \t\t * allocate a sufficiently large bvec array and may shorten the\n \t\t * request.\n \t\t */\n-\t\tif (async || user_backed_iter(iter)) {\n+\t\tif (user_backed_iter(iter)) {\n \t\t\tn = netfs_extract_user_iter(iter, len, &wreq->iter, 0);\n \t\t\tif (n < 0) {\n \t\t\t\tret = n;\n@@ -77,6 +77,11 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \t\t\twreq->direct_bv_count = n;\n \t\t\twreq->direct_bv_unpin = iov_iter_extract_will_pin(iter);\n \t\t} else {\n+\t\t\t/* If this is a kernel-generated async DIO request,\n+\t\t\t * assume that any resources the iterator points to\n+\t\t\t * (eg. a bio_vec array) will persist till the end of\n+\t\t\t * the op.\n+\t\t\t */\n \t\t\twreq->iter = *iter;\n \t\t}\n \n",
                            "downstream_file_content": {}
                        }
                    ],
                    "upstream_patch_content": "From 3f6bc9e3ab9b127171d39f9ac6eca1abb693b731 Mon Sep 17 00:00:00 2001\nFrom: David Howells <dhowells@redhat.com>\nDate: Tue, 7 Jan 2025 18:39:27 +0000\nSubject: [PATCH] netfs: Fix kernel async DIO\n\nNetfslib needs to be able to handle kernel-initiated asynchronous DIO that\nis supplied with a bio_vec[] array.  Currently, because of the async flag,\nthis gets passed to netfs_extract_user_iter() which throws a warning and\nfails because it only handles IOVEC and UBUF iterators.  This can be\ntriggered through a combination of cifs and a loopback blockdev with\nsomething like:\n\n        mount //my/cifs/share /foo\n        dd if=/dev/zero of=/foo/m0 bs=4K count=1K\n        losetup --sector-size 4096 --direct-io=on /dev/loop2046 /foo/m0\n        echo hello >/dev/loop2046\n\nThis causes the following to appear in syslog:\n\n        WARNING: CPU: 2 PID: 109 at fs/netfs/iterator.c:50 netfs_extract_user_iter+0x170/0x250 [netfs]\n\nand the write to fail.\n\nFix this by removing the check in netfs_unbuffered_write_iter_locked() that\ncauses async kernel DIO writes to be handled as userspace writes.  Note\nthat this change relies on the kernel caller maintaining the existence of\nthe bio_vec array (or kvec[] or folio_queue) until the op is complete.\n\nFixes: 153a9961b551 (\"netfs: Implement unbuffered/DIO write support\")\nReported-by: Nicolas Baranger <nicolas.baranger@3xo.fr>\nCloses: https://lore.kernel.org/r/fedd8a40d54b2969097ffa4507979858@3xo.fr/\nSigned-off-by: David Howells <dhowells@redhat.com>\nLink: https://lore.kernel.org/r/608725.1736275167@warthog.procyon.org.uk\nTested-by: Nicolas Baranger <nicolas.baranger@3xo.fr>\nAcked-by: Paulo Alcantara (Red Hat) <pc@manguebit.com>\ncc: Steve French <smfrench@gmail.com>\ncc: Jeff Layton <jlayton@kernel.org>\ncc: netfs@lists.linux.dev\ncc: linux-cifs@vger.kernel.org\ncc: linux-fsdevel@vger.kernel.org\nSigned-off-by: Christian Brauner <brauner@kernel.org>\n---\n fs/netfs/direct_write.c | 7 ++++++-\n 1 file changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a/fs/netfs/direct_write.c b/fs/netfs/direct_write.c\nindex 173e8b5e6a93..f9421f3e6d37 100644\n--- a/fs/netfs/direct_write.c\n+++ b/fs/netfs/direct_write.c\n@@ -67,7 +67,7 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \t\t * allocate a sufficiently large bvec array and may shorten the\n \t\t * request.\n \t\t */\n-\t\tif (async || user_backed_iter(iter)) {\n+\t\tif (user_backed_iter(iter)) {\n \t\t\tn = netfs_extract_user_iter(iter, len, &wreq->iter, 0);\n \t\t\tif (n < 0) {\n \t\t\t\tret = n;\n@@ -77,6 +77,11 @@ ssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *\n \t\t\twreq->direct_bv_count = n;\n \t\t\twreq->direct_bv_unpin = iov_iter_extract_will_pin(iter);\n \t\t} else {\n+\t\t\t/* If this is a kernel-generated async DIO request,\n+\t\t\t * assume that any resources the iterator points to\n+\t\t\t * (eg. a bio_vec array) will persist till the end of\n+\t\t\t * the op.\n+\t\t\t */\n \t\t\twreq->iter = *iter;\n \t\t}\n \n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "fs/netfs/direct_write.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* Unbuffered and direct write support.\n *\n * Copyright (C) 2023 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/export.h>\n#include <linux/uio.h>\n#include \"internal.h\"\n\nstatic void netfs_cleanup_dio_write(struct netfs_io_request *wreq)\n{\n\tstruct inode *inode = wreq->inode;\n\tunsigned long long end = wreq->start + wreq->transferred;\n\n\tif (!wreq->error &&\n\t    i_size_read(inode) < end) {\n\t\tif (wreq->netfs_ops->update_i_size)\n\t\t\twreq->netfs_ops->update_i_size(inode, end);\n\t\telse\n\t\t\ti_size_write(inode, end);\n\t}\n}\n\n/*\n * Perform an unbuffered write where we may have to do an RMW operation on an\n * encrypted file.  This can also be used for direct I/O writes.\n */\nssize_t netfs_unbuffered_write_iter_locked(struct kiocb *iocb, struct iov_iter *iter,\n\t\t\t\t\t\t  struct netfs_group *netfs_group)\n{\n\tstruct netfs_io_request *wreq;\n\tunsigned long long start = iocb->ki_pos;\n\tunsigned long long end = start + iov_iter_count(iter);\n\tssize_t ret, n;\n\tsize_t len = iov_iter_count(iter);\n\tbool async = !is_sync_kiocb(iocb);\n\n\t_enter(\"\");\n\n\t/* We're going to need a bounce buffer if what we transmit is going to\n\t * be different in some way to the source buffer, e.g. because it gets\n\t * encrypted/compressed or because it needs expanding to a block size.\n\t */\n\t// TODO\n\n\t_debug(\"uw %llx-%llx\", start, end);\n\n\twreq = netfs_create_write_req(iocb->ki_filp->f_mapping, iocb->ki_filp, start,\n\t\t\t\t      iocb->ki_flags & IOCB_DIRECT ?\n\t\t\t\t      NETFS_DIO_WRITE : NETFS_UNBUFFERED_WRITE);\n\tif (IS_ERR(wreq))\n\t\treturn PTR_ERR(wreq);\n\n\twreq->io_streams[0].avail = true;\n\ttrace_netfs_write(wreq, (iocb->ki_flags & IOCB_DIRECT ?\n\t\t\t\t netfs_write_trace_dio_write :\n\t\t\t\t netfs_write_trace_unbuffered_write));\n\n\t{\n\t\t/* If this is an async op and we're not using a bounce buffer,\n\t\t * we have to save the source buffer as the iterator is only\n\t\t * good until we return.  In such a case, extract an iterator\n\t\t * to represent as much of the the output buffer as we can\n\t\t * manage.  Note that the extraction might not be able to\n\t\t * allocate a sufficiently large bvec array and may shorten the\n\t\t * request.\n\t\t */\n\t\tif (async || user_backed_iter(iter)) {\n\t\t\tn = netfs_extract_user_iter(iter, len, &wreq->iter, 0);\n\t\t\tif (n < 0) {\n\t\t\t\tret = n;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\twreq->direct_bv = (struct bio_vec *)wreq->iter.bvec;\n\t\t\twreq->direct_bv_count = n;\n\t\t\twreq->direct_bv_unpin = iov_iter_extract_will_pin(iter);\n\t\t} else {\n\t\t\twreq->iter = *iter;\n\t\t}\n\n\t\twreq->io_iter = wreq->iter;\n\t}\n\n\t__set_bit(NETFS_RREQ_USE_IO_ITER, &wreq->flags);\n\n\t/* Copy the data into the bounce buffer and encrypt it. */\n\t// TODO\n\n\t/* Dispatch the write. */\n\t__set_bit(NETFS_RREQ_UPLOAD_TO_SERVER, &wreq->flags);\n\tif (async)\n\t\twreq->iocb = iocb;\n\twreq->len = iov_iter_count(&wreq->io_iter);\n\twreq->cleanup = netfs_cleanup_dio_write;\n\tret = netfs_unbuffered_write(wreq, is_sync_kiocb(iocb), wreq->len);\n\tif (ret < 0) {\n\t\t_debug(\"begin = %zd\", ret);\n\t\tgoto out;\n\t}\n\n\tif (!async) {\n\t\ttrace_netfs_rreq(wreq, netfs_rreq_trace_wait_ip);\n\t\twait_on_bit(&wreq->flags, NETFS_RREQ_IN_PROGRESS,\n\t\t\t    TASK_UNINTERRUPTIBLE);\n\t\tsmp_rmb(); /* Read error/transferred after RIP flag */\n\t\tret = wreq->error;\n\t\tif (ret == 0) {\n\t\t\tret = wreq->transferred;\n\t\t\tiocb->ki_pos += ret;\n\t\t}\n\t} else {\n\t\tret = -EIOCBQUEUED;\n\t}\n\nout:\n\tnetfs_put_request(wreq, false, netfs_rreq_trace_put_return);\n\treturn ret;\n}\nEXPORT_SYMBOL(netfs_unbuffered_write_iter_locked);\n\n/**\n * netfs_unbuffered_write_iter - Unbuffered write to a file\n * @iocb: IO state structure\n * @from: iov_iter with data to write\n *\n * Do an unbuffered write to a file, writing the data directly to the server\n * and not lodging the data in the pagecache.\n *\n * Return:\n * * Negative error code if no data has been written at all of\n *   vfs_fsync_range() failed for a synchronous write\n * * Number of bytes written, even for truncated writes\n */\nssize_t netfs_unbuffered_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct netfs_inode *ictx = netfs_inode(inode);\n\tssize_t ret;\n\tloff_t pos = iocb->ki_pos;\n\tunsigned long long end = pos + iov_iter_count(from) - 1;\n\n\t_enter(\"%llx,%zx,%llx\", pos, iov_iter_count(from), i_size_read(inode));\n\n\tif (!iov_iter_count(from))\n\t\treturn 0;\n\n\ttrace_netfs_write_iter(iocb, from);\n\tnetfs_stat(&netfs_n_wh_dio_write);\n\n\tret = netfs_start_io_direct(inode);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = generic_write_checks(iocb, from);\n\tif (ret <= 0)\n\t\tgoto out;\n\tret = file_remove_privs(file);\n\tif (ret < 0)\n\t\tgoto out;\n\tret = file_update_time(file);\n\tif (ret < 0)\n\t\tgoto out;\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\t/* We could block if there are any pages in the range. */\n\t\tret = -EAGAIN;\n\t\tif (filemap_range_has_page(mapping, pos, end))\n\t\t\tif (filemap_invalidate_inode(inode, true, pos, end))\n\t\t\t\tgoto out;\n\t} else {\n\t\tret = filemap_write_and_wait_range(mapping, pos, end);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * After a write we want buffered reads to be sure to go to disk to get\n\t * the new data.  We invalidate clean cached page from the region we're\n\t * about to write.  We do this *before* the write so that we can return\n\t * without clobbering -EIOCBQUEUED from ->direct_IO().\n\t */\n\tret = filemap_invalidate_inode(inode, true, pos, end);\n\tif (ret < 0)\n\t\tgoto out;\n\tend = iocb->ki_pos + iov_iter_count(from);\n\tif (end > ictx->zero_point)\n\t\tictx->zero_point = end;\n\n\tfscache_invalidate(netfs_i_cookie(ictx), NULL, i_size_read(inode),\n\t\t\t   FSCACHE_INVAL_DIO_WRITE);\n\tret = netfs_unbuffered_write_iter_locked(iocb, from, NULL);\nout:\n\tnetfs_end_io_direct(inode);\n\treturn ret;\n}\nEXPORT_SYMBOL(netfs_unbuffered_write_iter);\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21644",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21644.json",
            "patch_attempts": [
                {
                    "upstream_commit": "f0ed39830e6064d62f9c5393505677a26569bb56",
                    "upstream_commit_date": "2025-01-09 10:38:56 +0100",
                    "upstream_patch": "9ab4981552930a9c45682d62424ba610edc3992d",
                    "total_versions_tested": 1,
                    "successful_patches": 1,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "09b94ddc58c6640cbbc7775a61a5387b8be71488",
                            "downstream_commit": "53a56817755ed7ed2c046e9592071e77d6b31072",
                            "commit_date": "2025-01-17 13:40:44 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file drivers/gpu/drm/xe/xe_gt.c\nHunk #1 succeeded at 386 (offset -1 lines).\nHunk #2 succeeded at 589 (offset -3 lines).\npatching file drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\npatching file drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h",
                            "downstream_patch_content": "commit 09b94ddc58c6640cbbc7775a61a5387b8be71488\nAuthor: Lucas De Marchi <lucas.demarchi@intel.com>\nDate:   Thu Jan 2 16:11:10 2025 -0800\n\n    drm/xe: Fix tlb invalidation when wedging\n    \n    [ Upstream commit 9ab4981552930a9c45682d62424ba610edc3992d ]\n    \n    If GuC fails to load, the driver wedges, but in the process it tries to\n    do stuff that may not be initialized yet. This moves the\n    xe_gt_tlb_invalidation_init() to be done earlier: as its own doc says,\n    it's a software-only initialization and should had been named with the\n    _early() suffix.\n    \n    Move it to be called by xe_gt_init_early(), so the locks and seqno are\n    initialized, avoiding a NULL ptr deref when wedging:\n    \n            xe 0000:03:00.0: [drm] *ERROR* GT0: load failed: status: Reset = 0, BootROM = 0x50, UKernel = 0x00, MIA = 0x00, Auth = 0x01\n            xe 0000:03:00.0: [drm] *ERROR* GT0: firmware signature verification failed\n            xe 0000:03:00.0: [drm] *ERROR* CRITICAL: Xe has declared device 0000:03:00.0 as wedged.\n            ...\n            BUG: kernel NULL pointer dereference, address: 0000000000000000\n            #PF: supervisor read access in kernel mode\n            #PF: error_code(0x0000) - not-present page\n            PGD 0 P4D 0\n            Oops: Oops: 0000 [#1] PREEMPT SMP NOPTI\n            CPU: 9 UID: 0 PID: 3908 Comm: modprobe Tainted: G     U  W          6.13.0-rc4-xe+ #3\n            Tainted: [U]=USER, [W]=WARN\n            Hardware name: Intel Corporation Alder Lake Client Platform/AlderLake-S ADP-S DDR5 UDIMM CRB, BIOS ADLSFWI1.R00.3275.A00.2207010640 07/01/2022\n            RIP: 0010:xe_gt_tlb_invalidation_reset+0x75/0x110 [xe]\n    \n    This can be easily triggered by poking the GuC binary to force a\n    signature failure. There will still be an extra message,\n    \n            xe 0000:03:00.0: [drm] *ERROR* GT0: GuC mmio request 0x4100: no reply 0x4100\n    \n    but that's better than a NULL ptr deref.\n    \n    Closes: https://gitlab.freedesktop.org/drm/xe/kernel/-/issues/3956\n    Fixes: c9474b726b93 (\"drm/xe: Wedge the entire device\")\n    Reviewed-by: Matthew Brost <matthew.brost@intel.com>\n    Link: https://patchwork.freedesktop.org/patch/msgid/20250103001111.331684-2-lucas.demarchi@intel.com\n    Signed-off-by: Lucas De Marchi <lucas.demarchi@intel.com>\n    (cherry picked from commit 5001ef3af8f2c972d6fd9c5221a8457556f8bea6)\n    Signed-off-by: Thomas Hellstr\u00f6m <thomas.hellstrom@linux.intel.com>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/drivers/gpu/drm/xe/xe_gt.c b/drivers/gpu/drm/xe/xe_gt.c\nindex d5fd6a089b7c..b940688c3613 100644\n--- a/drivers/gpu/drm/xe/xe_gt.c\n+++ b/drivers/gpu/drm/xe/xe_gt.c\n@@ -386,6 +386,10 @@ int xe_gt_init_early(struct xe_gt *gt)\n \txe_force_wake_init_gt(gt, gt_to_fw(gt));\n \tspin_lock_init(&gt->global_invl_lock);\n \n+\terr = xe_gt_tlb_invalidation_init_early(gt);\n+\tif (err)\n+\t\treturn err;\n+\n \treturn 0;\n }\n \n@@ -585,10 +589,6 @@ int xe_gt_init(struct xe_gt *gt)\n \t\txe_hw_fence_irq_init(&gt->fence_irq[i]);\n \t}\n \n-\terr = xe_gt_tlb_invalidation_init(gt);\n-\tif (err)\n-\t\treturn err;\n-\n \terr = xe_gt_pagefault_init(gt);\n \tif (err)\n \t\treturn err;\ndiff --git a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\nindex 7e385940df08..ace1fe831a7b 100644\n--- a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\n+++ b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\n@@ -106,7 +106,7 @@ static void xe_gt_tlb_fence_timeout(struct work_struct *work)\n }\n \n /**\n- * xe_gt_tlb_invalidation_init - Initialize GT TLB invalidation state\n+ * xe_gt_tlb_invalidation_init_early - Initialize GT TLB invalidation state\n  * @gt: graphics tile\n  *\n  * Initialize GT TLB invalidation state, purely software initialization, should\n@@ -114,7 +114,7 @@ static void xe_gt_tlb_fence_timeout(struct work_struct *work)\n  *\n  * Return: 0 on success, negative error code on error.\n  */\n-int xe_gt_tlb_invalidation_init(struct xe_gt *gt)\n+int xe_gt_tlb_invalidation_init_early(struct xe_gt *gt)\n {\n \tgt->tlb_invalidation.seqno = 1;\n \tINIT_LIST_HEAD(&gt->tlb_invalidation.pending_fences);\ndiff --git a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h\nindex 00b1c6c01e8d..672acfcdf0d7 100644\n--- a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h\n+++ b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h\n@@ -14,7 +14,8 @@ struct xe_gt;\n struct xe_guc;\n struct xe_vma;\n \n-int xe_gt_tlb_invalidation_init(struct xe_gt *gt);\n+int xe_gt_tlb_invalidation_init_early(struct xe_gt *gt);\n+\n void xe_gt_tlb_invalidation_reset(struct xe_gt *gt);\n int xe_gt_tlb_invalidation_ggtt(struct xe_gt *gt);\n int xe_gt_tlb_invalidation_vma(struct xe_gt *gt,\n",
                            "downstream_file_content": {}
                        }
                    ],
                    "upstream_patch_content": "From 9ab4981552930a9c45682d62424ba610edc3992d Mon Sep 17 00:00:00 2001\nFrom: Lucas De Marchi <lucas.demarchi@intel.com>\nDate: Thu, 2 Jan 2025 16:11:10 -0800\nSubject: [PATCH] drm/xe: Fix tlb invalidation when wedging\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\nIf GuC fails to load, the driver wedges, but in the process it tries to\ndo stuff that may not be initialized yet. This moves the\nxe_gt_tlb_invalidation_init() to be done earlier: as its own doc says,\nit's a software-only initialization and should had been named with the\n_early() suffix.\n\nMove it to be called by xe_gt_init_early(), so the locks and seqno are\ninitialized, avoiding a NULL ptr deref when wedging:\n\n\txe 0000:03:00.0: [drm] *ERROR* GT0: load failed: status: Reset = 0, BootROM = 0x50, UKernel = 0x00, MIA = 0x00, Auth = 0x01\n\txe 0000:03:00.0: [drm] *ERROR* GT0: firmware signature verification failed\n\txe 0000:03:00.0: [drm] *ERROR* CRITICAL: Xe has declared device 0000:03:00.0 as wedged.\n\t...\n\tBUG: kernel NULL pointer dereference, address: 0000000000000000\n\t#PF: supervisor read access in kernel mode\n\t#PF: error_code(0x0000) - not-present page\n\tPGD 0 P4D 0\n\tOops: Oops: 0000 [#1] PREEMPT SMP NOPTI\n\tCPU: 9 UID: 0 PID: 3908 Comm: modprobe Tainted: G     U  W          6.13.0-rc4-xe+ #3\n\tTainted: [U]=USER, [W]=WARN\n\tHardware name: Intel Corporation Alder Lake Client Platform/AlderLake-S ADP-S DDR5 UDIMM CRB, BIOS ADLSFWI1.R00.3275.A00.2207010640 07/01/2022\n\tRIP: 0010:xe_gt_tlb_invalidation_reset+0x75/0x110 [xe]\n\nThis can be easily triggered by poking the GuC binary to force a\nsignature failure. There will still be an extra message,\n\n\txe 0000:03:00.0: [drm] *ERROR* GT0: GuC mmio request 0x4100: no reply 0x4100\n\nbut that's better than a NULL ptr deref.\n\nCloses: https://gitlab.freedesktop.org/drm/xe/kernel/-/issues/3956\nFixes: c9474b726b93 (\"drm/xe: Wedge the entire device\")\nReviewed-by: Matthew Brost <matthew.brost@intel.com>\nLink: https://patchwork.freedesktop.org/patch/msgid/20250103001111.331684-2-lucas.demarchi@intel.com\nSigned-off-by: Lucas De Marchi <lucas.demarchi@intel.com>\n(cherry picked from commit 5001ef3af8f2c972d6fd9c5221a8457556f8bea6)\nSigned-off-by: Thomas Hellstr\u00f6m <thomas.hellstrom@linux.intel.com>\n---\n drivers/gpu/drm/xe/xe_gt.c                  | 8 ++++----\n drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c | 4 ++--\n drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h | 3 ++-\n 3 files changed, 8 insertions(+), 7 deletions(-)\n\ndiff --git a/drivers/gpu/drm/xe/xe_gt.c b/drivers/gpu/drm/xe/xe_gt.c\nindex d6744be01a68..94d468d01253 100644\n--- a/drivers/gpu/drm/xe/xe_gt.c\n+++ b/drivers/gpu/drm/xe/xe_gt.c\n@@ -387,6 +387,10 @@ int xe_gt_init_early(struct xe_gt *gt)\n \txe_force_wake_init_gt(gt, gt_to_fw(gt));\n \tspin_lock_init(&gt->global_invl_lock);\n \n+\terr = xe_gt_tlb_invalidation_init_early(gt);\n+\tif (err)\n+\t\treturn err;\n+\n \treturn 0;\n }\n \n@@ -588,10 +592,6 @@ int xe_gt_init(struct xe_gt *gt)\n \t\txe_hw_fence_irq_init(&gt->fence_irq[i]);\n \t}\n \n-\terr = xe_gt_tlb_invalidation_init(gt);\n-\tif (err)\n-\t\treturn err;\n-\n \terr = xe_gt_pagefault_init(gt);\n \tif (err)\n \t\treturn err;\ndiff --git a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\nindex 6146d1776bda..0a0af413770e 100644\n--- a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\n+++ b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c\n@@ -106,7 +106,7 @@ static void xe_gt_tlb_fence_timeout(struct work_struct *work)\n }\n \n /**\n- * xe_gt_tlb_invalidation_init - Initialize GT TLB invalidation state\n+ * xe_gt_tlb_invalidation_init_early - Initialize GT TLB invalidation state\n  * @gt: graphics tile\n  *\n  * Initialize GT TLB invalidation state, purely software initialization, should\n@@ -114,7 +114,7 @@ static void xe_gt_tlb_fence_timeout(struct work_struct *work)\n  *\n  * Return: 0 on success, negative error code on error.\n  */\n-int xe_gt_tlb_invalidation_init(struct xe_gt *gt)\n+int xe_gt_tlb_invalidation_init_early(struct xe_gt *gt)\n {\n \tgt->tlb_invalidation.seqno = 1;\n \tINIT_LIST_HEAD(&gt->tlb_invalidation.pending_fences);\ndiff --git a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h\nindex 00b1c6c01e8d..672acfcdf0d7 100644\n--- a/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h\n+++ b/drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h\n@@ -14,7 +14,8 @@ struct xe_gt;\n struct xe_guc;\n struct xe_vma;\n \n-int xe_gt_tlb_invalidation_init(struct xe_gt *gt);\n+int xe_gt_tlb_invalidation_init_early(struct xe_gt *gt);\n+\n void xe_gt_tlb_invalidation_reset(struct xe_gt *gt);\n int xe_gt_tlb_invalidation_ggtt(struct xe_gt *gt);\n int xe_gt_tlb_invalidation_vma(struct xe_gt *gt,\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "drivers/gpu/drm/xe/xe_gt.c": "// SPDX-License-Identifier: MIT\n/*\n * Copyright \u00a9 2022 Intel Corporation\n */\n\n#include \"xe_gt.h\"\n\n#include <linux/minmax.h>\n\n#include <drm/drm_managed.h>\n#include <uapi/drm/xe_drm.h>\n\n#include <generated/xe_wa_oob.h>\n\n#include \"instructions/xe_gfxpipe_commands.h\"\n#include \"instructions/xe_mi_commands.h\"\n#include \"regs/xe_gt_regs.h\"\n#include \"xe_assert.h\"\n#include \"xe_bb.h\"\n#include \"xe_bo.h\"\n#include \"xe_device.h\"\n#include \"xe_exec_queue.h\"\n#include \"xe_execlist.h\"\n#include \"xe_force_wake.h\"\n#include \"xe_ggtt.h\"\n#include \"xe_gsc.h\"\n#include \"xe_gt_ccs_mode.h\"\n#include \"xe_gt_clock.h\"\n#include \"xe_gt_freq.h\"\n#include \"xe_gt_idle.h\"\n#include \"xe_gt_mcr.h\"\n#include \"xe_gt_pagefault.h\"\n#include \"xe_gt_printk.h\"\n#include \"xe_gt_sriov_pf.h\"\n#include \"xe_gt_sysfs.h\"\n#include \"xe_gt_tlb_invalidation.h\"\n#include \"xe_gt_topology.h\"\n#include \"xe_guc_exec_queue_types.h\"\n#include \"xe_guc_pc.h\"\n#include \"xe_hw_fence.h\"\n#include \"xe_hw_engine_class_sysfs.h\"\n#include \"xe_irq.h\"\n#include \"xe_lmtt.h\"\n#include \"xe_lrc.h\"\n#include \"xe_map.h\"\n#include \"xe_migrate.h\"\n#include \"xe_mmio.h\"\n#include \"xe_pat.h\"\n#include \"xe_pm.h\"\n#include \"xe_mocs.h\"\n#include \"xe_reg_sr.h\"\n#include \"xe_ring_ops.h\"\n#include \"xe_sa.h\"\n#include \"xe_sched_job.h\"\n#include \"xe_sriov.h\"\n#include \"xe_tuning.h\"\n#include \"xe_uc.h\"\n#include \"xe_uc_fw.h\"\n#include \"xe_vm.h\"\n#include \"xe_wa.h\"\n#include \"xe_wopcm.h\"\n\nstatic void gt_fini(struct drm_device *drm, void *arg)\n{\n\tstruct xe_gt *gt = arg;\n\n\tdestroy_workqueue(gt->ordered_wq);\n}\n\nstruct xe_gt *xe_gt_alloc(struct xe_tile *tile)\n{\n\tstruct xe_gt *gt;\n\tint err;\n\n\tgt = drmm_kzalloc(&tile_to_xe(tile)->drm, sizeof(*gt), GFP_KERNEL);\n\tif (!gt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tgt->tile = tile;\n\tgt->ordered_wq = alloc_ordered_workqueue(\"gt-ordered-wq\", 0);\n\n\terr = drmm_add_action_or_reset(&gt_to_xe(gt)->drm, gt_fini, gt);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn gt;\n}\n\nvoid xe_gt_sanitize(struct xe_gt *gt)\n{\n\t/*\n\t * FIXME: if xe_uc_sanitize is called here, on TGL driver will not\n\t * reload\n\t */\n\tgt->uc.guc.submission_state.enabled = false;\n}\n\nstatic void xe_gt_enable_host_l2_vram(struct xe_gt *gt)\n{\n\tu32 reg;\n\tint err;\n\n\tif (!XE_WA(gt, 16023588340))\n\t\treturn;\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FW_GT);\n\tif (WARN_ON(err))\n\t\treturn;\n\n\tif (!xe_gt_is_media_type(gt)) {\n\t\treg = xe_gt_mcr_unicast_read_any(gt, XE2_GAMREQSTRM_CTRL);\n\t\treg |= CG_DIS_CNTLBUS;\n\t\txe_gt_mcr_multicast_write(gt, XE2_GAMREQSTRM_CTRL, reg);\n\t}\n\n\txe_gt_mcr_multicast_write(gt, XEHPC_L3CLOS_MASK(3), 0x3);\n\txe_force_wake_put(gt_to_fw(gt), XE_FW_GT);\n}\n\nstatic void xe_gt_disable_host_l2_vram(struct xe_gt *gt)\n{\n\tu32 reg;\n\tint err;\n\n\tif (!XE_WA(gt, 16023588340))\n\t\treturn;\n\n\tif (xe_gt_is_media_type(gt))\n\t\treturn;\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FW_GT);\n\tif (WARN_ON(err))\n\t\treturn;\n\n\treg = xe_gt_mcr_unicast_read_any(gt, XE2_GAMREQSTRM_CTRL);\n\treg &= ~CG_DIS_CNTLBUS;\n\txe_gt_mcr_multicast_write(gt, XE2_GAMREQSTRM_CTRL, reg);\n\n\txe_force_wake_put(gt_to_fw(gt), XE_FW_GT);\n}\n\n/**\n * xe_gt_remove() - Clean up the GT structures before driver removal\n * @gt: the GT object\n *\n * This function should only act on objects/structures that must be cleaned\n * before the driver removal callback is complete and therefore can't be\n * deferred to a drmm action.\n */\nvoid xe_gt_remove(struct xe_gt *gt)\n{\n\tint i;\n\n\txe_uc_remove(&gt->uc);\n\n\tfor (i = 0; i < XE_ENGINE_CLASS_MAX; ++i)\n\t\txe_hw_fence_irq_finish(&gt->fence_irq[i]);\n\n\txe_gt_disable_host_l2_vram(gt);\n}\n\nstatic void gt_reset_worker(struct work_struct *w);\n\nstatic int emit_nop_job(struct xe_gt *gt, struct xe_exec_queue *q)\n{\n\tstruct xe_sched_job *job;\n\tstruct xe_bb *bb;\n\tstruct dma_fence *fence;\n\tlong timeout;\n\n\tbb = xe_bb_new(gt, 4, false);\n\tif (IS_ERR(bb))\n\t\treturn PTR_ERR(bb);\n\n\tjob = xe_bb_create_job(q, bb);\n\tif (IS_ERR(job)) {\n\t\txe_bb_free(bb, NULL);\n\t\treturn PTR_ERR(job);\n\t}\n\n\txe_sched_job_arm(job);\n\tfence = dma_fence_get(&job->drm.s_fence->finished);\n\txe_sched_job_push(job);\n\n\ttimeout = dma_fence_wait_timeout(fence, false, HZ);\n\tdma_fence_put(fence);\n\txe_bb_free(bb, NULL);\n\tif (timeout < 0)\n\t\treturn timeout;\n\telse if (!timeout)\n\t\treturn -ETIME;\n\n\treturn 0;\n}\n\n/*\n * Convert back from encoded value to type-safe, only to be used when reg.mcr\n * is true\n */\nstatic struct xe_reg_mcr to_xe_reg_mcr(const struct xe_reg reg)\n{\n\treturn (const struct xe_reg_mcr){.__reg.raw = reg.raw };\n}\n\nstatic int emit_wa_job(struct xe_gt *gt, struct xe_exec_queue *q)\n{\n\tstruct xe_reg_sr *sr = &q->hwe->reg_lrc;\n\tstruct xe_reg_sr_entry *entry;\n\tunsigned long idx;\n\tstruct xe_sched_job *job;\n\tstruct xe_bb *bb;\n\tstruct dma_fence *fence;\n\tlong timeout;\n\tint count = 0;\n\n\tif (q->hwe->class == XE_ENGINE_CLASS_RENDER)\n\t\t/* Big enough to emit all of the context's 3DSTATE */\n\t\tbb = xe_bb_new(gt, xe_gt_lrc_size(gt, q->hwe->class), false);\n\telse\n\t\t/* Just pick a large BB size */\n\t\tbb = xe_bb_new(gt, SZ_4K, false);\n\n\tif (IS_ERR(bb))\n\t\treturn PTR_ERR(bb);\n\n\txa_for_each(&sr->xa, idx, entry)\n\t\t++count;\n\n\tif (count) {\n\t\txe_gt_dbg(gt, \"LRC WA %s save-restore batch\\n\", sr->name);\n\n\t\tbb->cs[bb->len++] = MI_LOAD_REGISTER_IMM | MI_LRI_NUM_REGS(count);\n\n\t\txa_for_each(&sr->xa, idx, entry) {\n\t\t\tstruct xe_reg reg = entry->reg;\n\t\t\tstruct xe_reg_mcr reg_mcr = to_xe_reg_mcr(reg);\n\t\t\tu32 val;\n\n\t\t\t/*\n\t\t\t * Skip reading the register if it's not really needed\n\t\t\t */\n\t\t\tif (reg.masked)\n\t\t\t\tval = entry->clr_bits << 16;\n\t\t\telse if (entry->clr_bits + 1)\n\t\t\t\tval = (reg.mcr ?\n\t\t\t\t       xe_gt_mcr_unicast_read_any(gt, reg_mcr) :\n\t\t\t\t       xe_mmio_read32(gt, reg)) & (~entry->clr_bits);\n\t\t\telse\n\t\t\t\tval = 0;\n\n\t\t\tval |= entry->set_bits;\n\n\t\t\tbb->cs[bb->len++] = reg.addr;\n\t\t\tbb->cs[bb->len++] = val;\n\t\t\txe_gt_dbg(gt, \"REG[0x%x] = 0x%08x\", reg.addr, val);\n\t\t}\n\t}\n\n\txe_lrc_emit_hwe_state_instructions(q, bb);\n\n\tjob = xe_bb_create_job(q, bb);\n\tif (IS_ERR(job)) {\n\t\txe_bb_free(bb, NULL);\n\t\treturn PTR_ERR(job);\n\t}\n\n\txe_sched_job_arm(job);\n\tfence = dma_fence_get(&job->drm.s_fence->finished);\n\txe_sched_job_push(job);\n\n\ttimeout = dma_fence_wait_timeout(fence, false, HZ);\n\tdma_fence_put(fence);\n\txe_bb_free(bb, NULL);\n\tif (timeout < 0)\n\t\treturn timeout;\n\telse if (!timeout)\n\t\treturn -ETIME;\n\n\treturn 0;\n}\n\nint xe_gt_record_default_lrcs(struct xe_gt *gt)\n{\n\tstruct xe_device *xe = gt_to_xe(gt);\n\tstruct xe_hw_engine *hwe;\n\tenum xe_hw_engine_id id;\n\tint err = 0;\n\n\tfor_each_hw_engine(hwe, gt, id) {\n\t\tstruct xe_exec_queue *q, *nop_q;\n\t\tvoid *default_lrc;\n\n\t\tif (gt->default_lrc[hwe->class])\n\t\t\tcontinue;\n\n\t\txe_reg_sr_init(&hwe->reg_lrc, hwe->name, xe);\n\t\txe_wa_process_lrc(hwe);\n\t\txe_hw_engine_setup_default_lrc_state(hwe);\n\t\txe_tuning_process_lrc(hwe);\n\n\t\tdefault_lrc = drmm_kzalloc(&xe->drm,\n\t\t\t\t\t   xe_gt_lrc_size(gt, hwe->class),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!default_lrc)\n\t\t\treturn -ENOMEM;\n\n\t\tq = xe_exec_queue_create(xe, NULL, BIT(hwe->logical_instance), 1,\n\t\t\t\t\t hwe, EXEC_QUEUE_FLAG_KERNEL, 0);\n\t\tif (IS_ERR(q)) {\n\t\t\terr = PTR_ERR(q);\n\t\t\txe_gt_err(gt, \"hwe %s: xe_exec_queue_create failed (%pe)\\n\",\n\t\t\t\t  hwe->name, q);\n\t\t\treturn err;\n\t\t}\n\n\t\t/* Prime golden LRC with known good state */\n\t\terr = emit_wa_job(gt, q);\n\t\tif (err) {\n\t\t\txe_gt_err(gt, \"hwe %s: emit_wa_job failed (%pe) guc_id=%u\\n\",\n\t\t\t\t  hwe->name, ERR_PTR(err), q->guc->id);\n\t\t\tgoto put_exec_queue;\n\t\t}\n\n\t\tnop_q = xe_exec_queue_create(xe, NULL, BIT(hwe->logical_instance),\n\t\t\t\t\t     1, hwe, EXEC_QUEUE_FLAG_KERNEL, 0);\n\t\tif (IS_ERR(nop_q)) {\n\t\t\terr = PTR_ERR(nop_q);\n\t\t\txe_gt_err(gt, \"hwe %s: nop xe_exec_queue_create failed (%pe)\\n\",\n\t\t\t\t  hwe->name, nop_q);\n\t\t\tgoto put_exec_queue;\n\t\t}\n\n\t\t/* Switch to different LRC */\n\t\terr = emit_nop_job(gt, nop_q);\n\t\tif (err) {\n\t\t\txe_gt_err(gt, \"hwe %s: nop emit_nop_job failed (%pe) guc_id=%u\\n\",\n\t\t\t\t  hwe->name, ERR_PTR(err), nop_q->guc->id);\n\t\t\tgoto put_nop_q;\n\t\t}\n\n\t\t/* Reload golden LRC to record the effect of any indirect W/A */\n\t\terr = emit_nop_job(gt, q);\n\t\tif (err) {\n\t\t\txe_gt_err(gt, \"hwe %s: emit_nop_job failed (%pe) guc_id=%u\\n\",\n\t\t\t\t  hwe->name, ERR_PTR(err), q->guc->id);\n\t\t\tgoto put_nop_q;\n\t\t}\n\n\t\txe_map_memcpy_from(xe, default_lrc,\n\t\t\t\t   &q->lrc[0]->bo->vmap,\n\t\t\t\t   xe_lrc_pphwsp_offset(q->lrc[0]),\n\t\t\t\t   xe_gt_lrc_size(gt, hwe->class));\n\n\t\tgt->default_lrc[hwe->class] = default_lrc;\nput_nop_q:\n\t\txe_exec_queue_put(nop_q);\nput_exec_queue:\n\t\txe_exec_queue_put(q);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint xe_gt_init_early(struct xe_gt *gt)\n{\n\tint err;\n\n\tif (IS_SRIOV_PF(gt_to_xe(gt))) {\n\t\terr = xe_gt_sriov_pf_init_early(gt);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\txe_reg_sr_init(&gt->reg_sr, \"GT\", gt_to_xe(gt));\n\n\terr = xe_wa_init(gt);\n\tif (err)\n\t\treturn err;\n\n\txe_wa_process_gt(gt);\n\txe_wa_process_oob(gt);\n\txe_tuning_process_gt(gt);\n\n\txe_force_wake_init_gt(gt, gt_to_fw(gt));\n\tspin_lock_init(&gt->global_invl_lock);\n\n\treturn 0;\n}\n\nstatic void dump_pat_on_error(struct xe_gt *gt)\n{\n\tstruct drm_printer p;\n\tchar prefix[32];\n\n\tsnprintf(prefix, sizeof(prefix), \"[GT%u Error]\", gt->info.id);\n\tp = drm_dbg_printer(&gt_to_xe(gt)->drm, DRM_UT_DRIVER, prefix);\n\n\txe_pat_dump(gt, &p);\n}\n\nstatic int gt_fw_domain_init(struct xe_gt *gt)\n{\n\tint err, i;\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FW_GT);\n\tif (err)\n\t\tgoto err_hw_fence_irq;\n\n\tif (!xe_gt_is_media_type(gt)) {\n\t\terr = xe_ggtt_init(gt_to_tile(gt)->mem.ggtt);\n\t\tif (err)\n\t\t\tgoto err_force_wake;\n\t\tif (IS_SRIOV_PF(gt_to_xe(gt)))\n\t\t\txe_lmtt_init(&gt_to_tile(gt)->sriov.pf.lmtt);\n\t}\n\n\t/* Enable per hw engine IRQs */\n\txe_irq_enable_hwe(gt);\n\n\t/* Rerun MCR init as we now have hw engine list */\n\txe_gt_mcr_init(gt);\n\n\terr = xe_hw_engines_init_early(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\terr = xe_hw_engine_class_sysfs_init(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\t/* Initialize CCS mode sysfs after early initialization of HW engines */\n\terr = xe_gt_ccs_mode_sysfs_init(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\t/*\n\t * Stash hardware-reported version.  Since this register does not exist\n\t * on pre-MTL platforms, reading it there will (correctly) return 0.\n\t */\n\tgt->info.gmdid = xe_mmio_read32(gt, GMD_ID);\n\n\terr = xe_force_wake_put(gt_to_fw(gt), XE_FW_GT);\n\tXE_WARN_ON(err);\n\n\treturn 0;\n\nerr_force_wake:\n\tdump_pat_on_error(gt);\n\txe_force_wake_put(gt_to_fw(gt), XE_FW_GT);\nerr_hw_fence_irq:\n\tfor (i = 0; i < XE_ENGINE_CLASS_MAX; ++i)\n\t\txe_hw_fence_irq_finish(&gt->fence_irq[i]);\n\n\treturn err;\n}\n\nstatic int all_fw_domain_init(struct xe_gt *gt)\n{\n\tint err, i;\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FORCEWAKE_ALL);\n\tif (err)\n\t\tgoto err_hw_fence_irq;\n\n\txe_gt_mcr_set_implicit_defaults(gt);\n\txe_reg_sr_apply_mmio(&gt->reg_sr, gt);\n\n\terr = xe_gt_clock_init(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\txe_mocs_init(gt);\n\terr = xe_execlist_init(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\terr = xe_hw_engines_init(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\terr = xe_uc_init_post_hwconfig(&gt->uc);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\tif (!xe_gt_is_media_type(gt)) {\n\t\t/*\n\t\t * USM has its only SA pool to non-block behind user operations\n\t\t */\n\t\tif (gt_to_xe(gt)->info.has_usm) {\n\t\t\tstruct xe_device *xe = gt_to_xe(gt);\n\n\t\t\tgt->usm.bb_pool = xe_sa_bo_manager_init(gt_to_tile(gt),\n\t\t\t\t\t\t\t\tIS_DGFX(xe) ? SZ_1M : SZ_512K, 16);\n\t\t\tif (IS_ERR(gt->usm.bb_pool)) {\n\t\t\t\terr = PTR_ERR(gt->usm.bb_pool);\n\t\t\t\tgoto err_force_wake;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!xe_gt_is_media_type(gt)) {\n\t\tstruct xe_tile *tile = gt_to_tile(gt);\n\n\t\ttile->migrate = xe_migrate_init(tile);\n\t\tif (IS_ERR(tile->migrate)) {\n\t\t\terr = PTR_ERR(tile->migrate);\n\t\t\tgoto err_force_wake;\n\t\t}\n\t}\n\n\terr = xe_uc_init_hw(&gt->uc);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\t/* Configure default CCS mode of 1 engine with all resources */\n\tif (xe_gt_ccs_mode_enabled(gt)) {\n\t\tgt->ccs_mode = 1;\n\t\txe_gt_apply_ccs_mode(gt);\n\t}\n\n\tif (IS_SRIOV_PF(gt_to_xe(gt)) && !xe_gt_is_media_type(gt))\n\t\txe_lmtt_init_hw(&gt_to_tile(gt)->sriov.pf.lmtt);\n\n\tif (IS_SRIOV_PF(gt_to_xe(gt)))\n\t\txe_gt_sriov_pf_init_hw(gt);\n\n\terr = xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL);\n\tXE_WARN_ON(err);\n\n\treturn 0;\n\nerr_force_wake:\n\txe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL);\nerr_hw_fence_irq:\n\tfor (i = 0; i < XE_ENGINE_CLASS_MAX; ++i)\n\t\txe_hw_fence_irq_finish(&gt->fence_irq[i]);\n\n\treturn err;\n}\n\n/*\n * Initialize enough GT to be able to load GuC in order to obtain hwconfig and\n * enable CTB communication.\n */\nint xe_gt_init_hwconfig(struct xe_gt *gt)\n{\n\tint err;\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FW_GT);\n\tif (err)\n\t\tgoto out;\n\n\txe_gt_mcr_init_early(gt);\n\txe_pat_init(gt);\n\n\terr = xe_uc_init(&gt->uc);\n\tif (err)\n\t\tgoto out_fw;\n\n\terr = xe_uc_init_hwconfig(&gt->uc);\n\tif (err)\n\t\tgoto out_fw;\n\n\txe_gt_topology_init(gt);\n\txe_gt_mcr_init(gt);\n\txe_gt_enable_host_l2_vram(gt);\n\nout_fw:\n\txe_force_wake_put(gt_to_fw(gt), XE_FW_GT);\nout:\n\treturn err;\n}\n\nint xe_gt_init(struct xe_gt *gt)\n{\n\tint err;\n\tint i;\n\n\tINIT_WORK(&gt->reset.worker, gt_reset_worker);\n\n\tfor (i = 0; i < XE_ENGINE_CLASS_MAX; ++i) {\n\t\tgt->ring_ops[i] = xe_ring_ops_get(gt, i);\n\t\txe_hw_fence_irq_init(&gt->fence_irq[i]);\n\t}\n\n\terr = xe_gt_tlb_invalidation_init(gt);\n\tif (err)\n\t\treturn err;\n\n\terr = xe_gt_pagefault_init(gt);\n\tif (err)\n\t\treturn err;\n\n\txe_mocs_init_early(gt);\n\n\terr = xe_gt_sysfs_init(gt);\n\tif (err)\n\t\treturn err;\n\n\terr = gt_fw_domain_init(gt);\n\tif (err)\n\t\treturn err;\n\n\terr = xe_gt_idle_init(&gt->gtidle);\n\tif (err)\n\t\treturn err;\n\n\terr = xe_gt_freq_init(gt);\n\tif (err)\n\t\treturn err;\n\n\txe_force_wake_init_engines(gt, gt_to_fw(gt));\n\n\terr = all_fw_domain_init(gt);\n\tif (err)\n\t\treturn err;\n\n\txe_gt_record_user_engines(gt);\n\n\treturn 0;\n}\n\nvoid xe_gt_record_user_engines(struct xe_gt *gt)\n{\n\tstruct xe_hw_engine *hwe;\n\tenum xe_hw_engine_id id;\n\n\tgt->user_engines.mask = 0;\n\tmemset(gt->user_engines.instances_per_class, 0,\n\t       sizeof(gt->user_engines.instances_per_class));\n\n\tfor_each_hw_engine(hwe, gt, id) {\n\t\tif (xe_hw_engine_is_reserved(hwe))\n\t\t\tcontinue;\n\n\t\tgt->user_engines.mask |= BIT_ULL(id);\n\t\tgt->user_engines.instances_per_class[hwe->class]++;\n\t}\n\n\txe_gt_assert(gt, (gt->user_engines.mask | gt->info.engine_mask)\n\t\t     == gt->info.engine_mask);\n}\n\nstatic int do_gt_reset(struct xe_gt *gt)\n{\n\tint err;\n\n\txe_gsc_wa_14015076503(gt, true);\n\n\txe_mmio_write32(gt, GDRST, GRDOM_FULL);\n\terr = xe_mmio_wait32(gt, GDRST, GRDOM_FULL, 0, 5000, NULL, false);\n\tif (err)\n\t\txe_gt_err(gt, \"failed to clear GRDOM_FULL (%pe)\\n\",\n\t\t\t  ERR_PTR(err));\n\n\txe_gsc_wa_14015076503(gt, false);\n\n\treturn err;\n}\n\nstatic int vf_gt_restart(struct xe_gt *gt)\n{\n\tint err;\n\n\terr = xe_uc_sanitize_reset(&gt->uc);\n\tif (err)\n\t\treturn err;\n\n\terr = xe_uc_init_hw(&gt->uc);\n\tif (err)\n\t\treturn err;\n\n\terr = xe_uc_start(&gt->uc);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int do_gt_restart(struct xe_gt *gt)\n{\n\tstruct xe_hw_engine *hwe;\n\tenum xe_hw_engine_id id;\n\tint err;\n\n\tif (IS_SRIOV_VF(gt_to_xe(gt)))\n\t\treturn vf_gt_restart(gt);\n\n\txe_pat_init(gt);\n\n\txe_gt_enable_host_l2_vram(gt);\n\n\txe_gt_mcr_set_implicit_defaults(gt);\n\txe_reg_sr_apply_mmio(&gt->reg_sr, gt);\n\n\terr = xe_wopcm_init(&gt->uc.wopcm);\n\tif (err)\n\t\treturn err;\n\n\tfor_each_hw_engine(hwe, gt, id)\n\t\txe_hw_engine_enable_ring(hwe);\n\n\terr = xe_uc_sanitize_reset(&gt->uc);\n\tif (err)\n\t\treturn err;\n\n\terr = xe_uc_init_hw(&gt->uc);\n\tif (err)\n\t\treturn err;\n\n\tif (IS_SRIOV_PF(gt_to_xe(gt)) && !xe_gt_is_media_type(gt))\n\t\txe_lmtt_init_hw(&gt_to_tile(gt)->sriov.pf.lmtt);\n\n\tif (IS_SRIOV_PF(gt_to_xe(gt)))\n\t\txe_gt_sriov_pf_init_hw(gt);\n\n\txe_mocs_init(gt);\n\terr = xe_uc_start(&gt->uc);\n\tif (err)\n\t\treturn err;\n\n\tfor_each_hw_engine(hwe, gt, id) {\n\t\txe_reg_sr_apply_mmio(&hwe->reg_sr, gt);\n\t\txe_reg_sr_apply_whitelist(hwe);\n\t}\n\n\t/* Get CCS mode in sync between sw/hw */\n\txe_gt_apply_ccs_mode(gt);\n\n\t/* Restore GT freq to expected values */\n\txe_gt_sanitize_freq(gt);\n\n\tif (IS_SRIOV_PF(gt_to_xe(gt)))\n\t\txe_gt_sriov_pf_restart(gt);\n\n\treturn 0;\n}\n\nstatic int gt_reset(struct xe_gt *gt)\n{\n\tint err;\n\n\tif (xe_device_wedged(gt_to_xe(gt)))\n\t\treturn -ECANCELED;\n\n\t/* We only support GT resets with GuC submission */\n\tif (!xe_device_uc_enabled(gt_to_xe(gt)))\n\t\treturn -ENODEV;\n\n\txe_gt_info(gt, \"reset started\\n\");\n\n\txe_pm_runtime_get(gt_to_xe(gt));\n\n\tif (xe_fault_inject_gt_reset()) {\n\t\terr = -ECANCELED;\n\t\tgoto err_fail;\n\t}\n\n\txe_gt_sanitize(gt);\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FORCEWAKE_ALL);\n\tif (err)\n\t\tgoto err_msg;\n\n\txe_uc_gucrc_disable(&gt->uc);\n\txe_uc_stop_prepare(&gt->uc);\n\txe_gt_pagefault_reset(gt);\n\n\txe_uc_stop(&gt->uc);\n\n\txe_gt_tlb_invalidation_reset(gt);\n\n\terr = do_gt_reset(gt);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = do_gt_restart(gt);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL);\n\tXE_WARN_ON(err);\n\txe_pm_runtime_put(gt_to_xe(gt));\n\n\txe_gt_info(gt, \"reset done\\n\");\n\n\treturn 0;\n\nerr_out:\n\tXE_WARN_ON(xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL));\nerr_msg:\n\tXE_WARN_ON(xe_uc_start(&gt->uc));\nerr_fail:\n\txe_gt_err(gt, \"reset failed (%pe)\\n\", ERR_PTR(err));\n\n\txe_device_declare_wedged(gt_to_xe(gt));\n\txe_pm_runtime_put(gt_to_xe(gt));\n\n\treturn err;\n}\n\nstatic void gt_reset_worker(struct work_struct *w)\n{\n\tstruct xe_gt *gt = container_of(w, typeof(*gt), reset.worker);\n\n\tgt_reset(gt);\n}\n\nvoid xe_gt_reset_async(struct xe_gt *gt)\n{\n\txe_gt_info(gt, \"trying reset\\n\");\n\n\t/* Don't do a reset while one is already in flight */\n\tif (!xe_fault_inject_gt_reset() && xe_uc_reset_prepare(&gt->uc))\n\t\treturn;\n\n\txe_gt_info(gt, \"reset queued\\n\");\n\tqueue_work(gt->ordered_wq, &gt->reset.worker);\n}\n\nvoid xe_gt_suspend_prepare(struct xe_gt *gt)\n{\n\tXE_WARN_ON(xe_force_wake_get(gt_to_fw(gt), XE_FORCEWAKE_ALL));\n\n\txe_uc_stop_prepare(&gt->uc);\n\n\tXE_WARN_ON(xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL));\n}\n\nint xe_gt_suspend(struct xe_gt *gt)\n{\n\tint err;\n\n\txe_gt_dbg(gt, \"suspending\\n\");\n\txe_gt_sanitize(gt);\n\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FORCEWAKE_ALL);\n\tif (err)\n\t\tgoto err_msg;\n\n\terr = xe_uc_suspend(&gt->uc);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\txe_gt_idle_disable_pg(gt);\n\n\txe_gt_disable_host_l2_vram(gt);\n\n\tXE_WARN_ON(xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL));\n\txe_gt_dbg(gt, \"suspended\\n\");\n\n\treturn 0;\n\nerr_force_wake:\n\tXE_WARN_ON(xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL));\nerr_msg:\n\txe_gt_err(gt, \"suspend failed (%pe)\\n\", ERR_PTR(err));\n\n\treturn err;\n}\n\n/**\n * xe_gt_sanitize_freq() - Restore saved frequencies if necessary.\n * @gt: the GT object\n *\n * Called after driver init/GSC load completes to restore GT frequencies if we\n * limited them for any WAs.\n */\nint xe_gt_sanitize_freq(struct xe_gt *gt)\n{\n\tint ret = 0;\n\n\tif ((!xe_uc_fw_is_available(&gt->uc.gsc.fw) ||\n\t     xe_uc_fw_is_loaded(&gt->uc.gsc.fw) ||\n\t     xe_uc_fw_is_in_error_state(&gt->uc.gsc.fw)) &&\n\t    XE_WA(gt, 22019338487))\n\t\tret = xe_guc_pc_restore_stashed_freq(&gt->uc.guc.pc);\n\n\treturn ret;\n}\n\nint xe_gt_resume(struct xe_gt *gt)\n{\n\tint err;\n\n\txe_gt_dbg(gt, \"resuming\\n\");\n\terr = xe_force_wake_get(gt_to_fw(gt), XE_FORCEWAKE_ALL);\n\tif (err)\n\t\tgoto err_msg;\n\n\terr = do_gt_restart(gt);\n\tif (err)\n\t\tgoto err_force_wake;\n\n\txe_gt_idle_enable_pg(gt);\n\n\tXE_WARN_ON(xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL));\n\txe_gt_dbg(gt, \"resumed\\n\");\n\n\treturn 0;\n\nerr_force_wake:\n\tXE_WARN_ON(xe_force_wake_put(gt_to_fw(gt), XE_FORCEWAKE_ALL));\nerr_msg:\n\txe_gt_err(gt, \"resume failed (%pe)\\n\", ERR_PTR(err));\n\n\treturn err;\n}\n\nstruct xe_hw_engine *xe_gt_hw_engine(struct xe_gt *gt,\n\t\t\t\t     enum xe_engine_class class,\n\t\t\t\t     u16 instance, bool logical)\n{\n\tstruct xe_hw_engine *hwe;\n\tenum xe_hw_engine_id id;\n\n\tfor_each_hw_engine(hwe, gt, id)\n\t\tif (hwe->class == class &&\n\t\t    ((!logical && hwe->instance == instance) ||\n\t\t    (logical && hwe->logical_instance == instance)))\n\t\t\treturn hwe;\n\n\treturn NULL;\n}\n\nstruct xe_hw_engine *xe_gt_any_hw_engine_by_reset_domain(struct xe_gt *gt,\n\t\t\t\t\t\t\t enum xe_engine_class class)\n{\n\tstruct xe_hw_engine *hwe;\n\tenum xe_hw_engine_id id;\n\n\tfor_each_hw_engine(hwe, gt, id) {\n\t\tswitch (class) {\n\t\tcase XE_ENGINE_CLASS_RENDER:\n\t\tcase XE_ENGINE_CLASS_COMPUTE:\n\t\t\tif (hwe->class == XE_ENGINE_CLASS_RENDER ||\n\t\t\t    hwe->class == XE_ENGINE_CLASS_COMPUTE)\n\t\t\t\treturn hwe;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (hwe->class == class)\n\t\t\t\treturn hwe;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstruct xe_hw_engine *xe_gt_any_hw_engine(struct xe_gt *gt)\n{\n\tstruct xe_hw_engine *hwe;\n\tenum xe_hw_engine_id id;\n\n\tfor_each_hw_engine(hwe, gt, id)\n\t\treturn hwe;\n\n\treturn NULL;\n}\n\n/**\n * xe_gt_declare_wedged() - Declare GT wedged\n * @gt: the GT object\n *\n * Wedge the GT which stops all submission, saves desired debug state, and\n * cleans up anything which could timeout.\n */\nvoid xe_gt_declare_wedged(struct xe_gt *gt)\n{\n\txe_gt_assert(gt, gt_to_xe(gt)->wedged.mode);\n\n\txe_uc_declare_wedged(&gt->uc);\n\txe_gt_tlb_invalidation_reset(gt);\n}\n",
                        "drivers/gpu/drm/xe/xe_gt_tlb_invalidation.c": "// SPDX-License-Identifier: MIT\n/*\n * Copyright \u00a9 2023 Intel Corporation\n */\n\n#include \"xe_gt_tlb_invalidation.h\"\n\n#include \"abi/guc_actions_abi.h\"\n#include \"xe_device.h\"\n#include \"xe_force_wake.h\"\n#include \"xe_gt.h\"\n#include \"xe_gt_printk.h\"\n#include \"xe_guc.h\"\n#include \"xe_guc_ct.h\"\n#include \"xe_gt_stats.h\"\n#include \"xe_mmio.h\"\n#include \"xe_pm.h\"\n#include \"xe_sriov.h\"\n#include \"xe_trace.h\"\n#include \"regs/xe_guc_regs.h\"\n\n#define FENCE_STACK_BIT\t\tDMA_FENCE_FLAG_USER_BITS\n\n/*\n * TLB inval depends on pending commands in the CT queue and then the real\n * invalidation time. Double up the time to process full CT queue\n * just to be on the safe side.\n */\nstatic long tlb_timeout_jiffies(struct xe_gt *gt)\n{\n\t/* this reflects what HW/GuC needs to process TLB inv request */\n\tconst long hw_tlb_timeout = HZ / 4;\n\n\t/* this estimates actual delay caused by the CTB transport */\n\tlong delay = xe_guc_ct_queue_proc_time_jiffies(&gt->uc.guc.ct);\n\n\treturn hw_tlb_timeout + 2 * delay;\n}\n\nstatic void xe_gt_tlb_invalidation_fence_fini(struct xe_gt_tlb_invalidation_fence *fence)\n{\n\tif (WARN_ON_ONCE(!fence->gt))\n\t\treturn;\n\n\txe_pm_runtime_put(gt_to_xe(fence->gt));\n\tfence->gt = NULL; /* fini() should be called once */\n}\n\nstatic void\n__invalidation_fence_signal(struct xe_device *xe, struct xe_gt_tlb_invalidation_fence *fence)\n{\n\tbool stack = test_bit(FENCE_STACK_BIT, &fence->base.flags);\n\n\ttrace_xe_gt_tlb_invalidation_fence_signal(xe, fence);\n\txe_gt_tlb_invalidation_fence_fini(fence);\n\tdma_fence_signal(&fence->base);\n\tif (!stack)\n\t\tdma_fence_put(&fence->base);\n}\n\nstatic void\ninvalidation_fence_signal(struct xe_device *xe, struct xe_gt_tlb_invalidation_fence *fence)\n{\n\tlist_del(&fence->link);\n\t__invalidation_fence_signal(xe, fence);\n}\n\nvoid xe_gt_tlb_invalidation_fence_signal(struct xe_gt_tlb_invalidation_fence *fence)\n{\n\tif (WARN_ON_ONCE(!fence->gt))\n\t\treturn;\n\n\t__invalidation_fence_signal(gt_to_xe(fence->gt), fence);\n}\n\nstatic void xe_gt_tlb_fence_timeout(struct work_struct *work)\n{\n\tstruct xe_gt *gt = container_of(work, struct xe_gt,\n\t\t\t\t\ttlb_invalidation.fence_tdr.work);\n\tstruct xe_device *xe = gt_to_xe(gt);\n\tstruct xe_gt_tlb_invalidation_fence *fence, *next;\n\n\tLNL_FLUSH_WORK(&gt->uc.guc.ct.g2h_worker);\n\n\tspin_lock_irq(&gt->tlb_invalidation.pending_lock);\n\tlist_for_each_entry_safe(fence, next,\n\t\t\t\t &gt->tlb_invalidation.pending_fences, link) {\n\t\ts64 since_inval_ms = ktime_ms_delta(ktime_get(),\n\t\t\t\t\t\t    fence->invalidation_time);\n\n\t\tif (msecs_to_jiffies(since_inval_ms) < tlb_timeout_jiffies(gt))\n\t\t\tbreak;\n\n\t\ttrace_xe_gt_tlb_invalidation_fence_timeout(xe, fence);\n\t\txe_gt_err(gt, \"TLB invalidation fence timeout, seqno=%d recv=%d\",\n\t\t\t  fence->seqno, gt->tlb_invalidation.seqno_recv);\n\n\t\tfence->base.error = -ETIME;\n\t\tinvalidation_fence_signal(xe, fence);\n\t}\n\tif (!list_empty(&gt->tlb_invalidation.pending_fences))\n\t\tqueue_delayed_work(system_wq,\n\t\t\t\t   &gt->tlb_invalidation.fence_tdr,\n\t\t\t\t   tlb_timeout_jiffies(gt));\n\tspin_unlock_irq(&gt->tlb_invalidation.pending_lock);\n}\n\n/**\n * xe_gt_tlb_invalidation_init - Initialize GT TLB invalidation state\n * @gt: graphics tile\n *\n * Initialize GT TLB invalidation state, purely software initialization, should\n * be called once during driver load.\n *\n * Return: 0 on success, negative error code on error.\n */\nint xe_gt_tlb_invalidation_init(struct xe_gt *gt)\n{\n\tgt->tlb_invalidation.seqno = 1;\n\tINIT_LIST_HEAD(&gt->tlb_invalidation.pending_fences);\n\tspin_lock_init(&gt->tlb_invalidation.pending_lock);\n\tspin_lock_init(&gt->tlb_invalidation.lock);\n\tINIT_DELAYED_WORK(&gt->tlb_invalidation.fence_tdr,\n\t\t\t  xe_gt_tlb_fence_timeout);\n\n\treturn 0;\n}\n\n/**\n * xe_gt_tlb_invalidation_reset - Initialize GT TLB invalidation reset\n * @gt: graphics tile\n *\n * Signal any pending invalidation fences, should be called during a GT reset\n */\nvoid xe_gt_tlb_invalidation_reset(struct xe_gt *gt)\n{\n\tstruct xe_gt_tlb_invalidation_fence *fence, *next;\n\tint pending_seqno;\n\n\t/*\n\t * CT channel is already disabled at this point. No new TLB requests can\n\t * appear.\n\t */\n\n\tmutex_lock(&gt->uc.guc.ct.lock);\n\tspin_lock_irq(&gt->tlb_invalidation.pending_lock);\n\tcancel_delayed_work(&gt->tlb_invalidation.fence_tdr);\n\t/*\n\t * We might have various kworkers waiting for TLB flushes to complete\n\t * which are not tracked with an explicit TLB fence, however at this\n\t * stage that will never happen since the CT is already disabled, so\n\t * make sure we signal them here under the assumption that we have\n\t * completed a full GT reset.\n\t */\n\tif (gt->tlb_invalidation.seqno == 1)\n\t\tpending_seqno = TLB_INVALIDATION_SEQNO_MAX - 1;\n\telse\n\t\tpending_seqno = gt->tlb_invalidation.seqno - 1;\n\tWRITE_ONCE(gt->tlb_invalidation.seqno_recv, pending_seqno);\n\n\tlist_for_each_entry_safe(fence, next,\n\t\t\t\t &gt->tlb_invalidation.pending_fences, link)\n\t\tinvalidation_fence_signal(gt_to_xe(gt), fence);\n\tspin_unlock_irq(&gt->tlb_invalidation.pending_lock);\n\tmutex_unlock(&gt->uc.guc.ct.lock);\n}\n\nstatic bool tlb_invalidation_seqno_past(struct xe_gt *gt, int seqno)\n{\n\tint seqno_recv = READ_ONCE(gt->tlb_invalidation.seqno_recv);\n\n\tif (seqno - seqno_recv < -(TLB_INVALIDATION_SEQNO_MAX / 2))\n\t\treturn false;\n\n\tif (seqno - seqno_recv > (TLB_INVALIDATION_SEQNO_MAX / 2))\n\t\treturn true;\n\n\treturn seqno_recv >= seqno;\n}\n\nstatic int send_tlb_invalidation(struct xe_guc *guc,\n\t\t\t\t struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t\t u32 *action, int len)\n{\n\tstruct xe_gt *gt = guc_to_gt(guc);\n\tstruct xe_device *xe = gt_to_xe(gt);\n\tint seqno;\n\tint ret;\n\n\txe_gt_assert(gt, fence);\n\n\t/*\n\t * XXX: The seqno algorithm relies on TLB invalidation being processed\n\t * in order which they currently are, if that changes the algorithm will\n\t * need to be updated.\n\t */\n\n\tmutex_lock(&guc->ct.lock);\n\tseqno = gt->tlb_invalidation.seqno;\n\tfence->seqno = seqno;\n\ttrace_xe_gt_tlb_invalidation_fence_send(xe, fence);\n\taction[1] = seqno;\n\tret = xe_guc_ct_send_locked(&guc->ct, action, len,\n\t\t\t\t    G2H_LEN_DW_TLB_INVALIDATE, 1);\n\tif (!ret) {\n\t\tspin_lock_irq(&gt->tlb_invalidation.pending_lock);\n\t\t/*\n\t\t * We haven't actually published the TLB fence as per\n\t\t * pending_fences, but in theory our seqno could have already\n\t\t * been written as we acquired the pending_lock. In such a case\n\t\t * we can just go ahead and signal the fence here.\n\t\t */\n\t\tif (tlb_invalidation_seqno_past(gt, seqno)) {\n\t\t\t__invalidation_fence_signal(xe, fence);\n\t\t} else {\n\t\t\tfence->invalidation_time = ktime_get();\n\t\t\tlist_add_tail(&fence->link,\n\t\t\t\t      &gt->tlb_invalidation.pending_fences);\n\n\t\t\tif (list_is_singular(&gt->tlb_invalidation.pending_fences))\n\t\t\t\tqueue_delayed_work(system_wq,\n\t\t\t\t\t\t   &gt->tlb_invalidation.fence_tdr,\n\t\t\t\t\t\t   tlb_timeout_jiffies(gt));\n\t\t}\n\t\tspin_unlock_irq(&gt->tlb_invalidation.pending_lock);\n\t} else {\n\t\t__invalidation_fence_signal(xe, fence);\n\t}\n\tif (!ret) {\n\t\tgt->tlb_invalidation.seqno = (gt->tlb_invalidation.seqno + 1) %\n\t\t\tTLB_INVALIDATION_SEQNO_MAX;\n\t\tif (!gt->tlb_invalidation.seqno)\n\t\t\tgt->tlb_invalidation.seqno = 1;\n\t}\n\tmutex_unlock(&guc->ct.lock);\n\txe_gt_stats_incr(gt, XE_GT_STATS_ID_TLB_INVAL, 1);\n\n\treturn ret;\n}\n\n#define MAKE_INVAL_OP(type)\t((type << XE_GUC_TLB_INVAL_TYPE_SHIFT) | \\\n\t\tXE_GUC_TLB_INVAL_MODE_HEAVY << XE_GUC_TLB_INVAL_MODE_SHIFT | \\\n\t\tXE_GUC_TLB_INVAL_FLUSH_CACHE)\n\n/**\n * xe_gt_tlb_invalidation_guc - Issue a TLB invalidation on this GT for the GuC\n * @gt: graphics tile\n * @fence: invalidation fence which will be signal on TLB invalidation\n * completion\n *\n * Issue a TLB invalidation for the GuC. Completion of TLB is asynchronous and\n * caller can use the invalidation fence to wait for completion.\n *\n * Return: 0 on success, negative error code on error\n */\nstatic int xe_gt_tlb_invalidation_guc(struct xe_gt *gt,\n\t\t\t\t      struct xe_gt_tlb_invalidation_fence *fence)\n{\n\tu32 action[] = {\n\t\tXE_GUC_ACTION_TLB_INVALIDATION,\n\t\t0,  /* seqno, replaced in send_tlb_invalidation */\n\t\tMAKE_INVAL_OP(XE_GUC_TLB_INVAL_GUC),\n\t};\n\n\treturn send_tlb_invalidation(&gt->uc.guc, fence, action,\n\t\t\t\t     ARRAY_SIZE(action));\n}\n\n/**\n * xe_gt_tlb_invalidation_ggtt - Issue a TLB invalidation on this GT for the GGTT\n * @gt: graphics tile\n *\n * Issue a TLB invalidation for the GGTT. Completion of TLB invalidation is\n * synchronous.\n *\n * Return: 0 on success, negative error code on error\n */\nint xe_gt_tlb_invalidation_ggtt(struct xe_gt *gt)\n{\n\tstruct xe_device *xe = gt_to_xe(gt);\n\n\tif (xe_guc_ct_enabled(&gt->uc.guc.ct) &&\n\t    gt->uc.guc.submission_state.enabled) {\n\t\tstruct xe_gt_tlb_invalidation_fence fence;\n\t\tint ret;\n\n\t\txe_gt_tlb_invalidation_fence_init(gt, &fence, true);\n\t\tret = xe_gt_tlb_invalidation_guc(gt, &fence);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\txe_gt_tlb_invalidation_fence_wait(&fence);\n\t} else if (xe_device_uc_enabled(xe) && !xe_device_wedged(xe)) {\n\t\tif (IS_SRIOV_VF(xe))\n\t\t\treturn 0;\n\n\t\txe_gt_WARN_ON(gt, xe_force_wake_get(gt_to_fw(gt), XE_FW_GT));\n\t\tif (xe->info.platform == XE_PVC || GRAPHICS_VER(xe) >= 20) {\n\t\t\txe_mmio_write32(gt, PVC_GUC_TLB_INV_DESC1,\n\t\t\t\t\tPVC_GUC_TLB_INV_DESC1_INVALIDATE);\n\t\t\txe_mmio_write32(gt, PVC_GUC_TLB_INV_DESC0,\n\t\t\t\t\tPVC_GUC_TLB_INV_DESC0_VALID);\n\t\t} else {\n\t\t\txe_mmio_write32(gt, GUC_TLB_INV_CR,\n\t\t\t\t\tGUC_TLB_INV_CR_INVALIDATE);\n\t\t}\n\t\txe_force_wake_put(gt_to_fw(gt), XE_FW_GT);\n\t}\n\n\treturn 0;\n}\n\n/**\n * xe_gt_tlb_invalidation_range - Issue a TLB invalidation on this GT for an\n * address range\n *\n * @gt: graphics tile\n * @fence: invalidation fence which will be signal on TLB invalidation\n * completion\n * @start: start address\n * @end: end address\n * @asid: address space id\n *\n * Issue a range based TLB invalidation if supported, if not fallback to a full\n * TLB invalidation. Completion of TLB is asynchronous and caller can use\n * the invalidation fence to wait for completion.\n *\n * Return: Negative error code on error, 0 on success\n */\nint xe_gt_tlb_invalidation_range(struct xe_gt *gt,\n\t\t\t\t struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t\t u64 start, u64 end, u32 asid)\n{\n\tstruct xe_device *xe = gt_to_xe(gt);\n#define MAX_TLB_INVALIDATION_LEN\t7\n\tu32 action[MAX_TLB_INVALIDATION_LEN];\n\tint len = 0;\n\n\txe_gt_assert(gt, fence);\n\n\t/* Execlists not supported */\n\tif (gt_to_xe(gt)->info.force_execlist) {\n\t\t__invalidation_fence_signal(xe, fence);\n\t\treturn 0;\n\t}\n\n\taction[len++] = XE_GUC_ACTION_TLB_INVALIDATION;\n\taction[len++] = 0; /* seqno, replaced in send_tlb_invalidation */\n\tif (!xe->info.has_range_tlb_invalidation) {\n\t\taction[len++] = MAKE_INVAL_OP(XE_GUC_TLB_INVAL_FULL);\n\t} else {\n\t\tu64 orig_start = start;\n\t\tu64 length = end - start;\n\t\tu64 align;\n\n\t\tif (length < SZ_4K)\n\t\t\tlength = SZ_4K;\n\n\t\t/*\n\t\t * We need to invalidate a higher granularity if start address\n\t\t * is not aligned to length. When start is not aligned with\n\t\t * length we need to find the length large enough to create an\n\t\t * address mask covering the required range.\n\t\t */\n\t\talign = roundup_pow_of_two(length);\n\t\tstart = ALIGN_DOWN(start, align);\n\t\tend = ALIGN(end, align);\n\t\tlength = align;\n\t\twhile (start + length < end) {\n\t\t\tlength <<= 1;\n\t\t\tstart = ALIGN_DOWN(orig_start, length);\n\t\t}\n\n\t\t/*\n\t\t * Minimum invalidation size for a 2MB page that the hardware\n\t\t * expects is 16MB\n\t\t */\n\t\tif (length >= SZ_2M) {\n\t\t\tlength = max_t(u64, SZ_16M, length);\n\t\t\tstart = ALIGN_DOWN(orig_start, length);\n\t\t}\n\n\t\txe_gt_assert(gt, length >= SZ_4K);\n\t\txe_gt_assert(gt, is_power_of_2(length));\n\t\txe_gt_assert(gt, !(length & GENMASK(ilog2(SZ_16M) - 1,\n\t\t\t\t\t\t    ilog2(SZ_2M) + 1)));\n\t\txe_gt_assert(gt, IS_ALIGNED(start, length));\n\n\t\taction[len++] = MAKE_INVAL_OP(XE_GUC_TLB_INVAL_PAGE_SELECTIVE);\n\t\taction[len++] = asid;\n\t\taction[len++] = lower_32_bits(start);\n\t\taction[len++] = upper_32_bits(start);\n\t\taction[len++] = ilog2(length) - ilog2(SZ_4K);\n\t}\n\n\txe_gt_assert(gt, len <= MAX_TLB_INVALIDATION_LEN);\n\n\treturn send_tlb_invalidation(&gt->uc.guc, fence, action, len);\n}\n\n/**\n * xe_gt_tlb_invalidation_vma - Issue a TLB invalidation on this GT for a VMA\n * @gt: graphics tile\n * @fence: invalidation fence which will be signal on TLB invalidation\n * completion, can be NULL\n * @vma: VMA to invalidate\n *\n * Issue a range based TLB invalidation if supported, if not fallback to a full\n * TLB invalidation. Completion of TLB is asynchronous and caller can use\n * the invalidation fence to wait for completion.\n *\n * Return: Negative error code on error, 0 on success\n */\nint xe_gt_tlb_invalidation_vma(struct xe_gt *gt,\n\t\t\t       struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t       struct xe_vma *vma)\n{\n\txe_gt_assert(gt, vma);\n\n\treturn xe_gt_tlb_invalidation_range(gt, fence, xe_vma_start(vma),\n\t\t\t\t\t    xe_vma_end(vma),\n\t\t\t\t\t    xe_vma_vm(vma)->usm.asid);\n}\n\n/**\n * xe_guc_tlb_invalidation_done_handler - TLB invalidation done handler\n * @guc: guc\n * @msg: message indicating TLB invalidation done\n * @len: length of message\n *\n * Parse seqno of TLB invalidation, wake any waiters for seqno, and signal any\n * invalidation fences for seqno. Algorithm for this depends on seqno being\n * received in-order and asserts this assumption.\n *\n * Return: 0 on success, -EPROTO for malformed messages.\n */\nint xe_guc_tlb_invalidation_done_handler(struct xe_guc *guc, u32 *msg, u32 len)\n{\n\tstruct xe_gt *gt = guc_to_gt(guc);\n\tstruct xe_device *xe = gt_to_xe(gt);\n\tstruct xe_gt_tlb_invalidation_fence *fence, *next;\n\tunsigned long flags;\n\n\tif (unlikely(len != 1))\n\t\treturn -EPROTO;\n\n\t/*\n\t * This can also be run both directly from the IRQ handler and also in\n\t * process_g2h_msg(). Only one may process any individual CT message,\n\t * however the order they are processed here could result in skipping a\n\t * seqno. To handle that we just process all the seqnos from the last\n\t * seqno_recv up to and including the one in msg[0]. The delta should be\n\t * very small so there shouldn't be much of pending_fences we actually\n\t * need to iterate over here.\n\t *\n\t * From GuC POV we expect the seqnos to always appear in-order, so if we\n\t * see something later in the timeline we can be sure that anything\n\t * appearing earlier has already signalled, just that we have yet to\n\t * officially process the CT message like if racing against\n\t * process_g2h_msg().\n\t */\n\tspin_lock_irqsave(&gt->tlb_invalidation.pending_lock, flags);\n\tif (tlb_invalidation_seqno_past(gt, msg[0])) {\n\t\tspin_unlock_irqrestore(&gt->tlb_invalidation.pending_lock, flags);\n\t\treturn 0;\n\t}\n\n\tWRITE_ONCE(gt->tlb_invalidation.seqno_recv, msg[0]);\n\n\tlist_for_each_entry_safe(fence, next,\n\t\t\t\t &gt->tlb_invalidation.pending_fences, link) {\n\t\ttrace_xe_gt_tlb_invalidation_fence_recv(xe, fence);\n\n\t\tif (!tlb_invalidation_seqno_past(gt, fence->seqno))\n\t\t\tbreak;\n\n\t\tinvalidation_fence_signal(xe, fence);\n\t}\n\n\tif (!list_empty(&gt->tlb_invalidation.pending_fences))\n\t\tmod_delayed_work(system_wq,\n\t\t\t\t &gt->tlb_invalidation.fence_tdr,\n\t\t\t\t tlb_timeout_jiffies(gt));\n\telse\n\t\tcancel_delayed_work(&gt->tlb_invalidation.fence_tdr);\n\n\tspin_unlock_irqrestore(&gt->tlb_invalidation.pending_lock, flags);\n\n\treturn 0;\n}\n\nstatic const char *\ninvalidation_fence_get_driver_name(struct dma_fence *dma_fence)\n{\n\treturn \"xe\";\n}\n\nstatic const char *\ninvalidation_fence_get_timeline_name(struct dma_fence *dma_fence)\n{\n\treturn \"invalidation_fence\";\n}\n\nstatic const struct dma_fence_ops invalidation_fence_ops = {\n\t.get_driver_name = invalidation_fence_get_driver_name,\n\t.get_timeline_name = invalidation_fence_get_timeline_name,\n};\n\n/**\n * xe_gt_tlb_invalidation_fence_init - Initialize TLB invalidation fence\n * @gt: GT\n * @fence: TLB invalidation fence to initialize\n * @stack: fence is stack variable\n *\n * Initialize TLB invalidation fence for use. xe_gt_tlb_invalidation_fence_fini\n * will be automatically called when fence is signalled (all fences must signal),\n * even on error.\n */\nvoid xe_gt_tlb_invalidation_fence_init(struct xe_gt *gt,\n\t\t\t\t       struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t\t       bool stack)\n{\n\txe_pm_runtime_get_noresume(gt_to_xe(gt));\n\n\tspin_lock_irq(&gt->tlb_invalidation.lock);\n\tdma_fence_init(&fence->base, &invalidation_fence_ops,\n\t\t       &gt->tlb_invalidation.lock,\n\t\t       dma_fence_context_alloc(1), 1);\n\tspin_unlock_irq(&gt->tlb_invalidation.lock);\n\tINIT_LIST_HEAD(&fence->link);\n\tif (stack)\n\t\tset_bit(FENCE_STACK_BIT, &fence->base.flags);\n\telse\n\t\tdma_fence_get(&fence->base);\n\tfence->gt = gt;\n}\n",
                        "drivers/gpu/drm/xe/xe_gt_tlb_invalidation.h": "/* SPDX-License-Identifier: MIT */\n/*\n * Copyright \u00a9 2023 Intel Corporation\n */\n\n#ifndef _XE_GT_TLB_INVALIDATION_H_\n#define _XE_GT_TLB_INVALIDATION_H_\n\n#include <linux/types.h>\n\n#include \"xe_gt_tlb_invalidation_types.h\"\n\nstruct xe_gt;\nstruct xe_guc;\nstruct xe_vma;\n\nint xe_gt_tlb_invalidation_init(struct xe_gt *gt);\nvoid xe_gt_tlb_invalidation_reset(struct xe_gt *gt);\nint xe_gt_tlb_invalidation_ggtt(struct xe_gt *gt);\nint xe_gt_tlb_invalidation_vma(struct xe_gt *gt,\n\t\t\t       struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t       struct xe_vma *vma);\nint xe_gt_tlb_invalidation_range(struct xe_gt *gt,\n\t\t\t\t struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t\t u64 start, u64 end, u32 asid);\nint xe_guc_tlb_invalidation_done_handler(struct xe_guc *guc, u32 *msg, u32 len);\n\nvoid xe_gt_tlb_invalidation_fence_init(struct xe_gt *gt,\n\t\t\t\t       struct xe_gt_tlb_invalidation_fence *fence,\n\t\t\t\t       bool stack);\nvoid xe_gt_tlb_invalidation_fence_signal(struct xe_gt_tlb_invalidation_fence *fence);\n\nstatic inline void\nxe_gt_tlb_invalidation_fence_wait(struct xe_gt_tlb_invalidation_fence *fence)\n{\n\tdma_fence_wait(&fence->base, false);\n}\n\n#endif\t/* _XE_GT_TLB_INVALIDATION_ */\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21645",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21645.json",
            "patch_attempts": [
                {
                    "upstream_commit": "7e16ae558a87ac9099b6a93a43f19b42d809fd78",
                    "upstream_commit_date": "2025-01-07 17:25:18 +0200",
                    "upstream_patch": "dd410d784402c5775f66faf8b624e85e41c38aaf",
                    "total_versions_tested": 2,
                    "successful_patches": 2,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "5cc621085e2b7a9b1905a98f8e5a86bb4aea2016",
                            "downstream_commit": "7673030efe0f8ca1056d3849d61784c6caa052af",
                            "commit_date": "2025-01-17 13:36:16 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file drivers/platform/x86/amd/pmc/pmc.c\nHunk #1 succeeded at 878 (offset -69 lines).\nHunk #2 succeeded at 894 (offset -69 lines).",
                            "downstream_patch_content": "commit 5cc621085e2b7a9b1905a98f8e5a86bb4aea2016\nAuthor: Maciej S. Szmigiero <mail@maciej.szmigiero.name>\nDate:   Mon Jan 6 18:40:34 2025 +0100\n\n    platform/x86/amd/pmc: Only disable IRQ1 wakeup where i8042 actually enabled it\n    \n    [ Upstream commit dd410d784402c5775f66faf8b624e85e41c38aaf ]\n    \n    Wakeup for IRQ1 should be disabled only in cases where i8042 had\n    actually enabled it, otherwise \"wake_depth\" for this IRQ will try to\n    drop below zero and there will be an unpleasant WARN() logged:\n    \n    kernel: atkbd serio0: Disabling IRQ1 wakeup source to avoid platform firmware bug\n    kernel: ------------[ cut here ]------------\n    kernel: Unbalanced IRQ 1 wake disable\n    kernel: WARNING: CPU: 10 PID: 6431 at kernel/irq/manage.c:920 irq_set_irq_wake+0x147/0x1a0\n    \n    The PMC driver uses DEFINE_SIMPLE_DEV_PM_OPS() to define its dev_pm_ops\n    which sets amd_pmc_suspend_handler() to the .suspend, .freeze, and\n    .poweroff handlers. i8042_pm_suspend(), however, is only set as\n    the .suspend handler.\n    \n    Fix the issue by call PMC suspend handler only from the same set of\n    dev_pm_ops handlers as i8042_pm_suspend(), which currently means just\n    the .suspend handler.\n    \n    To reproduce this issue try hibernating (S4) the machine after a fresh boot\n    without putting it into s2idle first.\n    \n    Fixes: 8e60615e8932 (\"platform/x86/amd: pmc: Disable IRQ1 wakeup for RN/CZN\")\n    Reviewed-by: Mario Limonciello <mario.limonciello@amd.com>\n    Signed-off-by: Maciej S. Szmigiero <mail@maciej.szmigiero.name>\n    Link: https://lore.kernel.org/r/c8f28c002ca3c66fbeeb850904a1f43118e17200.1736184606.git.mail@maciej.szmigiero.name\n    [ij: edited the commit message.]\n    Reviewed-by: Ilpo J\u00e4rvinen <ilpo.jarvinen@linux.intel.com>\n    Signed-off-by: Ilpo J\u00e4rvinen <ilpo.jarvinen@linux.intel.com>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/drivers/platform/x86/amd/pmc/pmc.c b/drivers/platform/x86/amd/pmc/pmc.c\nindex f49b1bb258c7..70907e8f3ea9 100644\n--- a/drivers/platform/x86/amd/pmc/pmc.c\n+++ b/drivers/platform/x86/amd/pmc/pmc.c\n@@ -878,6 +878,10 @@ static int amd_pmc_suspend_handler(struct device *dev)\n {\n \tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n \n+\t/*\n+\t * Must be called only from the same set of dev_pm_ops handlers\n+\t * as i8042_pm_suspend() is called: currently just from .suspend.\n+\t */\n \tif (pdev->disable_8042_wakeup && !disable_workarounds) {\n \t\tint rc = amd_pmc_wa_irq1(pdev);\n \n@@ -890,7 +894,9 @@ static int amd_pmc_suspend_handler(struct device *dev)\n \treturn 0;\n }\n \n-static DEFINE_SIMPLE_DEV_PM_OPS(amd_pmc_pm, amd_pmc_suspend_handler, NULL);\n+static const struct dev_pm_ops amd_pmc_pm = {\n+\t.suspend = amd_pmc_suspend_handler,\n+};\n \n static const struct pci_device_id pmc_pci_ids[] = {\n \t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PS) },\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "b25778c87a6bce40c31e92364f08aa6240309e25",
                            "downstream_commit": "7922b1f058fe24a93730511dd0ae2e1630920096",
                            "commit_date": "2025-01-17 13:40:43 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file drivers/platform/x86/amd/pmc/pmc.c",
                            "downstream_patch_content": "commit b25778c87a6bce40c31e92364f08aa6240309e25\nAuthor: Maciej S. Szmigiero <mail@maciej.szmigiero.name>\nDate:   Mon Jan 6 18:40:34 2025 +0100\n\n    platform/x86/amd/pmc: Only disable IRQ1 wakeup where i8042 actually enabled it\n    \n    [ Upstream commit dd410d784402c5775f66faf8b624e85e41c38aaf ]\n    \n    Wakeup for IRQ1 should be disabled only in cases where i8042 had\n    actually enabled it, otherwise \"wake_depth\" for this IRQ will try to\n    drop below zero and there will be an unpleasant WARN() logged:\n    \n    kernel: atkbd serio0: Disabling IRQ1 wakeup source to avoid platform firmware bug\n    kernel: ------------[ cut here ]------------\n    kernel: Unbalanced IRQ 1 wake disable\n    kernel: WARNING: CPU: 10 PID: 6431 at kernel/irq/manage.c:920 irq_set_irq_wake+0x147/0x1a0\n    \n    The PMC driver uses DEFINE_SIMPLE_DEV_PM_OPS() to define its dev_pm_ops\n    which sets amd_pmc_suspend_handler() to the .suspend, .freeze, and\n    .poweroff handlers. i8042_pm_suspend(), however, is only set as\n    the .suspend handler.\n    \n    Fix the issue by call PMC suspend handler only from the same set of\n    dev_pm_ops handlers as i8042_pm_suspend(), which currently means just\n    the .suspend handler.\n    \n    To reproduce this issue try hibernating (S4) the machine after a fresh boot\n    without putting it into s2idle first.\n    \n    Fixes: 8e60615e8932 (\"platform/x86/amd: pmc: Disable IRQ1 wakeup for RN/CZN\")\n    Reviewed-by: Mario Limonciello <mario.limonciello@amd.com>\n    Signed-off-by: Maciej S. Szmigiero <mail@maciej.szmigiero.name>\n    Link: https://lore.kernel.org/r/c8f28c002ca3c66fbeeb850904a1f43118e17200.1736184606.git.mail@maciej.szmigiero.name\n    [ij: edited the commit message.]\n    Reviewed-by: Ilpo J\u00e4rvinen <ilpo.jarvinen@linux.intel.com>\n    Signed-off-by: Ilpo J\u00e4rvinen <ilpo.jarvinen@linux.intel.com>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/drivers/platform/x86/amd/pmc/pmc.c b/drivers/platform/x86/amd/pmc/pmc.c\nindex 5669f94c3d06..4d3acfe849bf 100644\n--- a/drivers/platform/x86/amd/pmc/pmc.c\n+++ b/drivers/platform/x86/amd/pmc/pmc.c\n@@ -947,6 +947,10 @@ static int amd_pmc_suspend_handler(struct device *dev)\n {\n \tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n \n+\t/*\n+\t * Must be called only from the same set of dev_pm_ops handlers\n+\t * as i8042_pm_suspend() is called: currently just from .suspend.\n+\t */\n \tif (pdev->disable_8042_wakeup && !disable_workarounds) {\n \t\tint rc = amd_pmc_wa_irq1(pdev);\n \n@@ -959,7 +963,9 @@ static int amd_pmc_suspend_handler(struct device *dev)\n \treturn 0;\n }\n \n-static DEFINE_SIMPLE_DEV_PM_OPS(amd_pmc_pm, amd_pmc_suspend_handler, NULL);\n+static const struct dev_pm_ops amd_pmc_pm = {\n+\t.suspend = amd_pmc_suspend_handler,\n+};\n \n static const struct pci_device_id pmc_pci_ids[] = {\n \t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PS) },\n",
                            "downstream_file_content": {
                                "drivers/platform/x86/amd/pmc/pmc.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * AMD SoC Power Management Controller Driver\n *\n * Copyright (c) 2020, Advanced Micro Devices, Inc.\n * All Rights Reserved.\n *\n * Author: Shyam Sundar S K <Shyam-sundar.S-k@amd.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <asm/amd_nb.h>\n#include <linux/acpi.h>\n#include <linux/bitfield.h>\n#include <linux/bits.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/limits.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/rtc.h>\n#include <linux/serio.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\n#include \"pmc.h\"\n\n/* SMU communication registers */\n#define AMD_PMC_REGISTER_RESPONSE\t0x980\n#define AMD_PMC_REGISTER_ARGUMENT\t0x9BC\n\n/* PMC Scratch Registers */\n#define AMD_PMC_SCRATCH_REG_CZN\t\t0x94\n#define AMD_PMC_SCRATCH_REG_YC\t\t0xD14\n#define AMD_PMC_SCRATCH_REG_1AH\t\t0xF14\n\n/* STB Registers */\n#define AMD_PMC_STB_PMI_0\t\t0x03E30600\n#define AMD_PMC_STB_S2IDLE_PREPARE\t0xC6000001\n#define AMD_PMC_STB_S2IDLE_RESTORE\t0xC6000002\n#define AMD_PMC_STB_S2IDLE_CHECK\t0xC6000003\n#define AMD_PMC_STB_DUMMY_PC\t\t0xC6000007\n\n/* STB S2D(Spill to DRAM) has different message port offset */\n#define AMD_S2D_REGISTER_MESSAGE\t0xA20\n#define AMD_S2D_REGISTER_RESPONSE\t0xA80\n#define AMD_S2D_REGISTER_ARGUMENT\t0xA88\n\n/* STB Spill to DRAM Parameters */\n#define S2D_TELEMETRY_BYTES_MAX\t\t0x100000U\n#define S2D_RSVD_RAM_SPACE\t\t0x100000\n#define S2D_TELEMETRY_DRAMBYTES_MAX\t0x1000000\n\n/* STB Spill to DRAM Message Definition */\n#define STB_FORCE_FLUSH_DATA\t\t0xCF\n\n/* Base address of SMU for mapping physical address to virtual address */\n#define AMD_PMC_MAPPING_SIZE\t\t0x01000\n#define AMD_PMC_BASE_ADDR_OFFSET\t0x10000\n#define AMD_PMC_BASE_ADDR_LO\t\t0x13B102E8\n#define AMD_PMC_BASE_ADDR_HI\t\t0x13B102EC\n#define AMD_PMC_BASE_ADDR_LO_MASK\tGENMASK(15, 0)\n#define AMD_PMC_BASE_ADDR_HI_MASK\tGENMASK(31, 20)\n\n/* SMU Response Codes */\n#define AMD_PMC_RESULT_OK                    0x01\n#define AMD_PMC_RESULT_CMD_REJECT_BUSY       0xFC\n#define AMD_PMC_RESULT_CMD_REJECT_PREREQ     0xFD\n#define AMD_PMC_RESULT_CMD_UNKNOWN           0xFE\n#define AMD_PMC_RESULT_FAILED                0xFF\n\n/* FCH SSC Registers */\n#define FCH_S0I3_ENTRY_TIME_L_OFFSET\t0x30\n#define FCH_S0I3_ENTRY_TIME_H_OFFSET\t0x34\n#define FCH_S0I3_EXIT_TIME_L_OFFSET\t0x38\n#define FCH_S0I3_EXIT_TIME_H_OFFSET\t0x3C\n#define FCH_SSC_MAPPING_SIZE\t\t0x800\n#define FCH_BASE_PHY_ADDR_LOW\t\t0xFED81100\n#define FCH_BASE_PHY_ADDR_HIGH\t\t0x00000000\n\n/* SMU Message Definations */\n#define SMU_MSG_GETSMUVERSION\t\t0x02\n#define SMU_MSG_LOG_GETDRAM_ADDR_HI\t0x04\n#define SMU_MSG_LOG_GETDRAM_ADDR_LO\t0x05\n#define SMU_MSG_LOG_START\t\t0x06\n#define SMU_MSG_LOG_RESET\t\t0x07\n#define SMU_MSG_LOG_DUMP_DATA\t\t0x08\n#define SMU_MSG_GET_SUP_CONSTRAINTS\t0x09\n\n#define PMC_MSG_DELAY_MIN_US\t\t50\n#define RESPONSE_REGISTER_LOOP_MAX\t20000\n\n#define DELAY_MIN_US\t\t2000\n#define DELAY_MAX_US\t\t3000\n#define FIFO_SIZE\t\t4096\n\nenum amd_pmc_def {\n\tMSG_TEST = 0x01,\n\tMSG_OS_HINT_PCO,\n\tMSG_OS_HINT_RN,\n};\n\nenum s2d_arg {\n\tS2D_TELEMETRY_SIZE = 0x01,\n\tS2D_PHYS_ADDR_LOW,\n\tS2D_PHYS_ADDR_HIGH,\n\tS2D_NUM_SAMPLES,\n\tS2D_DRAM_SIZE,\n};\n\nstruct amd_pmc_stb_v2_data {\n\tsize_t size;\n\tu8 data[] __counted_by(size);\n};\n\nstruct amd_pmc_bit_map {\n\tconst char *name;\n\tu32 bit_mask;\n};\n\nstatic const struct amd_pmc_bit_map soc15_ip_blk[] = {\n\t{\"DISPLAY\",\tBIT(0)},\n\t{\"CPU\",\t\tBIT(1)},\n\t{\"GFX\",\t\tBIT(2)},\n\t{\"VDD\",\t\tBIT(3)},\n\t{\"ACP\",\t\tBIT(4)},\n\t{\"VCN\",\t\tBIT(5)},\n\t{\"ISP\",\t\tBIT(6)},\n\t{\"NBIO\",\tBIT(7)},\n\t{\"DF\",\t\tBIT(8)},\n\t{\"USB3_0\",\tBIT(9)},\n\t{\"USB3_1\",\tBIT(10)},\n\t{\"LAPIC\",\tBIT(11)},\n\t{\"USB3_2\",\tBIT(12)},\n\t{\"USB3_3\",\tBIT(13)},\n\t{\"USB3_4\",\tBIT(14)},\n\t{\"USB4_0\",\tBIT(15)},\n\t{\"USB4_1\",\tBIT(16)},\n\t{\"MPM\",\t\tBIT(17)},\n\t{\"JPEG\",\tBIT(18)},\n\t{\"IPU\",\t\tBIT(19)},\n\t{\"UMSCH\",\tBIT(20)},\n\t{\"VPE\",\t\tBIT(21)},\n\t{}\n};\n\nstatic bool enable_stb;\nmodule_param(enable_stb, bool, 0644);\nMODULE_PARM_DESC(enable_stb, \"Enable the STB debug mechanism\");\n\nstatic bool disable_workarounds;\nmodule_param(disable_workarounds, bool, 0644);\nMODULE_PARM_DESC(disable_workarounds, \"Disable workarounds for platform bugs\");\n\nstatic bool dump_custom_stb;\nmodule_param(dump_custom_stb, bool, 0644);\nMODULE_PARM_DESC(dump_custom_stb, \"Enable to dump full STB buffer\");\n\nstatic struct amd_pmc_dev pmc;\nstatic int amd_pmc_send_cmd(struct amd_pmc_dev *dev, u32 arg, u32 *data, u8 msg, bool ret);\nstatic int amd_pmc_read_stb(struct amd_pmc_dev *dev, u32 *buf);\nstatic int amd_pmc_write_stb(struct amd_pmc_dev *dev, u32 data);\n\nstatic inline u32 amd_pmc_reg_read(struct amd_pmc_dev *dev, int reg_offset)\n{\n\treturn ioread32(dev->regbase + reg_offset);\n}\n\nstatic inline void amd_pmc_reg_write(struct amd_pmc_dev *dev, int reg_offset, u32 val)\n{\n\tiowrite32(val, dev->regbase + reg_offset);\n}\n\nstruct smu_metrics {\n\tu32 table_version;\n\tu32 hint_count;\n\tu32 s0i3_last_entry_status;\n\tu32 timein_s0i2;\n\tu64 timeentering_s0i3_lastcapture;\n\tu64 timeentering_s0i3_totaltime;\n\tu64 timeto_resume_to_os_lastcapture;\n\tu64 timeto_resume_to_os_totaltime;\n\tu64 timein_s0i3_lastcapture;\n\tu64 timein_s0i3_totaltime;\n\tu64 timein_swdrips_lastcapture;\n\tu64 timein_swdrips_totaltime;\n\tu64 timecondition_notmet_lastcapture[32];\n\tu64 timecondition_notmet_totaltime[32];\n} __packed;\n\nstatic int amd_pmc_stb_debugfs_open(struct inode *inode, struct file *filp)\n{\n\tstruct amd_pmc_dev *dev = filp->f_inode->i_private;\n\tu32 size = FIFO_SIZE * sizeof(u32);\n\tu32 *buf;\n\tint rc;\n\n\tbuf = kzalloc(size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\trc = amd_pmc_read_stb(dev, buf);\n\tif (rc) {\n\t\tkfree(buf);\n\t\treturn rc;\n\t}\n\n\tfilp->private_data = buf;\n\treturn rc;\n}\n\nstatic ssize_t amd_pmc_stb_debugfs_read(struct file *filp, char __user *buf, size_t size,\n\t\t\t\t\tloff_t *pos)\n{\n\tif (!filp->private_data)\n\t\treturn -EINVAL;\n\n\treturn simple_read_from_buffer(buf, size, pos, filp->private_data,\n\t\t\t\t       FIFO_SIZE * sizeof(u32));\n}\n\nstatic int amd_pmc_stb_debugfs_release(struct inode *inode, struct file *filp)\n{\n\tkfree(filp->private_data);\n\treturn 0;\n}\n\nstatic const struct file_operations amd_pmc_stb_debugfs_fops = {\n\t.owner = THIS_MODULE,\n\t.open = amd_pmc_stb_debugfs_open,\n\t.read = amd_pmc_stb_debugfs_read,\n\t.release = amd_pmc_stb_debugfs_release,\n};\n\n/* Enhanced STB Firmware Reporting Mechanism */\nstatic int amd_pmc_stb_handle_efr(struct file *filp)\n{\n\tstruct amd_pmc_dev *dev = filp->f_inode->i_private;\n\tstruct amd_pmc_stb_v2_data *stb_data_arr;\n\tu32 fsize;\n\n\tfsize = dev->dram_size - S2D_RSVD_RAM_SPACE;\n\tstb_data_arr = kmalloc(struct_size(stb_data_arr, data, fsize), GFP_KERNEL);\n\tif (!stb_data_arr)\n\t\treturn -ENOMEM;\n\n\tstb_data_arr->size = fsize;\n\tmemcpy_fromio(stb_data_arr->data, dev->stb_virt_addr, fsize);\n\tfilp->private_data = stb_data_arr;\n\n\treturn 0;\n}\n\nstatic int amd_pmc_stb_debugfs_open_v2(struct inode *inode, struct file *filp)\n{\n\tstruct amd_pmc_dev *dev = filp->f_inode->i_private;\n\tu32 fsize, num_samples, val, stb_rdptr_offset = 0;\n\tstruct amd_pmc_stb_v2_data *stb_data_arr;\n\tint ret;\n\n\t/* Write dummy postcode while reading the STB buffer */\n\tret = amd_pmc_write_stb(dev, AMD_PMC_STB_DUMMY_PC);\n\tif (ret)\n\t\tdev_err(dev->dev, \"error writing to STB: %d\\n\", ret);\n\n\t/* Spill to DRAM num_samples uses separate SMU message port */\n\tdev->msg_port = 1;\n\n\tret = amd_pmc_send_cmd(dev, 0, &val, STB_FORCE_FLUSH_DATA, 1);\n\tif (ret)\n\t\tdev_dbg_once(dev->dev, \"S2D force flush not supported: %d\\n\", ret);\n\n\t/*\n\t * We have a custom stb size and the PMFW is supposed to give\n\t * the enhanced dram size. Note that we land here only for the\n\t * platforms that support enhanced dram size reporting.\n\t */\n\tif (dump_custom_stb)\n\t\treturn amd_pmc_stb_handle_efr(filp);\n\n\t/* Get the num_samples to calculate the last push location */\n\tret = amd_pmc_send_cmd(dev, S2D_NUM_SAMPLES, &num_samples, dev->s2d_msg_id, true);\n\t/* Clear msg_port for other SMU operation */\n\tdev->msg_port = 0;\n\tif (ret) {\n\t\tdev_err(dev->dev, \"error: S2D_NUM_SAMPLES not supported : %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tfsize = min(num_samples, S2D_TELEMETRY_BYTES_MAX);\n\tstb_data_arr = kmalloc(struct_size(stb_data_arr, data, fsize), GFP_KERNEL);\n\tif (!stb_data_arr)\n\t\treturn -ENOMEM;\n\n\tstb_data_arr->size = fsize;\n\n\t/*\n\t * Start capturing data from the last push location.\n\t * This is for general cases, where the stb limits\n\t * are meant for standard usage.\n\t */\n\tif (num_samples > S2D_TELEMETRY_BYTES_MAX) {\n\t\t/* First read oldest data starting 1 behind last write till end of ringbuffer */\n\t\tstb_rdptr_offset = num_samples % S2D_TELEMETRY_BYTES_MAX;\n\t\tfsize = S2D_TELEMETRY_BYTES_MAX - stb_rdptr_offset;\n\n\t\tmemcpy_fromio(stb_data_arr->data, dev->stb_virt_addr + stb_rdptr_offset, fsize);\n\t\t/* Second copy the newer samples from offset 0 - last write */\n\t\tmemcpy_fromio(stb_data_arr->data + fsize, dev->stb_virt_addr, stb_rdptr_offset);\n\t} else {\n\t\tmemcpy_fromio(stb_data_arr->data, dev->stb_virt_addr, fsize);\n\t}\n\n\tfilp->private_data = stb_data_arr;\n\n\treturn 0;\n}\n\nstatic ssize_t amd_pmc_stb_debugfs_read_v2(struct file *filp, char __user *buf, size_t size,\n\t\t\t\t\t   loff_t *pos)\n{\n\tstruct amd_pmc_stb_v2_data *data = filp->private_data;\n\n\treturn simple_read_from_buffer(buf, size, pos, data->data, data->size);\n}\n\nstatic int amd_pmc_stb_debugfs_release_v2(struct inode *inode, struct file *filp)\n{\n\tkfree(filp->private_data);\n\treturn 0;\n}\n\nstatic const struct file_operations amd_pmc_stb_debugfs_fops_v2 = {\n\t.owner = THIS_MODULE,\n\t.open = amd_pmc_stb_debugfs_open_v2,\n\t.read = amd_pmc_stb_debugfs_read_v2,\n\t.release = amd_pmc_stb_debugfs_release_v2,\n};\n\nstatic void amd_pmc_get_ip_info(struct amd_pmc_dev *dev)\n{\n\tswitch (dev->cpu_id) {\n\tcase AMD_CPU_ID_PCO:\n\tcase AMD_CPU_ID_RN:\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\t\tdev->num_ips = 12;\n\t\tdev->s2d_msg_id = 0xBE;\n\t\tdev->smu_msg = 0x538;\n\t\tbreak;\n\tcase AMD_CPU_ID_PS:\n\t\tdev->num_ips = 21;\n\t\tdev->s2d_msg_id = 0x85;\n\t\tdev->smu_msg = 0x538;\n\t\tbreak;\n\tcase PCI_DEVICE_ID_AMD_1AH_M20H_ROOT:\n\tcase PCI_DEVICE_ID_AMD_1AH_M60H_ROOT:\n\t\tdev->num_ips = 22;\n\t\tdev->s2d_msg_id = 0xDE;\n\t\tdev->smu_msg = 0x938;\n\t\tbreak;\n\t}\n}\n\nstatic int amd_pmc_setup_smu_logging(struct amd_pmc_dev *dev)\n{\n\tif (dev->cpu_id == AMD_CPU_ID_PCO) {\n\t\tdev_warn_once(dev->dev, \"SMU debugging info not supported on this platform\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Get Active devices list from SMU */\n\tif (!dev->active_ips)\n\t\tamd_pmc_send_cmd(dev, 0, &dev->active_ips, SMU_MSG_GET_SUP_CONSTRAINTS, true);\n\n\t/* Get dram address */\n\tif (!dev->smu_virt_addr) {\n\t\tu32 phys_addr_low, phys_addr_hi;\n\t\tu64 smu_phys_addr;\n\n\t\tamd_pmc_send_cmd(dev, 0, &phys_addr_low, SMU_MSG_LOG_GETDRAM_ADDR_LO, true);\n\t\tamd_pmc_send_cmd(dev, 0, &phys_addr_hi, SMU_MSG_LOG_GETDRAM_ADDR_HI, true);\n\t\tsmu_phys_addr = ((u64)phys_addr_hi << 32 | phys_addr_low);\n\n\t\tdev->smu_virt_addr = devm_ioremap(dev->dev, smu_phys_addr,\n\t\t\t\t\t\t  sizeof(struct smu_metrics));\n\t\tif (!dev->smu_virt_addr)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Start the logging */\n\tamd_pmc_send_cmd(dev, 0, NULL, SMU_MSG_LOG_RESET, false);\n\tamd_pmc_send_cmd(dev, 0, NULL, SMU_MSG_LOG_START, false);\n\n\treturn 0;\n}\n\nstatic int get_metrics_table(struct amd_pmc_dev *pdev, struct smu_metrics *table)\n{\n\tif (!pdev->smu_virt_addr) {\n\t\tint ret = amd_pmc_setup_smu_logging(pdev);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (pdev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn -ENODEV;\n\tmemcpy_fromio(table, pdev->smu_virt_addr, sizeof(struct smu_metrics));\n\treturn 0;\n}\n\nstatic void amd_pmc_validate_deepest(struct amd_pmc_dev *pdev)\n{\n\tstruct smu_metrics table;\n\n\tif (get_metrics_table(pdev, &table))\n\t\treturn;\n\n\tif (!table.s0i3_last_entry_status)\n\t\tdev_warn(pdev->dev, \"Last suspend didn't reach deepest state\\n\");\n\tpm_report_hw_sleep_time(table.s0i3_last_entry_status ?\n\t\t\t\ttable.timein_s0i3_lastcapture : 0);\n}\n\nstatic int amd_pmc_get_smu_version(struct amd_pmc_dev *dev)\n{\n\tint rc;\n\tu32 val;\n\n\tif (dev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn -ENODEV;\n\n\trc = amd_pmc_send_cmd(dev, 0, &val, SMU_MSG_GETSMUVERSION, true);\n\tif (rc)\n\t\treturn rc;\n\n\tdev->smu_program = (val >> 24) & GENMASK(7, 0);\n\tdev->major = (val >> 16) & GENMASK(7, 0);\n\tdev->minor = (val >> 8) & GENMASK(7, 0);\n\tdev->rev = (val >> 0) & GENMASK(7, 0);\n\n\tdev_dbg(dev->dev, \"SMU program %u version is %u.%u.%u\\n\",\n\t\tdev->smu_program, dev->major, dev->minor, dev->rev);\n\n\treturn 0;\n}\n\nstatic ssize_t smu_fw_version_show(struct device *d, struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\tstruct amd_pmc_dev *dev = dev_get_drvdata(d);\n\n\tif (!dev->major) {\n\t\tint rc = amd_pmc_get_smu_version(dev);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn sysfs_emit(buf, \"%u.%u.%u\\n\", dev->major, dev->minor, dev->rev);\n}\n\nstatic ssize_t smu_program_show(struct device *d, struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\tstruct amd_pmc_dev *dev = dev_get_drvdata(d);\n\n\tif (!dev->major) {\n\t\tint rc = amd_pmc_get_smu_version(dev);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn sysfs_emit(buf, \"%u\\n\", dev->smu_program);\n}\n\nstatic DEVICE_ATTR_RO(smu_fw_version);\nstatic DEVICE_ATTR_RO(smu_program);\n\nstatic umode_t pmc_attr_is_visible(struct kobject *kobj, struct attribute *attr, int idx)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n\n\tif (pdev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn 0;\n\treturn 0444;\n}\n\nstatic struct attribute *pmc_attrs[] = {\n\t&dev_attr_smu_fw_version.attr,\n\t&dev_attr_smu_program.attr,\n\tNULL,\n};\n\nstatic struct attribute_group pmc_attr_group = {\n\t.attrs = pmc_attrs,\n\t.is_visible = pmc_attr_is_visible,\n};\n\nstatic const struct attribute_group *pmc_groups[] = {\n\t&pmc_attr_group,\n\tNULL,\n};\n\nstatic int smu_fw_info_show(struct seq_file *s, void *unused)\n{\n\tstruct amd_pmc_dev *dev = s->private;\n\tstruct smu_metrics table;\n\tint idx;\n\n\tif (get_metrics_table(dev, &table))\n\t\treturn -EINVAL;\n\n\tseq_puts(s, \"\\n=== SMU Statistics ===\\n\");\n\tseq_printf(s, \"Table Version: %d\\n\", table.table_version);\n\tseq_printf(s, \"Hint Count: %d\\n\", table.hint_count);\n\tseq_printf(s, \"Last S0i3 Status: %s\\n\", table.s0i3_last_entry_status ? \"Success\" :\n\t\t   \"Unknown/Fail\");\n\tseq_printf(s, \"Time (in us) to S0i3: %lld\\n\", table.timeentering_s0i3_lastcapture);\n\tseq_printf(s, \"Time (in us) in S0i3: %lld\\n\", table.timein_s0i3_lastcapture);\n\tseq_printf(s, \"Time (in us) to resume from S0i3: %lld\\n\",\n\t\t   table.timeto_resume_to_os_lastcapture);\n\n\tseq_puts(s, \"\\n=== Active time (in us) ===\\n\");\n\tfor (idx = 0 ; idx < dev->num_ips ; idx++) {\n\t\tif (soc15_ip_blk[idx].bit_mask & dev->active_ips)\n\t\t\tseq_printf(s, \"%-8s : %lld\\n\", soc15_ip_blk[idx].name,\n\t\t\t\t   table.timecondition_notmet_lastcapture[idx]);\n\t}\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(smu_fw_info);\n\nstatic int s0ix_stats_show(struct seq_file *s, void *unused)\n{\n\tstruct amd_pmc_dev *dev = s->private;\n\tu64 entry_time, exit_time, residency;\n\n\t/* Use FCH registers to get the S0ix stats */\n\tif (!dev->fch_virt_addr) {\n\t\tu32 base_addr_lo = FCH_BASE_PHY_ADDR_LOW;\n\t\tu32 base_addr_hi = FCH_BASE_PHY_ADDR_HIGH;\n\t\tu64 fch_phys_addr = ((u64)base_addr_hi << 32 | base_addr_lo);\n\n\t\tdev->fch_virt_addr = devm_ioremap(dev->dev, fch_phys_addr, FCH_SSC_MAPPING_SIZE);\n\t\tif (!dev->fch_virt_addr)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tentry_time = ioread32(dev->fch_virt_addr + FCH_S0I3_ENTRY_TIME_H_OFFSET);\n\tentry_time = entry_time << 32 | ioread32(dev->fch_virt_addr + FCH_S0I3_ENTRY_TIME_L_OFFSET);\n\n\texit_time = ioread32(dev->fch_virt_addr + FCH_S0I3_EXIT_TIME_H_OFFSET);\n\texit_time = exit_time << 32 | ioread32(dev->fch_virt_addr + FCH_S0I3_EXIT_TIME_L_OFFSET);\n\n\t/* It's in 48MHz. We need to convert it */\n\tresidency = exit_time - entry_time;\n\tdo_div(residency, 48);\n\n\tseq_puts(s, \"=== S0ix statistics ===\\n\");\n\tseq_printf(s, \"S0ix Entry Time: %lld\\n\", entry_time);\n\tseq_printf(s, \"S0ix Exit Time: %lld\\n\", exit_time);\n\tseq_printf(s, \"Residency Time: %lld\\n\", residency);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(s0ix_stats);\n\nstatic int amd_pmc_idlemask_read(struct amd_pmc_dev *pdev, struct device *dev,\n\t\t\t\t struct seq_file *s)\n{\n\tu32 val;\n\tint rc;\n\n\tswitch (pdev->cpu_id) {\n\tcase AMD_CPU_ID_CZN:\n\t\t/* we haven't yet read SMU version */\n\t\tif (!pdev->major) {\n\t\t\trc = amd_pmc_get_smu_version(pdev);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t\tif (pdev->major > 56 || (pdev->major >= 55 && pdev->minor >= 37))\n\t\t\tval = amd_pmc_reg_read(pdev, AMD_PMC_SCRATCH_REG_CZN);\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\tcase AMD_CPU_ID_PS:\n\t\tval = amd_pmc_reg_read(pdev, AMD_PMC_SCRATCH_REG_YC);\n\t\tbreak;\n\tcase PCI_DEVICE_ID_AMD_1AH_M20H_ROOT:\n\tcase PCI_DEVICE_ID_AMD_1AH_M60H_ROOT:\n\t\tval = amd_pmc_reg_read(pdev, AMD_PMC_SCRATCH_REG_1AH);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev)\n\t\tpm_pr_dbg(\"SMU idlemask s0i3: 0x%x\\n\", val);\n\n\tif (s)\n\t\tseq_printf(s, \"SMU idlemask : 0x%x\\n\", val);\n\n\treturn 0;\n}\n\nstatic int amd_pmc_idlemask_show(struct seq_file *s, void *unused)\n{\n\treturn amd_pmc_idlemask_read(s->private, NULL, s);\n}\nDEFINE_SHOW_ATTRIBUTE(amd_pmc_idlemask);\n\nstatic void amd_pmc_dbgfs_unregister(struct amd_pmc_dev *dev)\n{\n\tdebugfs_remove_recursive(dev->dbgfs_dir);\n}\n\nstatic bool amd_pmc_is_stb_supported(struct amd_pmc_dev *dev)\n{\n\tswitch (dev->cpu_id) {\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\tcase AMD_CPU_ID_PS:\n\tcase PCI_DEVICE_ID_AMD_1AH_M20H_ROOT:\n\tcase PCI_DEVICE_ID_AMD_1AH_M60H_ROOT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void amd_pmc_dbgfs_register(struct amd_pmc_dev *dev)\n{\n\tdev->dbgfs_dir = debugfs_create_dir(\"amd_pmc\", NULL);\n\tdebugfs_create_file(\"smu_fw_info\", 0644, dev->dbgfs_dir, dev,\n\t\t\t    &smu_fw_info_fops);\n\tdebugfs_create_file(\"s0ix_stats\", 0644, dev->dbgfs_dir, dev,\n\t\t\t    &s0ix_stats_fops);\n\tdebugfs_create_file(\"amd_pmc_idlemask\", 0644, dev->dbgfs_dir, dev,\n\t\t\t    &amd_pmc_idlemask_fops);\n\t/* Enable STB only when the module_param is set */\n\tif (enable_stb) {\n\t\tif (amd_pmc_is_stb_supported(dev))\n\t\t\tdebugfs_create_file(\"stb_read\", 0644, dev->dbgfs_dir, dev,\n\t\t\t\t\t    &amd_pmc_stb_debugfs_fops_v2);\n\t\telse\n\t\t\tdebugfs_create_file(\"stb_read\", 0644, dev->dbgfs_dir, dev,\n\t\t\t\t\t    &amd_pmc_stb_debugfs_fops);\n\t}\n}\n\nstatic void amd_pmc_dump_registers(struct amd_pmc_dev *dev)\n{\n\tu32 value, message, argument, response;\n\n\tif (dev->msg_port) {\n\t\tmessage = AMD_S2D_REGISTER_MESSAGE;\n\t\targument = AMD_S2D_REGISTER_ARGUMENT;\n\t\tresponse = AMD_S2D_REGISTER_RESPONSE;\n\t} else {\n\t\tmessage = dev->smu_msg;\n\t\targument = AMD_PMC_REGISTER_ARGUMENT;\n\t\tresponse = AMD_PMC_REGISTER_RESPONSE;\n\t}\n\n\tvalue = amd_pmc_reg_read(dev, response);\n\tdev_dbg(dev->dev, \"AMD_%s_REGISTER_RESPONSE:%x\\n\", dev->msg_port ? \"S2D\" : \"PMC\", value);\n\n\tvalue = amd_pmc_reg_read(dev, argument);\n\tdev_dbg(dev->dev, \"AMD_%s_REGISTER_ARGUMENT:%x\\n\", dev->msg_port ? \"S2D\" : \"PMC\", value);\n\n\tvalue = amd_pmc_reg_read(dev, message);\n\tdev_dbg(dev->dev, \"AMD_%s_REGISTER_MESSAGE:%x\\n\", dev->msg_port ? \"S2D\" : \"PMC\", value);\n}\n\nstatic int amd_pmc_send_cmd(struct amd_pmc_dev *dev, u32 arg, u32 *data, u8 msg, bool ret)\n{\n\tint rc;\n\tu32 val, message, argument, response;\n\n\tmutex_lock(&dev->lock);\n\n\tif (dev->msg_port) {\n\t\tmessage = AMD_S2D_REGISTER_MESSAGE;\n\t\targument = AMD_S2D_REGISTER_ARGUMENT;\n\t\tresponse = AMD_S2D_REGISTER_RESPONSE;\n\t} else {\n\t\tmessage = dev->smu_msg;\n\t\targument = AMD_PMC_REGISTER_ARGUMENT;\n\t\tresponse = AMD_PMC_REGISTER_RESPONSE;\n\t}\n\n\t/* Wait until we get a valid response */\n\trc = readx_poll_timeout(ioread32, dev->regbase + response,\n\t\t\t\tval, val != 0, PMC_MSG_DELAY_MIN_US,\n\t\t\t\tPMC_MSG_DELAY_MIN_US * RESPONSE_REGISTER_LOOP_MAX);\n\tif (rc) {\n\t\tdev_err(dev->dev, \"failed to talk to SMU\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\t/* Write zero to response register */\n\tamd_pmc_reg_write(dev, response, 0);\n\n\t/* Write argument into response register */\n\tamd_pmc_reg_write(dev, argument, arg);\n\n\t/* Write message ID to message ID register */\n\tamd_pmc_reg_write(dev, message, msg);\n\n\t/* Wait until we get a valid response */\n\trc = readx_poll_timeout(ioread32, dev->regbase + response,\n\t\t\t\tval, val != 0, PMC_MSG_DELAY_MIN_US,\n\t\t\t\tPMC_MSG_DELAY_MIN_US * RESPONSE_REGISTER_LOOP_MAX);\n\tif (rc) {\n\t\tdev_err(dev->dev, \"SMU response timed out\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (val) {\n\tcase AMD_PMC_RESULT_OK:\n\t\tif (ret) {\n\t\t\t/* PMFW may take longer time to return back the data */\n\t\t\tusleep_range(DELAY_MIN_US, 10 * DELAY_MAX_US);\n\t\t\t*data = amd_pmc_reg_read(dev, argument);\n\t\t}\n\t\tbreak;\n\tcase AMD_PMC_RESULT_CMD_REJECT_BUSY:\n\t\tdev_err(dev->dev, \"SMU not ready. err: 0x%x\\n\", val);\n\t\trc = -EBUSY;\n\t\tgoto out_unlock;\n\tcase AMD_PMC_RESULT_CMD_UNKNOWN:\n\t\tdev_err(dev->dev, \"SMU cmd unknown. err: 0x%x\\n\", val);\n\t\trc = -EINVAL;\n\t\tgoto out_unlock;\n\tcase AMD_PMC_RESULT_CMD_REJECT_PREREQ:\n\tcase AMD_PMC_RESULT_FAILED:\n\tdefault:\n\t\tdev_err(dev->dev, \"SMU cmd failed. err: 0x%x\\n\", val);\n\t\trc = -EIO;\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tmutex_unlock(&dev->lock);\n\tamd_pmc_dump_registers(dev);\n\treturn rc;\n}\n\nstatic int amd_pmc_get_os_hint(struct amd_pmc_dev *dev)\n{\n\tswitch (dev->cpu_id) {\n\tcase AMD_CPU_ID_PCO:\n\t\treturn MSG_OS_HINT_PCO;\n\tcase AMD_CPU_ID_RN:\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\tcase AMD_CPU_ID_PS:\n\tcase PCI_DEVICE_ID_AMD_1AH_M20H_ROOT:\n\tcase PCI_DEVICE_ID_AMD_1AH_M60H_ROOT:\n\t\treturn MSG_OS_HINT_RN;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int amd_pmc_wa_irq1(struct amd_pmc_dev *pdev)\n{\n\tstruct device *d;\n\tint rc;\n\n\t/* cezanne platform firmware has a fix in 64.66.0 */\n\tif (pdev->cpu_id == AMD_CPU_ID_CZN) {\n\t\tif (!pdev->major) {\n\t\t\trc = amd_pmc_get_smu_version(pdev);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\n\t\tif (pdev->major > 64 || (pdev->major == 64 && pdev->minor > 65))\n\t\t\treturn 0;\n\t}\n\n\td = bus_find_device_by_name(&serio_bus, NULL, \"serio0\");\n\tif (!d)\n\t\treturn 0;\n\tif (device_may_wakeup(d)) {\n\t\tdev_info_once(d, \"Disabling IRQ1 wakeup source to avoid platform firmware bug\\n\");\n\t\tdisable_irq_wake(1);\n\t\tdevice_set_wakeup_enable(d, false);\n\t}\n\tput_device(d);\n\n\treturn 0;\n}\n\nstatic int amd_pmc_verify_czn_rtc(struct amd_pmc_dev *pdev, u32 *arg)\n{\n\tstruct rtc_device *rtc_device;\n\ttime64_t then, now, duration;\n\tstruct rtc_wkalrm alarm;\n\tstruct rtc_time tm;\n\tint rc;\n\n\t/* we haven't yet read SMU version */\n\tif (!pdev->major) {\n\t\trc = amd_pmc_get_smu_version(pdev);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (pdev->major < 64 || (pdev->major == 64 && pdev->minor < 53))\n\t\treturn 0;\n\n\trtc_device = rtc_class_open(\"rtc0\");\n\tif (!rtc_device)\n\t\treturn 0;\n\trc = rtc_read_alarm(rtc_device, &alarm);\n\tif (rc)\n\t\treturn rc;\n\tif (!alarm.enabled) {\n\t\tdev_dbg(pdev->dev, \"alarm not enabled\\n\");\n\t\treturn 0;\n\t}\n\trc = rtc_read_time(rtc_device, &tm);\n\tif (rc)\n\t\treturn rc;\n\tthen = rtc_tm_to_time64(&alarm.time);\n\tnow = rtc_tm_to_time64(&tm);\n\tduration = then-now;\n\n\t/* in the past */\n\tif (then < now)\n\t\treturn 0;\n\n\t/* will be stored in upper 16 bits of s0i3 hint argument,\n\t * so timer wakeup from s0i3 is limited to ~18 hours or less\n\t */\n\tif (duration <= 4 || duration > U16_MAX)\n\t\treturn -EINVAL;\n\n\t*arg |= (duration << 16);\n\trc = rtc_alarm_irq_enable(rtc_device, 0);\n\tpm_pr_dbg(\"wakeup timer programmed for %lld seconds\\n\", duration);\n\n\treturn rc;\n}\n\nstatic void amd_pmc_s2idle_prepare(void)\n{\n\tstruct amd_pmc_dev *pdev = &pmc;\n\tint rc;\n\tu8 msg;\n\tu32 arg = 1;\n\n\t/* Reset and Start SMU logging - to monitor the s0i3 stats */\n\tamd_pmc_setup_smu_logging(pdev);\n\n\t/* Activate CZN specific platform bug workarounds */\n\tif (pdev->cpu_id == AMD_CPU_ID_CZN && !disable_workarounds) {\n\t\trc = amd_pmc_verify_czn_rtc(pdev, &arg);\n\t\tif (rc) {\n\t\t\tdev_err(pdev->dev, \"failed to set RTC: %d\\n\", rc);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tmsg = amd_pmc_get_os_hint(pdev);\n\trc = amd_pmc_send_cmd(pdev, arg, NULL, msg, false);\n\tif (rc) {\n\t\tdev_err(pdev->dev, \"suspend failed: %d\\n\", rc);\n\t\treturn;\n\t}\n\n\trc = amd_pmc_write_stb(pdev, AMD_PMC_STB_S2IDLE_PREPARE);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"error writing to STB: %d\\n\", rc);\n}\n\nstatic void amd_pmc_s2idle_check(void)\n{\n\tstruct amd_pmc_dev *pdev = &pmc;\n\tstruct smu_metrics table;\n\tint rc;\n\n\t/* CZN: Ensure that future s0i3 entry attempts at least 10ms passed */\n\tif (pdev->cpu_id == AMD_CPU_ID_CZN && !get_metrics_table(pdev, &table) &&\n\t    table.s0i3_last_entry_status)\n\t\tusleep_range(10000, 20000);\n\n\t/* Dump the IdleMask before we add to the STB */\n\tamd_pmc_idlemask_read(pdev, pdev->dev, NULL);\n\n\trc = amd_pmc_write_stb(pdev, AMD_PMC_STB_S2IDLE_CHECK);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"error writing to STB: %d\\n\", rc);\n}\n\nstatic int amd_pmc_dump_data(struct amd_pmc_dev *pdev)\n{\n\tif (pdev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn -ENODEV;\n\n\treturn amd_pmc_send_cmd(pdev, 0, NULL, SMU_MSG_LOG_DUMP_DATA, false);\n}\n\nstatic void amd_pmc_s2idle_restore(void)\n{\n\tstruct amd_pmc_dev *pdev = &pmc;\n\tint rc;\n\tu8 msg;\n\n\tmsg = amd_pmc_get_os_hint(pdev);\n\trc = amd_pmc_send_cmd(pdev, 0, NULL, msg, false);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"resume failed: %d\\n\", rc);\n\n\t/* Let SMU know that we are looking for stats */\n\tamd_pmc_dump_data(pdev);\n\n\trc = amd_pmc_write_stb(pdev, AMD_PMC_STB_S2IDLE_RESTORE);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"error writing to STB: %d\\n\", rc);\n\n\t/* Notify on failed entry */\n\tamd_pmc_validate_deepest(pdev);\n\n\tamd_pmc_process_restore_quirks(pdev);\n}\n\nstatic struct acpi_s2idle_dev_ops amd_pmc_s2idle_dev_ops = {\n\t.prepare = amd_pmc_s2idle_prepare,\n\t.check = amd_pmc_s2idle_check,\n\t.restore = amd_pmc_s2idle_restore,\n};\n\nstatic int amd_pmc_suspend_handler(struct device *dev)\n{\n\tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n\n\tif (pdev->disable_8042_wakeup && !disable_workarounds) {\n\t\tint rc = amd_pmc_wa_irq1(pdev);\n\n\t\tif (rc) {\n\t\t\tdev_err(pdev->dev, \"failed to adjust keyboard wakeup: %d\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic DEFINE_SIMPLE_DEV_PM_OPS(amd_pmc_pm, amd_pmc_suspend_handler, NULL);\n\nstatic const struct pci_device_id pmc_pci_ids[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PS) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_CB) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_YC) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_CZN) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_RN) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PCO) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_RV) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_SP) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_1AH_M20H_ROOT) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_1AH_M60H_ROOT) },\n\t{ }\n};\n\nstatic int amd_pmc_s2d_init(struct amd_pmc_dev *dev)\n{\n\tu32 phys_addr_low, phys_addr_hi;\n\tu64 stb_phys_addr;\n\tu32 size = 0;\n\tint ret;\n\n\t/* Spill to DRAM feature uses separate SMU message port */\n\tdev->msg_port = 1;\n\n\tamd_pmc_send_cmd(dev, S2D_TELEMETRY_SIZE, &size, dev->s2d_msg_id, true);\n\tif (size != S2D_TELEMETRY_BYTES_MAX)\n\t\treturn -EIO;\n\n\t/* Get DRAM size */\n\tret = amd_pmc_send_cmd(dev, S2D_DRAM_SIZE, &dev->dram_size, dev->s2d_msg_id, true);\n\tif (ret || !dev->dram_size)\n\t\tdev->dram_size = S2D_TELEMETRY_DRAMBYTES_MAX;\n\n\t/* Get STB DRAM address */\n\tamd_pmc_send_cmd(dev, S2D_PHYS_ADDR_LOW, &phys_addr_low, dev->s2d_msg_id, true);\n\tamd_pmc_send_cmd(dev, S2D_PHYS_ADDR_HIGH, &phys_addr_hi, dev->s2d_msg_id, true);\n\n\tif (!phys_addr_hi && !phys_addr_low) {\n\t\tdev_err(dev->dev, \"STB is not enabled on the system; disable enable_stb or contact system vendor\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tstb_phys_addr = ((u64)phys_addr_hi << 32 | phys_addr_low);\n\n\t/* Clear msg_port for other SMU operation */\n\tdev->msg_port = 0;\n\n\tdev->stb_virt_addr = devm_ioremap(dev->dev, stb_phys_addr, dev->dram_size);\n\tif (!dev->stb_virt_addr)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int amd_pmc_write_stb(struct amd_pmc_dev *dev, u32 data)\n{\n\tint err;\n\n\terr = amd_smn_write(0, AMD_PMC_STB_PMI_0, data);\n\tif (err) {\n\t\tdev_err(dev->dev, \"failed to write data in stb: 0x%X\\n\", AMD_PMC_STB_PMI_0);\n\t\treturn pcibios_err_to_errno(err);\n\t}\n\n\treturn 0;\n}\n\nstatic int amd_pmc_read_stb(struct amd_pmc_dev *dev, u32 *buf)\n{\n\tint i, err;\n\n\tfor (i = 0; i < FIFO_SIZE; i++) {\n\t\terr = amd_smn_read(0, AMD_PMC_STB_PMI_0, buf++);\n\t\tif (err) {\n\t\t\tdev_err(dev->dev, \"error reading data from stb: 0x%X\\n\", AMD_PMC_STB_PMI_0);\n\t\t\treturn pcibios_err_to_errno(err);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amd_pmc_probe(struct platform_device *pdev)\n{\n\tstruct amd_pmc_dev *dev = &pmc;\n\tstruct pci_dev *rdev;\n\tu32 base_addr_lo, base_addr_hi;\n\tu64 base_addr;\n\tint err;\n\tu32 val;\n\n\tdev->dev = &pdev->dev;\n\n\trdev = pci_get_domain_bus_and_slot(0, 0, PCI_DEVFN(0, 0));\n\tif (!rdev || !pci_match_id(pmc_pci_ids, rdev)) {\n\t\terr = -ENODEV;\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tdev->cpu_id = rdev->device;\n\n\tif (dev->cpu_id == AMD_CPU_ID_SP) {\n\t\tdev_warn_once(dev->dev, \"S0i3 is not supported on this hardware\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tdev->rdev = rdev;\n\terr = amd_smn_read(0, AMD_PMC_BASE_ADDR_LO, &val);\n\tif (err) {\n\t\tdev_err(dev->dev, \"error reading 0x%x\\n\", AMD_PMC_BASE_ADDR_LO);\n\t\terr = pcibios_err_to_errno(err);\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tbase_addr_lo = val & AMD_PMC_BASE_ADDR_HI_MASK;\n\n\terr = amd_smn_read(0, AMD_PMC_BASE_ADDR_HI, &val);\n\tif (err) {\n\t\tdev_err(dev->dev, \"error reading 0x%x\\n\", AMD_PMC_BASE_ADDR_HI);\n\t\terr = pcibios_err_to_errno(err);\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tbase_addr_hi = val & AMD_PMC_BASE_ADDR_LO_MASK;\n\tbase_addr = ((u64)base_addr_hi << 32 | base_addr_lo);\n\n\tdev->regbase = devm_ioremap(dev->dev, base_addr + AMD_PMC_BASE_ADDR_OFFSET,\n\t\t\t\t    AMD_PMC_MAPPING_SIZE);\n\tif (!dev->regbase) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tmutex_init(&dev->lock);\n\n\t/* Get num of IP blocks within the SoC */\n\tamd_pmc_get_ip_info(dev);\n\n\tif (enable_stb && amd_pmc_is_stb_supported(dev)) {\n\t\terr = amd_pmc_s2d_init(dev);\n\t\tif (err)\n\t\t\tgoto err_pci_dev_put;\n\t}\n\n\tplatform_set_drvdata(pdev, dev);\n\tif (IS_ENABLED(CONFIG_SUSPEND)) {\n\t\terr = acpi_register_lps0_dev(&amd_pmc_s2idle_dev_ops);\n\t\tif (err)\n\t\t\tdev_warn(dev->dev, \"failed to register LPS0 sleep handler, expect increased power consumption\\n\");\n\t\tif (!disable_workarounds)\n\t\t\tamd_pmc_quirks_init(dev);\n\t}\n\n\tamd_pmc_dbgfs_register(dev);\n\tif (IS_ENABLED(CONFIG_AMD_MP2_STB))\n\t\tamd_mp2_stb_init(dev);\n\tpm_report_max_hw_sleep(U64_MAX);\n\treturn 0;\n\nerr_pci_dev_put:\n\tpci_dev_put(rdev);\n\treturn err;\n}\n\nstatic void amd_pmc_remove(struct platform_device *pdev)\n{\n\tstruct amd_pmc_dev *dev = platform_get_drvdata(pdev);\n\n\tif (IS_ENABLED(CONFIG_SUSPEND))\n\t\tacpi_unregister_lps0_dev(&amd_pmc_s2idle_dev_ops);\n\tamd_pmc_dbgfs_unregister(dev);\n\tpci_dev_put(dev->rdev);\n\tif (IS_ENABLED(CONFIG_AMD_MP2_STB))\n\t\tamd_mp2_stb_deinit(dev);\n\tmutex_destroy(&dev->lock);\n}\n\nstatic const struct acpi_device_id amd_pmc_acpi_ids[] = {\n\t{\"AMDI0005\", 0},\n\t{\"AMDI0006\", 0},\n\t{\"AMDI0007\", 0},\n\t{\"AMDI0008\", 0},\n\t{\"AMDI0009\", 0},\n\t{\"AMDI000A\", 0},\n\t{\"AMDI000B\", 0},\n\t{\"AMD0004\", 0},\n\t{\"AMD0005\", 0},\n\t{ }\n};\nMODULE_DEVICE_TABLE(acpi, amd_pmc_acpi_ids);\n\nstatic struct platform_driver amd_pmc_driver = {\n\t.driver = {\n\t\t.name = \"amd_pmc\",\n\t\t.acpi_match_table = amd_pmc_acpi_ids,\n\t\t.dev_groups = pmc_groups,\n\t\t.pm = pm_sleep_ptr(&amd_pmc_pm),\n\t},\n\t.probe = amd_pmc_probe,\n\t.remove_new = amd_pmc_remove,\n};\nmodule_platform_driver(amd_pmc_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"AMD PMC Driver\");\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From dd410d784402c5775f66faf8b624e85e41c38aaf Mon Sep 17 00:00:00 2001\nFrom: \"Maciej S. Szmigiero\" <mail@maciej.szmigiero.name>\nDate: Mon, 6 Jan 2025 18:40:34 +0100\nSubject: [PATCH] platform/x86/amd/pmc: Only disable IRQ1 wakeup where i8042\n actually enabled it\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\n\nWakeup for IRQ1 should be disabled only in cases where i8042 had\nactually enabled it, otherwise \"wake_depth\" for this IRQ will try to\ndrop below zero and there will be an unpleasant WARN() logged:\n\nkernel: atkbd serio0: Disabling IRQ1 wakeup source to avoid platform firmware bug\nkernel: ------------[ cut here ]------------\nkernel: Unbalanced IRQ 1 wake disable\nkernel: WARNING: CPU: 10 PID: 6431 at kernel/irq/manage.c:920 irq_set_irq_wake+0x147/0x1a0\n\nThe PMC driver uses DEFINE_SIMPLE_DEV_PM_OPS() to define its dev_pm_ops\nwhich sets amd_pmc_suspend_handler() to the .suspend, .freeze, and\n.poweroff handlers. i8042_pm_suspend(), however, is only set as\nthe .suspend handler.\n\nFix the issue by call PMC suspend handler only from the same set of\ndev_pm_ops handlers as i8042_pm_suspend(), which currently means just\nthe .suspend handler.\n\nTo reproduce this issue try hibernating (S4) the machine after a fresh boot\nwithout putting it into s2idle first.\n\nFixes: 8e60615e8932 (\"platform/x86/amd: pmc: Disable IRQ1 wakeup for RN/CZN\")\nReviewed-by: Mario Limonciello <mario.limonciello@amd.com>\nSigned-off-by: Maciej S. Szmigiero <mail@maciej.szmigiero.name>\nLink: https://lore.kernel.org/r/c8f28c002ca3c66fbeeb850904a1f43118e17200.1736184606.git.mail@maciej.szmigiero.name\n[ij: edited the commit message.]\nReviewed-by: Ilpo J\u00e4rvinen <ilpo.jarvinen@linux.intel.com>\nSigned-off-by: Ilpo J\u00e4rvinen <ilpo.jarvinen@linux.intel.com>\n---\n drivers/platform/x86/amd/pmc/pmc.c | 8 +++++++-\n 1 file changed, 7 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/platform/x86/amd/pmc/pmc.c b/drivers/platform/x86/amd/pmc/pmc.c\nindex 26b878ee5191..a254debb9256 100644\n--- a/drivers/platform/x86/amd/pmc/pmc.c\n+++ b/drivers/platform/x86/amd/pmc/pmc.c\n@@ -947,6 +947,10 @@ static int amd_pmc_suspend_handler(struct device *dev)\n {\n \tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n \n+\t/*\n+\t * Must be called only from the same set of dev_pm_ops handlers\n+\t * as i8042_pm_suspend() is called: currently just from .suspend.\n+\t */\n \tif (pdev->disable_8042_wakeup && !disable_workarounds) {\n \t\tint rc = amd_pmc_wa_irq1(pdev);\n \n@@ -959,7 +963,9 @@ static int amd_pmc_suspend_handler(struct device *dev)\n \treturn 0;\n }\n \n-static DEFINE_SIMPLE_DEV_PM_OPS(amd_pmc_pm, amd_pmc_suspend_handler, NULL);\n+static const struct dev_pm_ops amd_pmc_pm = {\n+\t.suspend = amd_pmc_suspend_handler,\n+};\n \n static const struct pci_device_id pmc_pci_ids[] = {\n \t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PS) },\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "drivers/platform/x86/amd/pmc/pmc.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * AMD SoC Power Management Controller Driver\n *\n * Copyright (c) 2020, Advanced Micro Devices, Inc.\n * All Rights Reserved.\n *\n * Author: Shyam Sundar S K <Shyam-sundar.S-k@amd.com>\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <asm/amd_nb.h>\n#include <linux/acpi.h>\n#include <linux/bitfield.h>\n#include <linux/bits.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/limits.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/rtc.h>\n#include <linux/serio.h>\n#include <linux/suspend.h>\n#include <linux/seq_file.h>\n#include <linux/uaccess.h>\n\n#include \"pmc.h\"\n\n/* SMU communication registers */\n#define AMD_PMC_REGISTER_MESSAGE\t0x538\n#define AMD_PMC_REGISTER_RESPONSE\t0x980\n#define AMD_PMC_REGISTER_ARGUMENT\t0x9BC\n\n/* PMC Scratch Registers */\n#define AMD_PMC_SCRATCH_REG_CZN\t\t0x94\n#define AMD_PMC_SCRATCH_REG_YC\t\t0xD14\n\n/* STB Registers */\n#define AMD_PMC_STB_PMI_0\t\t0x03E30600\n#define AMD_PMC_STB_S2IDLE_PREPARE\t0xC6000001\n#define AMD_PMC_STB_S2IDLE_RESTORE\t0xC6000002\n#define AMD_PMC_STB_S2IDLE_CHECK\t0xC6000003\n#define AMD_PMC_STB_DUMMY_PC\t\t0xC6000007\n\n/* STB S2D(Spill to DRAM) has different message port offset */\n#define AMD_S2D_REGISTER_MESSAGE\t0xA20\n#define AMD_S2D_REGISTER_RESPONSE\t0xA80\n#define AMD_S2D_REGISTER_ARGUMENT\t0xA88\n\n/* STB Spill to DRAM Parameters */\n#define S2D_TELEMETRY_BYTES_MAX\t\t0x100000\n#define S2D_TELEMETRY_DRAMBYTES_MAX\t0x1000000\n\n/* Base address of SMU for mapping physical address to virtual address */\n#define AMD_PMC_MAPPING_SIZE\t\t0x01000\n#define AMD_PMC_BASE_ADDR_OFFSET\t0x10000\n#define AMD_PMC_BASE_ADDR_LO\t\t0x13B102E8\n#define AMD_PMC_BASE_ADDR_HI\t\t0x13B102EC\n#define AMD_PMC_BASE_ADDR_LO_MASK\tGENMASK(15, 0)\n#define AMD_PMC_BASE_ADDR_HI_MASK\tGENMASK(31, 20)\n\n/* SMU Response Codes */\n#define AMD_PMC_RESULT_OK                    0x01\n#define AMD_PMC_RESULT_CMD_REJECT_BUSY       0xFC\n#define AMD_PMC_RESULT_CMD_REJECT_PREREQ     0xFD\n#define AMD_PMC_RESULT_CMD_UNKNOWN           0xFE\n#define AMD_PMC_RESULT_FAILED                0xFF\n\n/* FCH SSC Registers */\n#define FCH_S0I3_ENTRY_TIME_L_OFFSET\t0x30\n#define FCH_S0I3_ENTRY_TIME_H_OFFSET\t0x34\n#define FCH_S0I3_EXIT_TIME_L_OFFSET\t0x38\n#define FCH_S0I3_EXIT_TIME_H_OFFSET\t0x3C\n#define FCH_SSC_MAPPING_SIZE\t\t0x800\n#define FCH_BASE_PHY_ADDR_LOW\t\t0xFED81100\n#define FCH_BASE_PHY_ADDR_HIGH\t\t0x00000000\n\n/* SMU Message Definations */\n#define SMU_MSG_GETSMUVERSION\t\t0x02\n#define SMU_MSG_LOG_GETDRAM_ADDR_HI\t0x04\n#define SMU_MSG_LOG_GETDRAM_ADDR_LO\t0x05\n#define SMU_MSG_LOG_START\t\t0x06\n#define SMU_MSG_LOG_RESET\t\t0x07\n#define SMU_MSG_LOG_DUMP_DATA\t\t0x08\n#define SMU_MSG_GET_SUP_CONSTRAINTS\t0x09\n\n#define PMC_MSG_DELAY_MIN_US\t\t50\n#define RESPONSE_REGISTER_LOOP_MAX\t20000\n\n#define DELAY_MIN_US\t\t2000\n#define DELAY_MAX_US\t\t3000\n#define FIFO_SIZE\t\t4096\n\nenum amd_pmc_def {\n\tMSG_TEST = 0x01,\n\tMSG_OS_HINT_PCO,\n\tMSG_OS_HINT_RN,\n};\n\nenum s2d_arg {\n\tS2D_TELEMETRY_SIZE = 0x01,\n\tS2D_PHYS_ADDR_LOW,\n\tS2D_PHYS_ADDR_HIGH,\n\tS2D_NUM_SAMPLES,\n\tS2D_DRAM_SIZE,\n};\n\nstruct amd_pmc_bit_map {\n\tconst char *name;\n\tu32 bit_mask;\n};\n\nstatic const struct amd_pmc_bit_map soc15_ip_blk[] = {\n\t{\"DISPLAY\",\tBIT(0)},\n\t{\"CPU\",\t\tBIT(1)},\n\t{\"GFX\",\t\tBIT(2)},\n\t{\"VDD\",\t\tBIT(3)},\n\t{\"ACP\",\t\tBIT(4)},\n\t{\"VCN\",\t\tBIT(5)},\n\t{\"ISP\",\t\tBIT(6)},\n\t{\"NBIO\",\tBIT(7)},\n\t{\"DF\",\t\tBIT(8)},\n\t{\"USB3_0\",\tBIT(9)},\n\t{\"USB3_1\",\tBIT(10)},\n\t{\"LAPIC\",\tBIT(11)},\n\t{\"USB3_2\",\tBIT(12)},\n\t{\"USB3_3\",\tBIT(13)},\n\t{\"USB3_4\",\tBIT(14)},\n\t{\"USB4_0\",\tBIT(15)},\n\t{\"USB4_1\",\tBIT(16)},\n\t{\"MPM\",\t\tBIT(17)},\n\t{\"JPEG\",\tBIT(18)},\n\t{\"IPU\",\t\tBIT(19)},\n\t{\"UMSCH\",\tBIT(20)},\n\t{}\n};\n\nstatic bool enable_stb;\nmodule_param(enable_stb, bool, 0644);\nMODULE_PARM_DESC(enable_stb, \"Enable the STB debug mechanism\");\n\nstatic bool disable_workarounds;\nmodule_param(disable_workarounds, bool, 0644);\nMODULE_PARM_DESC(disable_workarounds, \"Disable workarounds for platform bugs\");\n\nstatic struct amd_pmc_dev pmc;\nstatic int amd_pmc_send_cmd(struct amd_pmc_dev *dev, u32 arg, u32 *data, u8 msg, bool ret);\nstatic int amd_pmc_read_stb(struct amd_pmc_dev *dev, u32 *buf);\nstatic int amd_pmc_write_stb(struct amd_pmc_dev *dev, u32 data);\n\nstatic inline u32 amd_pmc_reg_read(struct amd_pmc_dev *dev, int reg_offset)\n{\n\treturn ioread32(dev->regbase + reg_offset);\n}\n\nstatic inline void amd_pmc_reg_write(struct amd_pmc_dev *dev, int reg_offset, u32 val)\n{\n\tiowrite32(val, dev->regbase + reg_offset);\n}\n\nstruct smu_metrics {\n\tu32 table_version;\n\tu32 hint_count;\n\tu32 s0i3_last_entry_status;\n\tu32 timein_s0i2;\n\tu64 timeentering_s0i3_lastcapture;\n\tu64 timeentering_s0i3_totaltime;\n\tu64 timeto_resume_to_os_lastcapture;\n\tu64 timeto_resume_to_os_totaltime;\n\tu64 timein_s0i3_lastcapture;\n\tu64 timein_s0i3_totaltime;\n\tu64 timein_swdrips_lastcapture;\n\tu64 timein_swdrips_totaltime;\n\tu64 timecondition_notmet_lastcapture[32];\n\tu64 timecondition_notmet_totaltime[32];\n} __packed;\n\nstatic int amd_pmc_stb_debugfs_open(struct inode *inode, struct file *filp)\n{\n\tstruct amd_pmc_dev *dev = filp->f_inode->i_private;\n\tu32 size = FIFO_SIZE * sizeof(u32);\n\tu32 *buf;\n\tint rc;\n\n\tbuf = kzalloc(size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\trc = amd_pmc_read_stb(dev, buf);\n\tif (rc) {\n\t\tkfree(buf);\n\t\treturn rc;\n\t}\n\n\tfilp->private_data = buf;\n\treturn rc;\n}\n\nstatic ssize_t amd_pmc_stb_debugfs_read(struct file *filp, char __user *buf, size_t size,\n\t\t\t\t\tloff_t *pos)\n{\n\tif (!filp->private_data)\n\t\treturn -EINVAL;\n\n\treturn simple_read_from_buffer(buf, size, pos, filp->private_data,\n\t\t\t\t       FIFO_SIZE * sizeof(u32));\n}\n\nstatic int amd_pmc_stb_debugfs_release(struct inode *inode, struct file *filp)\n{\n\tkfree(filp->private_data);\n\treturn 0;\n}\n\nstatic const struct file_operations amd_pmc_stb_debugfs_fops = {\n\t.owner = THIS_MODULE,\n\t.open = amd_pmc_stb_debugfs_open,\n\t.read = amd_pmc_stb_debugfs_read,\n\t.release = amd_pmc_stb_debugfs_release,\n};\n\nstatic int amd_pmc_stb_debugfs_open_v2(struct inode *inode, struct file *filp)\n{\n\tstruct amd_pmc_dev *dev = filp->f_inode->i_private;\n\tu32 *buf, fsize, num_samples, stb_rdptr_offset = 0;\n\tint ret;\n\n\t/* Write dummy postcode while reading the STB buffer */\n\tret = amd_pmc_write_stb(dev, AMD_PMC_STB_DUMMY_PC);\n\tif (ret)\n\t\tdev_err(dev->dev, \"error writing to STB: %d\\n\", ret);\n\n\tbuf = kzalloc(S2D_TELEMETRY_BYTES_MAX, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t/* Spill to DRAM num_samples uses separate SMU message port */\n\tdev->msg_port = 1;\n\n\t/* Get the num_samples to calculate the last push location */\n\tret = amd_pmc_send_cmd(dev, S2D_NUM_SAMPLES, &num_samples, dev->s2d_msg_id, true);\n\t/* Clear msg_port for other SMU operation */\n\tdev->msg_port = 0;\n\tif (ret) {\n\t\tdev_err(dev->dev, \"error: S2D_NUM_SAMPLES not supported : %d\\n\", ret);\n\t\tkfree(buf);\n\t\treturn ret;\n\t}\n\n\t/* Start capturing data from the last push location */\n\tif (num_samples > S2D_TELEMETRY_BYTES_MAX) {\n\t\tfsize  = S2D_TELEMETRY_BYTES_MAX;\n\t\tstb_rdptr_offset = num_samples - fsize;\n\t} else {\n\t\tfsize = num_samples;\n\t\tstb_rdptr_offset = 0;\n\t}\n\n\tmemcpy_fromio(buf, dev->stb_virt_addr + stb_rdptr_offset, fsize);\n\tfilp->private_data = buf;\n\n\treturn 0;\n}\n\nstatic ssize_t amd_pmc_stb_debugfs_read_v2(struct file *filp, char __user *buf, size_t size,\n\t\t\t\t\t   loff_t *pos)\n{\n\tif (!filp->private_data)\n\t\treturn -EINVAL;\n\n\treturn simple_read_from_buffer(buf, size, pos, filp->private_data,\n\t\t\t\t\tS2D_TELEMETRY_BYTES_MAX);\n}\n\nstatic int amd_pmc_stb_debugfs_release_v2(struct inode *inode, struct file *filp)\n{\n\tkfree(filp->private_data);\n\treturn 0;\n}\n\nstatic const struct file_operations amd_pmc_stb_debugfs_fops_v2 = {\n\t.owner = THIS_MODULE,\n\t.open = amd_pmc_stb_debugfs_open_v2,\n\t.read = amd_pmc_stb_debugfs_read_v2,\n\t.release = amd_pmc_stb_debugfs_release_v2,\n};\n\nstatic void amd_pmc_get_ip_info(struct amd_pmc_dev *dev)\n{\n\tswitch (dev->cpu_id) {\n\tcase AMD_CPU_ID_PCO:\n\tcase AMD_CPU_ID_RN:\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\t\tdev->num_ips = 12;\n\t\tdev->s2d_msg_id = 0xBE;\n\t\tbreak;\n\tcase AMD_CPU_ID_PS:\n\t\tdev->num_ips = 21;\n\t\tdev->s2d_msg_id = 0x85;\n\t\tbreak;\n\t}\n}\n\nstatic int amd_pmc_setup_smu_logging(struct amd_pmc_dev *dev)\n{\n\tif (dev->cpu_id == AMD_CPU_ID_PCO) {\n\t\tdev_warn_once(dev->dev, \"SMU debugging info not supported on this platform\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Get Active devices list from SMU */\n\tif (!dev->active_ips)\n\t\tamd_pmc_send_cmd(dev, 0, &dev->active_ips, SMU_MSG_GET_SUP_CONSTRAINTS, true);\n\n\t/* Get dram address */\n\tif (!dev->smu_virt_addr) {\n\t\tu32 phys_addr_low, phys_addr_hi;\n\t\tu64 smu_phys_addr;\n\n\t\tamd_pmc_send_cmd(dev, 0, &phys_addr_low, SMU_MSG_LOG_GETDRAM_ADDR_LO, true);\n\t\tamd_pmc_send_cmd(dev, 0, &phys_addr_hi, SMU_MSG_LOG_GETDRAM_ADDR_HI, true);\n\t\tsmu_phys_addr = ((u64)phys_addr_hi << 32 | phys_addr_low);\n\n\t\tdev->smu_virt_addr = devm_ioremap(dev->dev, smu_phys_addr,\n\t\t\t\t\t\t  sizeof(struct smu_metrics));\n\t\tif (!dev->smu_virt_addr)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/* Start the logging */\n\tamd_pmc_send_cmd(dev, 0, NULL, SMU_MSG_LOG_RESET, false);\n\tamd_pmc_send_cmd(dev, 0, NULL, SMU_MSG_LOG_START, false);\n\n\treturn 0;\n}\n\nstatic int get_metrics_table(struct amd_pmc_dev *pdev, struct smu_metrics *table)\n{\n\tif (!pdev->smu_virt_addr) {\n\t\tint ret = amd_pmc_setup_smu_logging(pdev);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (pdev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn -ENODEV;\n\tmemcpy_fromio(table, pdev->smu_virt_addr, sizeof(struct smu_metrics));\n\treturn 0;\n}\n\nstatic void amd_pmc_validate_deepest(struct amd_pmc_dev *pdev)\n{\n\tstruct smu_metrics table;\n\n\tif (get_metrics_table(pdev, &table))\n\t\treturn;\n\n\tif (!table.s0i3_last_entry_status)\n\t\tdev_warn(pdev->dev, \"Last suspend didn't reach deepest state\\n\");\n\tpm_report_hw_sleep_time(table.s0i3_last_entry_status ?\n\t\t\t\ttable.timein_s0i3_lastcapture : 0);\n}\n\nstatic int amd_pmc_get_smu_version(struct amd_pmc_dev *dev)\n{\n\tint rc;\n\tu32 val;\n\n\tif (dev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn -ENODEV;\n\n\trc = amd_pmc_send_cmd(dev, 0, &val, SMU_MSG_GETSMUVERSION, true);\n\tif (rc)\n\t\treturn rc;\n\n\tdev->smu_program = (val >> 24) & GENMASK(7, 0);\n\tdev->major = (val >> 16) & GENMASK(7, 0);\n\tdev->minor = (val >> 8) & GENMASK(7, 0);\n\tdev->rev = (val >> 0) & GENMASK(7, 0);\n\n\tdev_dbg(dev->dev, \"SMU program %u version is %u.%u.%u\\n\",\n\t\tdev->smu_program, dev->major, dev->minor, dev->rev);\n\n\treturn 0;\n}\n\nstatic ssize_t smu_fw_version_show(struct device *d, struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\tstruct amd_pmc_dev *dev = dev_get_drvdata(d);\n\n\tif (!dev->major) {\n\t\tint rc = amd_pmc_get_smu_version(dev);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn sysfs_emit(buf, \"%u.%u.%u\\n\", dev->major, dev->minor, dev->rev);\n}\n\nstatic ssize_t smu_program_show(struct device *d, struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\tstruct amd_pmc_dev *dev = dev_get_drvdata(d);\n\n\tif (!dev->major) {\n\t\tint rc = amd_pmc_get_smu_version(dev);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn sysfs_emit(buf, \"%u\\n\", dev->smu_program);\n}\n\nstatic DEVICE_ATTR_RO(smu_fw_version);\nstatic DEVICE_ATTR_RO(smu_program);\n\nstatic umode_t pmc_attr_is_visible(struct kobject *kobj, struct attribute *attr, int idx)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n\n\tif (pdev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn 0;\n\treturn 0444;\n}\n\nstatic struct attribute *pmc_attrs[] = {\n\t&dev_attr_smu_fw_version.attr,\n\t&dev_attr_smu_program.attr,\n\tNULL,\n};\n\nstatic struct attribute_group pmc_attr_group = {\n\t.attrs = pmc_attrs,\n\t.is_visible = pmc_attr_is_visible,\n};\n\nstatic const struct attribute_group *pmc_groups[] = {\n\t&pmc_attr_group,\n\tNULL,\n};\n\nstatic int smu_fw_info_show(struct seq_file *s, void *unused)\n{\n\tstruct amd_pmc_dev *dev = s->private;\n\tstruct smu_metrics table;\n\tint idx;\n\n\tif (get_metrics_table(dev, &table))\n\t\treturn -EINVAL;\n\n\tseq_puts(s, \"\\n=== SMU Statistics ===\\n\");\n\tseq_printf(s, \"Table Version: %d\\n\", table.table_version);\n\tseq_printf(s, \"Hint Count: %d\\n\", table.hint_count);\n\tseq_printf(s, \"Last S0i3 Status: %s\\n\", table.s0i3_last_entry_status ? \"Success\" :\n\t\t   \"Unknown/Fail\");\n\tseq_printf(s, \"Time (in us) to S0i3: %lld\\n\", table.timeentering_s0i3_lastcapture);\n\tseq_printf(s, \"Time (in us) in S0i3: %lld\\n\", table.timein_s0i3_lastcapture);\n\tseq_printf(s, \"Time (in us) to resume from S0i3: %lld\\n\",\n\t\t   table.timeto_resume_to_os_lastcapture);\n\n\tseq_puts(s, \"\\n=== Active time (in us) ===\\n\");\n\tfor (idx = 0 ; idx < dev->num_ips ; idx++) {\n\t\tif (soc15_ip_blk[idx].bit_mask & dev->active_ips)\n\t\t\tseq_printf(s, \"%-8s : %lld\\n\", soc15_ip_blk[idx].name,\n\t\t\t\t   table.timecondition_notmet_lastcapture[idx]);\n\t}\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(smu_fw_info);\n\nstatic int s0ix_stats_show(struct seq_file *s, void *unused)\n{\n\tstruct amd_pmc_dev *dev = s->private;\n\tu64 entry_time, exit_time, residency;\n\n\t/* Use FCH registers to get the S0ix stats */\n\tif (!dev->fch_virt_addr) {\n\t\tu32 base_addr_lo = FCH_BASE_PHY_ADDR_LOW;\n\t\tu32 base_addr_hi = FCH_BASE_PHY_ADDR_HIGH;\n\t\tu64 fch_phys_addr = ((u64)base_addr_hi << 32 | base_addr_lo);\n\n\t\tdev->fch_virt_addr = devm_ioremap(dev->dev, fch_phys_addr, FCH_SSC_MAPPING_SIZE);\n\t\tif (!dev->fch_virt_addr)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tentry_time = ioread32(dev->fch_virt_addr + FCH_S0I3_ENTRY_TIME_H_OFFSET);\n\tentry_time = entry_time << 32 | ioread32(dev->fch_virt_addr + FCH_S0I3_ENTRY_TIME_L_OFFSET);\n\n\texit_time = ioread32(dev->fch_virt_addr + FCH_S0I3_EXIT_TIME_H_OFFSET);\n\texit_time = exit_time << 32 | ioread32(dev->fch_virt_addr + FCH_S0I3_EXIT_TIME_L_OFFSET);\n\n\t/* It's in 48MHz. We need to convert it */\n\tresidency = exit_time - entry_time;\n\tdo_div(residency, 48);\n\n\tseq_puts(s, \"=== S0ix statistics ===\\n\");\n\tseq_printf(s, \"S0ix Entry Time: %lld\\n\", entry_time);\n\tseq_printf(s, \"S0ix Exit Time: %lld\\n\", exit_time);\n\tseq_printf(s, \"Residency Time: %lld\\n\", residency);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(s0ix_stats);\n\nstatic int amd_pmc_idlemask_read(struct amd_pmc_dev *pdev, struct device *dev,\n\t\t\t\t struct seq_file *s)\n{\n\tu32 val;\n\tint rc;\n\n\tswitch (pdev->cpu_id) {\n\tcase AMD_CPU_ID_CZN:\n\t\t/* we haven't yet read SMU version */\n\t\tif (!pdev->major) {\n\t\t\trc = amd_pmc_get_smu_version(pdev);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t\tif (pdev->major > 56 || (pdev->major >= 55 && pdev->minor >= 37))\n\t\t\tval = amd_pmc_reg_read(pdev, AMD_PMC_SCRATCH_REG_CZN);\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\tcase AMD_CPU_ID_PS:\n\t\tval = amd_pmc_reg_read(pdev, AMD_PMC_SCRATCH_REG_YC);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev)\n\t\tpm_pr_dbg(\"SMU idlemask s0i3: 0x%x\\n\", val);\n\n\tif (s)\n\t\tseq_printf(s, \"SMU idlemask : 0x%x\\n\", val);\n\n\treturn 0;\n}\n\nstatic int amd_pmc_idlemask_show(struct seq_file *s, void *unused)\n{\n\treturn amd_pmc_idlemask_read(s->private, NULL, s);\n}\nDEFINE_SHOW_ATTRIBUTE(amd_pmc_idlemask);\n\nstatic void amd_pmc_dbgfs_unregister(struct amd_pmc_dev *dev)\n{\n\tdebugfs_remove_recursive(dev->dbgfs_dir);\n}\n\nstatic bool amd_pmc_is_stb_supported(struct amd_pmc_dev *dev)\n{\n\tswitch (dev->cpu_id) {\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\tcase AMD_CPU_ID_PS:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void amd_pmc_dbgfs_register(struct amd_pmc_dev *dev)\n{\n\tdev->dbgfs_dir = debugfs_create_dir(\"amd_pmc\", NULL);\n\tdebugfs_create_file(\"smu_fw_info\", 0644, dev->dbgfs_dir, dev,\n\t\t\t    &smu_fw_info_fops);\n\tdebugfs_create_file(\"s0ix_stats\", 0644, dev->dbgfs_dir, dev,\n\t\t\t    &s0ix_stats_fops);\n\tdebugfs_create_file(\"amd_pmc_idlemask\", 0644, dev->dbgfs_dir, dev,\n\t\t\t    &amd_pmc_idlemask_fops);\n\t/* Enable STB only when the module_param is set */\n\tif (enable_stb) {\n\t\tif (amd_pmc_is_stb_supported(dev))\n\t\t\tdebugfs_create_file(\"stb_read\", 0644, dev->dbgfs_dir, dev,\n\t\t\t\t\t    &amd_pmc_stb_debugfs_fops_v2);\n\t\telse\n\t\t\tdebugfs_create_file(\"stb_read\", 0644, dev->dbgfs_dir, dev,\n\t\t\t\t\t    &amd_pmc_stb_debugfs_fops);\n\t}\n}\n\nstatic void amd_pmc_dump_registers(struct amd_pmc_dev *dev)\n{\n\tu32 value, message, argument, response;\n\n\tif (dev->msg_port) {\n\t\tmessage = AMD_S2D_REGISTER_MESSAGE;\n\t\targument = AMD_S2D_REGISTER_ARGUMENT;\n\t\tresponse = AMD_S2D_REGISTER_RESPONSE;\n\t} else {\n\t\tmessage = AMD_PMC_REGISTER_MESSAGE;\n\t\targument = AMD_PMC_REGISTER_ARGUMENT;\n\t\tresponse = AMD_PMC_REGISTER_RESPONSE;\n\t}\n\n\tvalue = amd_pmc_reg_read(dev, response);\n\tdev_dbg(dev->dev, \"AMD_%s_REGISTER_RESPONSE:%x\\n\", dev->msg_port ? \"S2D\" : \"PMC\", value);\n\n\tvalue = amd_pmc_reg_read(dev, argument);\n\tdev_dbg(dev->dev, \"AMD_%s_REGISTER_ARGUMENT:%x\\n\", dev->msg_port ? \"S2D\" : \"PMC\", value);\n\n\tvalue = amd_pmc_reg_read(dev, message);\n\tdev_dbg(dev->dev, \"AMD_%s_REGISTER_MESSAGE:%x\\n\", dev->msg_port ? \"S2D\" : \"PMC\", value);\n}\n\nstatic int amd_pmc_send_cmd(struct amd_pmc_dev *dev, u32 arg, u32 *data, u8 msg, bool ret)\n{\n\tint rc;\n\tu32 val, message, argument, response;\n\n\tmutex_lock(&dev->lock);\n\n\tif (dev->msg_port) {\n\t\tmessage = AMD_S2D_REGISTER_MESSAGE;\n\t\targument = AMD_S2D_REGISTER_ARGUMENT;\n\t\tresponse = AMD_S2D_REGISTER_RESPONSE;\n\t} else {\n\t\tmessage = AMD_PMC_REGISTER_MESSAGE;\n\t\targument = AMD_PMC_REGISTER_ARGUMENT;\n\t\tresponse = AMD_PMC_REGISTER_RESPONSE;\n\t}\n\n\t/* Wait until we get a valid response */\n\trc = readx_poll_timeout(ioread32, dev->regbase + response,\n\t\t\t\tval, val != 0, PMC_MSG_DELAY_MIN_US,\n\t\t\t\tPMC_MSG_DELAY_MIN_US * RESPONSE_REGISTER_LOOP_MAX);\n\tif (rc) {\n\t\tdev_err(dev->dev, \"failed to talk to SMU\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\t/* Write zero to response register */\n\tamd_pmc_reg_write(dev, response, 0);\n\n\t/* Write argument into response register */\n\tamd_pmc_reg_write(dev, argument, arg);\n\n\t/* Write message ID to message ID register */\n\tamd_pmc_reg_write(dev, message, msg);\n\n\t/* Wait until we get a valid response */\n\trc = readx_poll_timeout(ioread32, dev->regbase + response,\n\t\t\t\tval, val != 0, PMC_MSG_DELAY_MIN_US,\n\t\t\t\tPMC_MSG_DELAY_MIN_US * RESPONSE_REGISTER_LOOP_MAX);\n\tif (rc) {\n\t\tdev_err(dev->dev, \"SMU response timed out\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\tswitch (val) {\n\tcase AMD_PMC_RESULT_OK:\n\t\tif (ret) {\n\t\t\t/* PMFW may take longer time to return back the data */\n\t\t\tusleep_range(DELAY_MIN_US, 10 * DELAY_MAX_US);\n\t\t\t*data = amd_pmc_reg_read(dev, argument);\n\t\t}\n\t\tbreak;\n\tcase AMD_PMC_RESULT_CMD_REJECT_BUSY:\n\t\tdev_err(dev->dev, \"SMU not ready. err: 0x%x\\n\", val);\n\t\trc = -EBUSY;\n\t\tgoto out_unlock;\n\tcase AMD_PMC_RESULT_CMD_UNKNOWN:\n\t\tdev_err(dev->dev, \"SMU cmd unknown. err: 0x%x\\n\", val);\n\t\trc = -EINVAL;\n\t\tgoto out_unlock;\n\tcase AMD_PMC_RESULT_CMD_REJECT_PREREQ:\n\tcase AMD_PMC_RESULT_FAILED:\n\tdefault:\n\t\tdev_err(dev->dev, \"SMU cmd failed. err: 0x%x\\n\", val);\n\t\trc = -EIO;\n\t\tgoto out_unlock;\n\t}\n\nout_unlock:\n\tmutex_unlock(&dev->lock);\n\tamd_pmc_dump_registers(dev);\n\treturn rc;\n}\n\nstatic int amd_pmc_get_os_hint(struct amd_pmc_dev *dev)\n{\n\tswitch (dev->cpu_id) {\n\tcase AMD_CPU_ID_PCO:\n\t\treturn MSG_OS_HINT_PCO;\n\tcase AMD_CPU_ID_RN:\n\tcase AMD_CPU_ID_YC:\n\tcase AMD_CPU_ID_CB:\n\tcase AMD_CPU_ID_PS:\n\t\treturn MSG_OS_HINT_RN;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int amd_pmc_wa_irq1(struct amd_pmc_dev *pdev)\n{\n\tstruct device *d;\n\tint rc;\n\n\t/* cezanne platform firmware has a fix in 64.66.0 */\n\tif (pdev->cpu_id == AMD_CPU_ID_CZN) {\n\t\tif (!pdev->major) {\n\t\t\trc = amd_pmc_get_smu_version(pdev);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\n\t\tif (pdev->major > 64 || (pdev->major == 64 && pdev->minor > 65))\n\t\t\treturn 0;\n\t}\n\n\td = bus_find_device_by_name(&serio_bus, NULL, \"serio0\");\n\tif (!d)\n\t\treturn 0;\n\tif (device_may_wakeup(d)) {\n\t\tdev_info_once(d, \"Disabling IRQ1 wakeup source to avoid platform firmware bug\\n\");\n\t\tdisable_irq_wake(1);\n\t\tdevice_set_wakeup_enable(d, false);\n\t}\n\tput_device(d);\n\n\treturn 0;\n}\n\nstatic int amd_pmc_verify_czn_rtc(struct amd_pmc_dev *pdev, u32 *arg)\n{\n\tstruct rtc_device *rtc_device;\n\ttime64_t then, now, duration;\n\tstruct rtc_wkalrm alarm;\n\tstruct rtc_time tm;\n\tint rc;\n\n\t/* we haven't yet read SMU version */\n\tif (!pdev->major) {\n\t\trc = amd_pmc_get_smu_version(pdev);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (pdev->major < 64 || (pdev->major == 64 && pdev->minor < 53))\n\t\treturn 0;\n\n\trtc_device = rtc_class_open(\"rtc0\");\n\tif (!rtc_device)\n\t\treturn 0;\n\trc = rtc_read_alarm(rtc_device, &alarm);\n\tif (rc)\n\t\treturn rc;\n\tif (!alarm.enabled) {\n\t\tdev_dbg(pdev->dev, \"alarm not enabled\\n\");\n\t\treturn 0;\n\t}\n\trc = rtc_read_time(rtc_device, &tm);\n\tif (rc)\n\t\treturn rc;\n\tthen = rtc_tm_to_time64(&alarm.time);\n\tnow = rtc_tm_to_time64(&tm);\n\tduration = then-now;\n\n\t/* in the past */\n\tif (then < now)\n\t\treturn 0;\n\n\t/* will be stored in upper 16 bits of s0i3 hint argument,\n\t * so timer wakeup from s0i3 is limited to ~18 hours or less\n\t */\n\tif (duration <= 4 || duration > U16_MAX)\n\t\treturn -EINVAL;\n\n\t*arg |= (duration << 16);\n\trc = rtc_alarm_irq_enable(rtc_device, 0);\n\tpm_pr_dbg(\"wakeup timer programmed for %lld seconds\\n\", duration);\n\n\treturn rc;\n}\n\nstatic void amd_pmc_s2idle_prepare(void)\n{\n\tstruct amd_pmc_dev *pdev = &pmc;\n\tint rc;\n\tu8 msg;\n\tu32 arg = 1;\n\n\t/* Reset and Start SMU logging - to monitor the s0i3 stats */\n\tamd_pmc_setup_smu_logging(pdev);\n\n\t/* Activate CZN specific platform bug workarounds */\n\tif (pdev->cpu_id == AMD_CPU_ID_CZN && !disable_workarounds) {\n\t\trc = amd_pmc_verify_czn_rtc(pdev, &arg);\n\t\tif (rc) {\n\t\t\tdev_err(pdev->dev, \"failed to set RTC: %d\\n\", rc);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tmsg = amd_pmc_get_os_hint(pdev);\n\trc = amd_pmc_send_cmd(pdev, arg, NULL, msg, false);\n\tif (rc) {\n\t\tdev_err(pdev->dev, \"suspend failed: %d\\n\", rc);\n\t\treturn;\n\t}\n\n\trc = amd_pmc_write_stb(pdev, AMD_PMC_STB_S2IDLE_PREPARE);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"error writing to STB: %d\\n\", rc);\n}\n\nstatic void amd_pmc_s2idle_check(void)\n{\n\tstruct amd_pmc_dev *pdev = &pmc;\n\tstruct smu_metrics table;\n\tint rc;\n\n\t/* CZN: Ensure that future s0i3 entry attempts at least 10ms passed */\n\tif (pdev->cpu_id == AMD_CPU_ID_CZN && !get_metrics_table(pdev, &table) &&\n\t    table.s0i3_last_entry_status)\n\t\tusleep_range(10000, 20000);\n\n\t/* Dump the IdleMask before we add to the STB */\n\tamd_pmc_idlemask_read(pdev, pdev->dev, NULL);\n\n\trc = amd_pmc_write_stb(pdev, AMD_PMC_STB_S2IDLE_CHECK);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"error writing to STB: %d\\n\", rc);\n}\n\nstatic int amd_pmc_dump_data(struct amd_pmc_dev *pdev)\n{\n\tif (pdev->cpu_id == AMD_CPU_ID_PCO)\n\t\treturn -ENODEV;\n\n\treturn amd_pmc_send_cmd(pdev, 0, NULL, SMU_MSG_LOG_DUMP_DATA, false);\n}\n\nstatic void amd_pmc_s2idle_restore(void)\n{\n\tstruct amd_pmc_dev *pdev = &pmc;\n\tint rc;\n\tu8 msg;\n\n\tmsg = amd_pmc_get_os_hint(pdev);\n\trc = amd_pmc_send_cmd(pdev, 0, NULL, msg, false);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"resume failed: %d\\n\", rc);\n\n\t/* Let SMU know that we are looking for stats */\n\tamd_pmc_dump_data(pdev);\n\n\trc = amd_pmc_write_stb(pdev, AMD_PMC_STB_S2IDLE_RESTORE);\n\tif (rc)\n\t\tdev_err(pdev->dev, \"error writing to STB: %d\\n\", rc);\n\n\t/* Notify on failed entry */\n\tamd_pmc_validate_deepest(pdev);\n\n\tamd_pmc_process_restore_quirks(pdev);\n}\n\nstatic struct acpi_s2idle_dev_ops amd_pmc_s2idle_dev_ops = {\n\t.prepare = amd_pmc_s2idle_prepare,\n\t.check = amd_pmc_s2idle_check,\n\t.restore = amd_pmc_s2idle_restore,\n};\n\nstatic int amd_pmc_suspend_handler(struct device *dev)\n{\n\tstruct amd_pmc_dev *pdev = dev_get_drvdata(dev);\n\n\tif (pdev->disable_8042_wakeup && !disable_workarounds) {\n\t\tint rc = amd_pmc_wa_irq1(pdev);\n\n\t\tif (rc) {\n\t\t\tdev_err(pdev->dev, \"failed to adjust keyboard wakeup: %d\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic DEFINE_SIMPLE_DEV_PM_OPS(amd_pmc_pm, amd_pmc_suspend_handler, NULL);\n\nstatic const struct pci_device_id pmc_pci_ids[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PS) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_CB) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_YC) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_CZN) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_RN) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_PCO) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_RV) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, AMD_CPU_ID_SP) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_1AH_M20H_ROOT) },\n\t{ }\n};\n\nstatic int amd_pmc_s2d_init(struct amd_pmc_dev *dev)\n{\n\tu32 phys_addr_low, phys_addr_hi;\n\tu64 stb_phys_addr;\n\tu32 size = 0;\n\tint ret;\n\n\t/* Spill to DRAM feature uses separate SMU message port */\n\tdev->msg_port = 1;\n\n\t/* Get num of IP blocks within the SoC */\n\tamd_pmc_get_ip_info(dev);\n\n\tamd_pmc_send_cmd(dev, S2D_TELEMETRY_SIZE, &size, dev->s2d_msg_id, true);\n\tif (size != S2D_TELEMETRY_BYTES_MAX)\n\t\treturn -EIO;\n\n\t/* Get DRAM size */\n\tret = amd_pmc_send_cmd(dev, S2D_DRAM_SIZE, &dev->dram_size, dev->s2d_msg_id, true);\n\tif (ret || !dev->dram_size)\n\t\tdev->dram_size = S2D_TELEMETRY_DRAMBYTES_MAX;\n\n\t/* Get STB DRAM address */\n\tamd_pmc_send_cmd(dev, S2D_PHYS_ADDR_LOW, &phys_addr_low, dev->s2d_msg_id, true);\n\tamd_pmc_send_cmd(dev, S2D_PHYS_ADDR_HIGH, &phys_addr_hi, dev->s2d_msg_id, true);\n\n\tif (!phys_addr_hi && !phys_addr_low) {\n\t\tdev_err(dev->dev, \"STB is not enabled on the system; disable enable_stb or contact system vendor\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tstb_phys_addr = ((u64)phys_addr_hi << 32 | phys_addr_low);\n\n\t/* Clear msg_port for other SMU operation */\n\tdev->msg_port = 0;\n\n\tdev->stb_virt_addr = devm_ioremap(dev->dev, stb_phys_addr, dev->dram_size);\n\tif (!dev->stb_virt_addr)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int amd_pmc_write_stb(struct amd_pmc_dev *dev, u32 data)\n{\n\tint err;\n\n\terr = amd_smn_write(0, AMD_PMC_STB_PMI_0, data);\n\tif (err) {\n\t\tdev_err(dev->dev, \"failed to write data in stb: 0x%X\\n\", AMD_PMC_STB_PMI_0);\n\t\treturn pcibios_err_to_errno(err);\n\t}\n\n\treturn 0;\n}\n\nstatic int amd_pmc_read_stb(struct amd_pmc_dev *dev, u32 *buf)\n{\n\tint i, err;\n\n\tfor (i = 0; i < FIFO_SIZE; i++) {\n\t\terr = amd_smn_read(0, AMD_PMC_STB_PMI_0, buf++);\n\t\tif (err) {\n\t\t\tdev_err(dev->dev, \"error reading data from stb: 0x%X\\n\", AMD_PMC_STB_PMI_0);\n\t\t\treturn pcibios_err_to_errno(err);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amd_pmc_probe(struct platform_device *pdev)\n{\n\tstruct amd_pmc_dev *dev = &pmc;\n\tstruct pci_dev *rdev;\n\tu32 base_addr_lo, base_addr_hi;\n\tu64 base_addr;\n\tint err;\n\tu32 val;\n\n\tdev->dev = &pdev->dev;\n\n\trdev = pci_get_domain_bus_and_slot(0, 0, PCI_DEVFN(0, 0));\n\tif (!rdev || !pci_match_id(pmc_pci_ids, rdev)) {\n\t\terr = -ENODEV;\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tdev->cpu_id = rdev->device;\n\n\tif (dev->cpu_id == AMD_CPU_ID_SP) {\n\t\tdev_warn_once(dev->dev, \"S0i3 is not supported on this hardware\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tdev->rdev = rdev;\n\terr = amd_smn_read(0, AMD_PMC_BASE_ADDR_LO, &val);\n\tif (err) {\n\t\tdev_err(dev->dev, \"error reading 0x%x\\n\", AMD_PMC_BASE_ADDR_LO);\n\t\terr = pcibios_err_to_errno(err);\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tbase_addr_lo = val & AMD_PMC_BASE_ADDR_HI_MASK;\n\n\terr = amd_smn_read(0, AMD_PMC_BASE_ADDR_HI, &val);\n\tif (err) {\n\t\tdev_err(dev->dev, \"error reading 0x%x\\n\", AMD_PMC_BASE_ADDR_HI);\n\t\terr = pcibios_err_to_errno(err);\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tbase_addr_hi = val & AMD_PMC_BASE_ADDR_LO_MASK;\n\tbase_addr = ((u64)base_addr_hi << 32 | base_addr_lo);\n\n\tdev->regbase = devm_ioremap(dev->dev, base_addr + AMD_PMC_BASE_ADDR_OFFSET,\n\t\t\t\t    AMD_PMC_MAPPING_SIZE);\n\tif (!dev->regbase) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pci_dev_put;\n\t}\n\n\tmutex_init(&dev->lock);\n\n\tif (enable_stb && amd_pmc_is_stb_supported(dev)) {\n\t\terr = amd_pmc_s2d_init(dev);\n\t\tif (err)\n\t\t\tgoto err_pci_dev_put;\n\t}\n\n\tplatform_set_drvdata(pdev, dev);\n\tif (IS_ENABLED(CONFIG_SUSPEND)) {\n\t\terr = acpi_register_lps0_dev(&amd_pmc_s2idle_dev_ops);\n\t\tif (err)\n\t\t\tdev_warn(dev->dev, \"failed to register LPS0 sleep handler, expect increased power consumption\\n\");\n\t\tif (!disable_workarounds)\n\t\t\tamd_pmc_quirks_init(dev);\n\t}\n\n\tamd_pmc_dbgfs_register(dev);\n\tpm_report_max_hw_sleep(U64_MAX);\n\treturn 0;\n\nerr_pci_dev_put:\n\tpci_dev_put(rdev);\n\treturn err;\n}\n\nstatic void amd_pmc_remove(struct platform_device *pdev)\n{\n\tstruct amd_pmc_dev *dev = platform_get_drvdata(pdev);\n\n\tif (IS_ENABLED(CONFIG_SUSPEND))\n\t\tacpi_unregister_lps0_dev(&amd_pmc_s2idle_dev_ops);\n\tamd_pmc_dbgfs_unregister(dev);\n\tpci_dev_put(dev->rdev);\n\tmutex_destroy(&dev->lock);\n}\n\nstatic const struct acpi_device_id amd_pmc_acpi_ids[] = {\n\t{\"AMDI0005\", 0},\n\t{\"AMDI0006\", 0},\n\t{\"AMDI0007\", 0},\n\t{\"AMDI0008\", 0},\n\t{\"AMDI0009\", 0},\n\t{\"AMDI000A\", 0},\n\t{\"AMD0004\", 0},\n\t{\"AMD0005\", 0},\n\t{ }\n};\nMODULE_DEVICE_TABLE(acpi, amd_pmc_acpi_ids);\n\nstatic struct platform_driver amd_pmc_driver = {\n\t.driver = {\n\t\t.name = \"amd_pmc\",\n\t\t.acpi_match_table = amd_pmc_acpi_ids,\n\t\t.dev_groups = pmc_groups,\n\t\t.pm = pm_sleep_ptr(&amd_pmc_pm),\n\t},\n\t.probe = amd_pmc_probe,\n\t.remove_new = amd_pmc_remove,\n};\nmodule_platform_driver(amd_pmc_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"AMD PMC Driver\");\n"
                    }
                }
            ]
        },
        {
            "cve_id": "CVE-2025-21646",
            "cve_url": "./vulns/cve/published/2025/CVE-2025-21646.json",
            "patch_attempts": [
                {
                    "upstream_commit": "3ff93c5935610f1f53f4b730d9b7b76a9559968a",
                    "upstream_commit_date": "2025-01-07 15:55:25 +0100",
                    "upstream_patch": "8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8",
                    "total_versions_tested": 5,
                    "successful_patches": 5,
                    "failed_patches": 0,
                    "patch_results": [
                        {
                            "downstream_patch": "aabe47cf5ac5e1db2ae0635f189d836f67024904",
                            "downstream_commit": "ee7e40f7fb17f08a8cbae50553e5c2e10ae32fce",
                            "commit_date": "2025-01-17 13:34:40 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file fs/afs/afs.h\npatching file fs/afs/afs_vl.h\npatching file fs/afs/vl_alias.c\nHunk #1 succeeded at 302 (offset 49 lines).\nHunk #2 succeeded at 314 (offset 49 lines).\npatching file fs/afs/vlclient.c\nHunk #1 succeeded at 671 (offset -26 lines).",
                            "downstream_patch_content": "commit aabe47cf5ac5e1db2ae0635f189d836f67024904\nAuthor: David Howells <dhowells@redhat.com>\nDate:   Mon Jan 6 16:21:00 2025 +0000\n\n    afs: Fix the maximum cell name length\n    \n    [ Upstream commit 8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8 ]\n    \n    The kafs filesystem limits the maximum length of a cell to 256 bytes, but a\n    problem occurs if someone actually does that: kafs tries to create a\n    directory under /proc/net/afs/ with the name of the cell, but that fails\n    with a warning:\n    \n            WARNING: CPU: 0 PID: 9 at fs/proc/generic.c:405\n    \n    because procfs limits the maximum filename length to 255.\n    \n    However, the DNS limits the maximum lookup length and, by extension, the\n    maximum cell name, to 255 less two (length count and trailing NUL).\n    \n    Fix this by limiting the maximum acceptable cellname length to 253.  This\n    also allows us to be sure we can create the \"/afs/.<cell>/\" mountpoint too.\n    \n    Further, split the YFS VL record cell name maximum to be the 256 allowed by\n    the protocol and ignore the record retrieved by YFSVL.GetCellName if it\n    exceeds 253.\n    \n    Fixes: c3e9f888263b (\"afs: Implement client support for the YFSVL.GetCellName RPC op\")\n    Reported-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/r/6776d25d.050a0220.3a8527.0048.GAE@google.com/\n    Signed-off-by: David Howells <dhowells@redhat.com>\n    Link: https://lore.kernel.org/r/376236.1736180460@warthog.procyon.org.uk\n    Tested-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    cc: Marc Dionne <marc.dionne@auristor.com>\n    cc: linux-afs@lists.infradead.org\n    Signed-off-by: Christian Brauner <brauner@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/fs/afs/afs.h b/fs/afs/afs.h\nindex 432cb4b23961..3ea5f3e3c922 100644\n--- a/fs/afs/afs.h\n+++ b/fs/afs/afs.h\n@@ -10,7 +10,7 @@\n \n #include <linux/in.h>\n \n-#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n+#define AFS_MAXCELLNAME\t\t253  \t/* Maximum length of a cell name (DNS limited) */\n #define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n #define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n #define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\ndiff --git a/fs/afs/afs_vl.h b/fs/afs/afs_vl.h\nindex 9c65ffb8a523..8da0899fbc08 100644\n--- a/fs/afs/afs_vl.h\n+++ b/fs/afs/afs_vl.h\n@@ -13,6 +13,7 @@\n #define AFS_VL_PORT\t\t7003\t/* volume location service port */\n #define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n #define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n+#define YFS_VL_MAXCELLNAME\t256  \t/* Maximum length of a cell name in YFS protocol */\n \n enum AFSVL_Operations {\n \tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\ndiff --git a/fs/afs/vl_alias.c b/fs/afs/vl_alias.c\nindex f04a80e4f5c3..83cf1bfbe343 100644\n--- a/fs/afs/vl_alias.c\n+++ b/fs/afs/vl_alias.c\n@@ -302,6 +302,7 @@ static char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n {\n \tstruct afs_cell *master;\n+\tsize_t name_len;\n \tchar *cell_name;\n \n \tcell_name = afs_vl_get_cell_name(cell, key);\n@@ -313,8 +314,11 @@ static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n \t\treturn 0;\n \t}\n \n-\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n-\t\t\t\t NULL, false);\n+\tname_len = strlen(cell_name);\n+\tif (!name_len || name_len > AFS_MAXCELLNAME)\n+\t\tmaster = ERR_PTR(-EOPNOTSUPP);\n+\telse\n+\t\tmaster = afs_lookup_cell(cell->net, cell_name, name_len, NULL, false);\n \tkfree(cell_name);\n \tif (IS_ERR(master))\n \t\treturn PTR_ERR(master);\ndiff --git a/fs/afs/vlclient.c b/fs/afs/vlclient.c\nindex 00fca3c66ba6..16653f2ffe4f 100644\n--- a/fs/afs/vlclient.c\n+++ b/fs/afs/vlclient.c\n@@ -671,7 +671,7 @@ static int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n \t\t\treturn ret;\n \n \t\tnamesz = ntohl(call->tmp);\n-\t\tif (namesz > AFS_MAXCELLNAME)\n+\t\tif (namesz > YFS_VL_MAXCELLNAME)\n \t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n \t\tpaddedsz = (namesz + 3) & ~3;\n \t\tcall->count = namesz;\n",
                            "downstream_file_content": {}
                        },
                        {
                            "downstream_patch": "7673030efe0f8ca1056d3849d61784c6caa052af",
                            "downstream_commit": "271ae0edbfc942795c162e6cf20d2bc02bd7fde4",
                            "commit_date": "2025-01-17 13:36:16 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file fs/afs/afs.h\npatching file fs/afs/afs_vl.h\npatching file fs/afs/vl_alias.c\nHunk #1 succeeded at 302 (offset 49 lines).\nHunk #2 succeeded at 314 (offset 49 lines).\npatching file fs/afs/vlclient.c\nHunk #1 succeeded at 671 (offset -26 lines).",
                            "downstream_patch_content": "commit 7673030efe0f8ca1056d3849d61784c6caa052af\nAuthor: David Howells <dhowells@redhat.com>\nDate:   Mon Jan 6 16:21:00 2025 +0000\n\n    afs: Fix the maximum cell name length\n    \n    [ Upstream commit 8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8 ]\n    \n    The kafs filesystem limits the maximum length of a cell to 256 bytes, but a\n    problem occurs if someone actually does that: kafs tries to create a\n    directory under /proc/net/afs/ with the name of the cell, but that fails\n    with a warning:\n    \n            WARNING: CPU: 0 PID: 9 at fs/proc/generic.c:405\n    \n    because procfs limits the maximum filename length to 255.\n    \n    However, the DNS limits the maximum lookup length and, by extension, the\n    maximum cell name, to 255 less two (length count and trailing NUL).\n    \n    Fix this by limiting the maximum acceptable cellname length to 253.  This\n    also allows us to be sure we can create the \"/afs/.<cell>/\" mountpoint too.\n    \n    Further, split the YFS VL record cell name maximum to be the 256 allowed by\n    the protocol and ignore the record retrieved by YFSVL.GetCellName if it\n    exceeds 253.\n    \n    Fixes: c3e9f888263b (\"afs: Implement client support for the YFSVL.GetCellName RPC op\")\n    Reported-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/r/6776d25d.050a0220.3a8527.0048.GAE@google.com/\n    Signed-off-by: David Howells <dhowells@redhat.com>\n    Link: https://lore.kernel.org/r/376236.1736180460@warthog.procyon.org.uk\n    Tested-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    cc: Marc Dionne <marc.dionne@auristor.com>\n    cc: linux-afs@lists.infradead.org\n    Signed-off-by: Christian Brauner <brauner@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/fs/afs/afs.h b/fs/afs/afs.h\nindex 81815724db6c..25c17100798b 100644\n--- a/fs/afs/afs.h\n+++ b/fs/afs/afs.h\n@@ -10,7 +10,7 @@\n \n #include <linux/in.h>\n \n-#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n+#define AFS_MAXCELLNAME\t\t253  \t/* Maximum length of a cell name (DNS limited) */\n #define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n #define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n #define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\ndiff --git a/fs/afs/afs_vl.h b/fs/afs/afs_vl.h\nindex 9c65ffb8a523..8da0899fbc08 100644\n--- a/fs/afs/afs_vl.h\n+++ b/fs/afs/afs_vl.h\n@@ -13,6 +13,7 @@\n #define AFS_VL_PORT\t\t7003\t/* volume location service port */\n #define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n #define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n+#define YFS_VL_MAXCELLNAME\t256  \t/* Maximum length of a cell name in YFS protocol */\n \n enum AFSVL_Operations {\n \tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\ndiff --git a/fs/afs/vl_alias.c b/fs/afs/vl_alias.c\nindex f04a80e4f5c3..83cf1bfbe343 100644\n--- a/fs/afs/vl_alias.c\n+++ b/fs/afs/vl_alias.c\n@@ -302,6 +302,7 @@ static char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n {\n \tstruct afs_cell *master;\n+\tsize_t name_len;\n \tchar *cell_name;\n \n \tcell_name = afs_vl_get_cell_name(cell, key);\n@@ -313,8 +314,11 @@ static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n \t\treturn 0;\n \t}\n \n-\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n-\t\t\t\t NULL, false);\n+\tname_len = strlen(cell_name);\n+\tif (!name_len || name_len > AFS_MAXCELLNAME)\n+\t\tmaster = ERR_PTR(-EOPNOTSUPP);\n+\telse\n+\t\tmaster = afs_lookup_cell(cell->net, cell_name, name_len, NULL, false);\n \tkfree(cell_name);\n \tif (IS_ERR(master))\n \t\treturn PTR_ERR(master);\ndiff --git a/fs/afs/vlclient.c b/fs/afs/vlclient.c\nindex 00fca3c66ba6..16653f2ffe4f 100644\n--- a/fs/afs/vlclient.c\n+++ b/fs/afs/vlclient.c\n@@ -671,7 +671,7 @@ static int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n \t\t\treturn ret;\n \n \t\tnamesz = ntohl(call->tmp);\n-\t\tif (namesz > AFS_MAXCELLNAME)\n+\t\tif (namesz > YFS_VL_MAXCELLNAME)\n \t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n \t\tpaddedsz = (namesz + 3) & ~3;\n \t\tcall->count = namesz;\n",
                            "downstream_file_content": {
                                "fs/afs/afs.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS common types\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_H\n#define AFS_H\n\n#include <linux/in.h>\n\n#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n#define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n#define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n#define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\n#define AFS_MAXTYPES\t\t3\t/* Maximum number of volume types */\n#define AFSNAMEMAX\t\t256 \t/* Maximum length of a filename plus NUL */\n#define AFSPATHMAX\t\t1024\t/* Maximum length of a pathname plus NUL */\n#define AFSOPAQUEMAX\t\t1024\t/* Maximum length of an opaque field */\n\n#define AFS_VL_MAX_LIFESPAN\t120\n#define AFS_PROBE_MAX_LIFESPAN\t30\n\ntypedef u64\t\t\tafs_volid_t;\ntypedef u64\t\t\tafs_vnodeid_t;\ntypedef u64\t\t\tafs_dataversion_t;\n\ntypedef enum {\n\tAFSVL_RWVOL,\t\t\t/* read/write volume */\n\tAFSVL_ROVOL,\t\t\t/* read-only volume */\n\tAFSVL_BACKVOL,\t\t\t/* backup volume */\n} __attribute__((packed)) afs_voltype_t;\n\ntypedef enum {\n\tAFS_FTYPE_INVALID\t= 0,\n\tAFS_FTYPE_FILE\t\t= 1,\n\tAFS_FTYPE_DIR\t\t= 2,\n\tAFS_FTYPE_SYMLINK\t= 3,\n} afs_file_type_t;\n\ntypedef enum {\n\tAFS_LOCK_READ\t\t= 0,\t/* read lock request */\n\tAFS_LOCK_WRITE\t\t= 1,\t/* write lock request */\n} afs_lock_type_t;\n\n#define AFS_LOCKWAIT\t\t(5 * 60) /* time until a lock times out (seconds) */\n\n/*\n * AFS file identifier\n */\nstruct afs_fid {\n\tafs_volid_t\tvid;\t\t/* volume ID */\n\tafs_vnodeid_t\tvnode;\t\t/* Lower 64-bits of file index within volume */\n\tu32\t\tvnode_hi;\t/* Upper 32-bits of file index */\n\tu32\t\tunique;\t\t/* unique ID number (file index version) */\n};\n\n/*\n * AFS callback notification\n */\ntypedef enum {\n\tAFSCM_CB_UNTYPED\t= 0,\t/* no type set on CB break */\n\tAFSCM_CB_EXCLUSIVE\t= 1,\t/* CB exclusive to CM [not implemented] */\n\tAFSCM_CB_SHARED\t\t= 2,\t/* CB shared by other CM's */\n\tAFSCM_CB_DROPPED\t= 3,\t/* CB promise cancelled by file server */\n} afs_callback_type_t;\n\nstruct afs_callback {\n\ttime64_t\t\texpires_at;\t/* Time at which expires */\n\t//unsigned\t\tversion;\t/* Callback version */\n\t//afs_callback_type_t\ttype;\t\t/* Type of callback */\n};\n\nstruct afs_callback_break {\n\tstruct afs_fid\t\tfid;\t\t/* File identifier */\n\t//struct afs_callback\tcb;\t\t/* Callback details */\n};\n\n#define AFSCBMAX 50\t/* maximum callbacks transferred per bulk op */\n\nstruct afs_uuid {\n\t__be32\t\ttime_low;\t\t\t/* low part of timestamp */\n\t__be16\t\ttime_mid;\t\t\t/* mid part of timestamp */\n\t__be16\t\ttime_hi_and_version;\t\t/* high part of timestamp and version  */\n\t__s8\t\tclock_seq_hi_and_reserved;\t/* clock seq hi and variant */\n\t__s8\t\tclock_seq_low;\t\t\t/* clock seq low */\n\t__s8\t\tnode[6];\t\t\t/* spatially unique node ID (MAC addr) */\n};\n\n/*\n * AFS volume information\n */\nstruct afs_volume_info {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_voltype_t\t\ttype;\t\t/* type of this volume */\n\tafs_volid_t\t\ttype_vids[5];\t/* volume ID's for possible types for this vol */\n\n\t/* list of fileservers serving this volume */\n\tsize_t\t\t\tnservers;\t/* number of entries used in servers[] */\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* fileserver address */\n\t} servers[8];\n};\n\n/*\n * AFS security ACE access mask\n */\ntypedef u32 afs_access_t;\n#define AFS_ACE_READ\t\t0x00000001U\t/* - permission to read a file/dir */\n#define AFS_ACE_WRITE\t\t0x00000002U\t/* - permission to write/chmod a file */\n#define AFS_ACE_INSERT\t\t0x00000004U\t/* - permission to create dirent in a dir */\n#define AFS_ACE_LOOKUP\t\t0x00000008U\t/* - permission to lookup a file/dir in a dir */\n#define AFS_ACE_DELETE\t\t0x00000010U\t/* - permission to delete a dirent from a dir */\n#define AFS_ACE_LOCK\t\t0x00000020U\t/* - permission to lock a file */\n#define AFS_ACE_ADMINISTER\t0x00000040U\t/* - permission to change ACL */\n#define AFS_ACE_USER_A\t\t0x01000000U\t/* - 'A' user-defined permission */\n#define AFS_ACE_USER_B\t\t0x02000000U\t/* - 'B' user-defined permission */\n#define AFS_ACE_USER_C\t\t0x04000000U\t/* - 'C' user-defined permission */\n#define AFS_ACE_USER_D\t\t0x08000000U\t/* - 'D' user-defined permission */\n#define AFS_ACE_USER_E\t\t0x10000000U\t/* - 'E' user-defined permission */\n#define AFS_ACE_USER_F\t\t0x20000000U\t/* - 'F' user-defined permission */\n#define AFS_ACE_USER_G\t\t0x40000000U\t/* - 'G' user-defined permission */\n#define AFS_ACE_USER_H\t\t0x80000000U\t/* - 'H' user-defined permission */\n\n/*\n * AFS file status information\n */\nstruct afs_file_status {\n\tu64\t\t\tsize;\t\t/* file size */\n\tafs_dataversion_t\tdata_version;\t/* current data version */\n\tstruct timespec64\tmtime_client;\t/* Last time client changed data */\n\tstruct timespec64\tmtime_server;\t/* Last time server changed data */\n\ts64\t\t\tauthor;\t\t/* author ID */\n\ts64\t\t\towner;\t\t/* owner ID */\n\ts64\t\t\tgroup;\t\t/* group ID */\n\tafs_access_t\t\tcaller_access;\t/* access rights for authenticated caller */\n\tafs_access_t\t\tanon_access;\t/* access rights for unauthenticated caller */\n\tumode_t\t\t\tmode;\t\t/* UNIX mode */\n\tafs_file_type_t\t\ttype;\t\t/* file type */\n\tu32\t\t\tnlink;\t\t/* link count */\n\ts32\t\t\tlock_count;\t/* file lock count (0=UNLK -1=WRLCK +ve=#RDLCK */\n\tu32\t\t\tabort_code;\t/* Abort if bulk-fetching this failed */\n};\n\nstruct afs_status_cb {\n\tstruct afs_file_status\tstatus;\n\tstruct afs_callback\tcallback;\n\tbool\t\t\thave_status;\t/* True if status record was retrieved */\n\tbool\t\t\thave_cb;\t/* True if cb record was retrieved */\n\tbool\t\t\thave_error;\t/* True if status.abort_code indicates an error */\n};\n\n/*\n * AFS file status change request\n */\n\n#define AFS_SET_MTIME\t\t0x01\t\t/* set the mtime */\n#define AFS_SET_OWNER\t\t0x02\t\t/* set the owner ID */\n#define AFS_SET_GROUP\t\t0x04\t\t/* set the group ID (unsupported?) */\n#define AFS_SET_MODE\t\t0x08\t\t/* set the UNIX mode */\n#define AFS_SET_SEG_SIZE\t0x10\t\t/* set the segment size (unsupported) */\n\n/*\n * AFS volume synchronisation information\n */\nstruct afs_volsync {\n\ttime64_t\t\tcreation;\t/* volume creation time */\n};\n\n/*\n * AFS volume status record\n */\nstruct afs_volume_status {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_volid_t\t\tparent_id;\t/* parent volume ID */\n\tu8\t\t\tonline;\t\t/* true if volume currently online and available */\n\tu8\t\t\tin_service;\t/* true if volume currently in service */\n\tu8\t\t\tblessed;\t/* same as in_service */\n\tu8\t\t\tneeds_salvage;\t/* true if consistency checking required */\n\tu32\t\t\ttype;\t\t/* volume type (afs_voltype_t) */\n\tu64\t\t\tmin_quota;\t/* minimum space set aside (blocks) */\n\tu64\t\t\tmax_quota;\t/* maximum space this volume may occupy (blocks) */\n\tu64\t\t\tblocks_in_use;\t/* space this volume currently occupies (blocks) */\n\tu64\t\t\tpart_blocks_avail; /* space available in volume's partition */\n\tu64\t\t\tpart_max_blocks; /* size of volume's partition */\n\ts64\t\t\tvol_copy_date;\n\ts64\t\t\tvol_backup_date;\n};\n\n#define AFS_BLOCK_SIZE\t1024\n\n/*\n * XDR encoding of UUID in AFS.\n */\nstruct afs_uuid__xdr {\n\t__be32\t\ttime_low;\n\t__be32\t\ttime_mid;\n\t__be32\t\ttime_hi_and_version;\n\t__be32\t\tclock_seq_hi_and_reserved;\n\t__be32\t\tclock_seq_low;\n\t__be32\t\tnode[6];\n};\n\n#endif /* AFS_H */\n",
                                "fs/afs/afs_vl.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS Volume Location Service client interface\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_VL_H\n#define AFS_VL_H\n\n#include \"afs.h\"\n\n#define AFS_VL_PORT\t\t7003\t/* volume location service port */\n#define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n#define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n\nenum AFSVL_Operations {\n\tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\n\tVLGETENTRYBYNAME\t= 504,\t/* AFS Get VLDB entry by name */\n\tVLPROBE\t\t\t= 514,\t/* AFS probe VL service */\n\tVLGETENTRYBYIDU\t\t= 526,\t/* AFS Get VLDB entry by ID (UUID-variant) */\n\tVLGETENTRYBYNAMEU\t= 527,\t/* AFS Get VLDB entry by name (UUID-variant) */\n\tVLGETADDRSU\t\t= 533,\t/* AFS Get addrs for fileserver */\n\tYVLGETENDPOINTS\t\t= 64002, /* YFS Get endpoints for file/volume server */\n\tYVLGETCELLNAME\t\t= 64014, /* YFS Get actual cell name */\n\tVLGETCAPABILITIES\t= 65537, /* AFS Get server capabilities */\n};\n\nenum AFSVL_Errors {\n\tAFSVL_IDEXIST \t\t= 363520,\t/* Volume Id entry exists in vl database */\n\tAFSVL_IO \t\t= 363521,\t/* I/O related error */\n\tAFSVL_NAMEEXIST \t= 363522,\t/* Volume name entry exists in vl database */\n\tAFSVL_CREATEFAIL \t= 363523,\t/* Internal creation failure */\n\tAFSVL_NOENT \t\t= 363524,\t/* No such entry */\n\tAFSVL_EMPTY \t\t= 363525,\t/* Vl database is empty */\n\tAFSVL_ENTDELETED \t= 363526,\t/* Entry is deleted (soft delete) */\n\tAFSVL_BADNAME \t\t= 363527,\t/* Volume name is illegal */\n\tAFSVL_BADINDEX \t\t= 363528,\t/* Index is out of range */\n\tAFSVL_BADVOLTYPE \t= 363529,\t/* Bad volume type */\n\tAFSVL_BADSERVER \t= 363530,\t/* Illegal server number (out of range) */\n\tAFSVL_BADPARTITION \t= 363531,\t/* Bad partition number */\n\tAFSVL_REPSFULL \t\t= 363532,\t/* Run out of space for Replication sites */\n\tAFSVL_NOREPSERVER \t= 363533,\t/* No such Replication server site exists */\n\tAFSVL_DUPREPSERVER \t= 363534,\t/* Replication site already exists */\n\tAFSVL_RWNOTFOUND \t= 363535,\t/* Parent R/W entry not found */\n\tAFSVL_BADREFCOUNT \t= 363536,\t/* Illegal Reference Count number */\n\tAFSVL_SIZEEXCEEDED \t= 363537,\t/* Vl size for attributes exceeded */\n\tAFSVL_BADENTRY \t\t= 363538,\t/* Bad incoming vl entry */\n\tAFSVL_BADVOLIDBUMP \t= 363539,\t/* Illegal max volid increment */\n\tAFSVL_IDALREADYHASHED \t= 363540,\t/* RO/BACK id already hashed */\n\tAFSVL_ENTRYLOCKED \t= 363541,\t/* Vl entry is already locked */\n\tAFSVL_BADVOLOPER \t= 363542,\t/* Bad volume operation code */\n\tAFSVL_BADRELLOCKTYPE \t= 363543,\t/* Bad release lock type */\n\tAFSVL_RERELEASE \t= 363544,\t/* Status report: last release was aborted */\n\tAFSVL_BADSERVERFLAG \t= 363545,\t/* Invalid replication site server flag */\n\tAFSVL_PERM \t\t= 363546,\t/* No permission access */\n\tAFSVL_NOMEM \t\t= 363547,\t/* malloc/realloc failed to alloc enough memory */\n};\n\nenum {\n\tYFS_SERVER_INDEX\t= 0,\n\tYFS_SERVER_UUID\t\t= 1,\n\tYFS_SERVER_ENDPOINT\t= 2,\n};\n\nenum {\n\tYFS_ENDPOINT_IPV4\t= 0,\n\tYFS_ENDPOINT_IPV6\t= 1,\n};\n\n#define YFS_MAXENDPOINTS\t16\n\n/*\n * maps to \"struct vldbentry\" in vvl-spec.pdf\n */\nstruct afs_vldbentry {\n\tchar\t\tname[65];\t\t/* name of volume (with NUL char) */\n\tafs_voltype_t\ttype;\t\t\t/* volume type */\n\tunsigned\tnum_servers;\t\t/* num servers that hold instances of this vol */\n\tunsigned\tclone_id;\t\t/* cloning ID */\n\n\tunsigned\tflags;\n#define AFS_VLF_RWEXISTS\t0x1000\t\t/* R/W volume exists */\n#define AFS_VLF_ROEXISTS\t0x2000\t\t/* R/O volume exists */\n#define AFS_VLF_BACKEXISTS\t0x4000\t\t/* backup volume exists */\n\n\tafs_volid_t\tvolume_ids[3];\t\t/* volume IDs */\n\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* server address */\n\t\tunsigned\tpartition;\t/* partition ID on this server */\n\t\tunsigned\tflags;\t\t/* server specific flags */\n#define AFS_VLSF_NEWREPSITE\t0x0001\t/* Ignore all 'non-new' servers */\n#define AFS_VLSF_ROVOL\t\t0x0002\t/* this server holds a R/O instance of the volume */\n#define AFS_VLSF_RWVOL\t\t0x0004\t/* this server holds a R/W instance of the volume */\n#define AFS_VLSF_BACKVOL\t0x0008\t/* this server holds a backup instance of the volume */\n#define AFS_VLSF_UUID\t\t0x0010\t/* This server is referred to by its UUID */\n#define AFS_VLSF_DONTUSE\t0x0020\t/* This server ref should be ignored */\n\t} servers[8];\n};\n\n#define AFS_VLDB_MAXNAMELEN 65\n\n\nstruct afs_ListAddrByAttributes__xdr {\n\t__be32\t\t\tMask;\n#define AFS_VLADDR_IPADDR\t0x1\t/* Match by ->ipaddr */\n#define AFS_VLADDR_INDEX\t0x2\t/* Match by ->index */\n#define AFS_VLADDR_UUID\t\t0x4\t/* Match by ->uuid */\n\t__be32\t\t\tipaddr;\n\t__be32\t\t\tindex;\n\t__be32\t\t\tspare;\n\tstruct afs_uuid__xdr\tuuid;\n};\n\nstruct afs_uvldbentry__xdr {\n\t__be32\t\t\tname[AFS_VLDB_MAXNAMELEN];\n\t__be32\t\t\tnServers;\n\tstruct afs_uuid__xdr\tserverNumber[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverUnique[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverPartition[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverFlags[AFS_NMAXNSERVERS];\n\t__be32\t\t\tvolumeId[AFS_MAXTYPES];\n\t__be32\t\t\tcloneId;\n\t__be32\t\t\tflags;\n\t__be32\t\t\tspares1;\n\t__be32\t\t\tspares2;\n\t__be32\t\t\tspares3;\n\t__be32\t\t\tspares4;\n\t__be32\t\t\tspares5;\n\t__be32\t\t\tspares6;\n\t__be32\t\t\tspares7;\n\t__be32\t\t\tspares8;\n\t__be32\t\t\tspares9;\n};\n\nstruct afs_address_list {\n\trefcount_t\t\tusage;\n\tunsigned int\t\tversion;\n\tunsigned int\t\tnr_addrs;\n\tstruct sockaddr_rxrpc\taddrs[];\n};\n\nextern void afs_put_address_list(struct afs_address_list *alist);\n\n#endif /* AFS_VL_H */\n",
                                "fs/afs/vl_alias.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS cell alias detection\n *\n * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/namei.h>\n#include <keys/rxrpc-type.h>\n#include \"internal.h\"\n\n/*\n * Sample a volume.\n */\nstatic struct afs_volume *afs_sample_volume(struct afs_cell *cell, struct key *key,\n\t\t\t\t\t    const char *name, unsigned int namelen)\n{\n\tstruct afs_volume *volume;\n\tstruct afs_fs_context fc = {\n\t\t.type\t\t= 0, /* Explicitly leave it to the VLDB */\n\t\t.volnamesz\t= namelen,\n\t\t.volname\t= name,\n\t\t.net\t\t= cell->net,\n\t\t.cell\t\t= cell,\n\t\t.key\t\t= key, /* This might need to be something */\n\t};\n\n\tvolume = afs_create_volume(&fc);\n\t_leave(\" = %p\", volume);\n\treturn volume;\n}\n\n/*\n * Compare two addresses.\n */\nstatic int afs_compare_addrs(const struct sockaddr_rxrpc *srx_a,\n\t\t\t     const struct sockaddr_rxrpc *srx_b)\n{\n\tshort port_a, port_b;\n\tint addr_a, addr_b, diff;\n\n\tdiff = (short)srx_a->transport_type - (short)srx_b->transport_type;\n\tif (diff)\n\t\tgoto out;\n\n\tswitch (srx_a->transport_type) {\n\tcase AF_INET: {\n\t\tconst struct sockaddr_in *a = &srx_a->transport.sin;\n\t\tconst struct sockaddr_in *b = &srx_b->transport.sin;\n\t\taddr_a = ntohl(a->sin_addr.s_addr);\n\t\taddr_b = ntohl(b->sin_addr.s_addr);\n\t\tdiff = addr_a - addr_b;\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin_port);\n\t\t\tport_b = ntohs(b->sin_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase AF_INET6: {\n\t\tconst struct sockaddr_in6 *a = &srx_a->transport.sin6;\n\t\tconst struct sockaddr_in6 *b = &srx_b->transport.sin6;\n\t\tdiff = memcmp(&a->sin6_addr, &b->sin6_addr, 16);\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin6_port);\n\t\t\tport_b = ntohs(b->sin6_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\tWARN_ON(1);\n\t\tdiff = 1;\n\t}\n\nout:\n\treturn diff;\n}\n\n/*\n * Compare the address lists of a pair of fileservers.\n */\nstatic int afs_compare_fs_alists(const struct afs_server *server_a,\n\t\t\t\t const struct afs_server *server_b)\n{\n\tconst struct afs_addr_list *la, *lb;\n\tint a = 0, b = 0, addr_matches = 0;\n\n\tla = rcu_dereference(server_a->addresses);\n\tlb = rcu_dereference(server_b->addresses);\n\n\twhile (a < la->nr_addrs && b < lb->nr_addrs) {\n\t\tconst struct sockaddr_rxrpc *srx_a = &la->addrs[a];\n\t\tconst struct sockaddr_rxrpc *srx_b = &lb->addrs[b];\n\t\tint diff = afs_compare_addrs(srx_a, srx_b);\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\taddr_matches++;\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\treturn addr_matches;\n}\n\n/*\n * Compare the fileserver lists of two volumes.  The server lists are sorted in\n * order of ascending UUID.\n */\nstatic int afs_compare_volume_slists(const struct afs_volume *vol_a,\n\t\t\t\t     const struct afs_volume *vol_b)\n{\n\tconst struct afs_server_list *la, *lb;\n\tint i, a = 0, b = 0, uuid_matches = 0, addr_matches = 0;\n\n\tla = rcu_dereference(vol_a->servers);\n\tlb = rcu_dereference(vol_b->servers);\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tif (la->vids[i] != lb->vids[i])\n\t\t\treturn 0;\n\n\twhile (a < la->nr_servers && b < lb->nr_servers) {\n\t\tconst struct afs_server *server_a = la->servers[a].server;\n\t\tconst struct afs_server *server_b = lb->servers[b].server;\n\t\tint diff = memcmp(&server_a->uuid, &server_b->uuid, sizeof(uuid_t));\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\tuuid_matches++;\n\t\t\taddr_matches += afs_compare_fs_alists(server_a, server_b);\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\t_leave(\" = %d [um %d]\", addr_matches, uuid_matches);\n\treturn addr_matches;\n}\n\n/*\n * Compare root.cell volumes.\n */\nstatic int afs_compare_cell_roots(struct afs_cell *cell)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"\");\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (!p->root_volume)\n\t\t\tcontinue; /* Ignore cells that don't have a root.cell volume. */\n\n\t\tif (afs_compare_volume_slists(cell->root_volume, p->root_volume) != 0)\n\t\t\tgoto is_alias;\n\t}\n\n\trcu_read_unlock();\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\trcu_read_unlock();\n\tcell->alias_of = afs_use_cell(p, afs_cell_trace_use_alias);\n\treturn 1;\n}\n\n/*\n * Query the new cell for a volume from a cell we're already using.\n */\nstatic int afs_query_for_alias_one(struct afs_cell *cell, struct key *key,\n\t\t\t\t   struct afs_cell *p)\n{\n\tstruct afs_volume *volume, *pvol = NULL;\n\tint ret;\n\n\t/* Arbitrarily pick a volume from the list. */\n\tread_seqlock_excl(&p->volume_lock);\n\tif (!RB_EMPTY_ROOT(&p->volumes))\n\t\tpvol = afs_get_volume(rb_entry(p->volumes.rb_node,\n\t\t\t\t\t       struct afs_volume, cell_node),\n\t\t\t\t      afs_volume_trace_get_query_alias);\n\tread_sequnlock_excl(&p->volume_lock);\n\tif (!pvol)\n\t\treturn 0;\n\n\t_enter(\"%s:%s\", cell->name, pvol->name);\n\n\t/* And see if it's in the new cell. */\n\tvolume = afs_sample_volume(cell, key, pvol->name, pvol->name_len);\n\tif (IS_ERR(volume)) {\n\t\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\t\tif (PTR_ERR(volume) != -ENOMEDIUM)\n\t\t\treturn PTR_ERR(volume);\n\t\t/* That volume is not in the new cell, so not an alias */\n\t\treturn 0;\n\t}\n\n\t/* The new cell has a like-named volume also - compare volume ID,\n\t * server and address lists.\n\t */\n\tret = 0;\n\tif (pvol->vid == volume->vid) {\n\t\trcu_read_lock();\n\t\tif (afs_compare_volume_slists(volume, pvol))\n\t\t\tret = 1;\n\t\trcu_read_unlock();\n\t}\n\n\tafs_put_volume(cell->net, volume, afs_volume_trace_put_query_alias);\n\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\treturn ret;\n}\n\n/*\n * Query the new cell for volumes we know exist in cells we're already using.\n */\nstatic int afs_query_for_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"%s\", cell->name);\n\n\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\thlist_for_each_entry(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (RB_EMPTY_ROOT(&p->volumes))\n\t\t\tcontinue;\n\t\tif (p->root_volume)\n\t\t\tcontinue; /* Ignore cells that have a root.cell volume. */\n\t\tafs_use_cell(p, afs_cell_trace_use_check_alias);\n\t\tmutex_unlock(&cell->net->proc_cells_lock);\n\n\t\tif (afs_query_for_alias_one(cell, key, p) != 0)\n\t\t\tgoto is_alias;\n\n\t\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0) {\n\t\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\n\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t}\n\n\tmutex_unlock(&cell->net->proc_cells_lock);\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\tcell->alias_of = p; /* Transfer our ref */\n\treturn 1;\n}\n\n/*\n * Look up a VLDB record for a volume.\n */\nstatic char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_vl_cursor vc;\n\tchar *cell_name = ERR_PTR(-EDESTADDRREQ);\n\tbool skipped = false, not_skipped = false;\n\tint ret;\n\n\tif (!afs_begin_vlserver_operation(&vc, cell, key))\n\t\treturn ERR_PTR(-ERESTARTSYS);\n\n\twhile (afs_select_vlserver(&vc)) {\n\t\tif (!test_bit(AFS_VLSERVER_FL_IS_YFS, &vc.server->flags)) {\n\t\t\tvc.ac.error = -EOPNOTSUPP;\n\t\t\tskipped = true;\n\t\t\tcontinue;\n\t\t}\n\t\tnot_skipped = true;\n\t\tcell_name = afs_yfsvl_get_cell_name(&vc);\n\t}\n\n\tret = afs_end_vlserver_operation(&vc);\n\tif (skipped && !not_skipped)\n\t\tret = -EOPNOTSUPP;\n\treturn ret < 0 ? ERR_PTR(ret) : cell_name;\n}\n\nstatic int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *master;\n\tchar *cell_name;\n\n\tcell_name = afs_vl_get_cell_name(cell, key);\n\tif (IS_ERR(cell_name))\n\t\treturn PTR_ERR(cell_name);\n\n\tif (strcmp(cell_name, cell->name) == 0) {\n\t\tkfree(cell_name);\n\t\treturn 0;\n\t}\n\n\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n\t\t\t\t NULL, false);\n\tkfree(cell_name);\n\tif (IS_ERR(master))\n\t\treturn PTR_ERR(master);\n\n\tcell->alias_of = master; /* Transfer our ref */\n\treturn 1;\n}\n\nstatic int afs_do_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_volume *root_volume;\n\tint ret;\n\n\t_enter(\"%s\", cell->name);\n\n\tret = yfs_check_canonical_cell_name(cell, key);\n\tif (ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\t/* Try and get the root.cell volume for comparison with other cells */\n\troot_volume = afs_sample_volume(cell, key, \"root.cell\", 9);\n\tif (!IS_ERR(root_volume)) {\n\t\tcell->root_volume = root_volume;\n\t\treturn afs_compare_cell_roots(cell);\n\t}\n\n\tif (PTR_ERR(root_volume) != -ENOMEDIUM)\n\t\treturn PTR_ERR(root_volume);\n\n\t/* Okay, this cell doesn't have an root.cell volume.  We need to\n\t * locate some other random volume and use that to check.\n\t */\n\treturn afs_query_for_alias(cell, key);\n}\n\n/*\n * Check to see if a new cell is an alias of a cell we already have.  At this\n * point we have the cell's volume server list.\n *\n * Returns 0 if we didn't detect an alias, 1 if we found an alias and an error\n * if we had problems gathering the data required.  In the case the we did\n * detect an alias, cell->alias_of is set to point to the assumed master.\n */\nint afs_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_net *net = cell->net;\n\tint ret;\n\n\tif (mutex_lock_interruptible(&net->cells_alias_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\tif (test_bit(AFS_CELL_FL_CHECK_ALIAS, &cell->flags)) {\n\t\tret = afs_do_cell_detect_alias(cell, key);\n\t\tif (ret >= 0)\n\t\t\tclear_bit_unlock(AFS_CELL_FL_CHECK_ALIAS, &cell->flags);\n\t} else {\n\t\tret = cell->alias_of ? 1 : 0;\n\t}\n\n\tmutex_unlock(&net->cells_alias_lock);\n\n\tif (ret == 1)\n\t\tpr_notice(\"kAFS: Cell %s is an alias of %s\\n\",\n\t\t\t  cell->name, cell->alias_of->name);\n\treturn ret;\n}\n",
                                "fs/afs/vlclient.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS Volume Location Service client\n *\n * Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include \"afs_fs.h\"\n#include \"internal.h\"\n\n/*\n * Deliver reply data to a VL.GetEntryByNameU call.\n */\nstatic int afs_deliver_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tstruct afs_uvldbentry__xdr *uvldb;\n\tstruct afs_vldb_entry *entry;\n\tbool new_only = false;\n\tu32 tmp, nr_servers, vlflags;\n\tint i, ret;\n\n\t_enter(\"\");\n\n\tret = afs_transfer_reply(call);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* unmarshall the reply once we've received all of it */\n\tuvldb = call->buffer;\n\tentry = call->ret_vldb;\n\n\tnr_servers = ntohl(uvldb->nServers);\n\tif (nr_servers > AFS_NMAXNSERVERS)\n\t\tnr_servers = AFS_NMAXNSERVERS;\n\n\tfor (i = 0; i < ARRAY_SIZE(uvldb->name) - 1; i++)\n\t\tentry->name[i] = (u8)ntohl(uvldb->name[i]);\n\tentry->name[i] = 0;\n\tentry->name_len = strlen(entry->name);\n\n\t/* If there is a new replication site that we can use, ignore all the\n\t * sites that aren't marked as new.\n\t */\n\tfor (i = 0; i < nr_servers; i++) {\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (!(tmp & AFS_VLSF_DONTUSE) &&\n\t\t    (tmp & AFS_VLSF_NEWREPSITE))\n\t\t\tnew_only = true;\n\t}\n\n\tvlflags = ntohl(uvldb->flags);\n\tfor (i = 0; i < nr_servers; i++) {\n\t\tstruct afs_uuid__xdr *xdr;\n\t\tstruct afs_uuid *uuid;\n\t\tint j;\n\t\tint n = entry->nr_servers;\n\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (tmp & AFS_VLSF_DONTUSE ||\n\t\t    (new_only && !(tmp & AFS_VLSF_NEWREPSITE)))\n\t\t\tcontinue;\n\t\tif (tmp & AFS_VLSF_RWVOL) {\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RW;\n\t\t\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_BAK;\n\t\t}\n\t\tif (tmp & AFS_VLSF_ROVOL)\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RO;\n\t\tif (!entry->fs_mask[n])\n\t\t\tcontinue;\n\n\t\txdr = &uvldb->serverNumber[i];\n\t\tuuid = (struct afs_uuid *)&entry->fs_server[n];\n\t\tuuid->time_low\t\t\t= xdr->time_low;\n\t\tuuid->time_mid\t\t\t= htons(ntohl(xdr->time_mid));\n\t\tuuid->time_hi_and_version\t= htons(ntohl(xdr->time_hi_and_version));\n\t\tuuid->clock_seq_hi_and_reserved\t= (u8)ntohl(xdr->clock_seq_hi_and_reserved);\n\t\tuuid->clock_seq_low\t\t= (u8)ntohl(xdr->clock_seq_low);\n\t\tfor (j = 0; j < 6; j++)\n\t\t\tuuid->node[j] = (u8)ntohl(xdr->node[j]);\n\n\t\tentry->addr_version[n] = ntohl(uvldb->serverUnique[i]);\n\t\tentry->nr_servers++;\n\t}\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tentry->vid[i] = ntohl(uvldb->volumeId[i]);\n\n\tif (vlflags & AFS_VLF_RWEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RW, &entry->flags);\n\tif (vlflags & AFS_VLF_ROEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RO, &entry->flags);\n\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_BAK, &entry->flags);\n\n\tif (!(vlflags & (AFS_VLF_RWEXISTS | AFS_VLF_ROEXISTS | AFS_VLF_BACKEXISTS))) {\n\t\tentry->error = -ENOMEDIUM;\n\t\t__set_bit(AFS_VLDB_QUERY_ERROR, &entry->flags);\n\t}\n\n\t__set_bit(AFS_VLDB_QUERY_VALID, &entry->flags);\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tkfree(call->ret_vldb);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetEntryByNameU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetEntryByNameU = {\n\t.name\t\t= \"VL.GetEntryByNameU\",\n\t.op\t\t= afs_VL_GetEntryByNameU,\n\t.deliver\t= afs_deliver_vl_get_entry_by_name_u,\n\t.destructor\t= afs_destroy_vl_get_entry_by_name_u,\n};\n\n/*\n * Dispatch a get volume entry by name or ID operation (uuid variant).  If the\n * volname is a decimal number then it's a volume ID not a volume name.\n */\nstruct afs_vldb_entry *afs_vl_get_entry_by_name_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t\t  const char *volname,\n\t\t\t\t\t\t  int volnamesz)\n{\n\tstruct afs_vldb_entry *entry;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\tsize_t reqsz, padsz;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tpadsz = (4 - (volnamesz & 3)) & 3;\n\treqsz = 8 + volnamesz + padsz;\n\n\tentry = kzalloc(sizeof(struct afs_vldb_entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetEntryByNameU, reqsz,\n\t\t\t\t   sizeof(struct afs_uvldbentry__xdr));\n\tif (!call) {\n\t\tkfree(entry);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tcall->key = vc->key;\n\tcall->ret_vldb = entry;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETENTRYBYNAMEU);\n\t*bp++ = htonl(volnamesz);\n\tmemcpy(bp, volname, volnamesz);\n\tif (padsz > 0)\n\t\tmemset((void *)bp + volnamesz, 0, padsz);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_vldb_entry *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a VL.GetAddrsU call.\n *\n *\tGetAddrsU(IN ListAddrByAttributes *inaddr,\n *\t\t  OUT afsUUID *uuidp1,\n *\t\t  OUT uint32_t *uniquifier,\n *\t\t  OUT uint32_t *nentries,\n *\t\t  OUT bulkaddrs *blkaddrs);\n */\nstatic int afs_deliver_vl_get_addrs_u(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, nentries, count;\n\tint i, ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call,\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\t\tcall->unmarshall++;\n\n\t\t/* Extract the returned uuid, uniquifier, nentries and\n\t\t * blkaddrs size */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(struct afs_uuid__xdr);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tnentries\t= ntohl(*bp++);\n\t\tcount\t\t= ntohl(*bp);\n\n\t\tnentries = min(nentries, count);\n\t\talist = afs_alloc_addrlist(nentries, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\t\tcall->count = count;\n\t\tcall->count2 = nentries;\n\t\tcall->unmarshall++;\n\n\tmore_entries:\n\t\tcount = min(call->count, 4U);\n\t\tafs_extract_to_buf(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, call->count > 4);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tcount = min(call->count, 4U);\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (alist->nr_addrs < call->count2)\n\t\t\t\tafs_merge_fs_addr4(alist, *bp++, AFS_FS_PORT);\n\n\t\tcall->count -= count;\n\t\tif (call->count > 0)\n\t\t\tgoto more_entries;\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_vl_get_addrs_u_destructor(struct afs_call *call)\n{\n\tafs_put_addrlist(call->ret_alist);\n\treturn afs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetAddrsU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetAddrsU = {\n\t.name\t\t= \"VL.GetAddrsU\",\n\t.op\t\t= afs_VL_GetAddrsU,\n\t.deliver\t= afs_deliver_vl_get_addrs_u,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_vl_get_addrs_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t const uuid_t *uuid)\n{\n\tstruct afs_ListAddrByAttributes__xdr *r;\n\tconst struct afs_uuid *u = (const struct afs_uuid *)uuid;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\tint i;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetAddrsU,\n\t\t\t\t   sizeof(__be32) + sizeof(struct afs_ListAddrByAttributes__xdr),\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETADDRSU);\n\tr = (struct afs_ListAddrByAttributes__xdr *)bp;\n\tr->Mask\t\t= htonl(AFS_VLADDR_UUID);\n\tr->ipaddr\t= 0;\n\tr->index\t= 0;\n\tr->spare\t= 0;\n\tr->uuid.time_low\t\t\t= u->time_low;\n\tr->uuid.time_mid\t\t\t= htonl(ntohs(u->time_mid));\n\tr->uuid.time_hi_and_version\t\t= htonl(ntohs(u->time_hi_and_version));\n\tr->uuid.clock_seq_hi_and_reserved \t= htonl(u->clock_seq_hi_and_reserved);\n\tr->uuid.clock_seq_low\t\t\t= htonl(u->clock_seq_low);\n\tfor (i = 0; i < 6; i++)\n\t\tr->uuid.node[i] = htonl(u->node[i]);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to an VL.GetCapabilities operation.\n */\nstatic int afs_deliver_vl_get_capabilities(struct afs_call *call)\n{\n\tu32 count;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the capabilities word count */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcount = ntohl(call->tmp);\n\t\tcall->count = count;\n\t\tcall->count2 = count;\n\n\t\tcall->unmarshall++;\n\t\tafs_extract_discard(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract capabilities words */\n\tcase 2:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\t/* TODO: Examine capabilities */\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_capabilities(struct afs_call *call)\n{\n\tafs_put_vlserver(call->net, call->vlserver);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_RXVLGetCapabilities = {\n\t.name\t\t= \"VL.GetCapabilities\",\n\t.op\t\t= afs_VL_GetCapabilities,\n\t.deliver\t= afs_deliver_vl_get_capabilities,\n\t.done\t\t= afs_vlserver_probe_result,\n\t.destructor\t= afs_destroy_vl_get_capabilities,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nstruct afs_call *afs_vl_get_capabilities(struct afs_net *net,\n\t\t\t\t\t struct afs_addr_cursor *ac,\n\t\t\t\t\t struct key *key,\n\t\t\t\t\t struct afs_vlserver *server,\n\t\t\t\t\t unsigned int server_index)\n{\n\tstruct afs_call *call;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetCapabilities, 1 * 4, 16 * 4);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = key;\n\tcall->vlserver = afs_get_vlserver(server);\n\tcall->server_index = server_index;\n\tcall->upgrade = true;\n\tcall->async = true;\n\tcall->max_lifespan = AFS_PROBE_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETCAPABILITIES);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(ac, call, GFP_KERNEL);\n\treturn call;\n}\n\n/*\n * Deliver reply data to a YFSVL.GetEndpoints call.\n *\n *\tGetEndpoints(IN yfsServerAttributes *attr,\n *\t\t     OUT opr_uuid *uuid,\n *\t\t     OUT afs_int32 *uniquifier,\n *\t\t     OUT endpoints *fsEndpoints,\n *\t\t     OUT endpoints *volEndpoints)\n */\nstatic int afs_deliver_yfsvl_get_endpoints(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, size;\n\tint ret;\n\n\t_enter(\"{%u,%zu,%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count2);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call, sizeof(uuid_t) + 3 * sizeof(__be32));\n\t\tcall->unmarshall = 1;\n\n\t\t/* Extract the returned uuid, uniquifier, fsEndpoints count and\n\t\t * either the first fsEndpoint type or the volEndpoints\n\t\t * count if there are no fsEndpoints. */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(uuid_t);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tcall->count\t= ntohl(*bp++);\n\t\tcall->count2\t= ntohl(*bp); /* Type or next count */\n\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_num);\n\n\t\talist = afs_alloc_addrlist(call->count, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\n\t\tif (call->count == 0)\n\t\t\tgoto extract_volendpoints;\n\n\tnext_fsendpoint:\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\tsize += sizeof(__be32);\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 2;\n\n\t\tfallthrough;\t/* and extract fsEndpoints[] entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt4_len);\n\t\t\tafs_merge_fs_addr4(alist, bp[1], ntohl(bp[2]));\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt6_len);\n\t\t\tafs_merge_fs_addr6(alist, bp + 1, ntohl(bp[5]));\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count2 = ntohl(*bp++);\n\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_fsendpoint;\n\n\textract_volendpoints:\n\t\t/* Extract the list of volEndpoints. */\n\t\tcall->count = call->count2;\n\t\tif (!call->count)\n\t\t\tgoto end;\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\n\t\tafs_extract_to_buf(call, 1 * sizeof(__be32));\n\t\tcall->unmarshall = 3;\n\n\t\t/* Extract the type of volEndpoints[0].  Normally we would\n\t\t * extract the type of the next endpoint when we extract the\n\t\t * data of the current one, but this is the first...\n\t\t */\n\t\tfallthrough;\n\tcase 3:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\n\tnext_volendpoint:\n\t\tcall->count2 = ntohl(*bp++);\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\tif (call->count > 1)\n\t\t\tsize += sizeof(__be32); /* Get next type too */\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 4;\n\n\t\tfallthrough;\t/* and extract volEndpoints[] entries */\n\tcase 4:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt4_len);\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt6_len);\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_volendpoint;\n\n\tend:\n\t\tafs_extract_discard(call, 0);\n\t\tcall->unmarshall = 5;\n\n\t\tfallthrough;\t/* Done */\n\tcase 5:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tcall->unmarshall = 6;\n\t\tfallthrough;\n\n\tcase 6:\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * YFSVL.GetEndpoints operation type.\n */\nstatic const struct afs_call_type afs_YFSVLGetEndpoints = {\n\t.name\t\t= \"YFSVL.GetEndpoints\",\n\t.op\t\t= afs_YFSVL_GetEndpoints,\n\t.deliver\t= afs_deliver_yfsvl_get_endpoints,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_yfsvl_get_endpoints(struct afs_vl_cursor *vc,\n\t\t\t\t\t      const uuid_t *uuid)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetEndpoints,\n\t\t\t\t   sizeof(__be32) * 2 + sizeof(*uuid),\n\t\t\t\t   sizeof(struct in6_addr) + sizeof(__be32) * 3);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETENDPOINTS);\n\t*bp++ = htonl(YFS_SERVER_UUID);\n\tmemcpy(bp, uuid, sizeof(*uuid)); /* Type opr_uuid */\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a YFSVL.GetCellName operation.\n */\nstatic int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tchar *cell_name;\n\tu32 namesz, paddedsz;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the cell name length */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnamesz = ntohl(call->tmp);\n\t\tif (namesz > AFS_MAXCELLNAME)\n\t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n\t\tpaddedsz = (namesz + 3) & ~3;\n\t\tcall->count = namesz;\n\t\tcall->count2 = paddedsz - namesz;\n\n\t\tcell_name = kmalloc(namesz + 1, GFP_KERNEL);\n\t\tif (!cell_name)\n\t\t\treturn -ENOMEM;\n\t\tcell_name[namesz] = 0;\n\t\tcall->ret_str = cell_name;\n\n\t\tafs_extract_begin(call, cell_name, namesz);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract cell name */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tafs_extract_discard(call, call->count2);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract padding */\n\tcase 3:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tkfree(call->ret_str);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_YFSVLGetCellName = {\n\t.name\t\t= \"YFSVL.GetCellName\",\n\t.op\t\t= afs_YFSVL_GetCellName,\n\t.deliver\t= afs_deliver_yfsvl_get_cell_name,\n\t.destructor\t= afs_destroy_yfsvl_get_cell_name,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nchar *afs_yfsvl_get_cell_name(struct afs_vl_cursor *vc)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetCellName, 1 * 4, 0);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_str = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETCELLNAME);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (char *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "7922b1f058fe24a93730511dd0ae2e1630920096",
                            "downstream_commit": "2976e91a3e569cf2c92c9f71512c0ab1312fe965",
                            "commit_date": "2025-01-17 13:40:43 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file fs/afs/afs.h\npatching file fs/afs/afs_vl.h\npatching file fs/afs/vl_alias.c\npatching file fs/afs/vlclient.c",
                            "downstream_patch_content": "commit 7922b1f058fe24a93730511dd0ae2e1630920096\nAuthor: David Howells <dhowells@redhat.com>\nDate:   Mon Jan 6 16:21:00 2025 +0000\n\n    afs: Fix the maximum cell name length\n    \n    [ Upstream commit 8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8 ]\n    \n    The kafs filesystem limits the maximum length of a cell to 256 bytes, but a\n    problem occurs if someone actually does that: kafs tries to create a\n    directory under /proc/net/afs/ with the name of the cell, but that fails\n    with a warning:\n    \n            WARNING: CPU: 0 PID: 9 at fs/proc/generic.c:405\n    \n    because procfs limits the maximum filename length to 255.\n    \n    However, the DNS limits the maximum lookup length and, by extension, the\n    maximum cell name, to 255 less two (length count and trailing NUL).\n    \n    Fix this by limiting the maximum acceptable cellname length to 253.  This\n    also allows us to be sure we can create the \"/afs/.<cell>/\" mountpoint too.\n    \n    Further, split the YFS VL record cell name maximum to be the 256 allowed by\n    the protocol and ignore the record retrieved by YFSVL.GetCellName if it\n    exceeds 253.\n    \n    Fixes: c3e9f888263b (\"afs: Implement client support for the YFSVL.GetCellName RPC op\")\n    Reported-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/r/6776d25d.050a0220.3a8527.0048.GAE@google.com/\n    Signed-off-by: David Howells <dhowells@redhat.com>\n    Link: https://lore.kernel.org/r/376236.1736180460@warthog.procyon.org.uk\n    Tested-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    cc: Marc Dionne <marc.dionne@auristor.com>\n    cc: linux-afs@lists.infradead.org\n    Signed-off-by: Christian Brauner <brauner@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/fs/afs/afs.h b/fs/afs/afs.h\nindex b488072aee87..ec3db00bd081 100644\n--- a/fs/afs/afs.h\n+++ b/fs/afs/afs.h\n@@ -10,7 +10,7 @@\n \n #include <linux/in.h>\n \n-#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n+#define AFS_MAXCELLNAME\t\t253  \t/* Maximum length of a cell name (DNS limited) */\n #define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n #define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n #define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\ndiff --git a/fs/afs/afs_vl.h b/fs/afs/afs_vl.h\nindex a06296c8827d..b835e25a2c02 100644\n--- a/fs/afs/afs_vl.h\n+++ b/fs/afs/afs_vl.h\n@@ -13,6 +13,7 @@\n #define AFS_VL_PORT\t\t7003\t/* volume location service port */\n #define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n #define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n+#define YFS_VL_MAXCELLNAME\t256  \t/* Maximum length of a cell name in YFS protocol */\n \n enum AFSVL_Operations {\n \tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\ndiff --git a/fs/afs/vl_alias.c b/fs/afs/vl_alias.c\nindex 9f36e14f1c2d..f9e76b604f31 100644\n--- a/fs/afs/vl_alias.c\n+++ b/fs/afs/vl_alias.c\n@@ -253,6 +253,7 @@ static char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n {\n \tstruct afs_cell *master;\n+\tsize_t name_len;\n \tchar *cell_name;\n \n \tcell_name = afs_vl_get_cell_name(cell, key);\n@@ -264,8 +265,11 @@ static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n \t\treturn 0;\n \t}\n \n-\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n-\t\t\t\t NULL, false);\n+\tname_len = strlen(cell_name);\n+\tif (!name_len || name_len > AFS_MAXCELLNAME)\n+\t\tmaster = ERR_PTR(-EOPNOTSUPP);\n+\telse\n+\t\tmaster = afs_lookup_cell(cell->net, cell_name, name_len, NULL, false);\n \tkfree(cell_name);\n \tif (IS_ERR(master))\n \t\treturn PTR_ERR(master);\ndiff --git a/fs/afs/vlclient.c b/fs/afs/vlclient.c\nindex cac75f89b64a..55dd0fc5aad7 100644\n--- a/fs/afs/vlclient.c\n+++ b/fs/afs/vlclient.c\n@@ -697,7 +697,7 @@ static int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n \t\t\treturn ret;\n \n \t\tnamesz = ntohl(call->tmp);\n-\t\tif (namesz > AFS_MAXCELLNAME)\n+\t\tif (namesz > YFS_VL_MAXCELLNAME)\n \t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n \t\tpaddedsz = (namesz + 3) & ~3;\n \t\tcall->count = namesz;\n",
                            "downstream_file_content": {
                                "fs/afs/afs.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS common types\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_H\n#define AFS_H\n\n#include <linux/in.h>\n\n#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n#define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n#define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n#define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\n#define AFS_MAXTYPES\t\t3\t/* Maximum number of volume types */\n#define AFSNAMEMAX\t\t256 \t/* Maximum length of a filename plus NUL */\n#define AFSPATHMAX\t\t1024\t/* Maximum length of a pathname plus NUL */\n#define AFSOPAQUEMAX\t\t1024\t/* Maximum length of an opaque field */\n\n#define AFS_VL_MAX_LIFESPAN\t120\n#define AFS_PROBE_MAX_LIFESPAN\t30\n\ntypedef u64\t\t\tafs_volid_t;\ntypedef u64\t\t\tafs_vnodeid_t;\ntypedef u64\t\t\tafs_dataversion_t;\n\ntypedef enum {\n\tAFSVL_RWVOL,\t\t\t/* read/write volume */\n\tAFSVL_ROVOL,\t\t\t/* read-only volume */\n\tAFSVL_BACKVOL,\t\t\t/* backup volume */\n} __attribute__((packed)) afs_voltype_t;\n\ntypedef enum {\n\tAFS_FTYPE_INVALID\t= 0,\n\tAFS_FTYPE_FILE\t\t= 1,\n\tAFS_FTYPE_DIR\t\t= 2,\n\tAFS_FTYPE_SYMLINK\t= 3,\n} afs_file_type_t;\n\ntypedef enum {\n\tAFS_LOCK_READ\t\t= 0,\t/* read lock request */\n\tAFS_LOCK_WRITE\t\t= 1,\t/* write lock request */\n} afs_lock_type_t;\n\n#define AFS_LOCKWAIT\t\t(5 * 60) /* time until a lock times out (seconds) */\n\n/*\n * AFS file identifier\n */\nstruct afs_fid {\n\tafs_volid_t\tvid;\t\t/* volume ID */\n\tafs_vnodeid_t\tvnode;\t\t/* Lower 64-bits of file index within volume */\n\tu32\t\tvnode_hi;\t/* Upper 32-bits of file index */\n\tu32\t\tunique;\t\t/* unique ID number (file index version) */\n};\n\n/*\n * AFS callback notification\n */\ntypedef enum {\n\tAFSCM_CB_UNTYPED\t= 0,\t/* no type set on CB break */\n\tAFSCM_CB_EXCLUSIVE\t= 1,\t/* CB exclusive to CM [not implemented] */\n\tAFSCM_CB_SHARED\t\t= 2,\t/* CB shared by other CM's */\n\tAFSCM_CB_DROPPED\t= 3,\t/* CB promise cancelled by file server */\n} afs_callback_type_t;\n\nstruct afs_callback {\n\ttime64_t\t\texpires_at;\t/* Time at which expires */\n\t//unsigned\t\tversion;\t/* Callback version */\n\t//afs_callback_type_t\ttype;\t\t/* Type of callback */\n};\n\nstruct afs_callback_break {\n\tstruct afs_fid\t\tfid;\t\t/* File identifier */\n\t//struct afs_callback\tcb;\t\t/* Callback details */\n};\n\n#define AFSCBMAX 50\t/* maximum callbacks transferred per bulk op */\n\nstruct afs_uuid {\n\t__be32\t\ttime_low;\t\t\t/* low part of timestamp */\n\t__be16\t\ttime_mid;\t\t\t/* mid part of timestamp */\n\t__be16\t\ttime_hi_and_version;\t\t/* high part of timestamp and version  */\n\t__s8\t\tclock_seq_hi_and_reserved;\t/* clock seq hi and variant */\n\t__s8\t\tclock_seq_low;\t\t\t/* clock seq low */\n\t__s8\t\tnode[6];\t\t\t/* spatially unique node ID (MAC addr) */\n};\n\n/*\n * AFS volume information\n */\nstruct afs_volume_info {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_voltype_t\t\ttype;\t\t/* type of this volume */\n\tafs_volid_t\t\ttype_vids[5];\t/* volume ID's for possible types for this vol */\n\n\t/* list of fileservers serving this volume */\n\tsize_t\t\t\tnservers;\t/* number of entries used in servers[] */\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* fileserver address */\n\t} servers[8];\n};\n\n/*\n * AFS security ACE access mask\n */\ntypedef u32 afs_access_t;\n#define AFS_ACE_READ\t\t0x00000001U\t/* - permission to read a file/dir */\n#define AFS_ACE_WRITE\t\t0x00000002U\t/* - permission to write/chmod a file */\n#define AFS_ACE_INSERT\t\t0x00000004U\t/* - permission to create dirent in a dir */\n#define AFS_ACE_LOOKUP\t\t0x00000008U\t/* - permission to lookup a file/dir in a dir */\n#define AFS_ACE_DELETE\t\t0x00000010U\t/* - permission to delete a dirent from a dir */\n#define AFS_ACE_LOCK\t\t0x00000020U\t/* - permission to lock a file */\n#define AFS_ACE_ADMINISTER\t0x00000040U\t/* - permission to change ACL */\n#define AFS_ACE_USER_A\t\t0x01000000U\t/* - 'A' user-defined permission */\n#define AFS_ACE_USER_B\t\t0x02000000U\t/* - 'B' user-defined permission */\n#define AFS_ACE_USER_C\t\t0x04000000U\t/* - 'C' user-defined permission */\n#define AFS_ACE_USER_D\t\t0x08000000U\t/* - 'D' user-defined permission */\n#define AFS_ACE_USER_E\t\t0x10000000U\t/* - 'E' user-defined permission */\n#define AFS_ACE_USER_F\t\t0x20000000U\t/* - 'F' user-defined permission */\n#define AFS_ACE_USER_G\t\t0x40000000U\t/* - 'G' user-defined permission */\n#define AFS_ACE_USER_H\t\t0x80000000U\t/* - 'H' user-defined permission */\n\n/*\n * AFS file status information\n */\nstruct afs_file_status {\n\tu64\t\t\tsize;\t\t/* file size */\n\tafs_dataversion_t\tdata_version;\t/* current data version */\n\tstruct timespec64\tmtime_client;\t/* Last time client changed data */\n\tstruct timespec64\tmtime_server;\t/* Last time server changed data */\n\ts64\t\t\tauthor;\t\t/* author ID */\n\ts64\t\t\towner;\t\t/* owner ID */\n\ts64\t\t\tgroup;\t\t/* group ID */\n\tafs_access_t\t\tcaller_access;\t/* access rights for authenticated caller */\n\tafs_access_t\t\tanon_access;\t/* access rights for unauthenticated caller */\n\tumode_t\t\t\tmode;\t\t/* UNIX mode */\n\tafs_file_type_t\t\ttype;\t\t/* file type */\n\tu32\t\t\tnlink;\t\t/* link count */\n\ts32\t\t\tlock_count;\t/* file lock count (0=UNLK -1=WRLCK +ve=#RDLCK */\n\tu32\t\t\tabort_code;\t/* Abort if bulk-fetching this failed */\n};\n\nstruct afs_status_cb {\n\tstruct afs_file_status\tstatus;\n\tstruct afs_callback\tcallback;\n\tbool\t\t\thave_status;\t/* True if status record was retrieved */\n\tbool\t\t\thave_cb;\t/* True if cb record was retrieved */\n\tbool\t\t\thave_error;\t/* True if status.abort_code indicates an error */\n};\n\n/*\n * AFS file status change request\n */\n\n#define AFS_SET_MTIME\t\t0x01\t\t/* set the mtime */\n#define AFS_SET_OWNER\t\t0x02\t\t/* set the owner ID */\n#define AFS_SET_GROUP\t\t0x04\t\t/* set the group ID (unsupported?) */\n#define AFS_SET_MODE\t\t0x08\t\t/* set the UNIX mode */\n#define AFS_SET_SEG_SIZE\t0x10\t\t/* set the segment size (unsupported) */\n\n/*\n * AFS volume synchronisation information\n */\nstruct afs_volsync {\n\ttime64_t\t\tcreation;\t/* Volume creation time (or TIME64_MIN) */\n\ttime64_t\t\tupdate;\t\t/* Volume update time (or TIME64_MIN) */\n};\n\n/*\n * AFS volume status record\n */\nstruct afs_volume_status {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_volid_t\t\tparent_id;\t/* parent volume ID */\n\tu8\t\t\tonline;\t\t/* true if volume currently online and available */\n\tu8\t\t\tin_service;\t/* true if volume currently in service */\n\tu8\t\t\tblessed;\t/* same as in_service */\n\tu8\t\t\tneeds_salvage;\t/* true if consistency checking required */\n\tu32\t\t\ttype;\t\t/* volume type (afs_voltype_t) */\n\tu64\t\t\tmin_quota;\t/* minimum space set aside (blocks) */\n\tu64\t\t\tmax_quota;\t/* maximum space this volume may occupy (blocks) */\n\tu64\t\t\tblocks_in_use;\t/* space this volume currently occupies (blocks) */\n\tu64\t\t\tpart_blocks_avail; /* space available in volume's partition */\n\tu64\t\t\tpart_max_blocks; /* size of volume's partition */\n\ts64\t\t\tvol_copy_date;\n\ts64\t\t\tvol_backup_date;\n};\n\n#define AFS_BLOCK_SIZE\t1024\n\n/*\n * XDR encoding of UUID in AFS.\n */\nstruct afs_uuid__xdr {\n\t__be32\t\ttime_low;\n\t__be32\t\ttime_mid;\n\t__be32\t\ttime_hi_and_version;\n\t__be32\t\tclock_seq_hi_and_reserved;\n\t__be32\t\tclock_seq_low;\n\t__be32\t\tnode[6];\n};\n\n#endif /* AFS_H */\n",
                                "fs/afs/afs_vl.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS Volume Location Service client interface\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_VL_H\n#define AFS_VL_H\n\n#include \"afs.h\"\n\n#define AFS_VL_PORT\t\t7003\t/* volume location service port */\n#define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n#define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n\nenum AFSVL_Operations {\n\tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\n\tVLGETENTRYBYNAME\t= 504,\t/* AFS Get VLDB entry by name */\n\tVLPROBE\t\t\t= 514,\t/* AFS probe VL service */\n\tVLGETENTRYBYIDU\t\t= 526,\t/* AFS Get VLDB entry by ID (UUID-variant) */\n\tVLGETENTRYBYNAMEU\t= 527,\t/* AFS Get VLDB entry by name (UUID-variant) */\n\tVLGETADDRSU\t\t= 533,\t/* AFS Get addrs for fileserver */\n\tYVLGETENDPOINTS\t\t= 64002, /* YFS Get endpoints for file/volume server */\n\tYVLGETCELLNAME\t\t= 64014, /* YFS Get actual cell name */\n\tVLGETCAPABILITIES\t= 65537, /* AFS Get server capabilities */\n};\n\nenum AFSVL_Errors {\n\tAFSVL_IDEXIST \t\t= 363520,\t/* Volume Id entry exists in vl database */\n\tAFSVL_IO \t\t= 363521,\t/* I/O related error */\n\tAFSVL_NAMEEXIST \t= 363522,\t/* Volume name entry exists in vl database */\n\tAFSVL_CREATEFAIL \t= 363523,\t/* Internal creation failure */\n\tAFSVL_NOENT \t\t= 363524,\t/* No such entry */\n\tAFSVL_EMPTY \t\t= 363525,\t/* Vl database is empty */\n\tAFSVL_ENTDELETED \t= 363526,\t/* Entry is deleted (soft delete) */\n\tAFSVL_BADNAME \t\t= 363527,\t/* Volume name is illegal */\n\tAFSVL_BADINDEX \t\t= 363528,\t/* Index is out of range */\n\tAFSVL_BADVOLTYPE \t= 363529,\t/* Bad volume type */\n\tAFSVL_BADSERVER \t= 363530,\t/* Illegal server number (out of range) */\n\tAFSVL_BADPARTITION \t= 363531,\t/* Bad partition number */\n\tAFSVL_REPSFULL \t\t= 363532,\t/* Run out of space for Replication sites */\n\tAFSVL_NOREPSERVER \t= 363533,\t/* No such Replication server site exists */\n\tAFSVL_DUPREPSERVER \t= 363534,\t/* Replication site already exists */\n\tAFSVL_RWNOTFOUND \t= 363535,\t/* Parent R/W entry not found */\n\tAFSVL_BADREFCOUNT \t= 363536,\t/* Illegal Reference Count number */\n\tAFSVL_SIZEEXCEEDED \t= 363537,\t/* Vl size for attributes exceeded */\n\tAFSVL_BADENTRY \t\t= 363538,\t/* Bad incoming vl entry */\n\tAFSVL_BADVOLIDBUMP \t= 363539,\t/* Illegal max volid increment */\n\tAFSVL_IDALREADYHASHED \t= 363540,\t/* RO/BACK id already hashed */\n\tAFSVL_ENTRYLOCKED \t= 363541,\t/* Vl entry is already locked */\n\tAFSVL_BADVOLOPER \t= 363542,\t/* Bad volume operation code */\n\tAFSVL_BADRELLOCKTYPE \t= 363543,\t/* Bad release lock type */\n\tAFSVL_RERELEASE \t= 363544,\t/* Status report: last release was aborted */\n\tAFSVL_BADSERVERFLAG \t= 363545,\t/* Invalid replication site server flag */\n\tAFSVL_PERM \t\t= 363546,\t/* No permission access */\n\tAFSVL_NOMEM \t\t= 363547,\t/* malloc/realloc failed to alloc enough memory */\n};\n\nenum {\n\tYFS_SERVER_INDEX\t= 0,\n\tYFS_SERVER_UUID\t\t= 1,\n\tYFS_SERVER_ENDPOINT\t= 2,\n};\n\nenum {\n\tYFS_ENDPOINT_IPV4\t= 0,\n\tYFS_ENDPOINT_IPV6\t= 1,\n};\n\n#define YFS_MAXENDPOINTS\t16\n\n/*\n * maps to \"struct vldbentry\" in vvl-spec.pdf\n */\nstruct afs_vldbentry {\n\tchar\t\tname[65];\t\t/* name of volume (with NUL char) */\n\tafs_voltype_t\ttype;\t\t\t/* volume type */\n\tunsigned\tnum_servers;\t\t/* num servers that hold instances of this vol */\n\tunsigned\tclone_id;\t\t/* cloning ID */\n\n\tunsigned\tflags;\n#define AFS_VLF_RWEXISTS\t0x1000\t\t/* R/W volume exists */\n#define AFS_VLF_ROEXISTS\t0x2000\t\t/* R/O volume exists */\n#define AFS_VLF_BACKEXISTS\t0x4000\t\t/* backup volume exists */\n\n\tafs_volid_t\tvolume_ids[3];\t\t/* volume IDs */\n\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* server address */\n\t\tunsigned\tpartition;\t/* partition ID on this server */\n\t\tunsigned\tflags;\t\t/* server specific flags */\n#define AFS_VLSF_NEWREPSITE\t0x0001\t/* Ignore all 'non-new' servers */\n#define AFS_VLSF_ROVOL\t\t0x0002\t/* this server holds a R/O instance of the volume */\n#define AFS_VLSF_RWVOL\t\t0x0004\t/* this server holds a R/W instance of the volume */\n#define AFS_VLSF_BACKVOL\t0x0008\t/* this server holds a backup instance of the volume */\n#define AFS_VLSF_UUID\t\t0x0010\t/* This server is referred to by its UUID */\n#define AFS_VLSF_DONTUSE\t0x0020\t/* This server ref should be ignored */\n\t} servers[8];\n};\n\n#define AFS_VLDB_MAXNAMELEN 65\n\n\nstruct afs_ListAddrByAttributes__xdr {\n\t__be32\t\t\tMask;\n#define AFS_VLADDR_IPADDR\t0x1\t/* Match by ->ipaddr */\n#define AFS_VLADDR_INDEX\t0x2\t/* Match by ->index */\n#define AFS_VLADDR_UUID\t\t0x4\t/* Match by ->uuid */\n\t__be32\t\t\tipaddr;\n\t__be32\t\t\tindex;\n\t__be32\t\t\tspare;\n\tstruct afs_uuid__xdr\tuuid;\n};\n\nstruct afs_uvldbentry__xdr {\n\t__be32\t\t\tname[AFS_VLDB_MAXNAMELEN];\n\t__be32\t\t\tnServers;\n\tstruct afs_uuid__xdr\tserverNumber[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverUnique[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverPartition[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverFlags[AFS_NMAXNSERVERS];\n\t__be32\t\t\tvolumeId[AFS_MAXTYPES];\n\t__be32\t\t\tcloneId;\n\t__be32\t\t\tflags;\n\t__be32\t\t\tspares1;\n\t__be32\t\t\tspares2;\n\t__be32\t\t\tspares3;\n\t__be32\t\t\tspares4;\n\t__be32\t\t\tspares5;\n\t__be32\t\t\tspares6;\n\t__be32\t\t\tspares7;\n\t__be32\t\t\tspares8;\n\t__be32\t\t\tspares9;\n};\n\n#endif /* AFS_VL_H */\n",
                                "fs/afs/vl_alias.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS cell alias detection\n *\n * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/namei.h>\n#include <keys/rxrpc-type.h>\n#include \"internal.h\"\n\n/*\n * Sample a volume.\n */\nstatic struct afs_volume *afs_sample_volume(struct afs_cell *cell, struct key *key,\n\t\t\t\t\t    const char *name, unsigned int namelen)\n{\n\tstruct afs_volume *volume;\n\tstruct afs_fs_context fc = {\n\t\t.type\t\t= 0, /* Explicitly leave it to the VLDB */\n\t\t.volnamesz\t= namelen,\n\t\t.volname\t= name,\n\t\t.net\t\t= cell->net,\n\t\t.cell\t\t= cell,\n\t\t.key\t\t= key, /* This might need to be something */\n\t};\n\n\tvolume = afs_create_volume(&fc);\n\t_leave(\" = %p\", volume);\n\treturn volume;\n}\n\n/*\n * Compare the address lists of a pair of fileservers.\n */\nstatic int afs_compare_fs_alists(const struct afs_server *server_a,\n\t\t\t\t const struct afs_server *server_b)\n{\n\tconst struct afs_addr_list *la, *lb;\n\tint a = 0, b = 0, addr_matches = 0;\n\n\tla = rcu_dereference(server_a->endpoint_state)->addresses;\n\tlb = rcu_dereference(server_b->endpoint_state)->addresses;\n\n\twhile (a < la->nr_addrs && b < lb->nr_addrs) {\n\t\tunsigned long pa = (unsigned long)la->addrs[a].peer;\n\t\tunsigned long pb = (unsigned long)lb->addrs[b].peer;\n\t\tlong diff = pa - pb;\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\taddr_matches++;\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\treturn addr_matches;\n}\n\n/*\n * Compare the fileserver lists of two volumes.  The server lists are sorted in\n * order of ascending UUID.\n */\nstatic int afs_compare_volume_slists(const struct afs_volume *vol_a,\n\t\t\t\t     const struct afs_volume *vol_b)\n{\n\tconst struct afs_server_list *la, *lb;\n\tint i, a = 0, b = 0, uuid_matches = 0, addr_matches = 0;\n\n\tla = rcu_dereference(vol_a->servers);\n\tlb = rcu_dereference(vol_b->servers);\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tif (vol_a->vids[i] != vol_b->vids[i])\n\t\t\treturn 0;\n\n\twhile (a < la->nr_servers && b < lb->nr_servers) {\n\t\tconst struct afs_server *server_a = la->servers[a].server;\n\t\tconst struct afs_server *server_b = lb->servers[b].server;\n\t\tint diff = memcmp(&server_a->uuid, &server_b->uuid, sizeof(uuid_t));\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\tuuid_matches++;\n\t\t\taddr_matches += afs_compare_fs_alists(server_a, server_b);\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\t_leave(\" = %d [um %d]\", addr_matches, uuid_matches);\n\treturn addr_matches;\n}\n\n/*\n * Compare root.cell volumes.\n */\nstatic int afs_compare_cell_roots(struct afs_cell *cell)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"\");\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (!p->root_volume)\n\t\t\tcontinue; /* Ignore cells that don't have a root.cell volume. */\n\n\t\tif (afs_compare_volume_slists(cell->root_volume, p->root_volume) != 0)\n\t\t\tgoto is_alias;\n\t}\n\n\trcu_read_unlock();\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\trcu_read_unlock();\n\tcell->alias_of = afs_use_cell(p, afs_cell_trace_use_alias);\n\treturn 1;\n}\n\n/*\n * Query the new cell for a volume from a cell we're already using.\n */\nstatic int afs_query_for_alias_one(struct afs_cell *cell, struct key *key,\n\t\t\t\t   struct afs_cell *p)\n{\n\tstruct afs_volume *volume, *pvol = NULL;\n\tint ret;\n\n\t/* Arbitrarily pick a volume from the list. */\n\tread_seqlock_excl(&p->volume_lock);\n\tif (!RB_EMPTY_ROOT(&p->volumes))\n\t\tpvol = afs_get_volume(rb_entry(p->volumes.rb_node,\n\t\t\t\t\t       struct afs_volume, cell_node),\n\t\t\t\t      afs_volume_trace_get_query_alias);\n\tread_sequnlock_excl(&p->volume_lock);\n\tif (!pvol)\n\t\treturn 0;\n\n\t_enter(\"%s:%s\", cell->name, pvol->name);\n\n\t/* And see if it's in the new cell. */\n\tvolume = afs_sample_volume(cell, key, pvol->name, pvol->name_len);\n\tif (IS_ERR(volume)) {\n\t\tafs_put_volume(pvol, afs_volume_trace_put_query_alias);\n\t\tif (PTR_ERR(volume) != -ENOMEDIUM)\n\t\t\treturn PTR_ERR(volume);\n\t\t/* That volume is not in the new cell, so not an alias */\n\t\treturn 0;\n\t}\n\n\t/* The new cell has a like-named volume also - compare volume ID,\n\t * server and address lists.\n\t */\n\tret = 0;\n\tif (pvol->vid == volume->vid) {\n\t\trcu_read_lock();\n\t\tif (afs_compare_volume_slists(volume, pvol))\n\t\t\tret = 1;\n\t\trcu_read_unlock();\n\t}\n\n\tafs_put_volume(volume, afs_volume_trace_put_query_alias);\n\tafs_put_volume(pvol, afs_volume_trace_put_query_alias);\n\treturn ret;\n}\n\n/*\n * Query the new cell for volumes we know exist in cells we're already using.\n */\nstatic int afs_query_for_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"%s\", cell->name);\n\n\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\thlist_for_each_entry(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (RB_EMPTY_ROOT(&p->volumes))\n\t\t\tcontinue;\n\t\tif (p->root_volume)\n\t\t\tcontinue; /* Ignore cells that have a root.cell volume. */\n\t\tafs_use_cell(p, afs_cell_trace_use_check_alias);\n\t\tmutex_unlock(&cell->net->proc_cells_lock);\n\n\t\tif (afs_query_for_alias_one(cell, key, p) != 0)\n\t\t\tgoto is_alias;\n\n\t\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0) {\n\t\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\n\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t}\n\n\tmutex_unlock(&cell->net->proc_cells_lock);\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\tcell->alias_of = p; /* Transfer our ref */\n\treturn 1;\n}\n\n/*\n * Look up a VLDB record for a volume.\n */\nstatic char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_vl_cursor vc;\n\tchar *cell_name = ERR_PTR(-EDESTADDRREQ);\n\tbool skipped = false, not_skipped = false;\n\tint ret;\n\n\tif (!afs_begin_vlserver_operation(&vc, cell, key))\n\t\treturn ERR_PTR(-ERESTARTSYS);\n\n\twhile (afs_select_vlserver(&vc)) {\n\t\tif (!test_bit(AFS_VLSERVER_FL_IS_YFS, &vc.server->flags)) {\n\t\t\tvc.call_error = -EOPNOTSUPP;\n\t\t\tskipped = true;\n\t\t\tcontinue;\n\t\t}\n\t\tnot_skipped = true;\n\t\tcell_name = afs_yfsvl_get_cell_name(&vc);\n\t}\n\n\tret = afs_end_vlserver_operation(&vc);\n\tif (skipped && !not_skipped)\n\t\tret = -EOPNOTSUPP;\n\treturn ret < 0 ? ERR_PTR(ret) : cell_name;\n}\n\nstatic int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *master;\n\tchar *cell_name;\n\n\tcell_name = afs_vl_get_cell_name(cell, key);\n\tif (IS_ERR(cell_name))\n\t\treturn PTR_ERR(cell_name);\n\n\tif (strcmp(cell_name, cell->name) == 0) {\n\t\tkfree(cell_name);\n\t\treturn 0;\n\t}\n\n\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n\t\t\t\t NULL, false);\n\tkfree(cell_name);\n\tif (IS_ERR(master))\n\t\treturn PTR_ERR(master);\n\n\tcell->alias_of = master; /* Transfer our ref */\n\treturn 1;\n}\n\nstatic int afs_do_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_volume *root_volume;\n\tint ret;\n\n\t_enter(\"%s\", cell->name);\n\n\tret = yfs_check_canonical_cell_name(cell, key);\n\tif (ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\t/* Try and get the root.cell volume for comparison with other cells */\n\troot_volume = afs_sample_volume(cell, key, \"root.cell\", 9);\n\tif (!IS_ERR(root_volume)) {\n\t\tcell->root_volume = root_volume;\n\t\treturn afs_compare_cell_roots(cell);\n\t}\n\n\tif (PTR_ERR(root_volume) != -ENOMEDIUM)\n\t\treturn PTR_ERR(root_volume);\n\n\t/* Okay, this cell doesn't have an root.cell volume.  We need to\n\t * locate some other random volume and use that to check.\n\t */\n\treturn afs_query_for_alias(cell, key);\n}\n\n/*\n * Check to see if a new cell is an alias of a cell we already have.  At this\n * point we have the cell's volume server list.\n *\n * Returns 0 if we didn't detect an alias, 1 if we found an alias and an error\n * if we had problems gathering the data required.  In the case the we did\n * detect an alias, cell->alias_of is set to point to the assumed master.\n */\nint afs_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_net *net = cell->net;\n\tint ret;\n\n\tif (mutex_lock_interruptible(&net->cells_alias_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\tif (test_bit(AFS_CELL_FL_CHECK_ALIAS, &cell->flags)) {\n\t\tret = afs_do_cell_detect_alias(cell, key);\n\t\tif (ret >= 0)\n\t\t\tclear_bit_unlock(AFS_CELL_FL_CHECK_ALIAS, &cell->flags);\n\t} else {\n\t\tret = cell->alias_of ? 1 : 0;\n\t}\n\n\tmutex_unlock(&net->cells_alias_lock);\n\n\tif (ret == 1)\n\t\tpr_notice(\"kAFS: Cell %s is an alias of %s\\n\",\n\t\t\t  cell->name, cell->alias_of->name);\n\treturn ret;\n}\n",
                                "fs/afs/vlclient.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS Volume Location Service client\n *\n * Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include \"afs_fs.h\"\n#include \"internal.h\"\n\n/*\n * Deliver reply data to a VL.GetEntryByNameU call.\n */\nstatic int afs_deliver_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tstruct afs_uvldbentry__xdr *uvldb;\n\tstruct afs_vldb_entry *entry;\n\tu32 nr_servers, vlflags;\n\tint i, ret;\n\n\t_enter(\"\");\n\n\tret = afs_transfer_reply(call);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* unmarshall the reply once we've received all of it */\n\tuvldb = call->buffer;\n\tentry = call->ret_vldb;\n\n\tnr_servers = ntohl(uvldb->nServers);\n\tif (nr_servers > AFS_NMAXNSERVERS)\n\t\tnr_servers = AFS_NMAXNSERVERS;\n\n\tfor (i = 0; i < ARRAY_SIZE(uvldb->name) - 1; i++)\n\t\tentry->name[i] = (u8)ntohl(uvldb->name[i]);\n\tentry->name[i] = 0;\n\tentry->name_len = strlen(entry->name);\n\n\tvlflags = ntohl(uvldb->flags);\n\tfor (i = 0; i < nr_servers; i++) {\n\t\tstruct afs_uuid__xdr *xdr;\n\t\tstruct afs_uuid *uuid;\n\t\tu32 tmp = ntohl(uvldb->serverFlags[i]);\n\t\tint j;\n\t\tint n = entry->nr_servers;\n\n\t\tif (tmp & AFS_VLSF_RWVOL) {\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RW;\n\t\t\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_BAK;\n\t\t}\n\t\tif (tmp & AFS_VLSF_ROVOL)\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RO;\n\t\tif (!entry->fs_mask[n])\n\t\t\tcontinue;\n\n\t\txdr = &uvldb->serverNumber[i];\n\t\tuuid = (struct afs_uuid *)&entry->fs_server[n];\n\t\tuuid->time_low\t\t\t= xdr->time_low;\n\t\tuuid->time_mid\t\t\t= htons(ntohl(xdr->time_mid));\n\t\tuuid->time_hi_and_version\t= htons(ntohl(xdr->time_hi_and_version));\n\t\tuuid->clock_seq_hi_and_reserved\t= (u8)ntohl(xdr->clock_seq_hi_and_reserved);\n\t\tuuid->clock_seq_low\t\t= (u8)ntohl(xdr->clock_seq_low);\n\t\tfor (j = 0; j < 6; j++)\n\t\t\tuuid->node[j] = (u8)ntohl(xdr->node[j]);\n\n\t\tentry->vlsf_flags[n] = tmp;\n\t\tentry->addr_version[n] = ntohl(uvldb->serverUnique[i]);\n\t\tentry->nr_servers++;\n\t}\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tentry->vid[i] = ntohl(uvldb->volumeId[i]);\n\n\tif (vlflags & AFS_VLF_RWEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RW, &entry->flags);\n\tif (vlflags & AFS_VLF_ROEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RO, &entry->flags);\n\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_BAK, &entry->flags);\n\n\tif (!(vlflags & (AFS_VLF_RWEXISTS | AFS_VLF_ROEXISTS | AFS_VLF_BACKEXISTS))) {\n\t\tentry->error = -ENOMEDIUM;\n\t\t__set_bit(AFS_VLDB_QUERY_ERROR, &entry->flags);\n\t}\n\n\t__set_bit(AFS_VLDB_QUERY_VALID, &entry->flags);\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * VL.GetEntryByNameU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetEntryByNameU = {\n\t.name\t\t= \"VL.GetEntryByNameU\",\n\t.op\t\t= afs_VL_GetEntryByNameU,\n\t.deliver\t= afs_deliver_vl_get_entry_by_name_u,\n\t.destructor\t= afs_flat_call_destructor,\n};\n\n/*\n * Dispatch a get volume entry by name or ID operation (uuid variant).  If the\n * volname is a decimal number then it's a volume ID not a volume name.\n */\nstruct afs_vldb_entry *afs_vl_get_entry_by_name_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t\t  const char *volname,\n\t\t\t\t\t\t  int volnamesz)\n{\n\tstruct afs_vldb_entry *entry;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\tsize_t reqsz, padsz;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tpadsz = (4 - (volnamesz & 3)) & 3;\n\treqsz = 8 + volnamesz + padsz;\n\n\tentry = kzalloc(sizeof(struct afs_vldb_entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetEntryByNameU, reqsz,\n\t\t\t\t   sizeof(struct afs_uvldbentry__xdr));\n\tif (!call) {\n\t\tkfree(entry);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tcall->key = vc->key;\n\tcall->ret_vldb = entry;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\tcall->peer = rxrpc_kernel_get_peer(vc->alist->addrs[vc->addr_index].peer);\n\tcall->service_id = vc->server->service_id;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETENTRYBYNAMEU);\n\t*bp++ = htonl(volnamesz);\n\tmemcpy(bp, volname, volnamesz);\n\tif (padsz > 0)\n\t\tmemset((void *)bp + volnamesz, 0, padsz);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(call, GFP_KERNEL);\n\tafs_wait_for_call_to_complete(call);\n\tvc->call_abort_code\t= call->abort_code;\n\tvc->call_error\t\t= call->error;\n\tvc->call_responded\t= call->responded;\n\tafs_put_call(call);\n\tif (vc->call_error) {\n\t\tkfree(entry);\n\t\treturn ERR_PTR(vc->call_error);\n\t}\n\treturn entry;\n}\n\n/*\n * Deliver reply data to a VL.GetAddrsU call.\n *\n *\tGetAddrsU(IN ListAddrByAttributes *inaddr,\n *\t\t  OUT afsUUID *uuidp1,\n *\t\t  OUT uint32_t *uniquifier,\n *\t\t  OUT uint32_t *nentries,\n *\t\t  OUT bulkaddrs *blkaddrs);\n */\nstatic int afs_deliver_vl_get_addrs_u(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, nentries, count;\n\tint i, ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call,\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\t\tcall->unmarshall++;\n\n\t\t/* Extract the returned uuid, uniquifier, nentries and\n\t\t * blkaddrs size */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(struct afs_uuid__xdr);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tnentries\t= ntohl(*bp++);\n\t\tcount\t\t= ntohl(*bp);\n\n\t\tnentries = min(nentries, count);\n\t\talist = afs_alloc_addrlist(nentries);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\t\tcall->count = count;\n\t\tcall->count2 = nentries;\n\t\tcall->unmarshall++;\n\n\tmore_entries:\n\t\tcount = min(call->count, 4U);\n\t\tafs_extract_to_buf(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, call->count > 4);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tcount = min(call->count, 4U);\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tif (alist->nr_addrs < call->count2) {\n\t\t\t\tret = afs_merge_fs_addr4(call->net, alist, *bp++, AFS_FS_PORT);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tcall->count -= count;\n\t\tif (call->count > 0)\n\t\t\tgoto more_entries;\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * VL.GetAddrsU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetAddrsU = {\n\t.name\t\t= \"VL.GetAddrsU\",\n\t.op\t\t= afs_VL_GetAddrsU,\n\t.deliver\t= afs_deliver_vl_get_addrs_u,\n\t.destructor\t= afs_flat_call_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_vl_get_addrs_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t const uuid_t *uuid)\n{\n\tstruct afs_ListAddrByAttributes__xdr *r;\n\tstruct afs_addr_list *alist;\n\tconst struct afs_uuid *u = (const struct afs_uuid *)uuid;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\tint i;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetAddrsU,\n\t\t\t\t   sizeof(__be32) + sizeof(struct afs_ListAddrByAttributes__xdr),\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\tcall->peer = rxrpc_kernel_get_peer(vc->alist->addrs[vc->addr_index].peer);\n\tcall->service_id = vc->server->service_id;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETADDRSU);\n\tr = (struct afs_ListAddrByAttributes__xdr *)bp;\n\tr->Mask\t\t= htonl(AFS_VLADDR_UUID);\n\tr->ipaddr\t= 0;\n\tr->index\t= 0;\n\tr->spare\t= 0;\n\tr->uuid.time_low\t\t\t= u->time_low;\n\tr->uuid.time_mid\t\t\t= htonl(ntohs(u->time_mid));\n\tr->uuid.time_hi_and_version\t\t= htonl(ntohs(u->time_hi_and_version));\n\tr->uuid.clock_seq_hi_and_reserved \t= htonl(u->clock_seq_hi_and_reserved);\n\tr->uuid.clock_seq_low\t\t\t= htonl(u->clock_seq_low);\n\tfor (i = 0; i < 6; i++)\n\t\tr->uuid.node[i] = htonl(u->node[i]);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(call, GFP_KERNEL);\n\tafs_wait_for_call_to_complete(call);\n\tvc->call_abort_code\t= call->abort_code;\n\tvc->call_error\t\t= call->error;\n\tvc->call_responded\t= call->responded;\n\talist\t\t\t= call->ret_alist;\n\tafs_put_call(call);\n\tif (vc->call_error) {\n\t\tafs_put_addrlist(alist, afs_alist_trace_put_getaddru);\n\t\treturn ERR_PTR(vc->call_error);\n\t}\n\treturn alist;\n}\n\n/*\n * Deliver reply data to an VL.GetCapabilities operation.\n */\nstatic int afs_deliver_vl_get_capabilities(struct afs_call *call)\n{\n\tu32 count;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the capabilities word count */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcount = ntohl(call->tmp);\n\t\tcall->count = count;\n\t\tcall->count2 = count;\n\n\t\tcall->unmarshall++;\n\t\tafs_extract_discard(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract capabilities words */\n\tcase 2:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\t/* TODO: Examine capabilities */\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_capabilities(struct afs_call *call)\n{\n\tafs_put_addrlist(call->vl_probe, afs_alist_trace_put_vlgetcaps);\n\tafs_put_vlserver(call->net, call->vlserver);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_RXVLGetCapabilities = {\n\t.name\t\t= \"VL.GetCapabilities\",\n\t.op\t\t= afs_VL_GetCapabilities,\n\t.deliver\t= afs_deliver_vl_get_capabilities,\n\t.done\t\t= afs_vlserver_probe_result,\n\t.destructor\t= afs_destroy_vl_get_capabilities,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nstruct afs_call *afs_vl_get_capabilities(struct afs_net *net,\n\t\t\t\t\t struct afs_addr_list *alist,\n\t\t\t\t\t unsigned int addr_index,\n\t\t\t\t\t struct key *key,\n\t\t\t\t\t struct afs_vlserver *server,\n\t\t\t\t\t unsigned int server_index)\n{\n\tstruct afs_call *call;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetCapabilities, 1 * 4, 16 * 4);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = key;\n\tcall->vlserver = afs_get_vlserver(server);\n\tcall->server_index = server_index;\n\tcall->peer = rxrpc_kernel_get_peer(alist->addrs[addr_index].peer);\n\tcall->vl_probe = afs_get_addrlist(alist, afs_alist_trace_get_vlgetcaps);\n\tcall->probe_index = addr_index;\n\tcall->service_id = server->service_id;\n\tcall->upgrade = true;\n\tcall->async = true;\n\tcall->max_lifespan = AFS_PROBE_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETCAPABILITIES);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(call, GFP_KERNEL);\n\treturn call;\n}\n\n/*\n * Deliver reply data to a YFSVL.GetEndpoints call.\n *\n *\tGetEndpoints(IN yfsServerAttributes *attr,\n *\t\t     OUT opr_uuid *uuid,\n *\t\t     OUT afs_int32 *uniquifier,\n *\t\t     OUT endpoints *fsEndpoints,\n *\t\t     OUT endpoints *volEndpoints)\n */\nstatic int afs_deliver_yfsvl_get_endpoints(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, size;\n\tint ret;\n\n\t_enter(\"{%u,%zu,%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count2);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call, sizeof(uuid_t) + 3 * sizeof(__be32));\n\t\tcall->unmarshall = 1;\n\n\t\t/* Extract the returned uuid, uniquifier, fsEndpoints count and\n\t\t * either the first fsEndpoint type or the volEndpoints\n\t\t * count if there are no fsEndpoints. */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(uuid_t);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tcall->count\t= ntohl(*bp++);\n\t\tcall->count2\t= ntohl(*bp); /* Type or next count */\n\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_num);\n\n\t\talist = afs_alloc_addrlist(call->count);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\n\t\tif (call->count == 0)\n\t\t\tgoto extract_volendpoints;\n\n\tnext_fsendpoint:\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\tsize += sizeof(__be32);\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 2;\n\n\t\tfallthrough;\t/* and extract fsEndpoints[] entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt4_len);\n\t\t\tret = afs_merge_fs_addr4(call->net, alist, bp[1], ntohl(bp[2]));\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt6_len);\n\t\t\tret = afs_merge_fs_addr6(call->net, alist, bp + 1, ntohl(bp[5]));\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count2 = ntohl(*bp++);\n\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_fsendpoint;\n\n\textract_volendpoints:\n\t\t/* Extract the list of volEndpoints. */\n\t\tcall->count = call->count2;\n\t\tif (!call->count)\n\t\t\tgoto end;\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\n\t\tafs_extract_to_buf(call, 1 * sizeof(__be32));\n\t\tcall->unmarshall = 3;\n\n\t\t/* Extract the type of volEndpoints[0].  Normally we would\n\t\t * extract the type of the next endpoint when we extract the\n\t\t * data of the current one, but this is the first...\n\t\t */\n\t\tfallthrough;\n\tcase 3:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\n\tnext_volendpoint:\n\t\tcall->count2 = ntohl(*bp++);\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\tif (call->count > 1)\n\t\t\tsize += sizeof(__be32); /* Get next type too */\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 4;\n\n\t\tfallthrough;\t/* and extract volEndpoints[] entries */\n\tcase 4:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt4_len);\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt6_len);\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_volendpoint;\n\n\tend:\n\t\tafs_extract_discard(call, 0);\n\t\tcall->unmarshall = 5;\n\n\t\tfallthrough;\t/* Done */\n\tcase 5:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tcall->unmarshall = 6;\n\t\tfallthrough;\n\n\tcase 6:\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * YFSVL.GetEndpoints operation type.\n */\nstatic const struct afs_call_type afs_YFSVLGetEndpoints = {\n\t.name\t\t= \"YFSVL.GetEndpoints\",\n\t.op\t\t= afs_YFSVL_GetEndpoints,\n\t.deliver\t= afs_deliver_yfsvl_get_endpoints,\n\t.destructor\t= afs_flat_call_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_yfsvl_get_endpoints(struct afs_vl_cursor *vc,\n\t\t\t\t\t      const uuid_t *uuid)\n{\n\tstruct afs_addr_list *alist;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetEndpoints,\n\t\t\t\t   sizeof(__be32) * 2 + sizeof(*uuid),\n\t\t\t\t   sizeof(struct in6_addr) + sizeof(__be32) * 3);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\tcall->peer = rxrpc_kernel_get_peer(vc->alist->addrs[vc->addr_index].peer);\n\tcall->service_id = vc->server->service_id;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETENDPOINTS);\n\t*bp++ = htonl(YFS_SERVER_UUID);\n\tmemcpy(bp, uuid, sizeof(*uuid)); /* Type opr_uuid */\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(call, GFP_KERNEL);\n\tafs_wait_for_call_to_complete(call);\n\tvc->call_abort_code\t= call->abort_code;\n\tvc->call_error\t\t= call->error;\n\tvc->call_responded\t= call->responded;\n\talist\t\t\t= call->ret_alist;\n\tafs_put_call(call);\n\tif (vc->call_error) {\n\t\tafs_put_addrlist(alist, afs_alist_trace_put_getaddru);\n\t\treturn ERR_PTR(vc->call_error);\n\t}\n\treturn alist;\n}\n\n/*\n * Deliver reply data to a YFSVL.GetCellName operation.\n */\nstatic int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tchar *cell_name;\n\tu32 namesz, paddedsz;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the cell name length */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnamesz = ntohl(call->tmp);\n\t\tif (namesz > AFS_MAXCELLNAME)\n\t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n\t\tpaddedsz = (namesz + 3) & ~3;\n\t\tcall->count = namesz;\n\t\tcall->count2 = paddedsz - namesz;\n\n\t\tcell_name = kmalloc(namesz + 1, GFP_KERNEL);\n\t\tif (!cell_name)\n\t\t\treturn -ENOMEM;\n\t\tcell_name[namesz] = 0;\n\t\tcall->ret_str = cell_name;\n\n\t\tafs_extract_begin(call, cell_name, namesz);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract cell name */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tafs_extract_discard(call, call->count2);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract padding */\n\tcase 3:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_YFSVLGetCellName = {\n\t.name\t\t= \"YFSVL.GetCellName\",\n\t.op\t\t= afs_YFSVL_GetCellName,\n\t.deliver\t= afs_deliver_yfsvl_get_cell_name,\n\t.destructor\t= afs_flat_call_destructor,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nchar *afs_yfsvl_get_cell_name(struct afs_vl_cursor *vc)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\tchar *cellname;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetCellName, 1 * 4, 0);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_str = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\tcall->peer = rxrpc_kernel_get_peer(vc->alist->addrs[vc->addr_index].peer);\n\tcall->service_id = vc->server->service_id;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETCELLNAME);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(call, GFP_KERNEL);\n\tafs_wait_for_call_to_complete(call);\n\tvc->call_abort_code\t= call->abort_code;\n\tvc->call_error\t\t= call->error;\n\tvc->call_responded\t= call->responded;\n\tcellname\t\t= call->ret_str;\n\tafs_put_call(call);\n\tif (vc->call_error) {\n\t\tkfree(cellname);\n\t\treturn ERR_PTR(vc->call_error);\n\t}\n\treturn cellname;\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "7cb3e77e9b4e6ffa325a5559393d3283c9af3d01",
                            "downstream_commit": "781c743e18bfd9b7dc0383f036ae952bd1486f21",
                            "commit_date": "2025-01-23 17:15:50 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file fs/afs/afs.h\npatching file fs/afs/afs_vl.h\npatching file fs/afs/vl_alias.c\nHunk #1 succeeded at 302 (offset 49 lines).\nHunk #2 succeeded at 314 (offset 49 lines).\npatching file fs/afs/vlclient.c\nHunk #1 succeeded at 671 (offset -26 lines).",
                            "downstream_patch_content": "commit 7cb3e77e9b4e6ffa325a5559393d3283c9af3d01\nAuthor: David Howells <dhowells@redhat.com>\nDate:   Mon Jan 6 16:21:00 2025 +0000\n\n    afs: Fix the maximum cell name length\n    \n    [ Upstream commit 8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8 ]\n    \n    The kafs filesystem limits the maximum length of a cell to 256 bytes, but a\n    problem occurs if someone actually does that: kafs tries to create a\n    directory under /proc/net/afs/ with the name of the cell, but that fails\n    with a warning:\n    \n            WARNING: CPU: 0 PID: 9 at fs/proc/generic.c:405\n    \n    because procfs limits the maximum filename length to 255.\n    \n    However, the DNS limits the maximum lookup length and, by extension, the\n    maximum cell name, to 255 less two (length count and trailing NUL).\n    \n    Fix this by limiting the maximum acceptable cellname length to 253.  This\n    also allows us to be sure we can create the \"/afs/.<cell>/\" mountpoint too.\n    \n    Further, split the YFS VL record cell name maximum to be the 256 allowed by\n    the protocol and ignore the record retrieved by YFSVL.GetCellName if it\n    exceeds 253.\n    \n    Fixes: c3e9f888263b (\"afs: Implement client support for the YFSVL.GetCellName RPC op\")\n    Reported-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/r/6776d25d.050a0220.3a8527.0048.GAE@google.com/\n    Signed-off-by: David Howells <dhowells@redhat.com>\n    Link: https://lore.kernel.org/r/376236.1736180460@warthog.procyon.org.uk\n    Tested-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    cc: Marc Dionne <marc.dionne@auristor.com>\n    cc: linux-afs@lists.infradead.org\n    Signed-off-by: Christian Brauner <brauner@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/fs/afs/afs.h b/fs/afs/afs.h\nindex 432cb4b23961..3ea5f3e3c922 100644\n--- a/fs/afs/afs.h\n+++ b/fs/afs/afs.h\n@@ -10,7 +10,7 @@\n \n #include <linux/in.h>\n \n-#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n+#define AFS_MAXCELLNAME\t\t253  \t/* Maximum length of a cell name (DNS limited) */\n #define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n #define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n #define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\ndiff --git a/fs/afs/afs_vl.h b/fs/afs/afs_vl.h\nindex 9c65ffb8a523..8da0899fbc08 100644\n--- a/fs/afs/afs_vl.h\n+++ b/fs/afs/afs_vl.h\n@@ -13,6 +13,7 @@\n #define AFS_VL_PORT\t\t7003\t/* volume location service port */\n #define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n #define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n+#define YFS_VL_MAXCELLNAME\t256  \t/* Maximum length of a cell name in YFS protocol */\n \n enum AFSVL_Operations {\n \tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\ndiff --git a/fs/afs/vl_alias.c b/fs/afs/vl_alias.c\nindex f04a80e4f5c3..83cf1bfbe343 100644\n--- a/fs/afs/vl_alias.c\n+++ b/fs/afs/vl_alias.c\n@@ -302,6 +302,7 @@ static char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n {\n \tstruct afs_cell *master;\n+\tsize_t name_len;\n \tchar *cell_name;\n \n \tcell_name = afs_vl_get_cell_name(cell, key);\n@@ -313,8 +314,11 @@ static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n \t\treturn 0;\n \t}\n \n-\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n-\t\t\t\t NULL, false);\n+\tname_len = strlen(cell_name);\n+\tif (!name_len || name_len > AFS_MAXCELLNAME)\n+\t\tmaster = ERR_PTR(-EOPNOTSUPP);\n+\telse\n+\t\tmaster = afs_lookup_cell(cell->net, cell_name, name_len, NULL, false);\n \tkfree(cell_name);\n \tif (IS_ERR(master))\n \t\treturn PTR_ERR(master);\ndiff --git a/fs/afs/vlclient.c b/fs/afs/vlclient.c\nindex 00fca3c66ba6..16653f2ffe4f 100644\n--- a/fs/afs/vlclient.c\n+++ b/fs/afs/vlclient.c\n@@ -671,7 +671,7 @@ static int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n \t\t\treturn ret;\n \n \t\tnamesz = ntohl(call->tmp);\n-\t\tif (namesz > AFS_MAXCELLNAME)\n+\t\tif (namesz > YFS_VL_MAXCELLNAME)\n \t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n \t\tpaddedsz = (namesz + 3) & ~3;\n \t\tcall->count = namesz;\n",
                            "downstream_file_content": {
                                "fs/afs/afs.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS common types\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_H\n#define AFS_H\n\n#include <linux/in.h>\n\n#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n#define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n#define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n#define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\n#define AFS_MAXTYPES\t\t3\t/* Maximum number of volume types */\n#define AFSNAMEMAX\t\t256 \t/* Maximum length of a filename plus NUL */\n#define AFSPATHMAX\t\t1024\t/* Maximum length of a pathname plus NUL */\n#define AFSOPAQUEMAX\t\t1024\t/* Maximum length of an opaque field */\n\n#define AFS_VL_MAX_LIFESPAN\t(120 * HZ)\n#define AFS_PROBE_MAX_LIFESPAN\t(30 * HZ)\n\ntypedef u64\t\t\tafs_volid_t;\ntypedef u64\t\t\tafs_vnodeid_t;\ntypedef u64\t\t\tafs_dataversion_t;\n\ntypedef enum {\n\tAFSVL_RWVOL,\t\t\t/* read/write volume */\n\tAFSVL_ROVOL,\t\t\t/* read-only volume */\n\tAFSVL_BACKVOL,\t\t\t/* backup volume */\n} __attribute__((packed)) afs_voltype_t;\n\ntypedef enum {\n\tAFS_FTYPE_INVALID\t= 0,\n\tAFS_FTYPE_FILE\t\t= 1,\n\tAFS_FTYPE_DIR\t\t= 2,\n\tAFS_FTYPE_SYMLINK\t= 3,\n} afs_file_type_t;\n\ntypedef enum {\n\tAFS_LOCK_READ\t\t= 0,\t/* read lock request */\n\tAFS_LOCK_WRITE\t\t= 1,\t/* write lock request */\n} afs_lock_type_t;\n\n#define AFS_LOCKWAIT\t\t(5 * 60) /* time until a lock times out (seconds) */\n\n/*\n * AFS file identifier\n */\nstruct afs_fid {\n\tafs_volid_t\tvid;\t\t/* volume ID */\n\tafs_vnodeid_t\tvnode;\t\t/* Lower 64-bits of file index within volume */\n\tu32\t\tvnode_hi;\t/* Upper 32-bits of file index */\n\tu32\t\tunique;\t\t/* unique ID number (file index version) */\n};\n\n/*\n * AFS callback notification\n */\ntypedef enum {\n\tAFSCM_CB_UNTYPED\t= 0,\t/* no type set on CB break */\n\tAFSCM_CB_EXCLUSIVE\t= 1,\t/* CB exclusive to CM [not implemented] */\n\tAFSCM_CB_SHARED\t\t= 2,\t/* CB shared by other CM's */\n\tAFSCM_CB_DROPPED\t= 3,\t/* CB promise cancelled by file server */\n} afs_callback_type_t;\n\nstruct afs_callback {\n\ttime64_t\t\texpires_at;\t/* Time at which expires */\n\t//unsigned\t\tversion;\t/* Callback version */\n\t//afs_callback_type_t\ttype;\t\t/* Type of callback */\n};\n\nstruct afs_callback_break {\n\tstruct afs_fid\t\tfid;\t\t/* File identifier */\n\t//struct afs_callback\tcb;\t\t/* Callback details */\n};\n\n#define AFSCBMAX 50\t/* maximum callbacks transferred per bulk op */\n\nstruct afs_uuid {\n\t__be32\t\ttime_low;\t\t\t/* low part of timestamp */\n\t__be16\t\ttime_mid;\t\t\t/* mid part of timestamp */\n\t__be16\t\ttime_hi_and_version;\t\t/* high part of timestamp and version  */\n\t__s8\t\tclock_seq_hi_and_reserved;\t/* clock seq hi and variant */\n\t__s8\t\tclock_seq_low;\t\t\t/* clock seq low */\n\t__s8\t\tnode[6];\t\t\t/* spatially unique node ID (MAC addr) */\n};\n\n/*\n * AFS volume information\n */\nstruct afs_volume_info {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_voltype_t\t\ttype;\t\t/* type of this volume */\n\tafs_volid_t\t\ttype_vids[5];\t/* volume ID's for possible types for this vol */\n\n\t/* list of fileservers serving this volume */\n\tsize_t\t\t\tnservers;\t/* number of entries used in servers[] */\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* fileserver address */\n\t} servers[8];\n};\n\n/*\n * AFS security ACE access mask\n */\ntypedef u32 afs_access_t;\n#define AFS_ACE_READ\t\t0x00000001U\t/* - permission to read a file/dir */\n#define AFS_ACE_WRITE\t\t0x00000002U\t/* - permission to write/chmod a file */\n#define AFS_ACE_INSERT\t\t0x00000004U\t/* - permission to create dirent in a dir */\n#define AFS_ACE_LOOKUP\t\t0x00000008U\t/* - permission to lookup a file/dir in a dir */\n#define AFS_ACE_DELETE\t\t0x00000010U\t/* - permission to delete a dirent from a dir */\n#define AFS_ACE_LOCK\t\t0x00000020U\t/* - permission to lock a file */\n#define AFS_ACE_ADMINISTER\t0x00000040U\t/* - permission to change ACL */\n#define AFS_ACE_USER_A\t\t0x01000000U\t/* - 'A' user-defined permission */\n#define AFS_ACE_USER_B\t\t0x02000000U\t/* - 'B' user-defined permission */\n#define AFS_ACE_USER_C\t\t0x04000000U\t/* - 'C' user-defined permission */\n#define AFS_ACE_USER_D\t\t0x08000000U\t/* - 'D' user-defined permission */\n#define AFS_ACE_USER_E\t\t0x10000000U\t/* - 'E' user-defined permission */\n#define AFS_ACE_USER_F\t\t0x20000000U\t/* - 'F' user-defined permission */\n#define AFS_ACE_USER_G\t\t0x40000000U\t/* - 'G' user-defined permission */\n#define AFS_ACE_USER_H\t\t0x80000000U\t/* - 'H' user-defined permission */\n\n/*\n * AFS file status information\n */\nstruct afs_file_status {\n\tu64\t\t\tsize;\t\t/* file size */\n\tafs_dataversion_t\tdata_version;\t/* current data version */\n\tstruct timespec64\tmtime_client;\t/* Last time client changed data */\n\tstruct timespec64\tmtime_server;\t/* Last time server changed data */\n\ts64\t\t\tauthor;\t\t/* author ID */\n\ts64\t\t\towner;\t\t/* owner ID */\n\ts64\t\t\tgroup;\t\t/* group ID */\n\tafs_access_t\t\tcaller_access;\t/* access rights for authenticated caller */\n\tafs_access_t\t\tanon_access;\t/* access rights for unauthenticated caller */\n\tumode_t\t\t\tmode;\t\t/* UNIX mode */\n\tafs_file_type_t\t\ttype;\t\t/* file type */\n\tu32\t\t\tnlink;\t\t/* link count */\n\ts32\t\t\tlock_count;\t/* file lock count (0=UNLK -1=WRLCK +ve=#RDLCK */\n\tu32\t\t\tabort_code;\t/* Abort if bulk-fetching this failed */\n};\n\nstruct afs_status_cb {\n\tstruct afs_file_status\tstatus;\n\tstruct afs_callback\tcallback;\n\tbool\t\t\thave_status;\t/* True if status record was retrieved */\n\tbool\t\t\thave_cb;\t/* True if cb record was retrieved */\n\tbool\t\t\thave_error;\t/* True if status.abort_code indicates an error */\n};\n\n/*\n * AFS file status change request\n */\n\n#define AFS_SET_MTIME\t\t0x01\t\t/* set the mtime */\n#define AFS_SET_OWNER\t\t0x02\t\t/* set the owner ID */\n#define AFS_SET_GROUP\t\t0x04\t\t/* set the group ID (unsupported?) */\n#define AFS_SET_MODE\t\t0x08\t\t/* set the UNIX mode */\n#define AFS_SET_SEG_SIZE\t0x10\t\t/* set the segment size (unsupported) */\n\n/*\n * AFS volume synchronisation information\n */\nstruct afs_volsync {\n\ttime64_t\t\tcreation;\t/* volume creation time */\n};\n\n/*\n * AFS volume status record\n */\nstruct afs_volume_status {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_volid_t\t\tparent_id;\t/* parent volume ID */\n\tu8\t\t\tonline;\t\t/* true if volume currently online and available */\n\tu8\t\t\tin_service;\t/* true if volume currently in service */\n\tu8\t\t\tblessed;\t/* same as in_service */\n\tu8\t\t\tneeds_salvage;\t/* true if consistency checking required */\n\tu32\t\t\ttype;\t\t/* volume type (afs_voltype_t) */\n\tu64\t\t\tmin_quota;\t/* minimum space set aside (blocks) */\n\tu64\t\t\tmax_quota;\t/* maximum space this volume may occupy (blocks) */\n\tu64\t\t\tblocks_in_use;\t/* space this volume currently occupies (blocks) */\n\tu64\t\t\tpart_blocks_avail; /* space available in volume's partition */\n\tu64\t\t\tpart_max_blocks; /* size of volume's partition */\n\ts64\t\t\tvol_copy_date;\n\ts64\t\t\tvol_backup_date;\n};\n\n#define AFS_BLOCK_SIZE\t1024\n\n/*\n * XDR encoding of UUID in AFS.\n */\nstruct afs_uuid__xdr {\n\t__be32\t\ttime_low;\n\t__be32\t\ttime_mid;\n\t__be32\t\ttime_hi_and_version;\n\t__be32\t\tclock_seq_hi_and_reserved;\n\t__be32\t\tclock_seq_low;\n\t__be32\t\tnode[6];\n};\n\n#endif /* AFS_H */\n",
                                "fs/afs/afs_vl.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS Volume Location Service client interface\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_VL_H\n#define AFS_VL_H\n\n#include \"afs.h\"\n\n#define AFS_VL_PORT\t\t7003\t/* volume location service port */\n#define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n#define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n\nenum AFSVL_Operations {\n\tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\n\tVLGETENTRYBYNAME\t= 504,\t/* AFS Get VLDB entry by name */\n\tVLPROBE\t\t\t= 514,\t/* AFS probe VL service */\n\tVLGETENTRYBYIDU\t\t= 526,\t/* AFS Get VLDB entry by ID (UUID-variant) */\n\tVLGETENTRYBYNAMEU\t= 527,\t/* AFS Get VLDB entry by name (UUID-variant) */\n\tVLGETADDRSU\t\t= 533,\t/* AFS Get addrs for fileserver */\n\tYVLGETENDPOINTS\t\t= 64002, /* YFS Get endpoints for file/volume server */\n\tYVLGETCELLNAME\t\t= 64014, /* YFS Get actual cell name */\n\tVLGETCAPABILITIES\t= 65537, /* AFS Get server capabilities */\n};\n\nenum AFSVL_Errors {\n\tAFSVL_IDEXIST \t\t= 363520,\t/* Volume Id entry exists in vl database */\n\tAFSVL_IO \t\t= 363521,\t/* I/O related error */\n\tAFSVL_NAMEEXIST \t= 363522,\t/* Volume name entry exists in vl database */\n\tAFSVL_CREATEFAIL \t= 363523,\t/* Internal creation failure */\n\tAFSVL_NOENT \t\t= 363524,\t/* No such entry */\n\tAFSVL_EMPTY \t\t= 363525,\t/* Vl database is empty */\n\tAFSVL_ENTDELETED \t= 363526,\t/* Entry is deleted (soft delete) */\n\tAFSVL_BADNAME \t\t= 363527,\t/* Volume name is illegal */\n\tAFSVL_BADINDEX \t\t= 363528,\t/* Index is out of range */\n\tAFSVL_BADVOLTYPE \t= 363529,\t/* Bad volume type */\n\tAFSVL_BADSERVER \t= 363530,\t/* Illegal server number (out of range) */\n\tAFSVL_BADPARTITION \t= 363531,\t/* Bad partition number */\n\tAFSVL_REPSFULL \t\t= 363532,\t/* Run out of space for Replication sites */\n\tAFSVL_NOREPSERVER \t= 363533,\t/* No such Replication server site exists */\n\tAFSVL_DUPREPSERVER \t= 363534,\t/* Replication site already exists */\n\tAFSVL_RWNOTFOUND \t= 363535,\t/* Parent R/W entry not found */\n\tAFSVL_BADREFCOUNT \t= 363536,\t/* Illegal Reference Count number */\n\tAFSVL_SIZEEXCEEDED \t= 363537,\t/* Vl size for attributes exceeded */\n\tAFSVL_BADENTRY \t\t= 363538,\t/* Bad incoming vl entry */\n\tAFSVL_BADVOLIDBUMP \t= 363539,\t/* Illegal max volid increment */\n\tAFSVL_IDALREADYHASHED \t= 363540,\t/* RO/BACK id already hashed */\n\tAFSVL_ENTRYLOCKED \t= 363541,\t/* Vl entry is already locked */\n\tAFSVL_BADVOLOPER \t= 363542,\t/* Bad volume operation code */\n\tAFSVL_BADRELLOCKTYPE \t= 363543,\t/* Bad release lock type */\n\tAFSVL_RERELEASE \t= 363544,\t/* Status report: last release was aborted */\n\tAFSVL_BADSERVERFLAG \t= 363545,\t/* Invalid replication site server flag */\n\tAFSVL_PERM \t\t= 363546,\t/* No permission access */\n\tAFSVL_NOMEM \t\t= 363547,\t/* malloc/realloc failed to alloc enough memory */\n};\n\nenum {\n\tYFS_SERVER_INDEX\t= 0,\n\tYFS_SERVER_UUID\t\t= 1,\n\tYFS_SERVER_ENDPOINT\t= 2,\n};\n\nenum {\n\tYFS_ENDPOINT_IPV4\t= 0,\n\tYFS_ENDPOINT_IPV6\t= 1,\n};\n\n#define YFS_MAXENDPOINTS\t16\n\n/*\n * maps to \"struct vldbentry\" in vvl-spec.pdf\n */\nstruct afs_vldbentry {\n\tchar\t\tname[65];\t\t/* name of volume (with NUL char) */\n\tafs_voltype_t\ttype;\t\t\t/* volume type */\n\tunsigned\tnum_servers;\t\t/* num servers that hold instances of this vol */\n\tunsigned\tclone_id;\t\t/* cloning ID */\n\n\tunsigned\tflags;\n#define AFS_VLF_RWEXISTS\t0x1000\t\t/* R/W volume exists */\n#define AFS_VLF_ROEXISTS\t0x2000\t\t/* R/O volume exists */\n#define AFS_VLF_BACKEXISTS\t0x4000\t\t/* backup volume exists */\n\n\tafs_volid_t\tvolume_ids[3];\t\t/* volume IDs */\n\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* server address */\n\t\tunsigned\tpartition;\t/* partition ID on this server */\n\t\tunsigned\tflags;\t\t/* server specific flags */\n#define AFS_VLSF_NEWREPSITE\t0x0001\t/* Ignore all 'non-new' servers */\n#define AFS_VLSF_ROVOL\t\t0x0002\t/* this server holds a R/O instance of the volume */\n#define AFS_VLSF_RWVOL\t\t0x0004\t/* this server holds a R/W instance of the volume */\n#define AFS_VLSF_BACKVOL\t0x0008\t/* this server holds a backup instance of the volume */\n#define AFS_VLSF_UUID\t\t0x0010\t/* This server is referred to by its UUID */\n#define AFS_VLSF_DONTUSE\t0x0020\t/* This server ref should be ignored */\n\t} servers[8];\n};\n\n#define AFS_VLDB_MAXNAMELEN 65\n\n\nstruct afs_ListAddrByAttributes__xdr {\n\t__be32\t\t\tMask;\n#define AFS_VLADDR_IPADDR\t0x1\t/* Match by ->ipaddr */\n#define AFS_VLADDR_INDEX\t0x2\t/* Match by ->index */\n#define AFS_VLADDR_UUID\t\t0x4\t/* Match by ->uuid */\n\t__be32\t\t\tipaddr;\n\t__be32\t\t\tindex;\n\t__be32\t\t\tspare;\n\tstruct afs_uuid__xdr\tuuid;\n};\n\nstruct afs_uvldbentry__xdr {\n\t__be32\t\t\tname[AFS_VLDB_MAXNAMELEN];\n\t__be32\t\t\tnServers;\n\tstruct afs_uuid__xdr\tserverNumber[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverUnique[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverPartition[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverFlags[AFS_NMAXNSERVERS];\n\t__be32\t\t\tvolumeId[AFS_MAXTYPES];\n\t__be32\t\t\tcloneId;\n\t__be32\t\t\tflags;\n\t__be32\t\t\tspares1;\n\t__be32\t\t\tspares2;\n\t__be32\t\t\tspares3;\n\t__be32\t\t\tspares4;\n\t__be32\t\t\tspares5;\n\t__be32\t\t\tspares6;\n\t__be32\t\t\tspares7;\n\t__be32\t\t\tspares8;\n\t__be32\t\t\tspares9;\n};\n\nstruct afs_address_list {\n\trefcount_t\t\tusage;\n\tunsigned int\t\tversion;\n\tunsigned int\t\tnr_addrs;\n\tstruct sockaddr_rxrpc\taddrs[];\n};\n\nextern void afs_put_address_list(struct afs_address_list *alist);\n\n#endif /* AFS_VL_H */\n",
                                "fs/afs/vl_alias.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS cell alias detection\n *\n * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/namei.h>\n#include <keys/rxrpc-type.h>\n#include \"internal.h\"\n\n/*\n * Sample a volume.\n */\nstatic struct afs_volume *afs_sample_volume(struct afs_cell *cell, struct key *key,\n\t\t\t\t\t    const char *name, unsigned int namelen)\n{\n\tstruct afs_volume *volume;\n\tstruct afs_fs_context fc = {\n\t\t.type\t\t= 0, /* Explicitly leave it to the VLDB */\n\t\t.volnamesz\t= namelen,\n\t\t.volname\t= name,\n\t\t.net\t\t= cell->net,\n\t\t.cell\t\t= cell,\n\t\t.key\t\t= key, /* This might need to be something */\n\t};\n\n\tvolume = afs_create_volume(&fc);\n\t_leave(\" = %p\", volume);\n\treturn volume;\n}\n\n/*\n * Compare two addresses.\n */\nstatic int afs_compare_addrs(const struct sockaddr_rxrpc *srx_a,\n\t\t\t     const struct sockaddr_rxrpc *srx_b)\n{\n\tshort port_a, port_b;\n\tint addr_a, addr_b, diff;\n\n\tdiff = (short)srx_a->transport_type - (short)srx_b->transport_type;\n\tif (diff)\n\t\tgoto out;\n\n\tswitch (srx_a->transport_type) {\n\tcase AF_INET: {\n\t\tconst struct sockaddr_in *a = &srx_a->transport.sin;\n\t\tconst struct sockaddr_in *b = &srx_b->transport.sin;\n\t\taddr_a = ntohl(a->sin_addr.s_addr);\n\t\taddr_b = ntohl(b->sin_addr.s_addr);\n\t\tdiff = addr_a - addr_b;\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin_port);\n\t\t\tport_b = ntohs(b->sin_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase AF_INET6: {\n\t\tconst struct sockaddr_in6 *a = &srx_a->transport.sin6;\n\t\tconst struct sockaddr_in6 *b = &srx_b->transport.sin6;\n\t\tdiff = memcmp(&a->sin6_addr, &b->sin6_addr, 16);\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin6_port);\n\t\t\tport_b = ntohs(b->sin6_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\tWARN_ON(1);\n\t\tdiff = 1;\n\t}\n\nout:\n\treturn diff;\n}\n\n/*\n * Compare the address lists of a pair of fileservers.\n */\nstatic int afs_compare_fs_alists(const struct afs_server *server_a,\n\t\t\t\t const struct afs_server *server_b)\n{\n\tconst struct afs_addr_list *la, *lb;\n\tint a = 0, b = 0, addr_matches = 0;\n\n\tla = rcu_dereference(server_a->addresses);\n\tlb = rcu_dereference(server_b->addresses);\n\n\twhile (a < la->nr_addrs && b < lb->nr_addrs) {\n\t\tconst struct sockaddr_rxrpc *srx_a = &la->addrs[a];\n\t\tconst struct sockaddr_rxrpc *srx_b = &lb->addrs[b];\n\t\tint diff = afs_compare_addrs(srx_a, srx_b);\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\taddr_matches++;\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\treturn addr_matches;\n}\n\n/*\n * Compare the fileserver lists of two volumes.  The server lists are sorted in\n * order of ascending UUID.\n */\nstatic int afs_compare_volume_slists(const struct afs_volume *vol_a,\n\t\t\t\t     const struct afs_volume *vol_b)\n{\n\tconst struct afs_server_list *la, *lb;\n\tint i, a = 0, b = 0, uuid_matches = 0, addr_matches = 0;\n\n\tla = rcu_dereference(vol_a->servers);\n\tlb = rcu_dereference(vol_b->servers);\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tif (la->vids[i] != lb->vids[i])\n\t\t\treturn 0;\n\n\twhile (a < la->nr_servers && b < lb->nr_servers) {\n\t\tconst struct afs_server *server_a = la->servers[a].server;\n\t\tconst struct afs_server *server_b = lb->servers[b].server;\n\t\tint diff = memcmp(&server_a->uuid, &server_b->uuid, sizeof(uuid_t));\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\tuuid_matches++;\n\t\t\taddr_matches += afs_compare_fs_alists(server_a, server_b);\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\t_leave(\" = %d [um %d]\", addr_matches, uuid_matches);\n\treturn addr_matches;\n}\n\n/*\n * Compare root.cell volumes.\n */\nstatic int afs_compare_cell_roots(struct afs_cell *cell)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"\");\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (!p->root_volume)\n\t\t\tcontinue; /* Ignore cells that don't have a root.cell volume. */\n\n\t\tif (afs_compare_volume_slists(cell->root_volume, p->root_volume) != 0)\n\t\t\tgoto is_alias;\n\t}\n\n\trcu_read_unlock();\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\trcu_read_unlock();\n\tcell->alias_of = afs_use_cell(p, afs_cell_trace_use_alias);\n\treturn 1;\n}\n\n/*\n * Query the new cell for a volume from a cell we're already using.\n */\nstatic int afs_query_for_alias_one(struct afs_cell *cell, struct key *key,\n\t\t\t\t   struct afs_cell *p)\n{\n\tstruct afs_volume *volume, *pvol = NULL;\n\tint ret;\n\n\t/* Arbitrarily pick a volume from the list. */\n\tread_seqlock_excl(&p->volume_lock);\n\tif (!RB_EMPTY_ROOT(&p->volumes))\n\t\tpvol = afs_get_volume(rb_entry(p->volumes.rb_node,\n\t\t\t\t\t       struct afs_volume, cell_node),\n\t\t\t\t      afs_volume_trace_get_query_alias);\n\tread_sequnlock_excl(&p->volume_lock);\n\tif (!pvol)\n\t\treturn 0;\n\n\t_enter(\"%s:%s\", cell->name, pvol->name);\n\n\t/* And see if it's in the new cell. */\n\tvolume = afs_sample_volume(cell, key, pvol->name, pvol->name_len);\n\tif (IS_ERR(volume)) {\n\t\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\t\tif (PTR_ERR(volume) != -ENOMEDIUM)\n\t\t\treturn PTR_ERR(volume);\n\t\t/* That volume is not in the new cell, so not an alias */\n\t\treturn 0;\n\t}\n\n\t/* The new cell has a like-named volume also - compare volume ID,\n\t * server and address lists.\n\t */\n\tret = 0;\n\tif (pvol->vid == volume->vid) {\n\t\trcu_read_lock();\n\t\tif (afs_compare_volume_slists(volume, pvol))\n\t\t\tret = 1;\n\t\trcu_read_unlock();\n\t}\n\n\tafs_put_volume(cell->net, volume, afs_volume_trace_put_query_alias);\n\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\treturn ret;\n}\n\n/*\n * Query the new cell for volumes we know exist in cells we're already using.\n */\nstatic int afs_query_for_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"%s\", cell->name);\n\n\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\thlist_for_each_entry(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (RB_EMPTY_ROOT(&p->volumes))\n\t\t\tcontinue;\n\t\tif (p->root_volume)\n\t\t\tcontinue; /* Ignore cells that have a root.cell volume. */\n\t\tafs_use_cell(p, afs_cell_trace_use_check_alias);\n\t\tmutex_unlock(&cell->net->proc_cells_lock);\n\n\t\tif (afs_query_for_alias_one(cell, key, p) != 0)\n\t\t\tgoto is_alias;\n\n\t\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0) {\n\t\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\n\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t}\n\n\tmutex_unlock(&cell->net->proc_cells_lock);\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\tcell->alias_of = p; /* Transfer our ref */\n\treturn 1;\n}\n\n/*\n * Look up a VLDB record for a volume.\n */\nstatic char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_vl_cursor vc;\n\tchar *cell_name = ERR_PTR(-EDESTADDRREQ);\n\tbool skipped = false, not_skipped = false;\n\tint ret;\n\n\tif (!afs_begin_vlserver_operation(&vc, cell, key))\n\t\treturn ERR_PTR(-ERESTARTSYS);\n\n\twhile (afs_select_vlserver(&vc)) {\n\t\tif (!test_bit(AFS_VLSERVER_FL_IS_YFS, &vc.server->flags)) {\n\t\t\tvc.ac.error = -EOPNOTSUPP;\n\t\t\tskipped = true;\n\t\t\tcontinue;\n\t\t}\n\t\tnot_skipped = true;\n\t\tcell_name = afs_yfsvl_get_cell_name(&vc);\n\t}\n\n\tret = afs_end_vlserver_operation(&vc);\n\tif (skipped && !not_skipped)\n\t\tret = -EOPNOTSUPP;\n\treturn ret < 0 ? ERR_PTR(ret) : cell_name;\n}\n\nstatic int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *master;\n\tchar *cell_name;\n\n\tcell_name = afs_vl_get_cell_name(cell, key);\n\tif (IS_ERR(cell_name))\n\t\treturn PTR_ERR(cell_name);\n\n\tif (strcmp(cell_name, cell->name) == 0) {\n\t\tkfree(cell_name);\n\t\treturn 0;\n\t}\n\n\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n\t\t\t\t NULL, false);\n\tkfree(cell_name);\n\tif (IS_ERR(master))\n\t\treturn PTR_ERR(master);\n\n\tcell->alias_of = master; /* Transfer our ref */\n\treturn 1;\n}\n\nstatic int afs_do_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_volume *root_volume;\n\tint ret;\n\n\t_enter(\"%s\", cell->name);\n\n\tret = yfs_check_canonical_cell_name(cell, key);\n\tif (ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\t/* Try and get the root.cell volume for comparison with other cells */\n\troot_volume = afs_sample_volume(cell, key, \"root.cell\", 9);\n\tif (!IS_ERR(root_volume)) {\n\t\tcell->root_volume = root_volume;\n\t\treturn afs_compare_cell_roots(cell);\n\t}\n\n\tif (PTR_ERR(root_volume) != -ENOMEDIUM)\n\t\treturn PTR_ERR(root_volume);\n\n\t/* Okay, this cell doesn't have an root.cell volume.  We need to\n\t * locate some other random volume and use that to check.\n\t */\n\treturn afs_query_for_alias(cell, key);\n}\n\n/*\n * Check to see if a new cell is an alias of a cell we already have.  At this\n * point we have the cell's volume server list.\n *\n * Returns 0 if we didn't detect an alias, 1 if we found an alias and an error\n * if we had problems gathering the data required.  In the case the we did\n * detect an alias, cell->alias_of is set to point to the assumed master.\n */\nint afs_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_net *net = cell->net;\n\tint ret;\n\n\tif (mutex_lock_interruptible(&net->cells_alias_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\tif (test_bit(AFS_CELL_FL_CHECK_ALIAS, &cell->flags)) {\n\t\tret = afs_do_cell_detect_alias(cell, key);\n\t\tif (ret >= 0)\n\t\t\tclear_bit_unlock(AFS_CELL_FL_CHECK_ALIAS, &cell->flags);\n\t} else {\n\t\tret = cell->alias_of ? 1 : 0;\n\t}\n\n\tmutex_unlock(&net->cells_alias_lock);\n\n\tif (ret == 1)\n\t\tpr_notice(\"kAFS: Cell %s is an alias of %s\\n\",\n\t\t\t  cell->name, cell->alias_of->name);\n\treturn ret;\n}\n",
                                "fs/afs/vlclient.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS Volume Location Service client\n *\n * Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include \"afs_fs.h\"\n#include \"internal.h\"\n\n/*\n * Deliver reply data to a VL.GetEntryByNameU call.\n */\nstatic int afs_deliver_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tstruct afs_uvldbentry__xdr *uvldb;\n\tstruct afs_vldb_entry *entry;\n\tbool new_only = false;\n\tu32 tmp, nr_servers, vlflags;\n\tint i, ret;\n\n\t_enter(\"\");\n\n\tret = afs_transfer_reply(call);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* unmarshall the reply once we've received all of it */\n\tuvldb = call->buffer;\n\tentry = call->ret_vldb;\n\n\tnr_servers = ntohl(uvldb->nServers);\n\tif (nr_servers > AFS_NMAXNSERVERS)\n\t\tnr_servers = AFS_NMAXNSERVERS;\n\n\tfor (i = 0; i < ARRAY_SIZE(uvldb->name) - 1; i++)\n\t\tentry->name[i] = (u8)ntohl(uvldb->name[i]);\n\tentry->name[i] = 0;\n\tentry->name_len = strlen(entry->name);\n\n\t/* If there is a new replication site that we can use, ignore all the\n\t * sites that aren't marked as new.\n\t */\n\tfor (i = 0; i < nr_servers; i++) {\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (!(tmp & AFS_VLSF_DONTUSE) &&\n\t\t    (tmp & AFS_VLSF_NEWREPSITE))\n\t\t\tnew_only = true;\n\t}\n\n\tvlflags = ntohl(uvldb->flags);\n\tfor (i = 0; i < nr_servers; i++) {\n\t\tstruct afs_uuid__xdr *xdr;\n\t\tstruct afs_uuid *uuid;\n\t\tint j;\n\t\tint n = entry->nr_servers;\n\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (tmp & AFS_VLSF_DONTUSE ||\n\t\t    (new_only && !(tmp & AFS_VLSF_NEWREPSITE)))\n\t\t\tcontinue;\n\t\tif (tmp & AFS_VLSF_RWVOL) {\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RW;\n\t\t\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_BAK;\n\t\t}\n\t\tif (tmp & AFS_VLSF_ROVOL)\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RO;\n\t\tif (!entry->fs_mask[n])\n\t\t\tcontinue;\n\n\t\txdr = &uvldb->serverNumber[i];\n\t\tuuid = (struct afs_uuid *)&entry->fs_server[n];\n\t\tuuid->time_low\t\t\t= xdr->time_low;\n\t\tuuid->time_mid\t\t\t= htons(ntohl(xdr->time_mid));\n\t\tuuid->time_hi_and_version\t= htons(ntohl(xdr->time_hi_and_version));\n\t\tuuid->clock_seq_hi_and_reserved\t= (u8)ntohl(xdr->clock_seq_hi_and_reserved);\n\t\tuuid->clock_seq_low\t\t= (u8)ntohl(xdr->clock_seq_low);\n\t\tfor (j = 0; j < 6; j++)\n\t\t\tuuid->node[j] = (u8)ntohl(xdr->node[j]);\n\n\t\tentry->addr_version[n] = ntohl(uvldb->serverUnique[i]);\n\t\tentry->nr_servers++;\n\t}\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tentry->vid[i] = ntohl(uvldb->volumeId[i]);\n\n\tif (vlflags & AFS_VLF_RWEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RW, &entry->flags);\n\tif (vlflags & AFS_VLF_ROEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RO, &entry->flags);\n\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_BAK, &entry->flags);\n\n\tif (!(vlflags & (AFS_VLF_RWEXISTS | AFS_VLF_ROEXISTS | AFS_VLF_BACKEXISTS))) {\n\t\tentry->error = -ENOMEDIUM;\n\t\t__set_bit(AFS_VLDB_QUERY_ERROR, &entry->flags);\n\t}\n\n\t__set_bit(AFS_VLDB_QUERY_VALID, &entry->flags);\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tkfree(call->ret_vldb);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetEntryByNameU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetEntryByNameU = {\n\t.name\t\t= \"VL.GetEntryByNameU\",\n\t.op\t\t= afs_VL_GetEntryByNameU,\n\t.deliver\t= afs_deliver_vl_get_entry_by_name_u,\n\t.destructor\t= afs_destroy_vl_get_entry_by_name_u,\n};\n\n/*\n * Dispatch a get volume entry by name or ID operation (uuid variant).  If the\n * volname is a decimal number then it's a volume ID not a volume name.\n */\nstruct afs_vldb_entry *afs_vl_get_entry_by_name_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t\t  const char *volname,\n\t\t\t\t\t\t  int volnamesz)\n{\n\tstruct afs_vldb_entry *entry;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\tsize_t reqsz, padsz;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tpadsz = (4 - (volnamesz & 3)) & 3;\n\treqsz = 8 + volnamesz + padsz;\n\n\tentry = kzalloc(sizeof(struct afs_vldb_entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetEntryByNameU, reqsz,\n\t\t\t\t   sizeof(struct afs_uvldbentry__xdr));\n\tif (!call) {\n\t\tkfree(entry);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tcall->key = vc->key;\n\tcall->ret_vldb = entry;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETENTRYBYNAMEU);\n\t*bp++ = htonl(volnamesz);\n\tmemcpy(bp, volname, volnamesz);\n\tif (padsz > 0)\n\t\tmemset((void *)bp + volnamesz, 0, padsz);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_vldb_entry *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a VL.GetAddrsU call.\n *\n *\tGetAddrsU(IN ListAddrByAttributes *inaddr,\n *\t\t  OUT afsUUID *uuidp1,\n *\t\t  OUT uint32_t *uniquifier,\n *\t\t  OUT uint32_t *nentries,\n *\t\t  OUT bulkaddrs *blkaddrs);\n */\nstatic int afs_deliver_vl_get_addrs_u(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, nentries, count;\n\tint i, ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call,\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\t\tcall->unmarshall++;\n\n\t\t/* Extract the returned uuid, uniquifier, nentries and\n\t\t * blkaddrs size */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(struct afs_uuid__xdr);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tnentries\t= ntohl(*bp++);\n\t\tcount\t\t= ntohl(*bp);\n\n\t\tnentries = min(nentries, count);\n\t\talist = afs_alloc_addrlist(nentries, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\t\tcall->count = count;\n\t\tcall->count2 = nentries;\n\t\tcall->unmarshall++;\n\n\tmore_entries:\n\t\tcount = min(call->count, 4U);\n\t\tafs_extract_to_buf(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, call->count > 4);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tcount = min(call->count, 4U);\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (alist->nr_addrs < call->count2)\n\t\t\t\tafs_merge_fs_addr4(alist, *bp++, AFS_FS_PORT);\n\n\t\tcall->count -= count;\n\t\tif (call->count > 0)\n\t\t\tgoto more_entries;\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_vl_get_addrs_u_destructor(struct afs_call *call)\n{\n\tafs_put_addrlist(call->ret_alist);\n\treturn afs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetAddrsU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetAddrsU = {\n\t.name\t\t= \"VL.GetAddrsU\",\n\t.op\t\t= afs_VL_GetAddrsU,\n\t.deliver\t= afs_deliver_vl_get_addrs_u,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_vl_get_addrs_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t const uuid_t *uuid)\n{\n\tstruct afs_ListAddrByAttributes__xdr *r;\n\tconst struct afs_uuid *u = (const struct afs_uuid *)uuid;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\tint i;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetAddrsU,\n\t\t\t\t   sizeof(__be32) + sizeof(struct afs_ListAddrByAttributes__xdr),\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETADDRSU);\n\tr = (struct afs_ListAddrByAttributes__xdr *)bp;\n\tr->Mask\t\t= htonl(AFS_VLADDR_UUID);\n\tr->ipaddr\t= 0;\n\tr->index\t= 0;\n\tr->spare\t= 0;\n\tr->uuid.time_low\t\t\t= u->time_low;\n\tr->uuid.time_mid\t\t\t= htonl(ntohs(u->time_mid));\n\tr->uuid.time_hi_and_version\t\t= htonl(ntohs(u->time_hi_and_version));\n\tr->uuid.clock_seq_hi_and_reserved \t= htonl(u->clock_seq_hi_and_reserved);\n\tr->uuid.clock_seq_low\t\t\t= htonl(u->clock_seq_low);\n\tfor (i = 0; i < 6; i++)\n\t\tr->uuid.node[i] = htonl(u->node[i]);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to an VL.GetCapabilities operation.\n */\nstatic int afs_deliver_vl_get_capabilities(struct afs_call *call)\n{\n\tu32 count;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the capabilities word count */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcount = ntohl(call->tmp);\n\t\tcall->count = count;\n\t\tcall->count2 = count;\n\n\t\tcall->unmarshall++;\n\t\tafs_extract_discard(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract capabilities words */\n\tcase 2:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\t/* TODO: Examine capabilities */\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_capabilities(struct afs_call *call)\n{\n\tafs_put_vlserver(call->net, call->vlserver);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_RXVLGetCapabilities = {\n\t.name\t\t= \"VL.GetCapabilities\",\n\t.op\t\t= afs_VL_GetCapabilities,\n\t.deliver\t= afs_deliver_vl_get_capabilities,\n\t.done\t\t= afs_vlserver_probe_result,\n\t.destructor\t= afs_destroy_vl_get_capabilities,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nstruct afs_call *afs_vl_get_capabilities(struct afs_net *net,\n\t\t\t\t\t struct afs_addr_cursor *ac,\n\t\t\t\t\t struct key *key,\n\t\t\t\t\t struct afs_vlserver *server,\n\t\t\t\t\t unsigned int server_index)\n{\n\tstruct afs_call *call;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetCapabilities, 1 * 4, 16 * 4);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = key;\n\tcall->vlserver = afs_get_vlserver(server);\n\tcall->server_index = server_index;\n\tcall->upgrade = true;\n\tcall->async = true;\n\tcall->max_lifespan = AFS_PROBE_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETCAPABILITIES);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(ac, call, GFP_KERNEL);\n\treturn call;\n}\n\n/*\n * Deliver reply data to a YFSVL.GetEndpoints call.\n *\n *\tGetEndpoints(IN yfsServerAttributes *attr,\n *\t\t     OUT opr_uuid *uuid,\n *\t\t     OUT afs_int32 *uniquifier,\n *\t\t     OUT endpoints *fsEndpoints,\n *\t\t     OUT endpoints *volEndpoints)\n */\nstatic int afs_deliver_yfsvl_get_endpoints(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, size;\n\tint ret;\n\n\t_enter(\"{%u,%zu,%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count2);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call, sizeof(uuid_t) + 3 * sizeof(__be32));\n\t\tcall->unmarshall = 1;\n\n\t\t/* Extract the returned uuid, uniquifier, fsEndpoints count and\n\t\t * either the first fsEndpoint type or the volEndpoints\n\t\t * count if there are no fsEndpoints. */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(uuid_t);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tcall->count\t= ntohl(*bp++);\n\t\tcall->count2\t= ntohl(*bp); /* Type or next count */\n\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_num);\n\n\t\talist = afs_alloc_addrlist(call->count, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\n\t\tif (call->count == 0)\n\t\t\tgoto extract_volendpoints;\n\n\tnext_fsendpoint:\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\tsize += sizeof(__be32);\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 2;\n\n\t\tfallthrough;\t/* and extract fsEndpoints[] entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt4_len);\n\t\t\tafs_merge_fs_addr4(alist, bp[1], ntohl(bp[2]));\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt6_len);\n\t\t\tafs_merge_fs_addr6(alist, bp + 1, ntohl(bp[5]));\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count2 = ntohl(*bp++);\n\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_fsendpoint;\n\n\textract_volendpoints:\n\t\t/* Extract the list of volEndpoints. */\n\t\tcall->count = call->count2;\n\t\tif (!call->count)\n\t\t\tgoto end;\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\n\t\tafs_extract_to_buf(call, 1 * sizeof(__be32));\n\t\tcall->unmarshall = 3;\n\n\t\t/* Extract the type of volEndpoints[0].  Normally we would\n\t\t * extract the type of the next endpoint when we extract the\n\t\t * data of the current one, but this is the first...\n\t\t */\n\t\tfallthrough;\n\tcase 3:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\n\tnext_volendpoint:\n\t\tcall->count2 = ntohl(*bp++);\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\tif (call->count > 1)\n\t\t\tsize += sizeof(__be32); /* Get next type too */\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 4;\n\n\t\tfallthrough;\t/* and extract volEndpoints[] entries */\n\tcase 4:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt4_len);\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt6_len);\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_volendpoint;\n\n\tend:\n\t\tafs_extract_discard(call, 0);\n\t\tcall->unmarshall = 5;\n\n\t\tfallthrough;\t/* Done */\n\tcase 5:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tcall->unmarshall = 6;\n\t\tfallthrough;\n\n\tcase 6:\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * YFSVL.GetEndpoints operation type.\n */\nstatic const struct afs_call_type afs_YFSVLGetEndpoints = {\n\t.name\t\t= \"YFSVL.GetEndpoints\",\n\t.op\t\t= afs_YFSVL_GetEndpoints,\n\t.deliver\t= afs_deliver_yfsvl_get_endpoints,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_yfsvl_get_endpoints(struct afs_vl_cursor *vc,\n\t\t\t\t\t      const uuid_t *uuid)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetEndpoints,\n\t\t\t\t   sizeof(__be32) * 2 + sizeof(*uuid),\n\t\t\t\t   sizeof(struct in6_addr) + sizeof(__be32) * 3);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETENDPOINTS);\n\t*bp++ = htonl(YFS_SERVER_UUID);\n\tmemcpy(bp, uuid, sizeof(*uuid)); /* Type opr_uuid */\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a YFSVL.GetCellName operation.\n */\nstatic int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tchar *cell_name;\n\tu32 namesz, paddedsz;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the cell name length */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnamesz = ntohl(call->tmp);\n\t\tif (namesz > AFS_MAXCELLNAME)\n\t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n\t\tpaddedsz = (namesz + 3) & ~3;\n\t\tcall->count = namesz;\n\t\tcall->count2 = paddedsz - namesz;\n\n\t\tcell_name = kmalloc(namesz + 1, GFP_KERNEL);\n\t\tif (!cell_name)\n\t\t\treturn -ENOMEM;\n\t\tcell_name[namesz] = 0;\n\t\tcall->ret_str = cell_name;\n\n\t\tafs_extract_begin(call, cell_name, namesz);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract cell name */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tafs_extract_discard(call, call->count2);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract padding */\n\tcase 3:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tkfree(call->ret_str);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_YFSVLGetCellName = {\n\t.name\t\t= \"YFSVL.GetCellName\",\n\t.op\t\t= afs_YFSVL_GetCellName,\n\t.deliver\t= afs_deliver_yfsvl_get_cell_name,\n\t.destructor\t= afs_destroy_yfsvl_get_cell_name,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nchar *afs_yfsvl_get_cell_name(struct afs_vl_cursor *vc)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetCellName, 1 * 4, 0);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_str = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETCELLNAME);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (char *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n"
                            }
                        },
                        {
                            "downstream_patch": "9340385468d056bb700b8f28df236b81fc86a079",
                            "downstream_commit": "a965f7f0ea3ae61b9165bed619d5d6da02c75f80",
                            "commit_date": "2025-02-01 18:22:19 +0100",
                            "result": "success",
                            "patch_apply_output": "patching file fs/afs/afs.h\npatching file fs/afs/afs_vl.h\npatching file fs/afs/vl_alias.c\nHunk #1 succeeded at 302 (offset 49 lines).\nHunk #2 succeeded at 314 (offset 49 lines).\npatching file fs/afs/vlclient.c\nHunk #1 succeeded at 670 (offset -27 lines).",
                            "downstream_patch_content": "commit 9340385468d056bb700b8f28df236b81fc86a079\nAuthor: David Howells <dhowells@redhat.com>\nDate:   Mon Jan 6 16:21:00 2025 +0000\n\n    afs: Fix the maximum cell name length\n    \n    [ Upstream commit 8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8 ]\n    \n    The kafs filesystem limits the maximum length of a cell to 256 bytes, but a\n    problem occurs if someone actually does that: kafs tries to create a\n    directory under /proc/net/afs/ with the name of the cell, but that fails\n    with a warning:\n    \n            WARNING: CPU: 0 PID: 9 at fs/proc/generic.c:405\n    \n    because procfs limits the maximum filename length to 255.\n    \n    However, the DNS limits the maximum lookup length and, by extension, the\n    maximum cell name, to 255 less two (length count and trailing NUL).\n    \n    Fix this by limiting the maximum acceptable cellname length to 253.  This\n    also allows us to be sure we can create the \"/afs/.<cell>/\" mountpoint too.\n    \n    Further, split the YFS VL record cell name maximum to be the 256 allowed by\n    the protocol and ignore the record retrieved by YFSVL.GetCellName if it\n    exceeds 253.\n    \n    Fixes: c3e9f888263b (\"afs: Implement client support for the YFSVL.GetCellName RPC op\")\n    Reported-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    Closes: https://lore.kernel.org/r/6776d25d.050a0220.3a8527.0048.GAE@google.com/\n    Signed-off-by: David Howells <dhowells@redhat.com>\n    Link: https://lore.kernel.org/r/376236.1736180460@warthog.procyon.org.uk\n    Tested-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\n    cc: Marc Dionne <marc.dionne@auristor.com>\n    cc: linux-afs@lists.infradead.org\n    Signed-off-by: Christian Brauner <brauner@kernel.org>\n    Signed-off-by: Sasha Levin <sashal@kernel.org>\n\ndiff --git a/fs/afs/afs.h b/fs/afs/afs.h\nindex 432cb4b23961..3ea5f3e3c922 100644\n--- a/fs/afs/afs.h\n+++ b/fs/afs/afs.h\n@@ -10,7 +10,7 @@\n \n #include <linux/in.h>\n \n-#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n+#define AFS_MAXCELLNAME\t\t253  \t/* Maximum length of a cell name (DNS limited) */\n #define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n #define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n #define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\ndiff --git a/fs/afs/afs_vl.h b/fs/afs/afs_vl.h\nindex 9c65ffb8a523..8da0899fbc08 100644\n--- a/fs/afs/afs_vl.h\n+++ b/fs/afs/afs_vl.h\n@@ -13,6 +13,7 @@\n #define AFS_VL_PORT\t\t7003\t/* volume location service port */\n #define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n #define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n+#define YFS_VL_MAXCELLNAME\t256  \t/* Maximum length of a cell name in YFS protocol */\n \n enum AFSVL_Operations {\n \tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\ndiff --git a/fs/afs/vl_alias.c b/fs/afs/vl_alias.c\nindex f04a80e4f5c3..83cf1bfbe343 100644\n--- a/fs/afs/vl_alias.c\n+++ b/fs/afs/vl_alias.c\n@@ -302,6 +302,7 @@ static char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n {\n \tstruct afs_cell *master;\n+\tsize_t name_len;\n \tchar *cell_name;\n \n \tcell_name = afs_vl_get_cell_name(cell, key);\n@@ -313,8 +314,11 @@ static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n \t\treturn 0;\n \t}\n \n-\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n-\t\t\t\t NULL, false);\n+\tname_len = strlen(cell_name);\n+\tif (!name_len || name_len > AFS_MAXCELLNAME)\n+\t\tmaster = ERR_PTR(-EOPNOTSUPP);\n+\telse\n+\t\tmaster = afs_lookup_cell(cell->net, cell_name, name_len, NULL, false);\n \tkfree(cell_name);\n \tif (IS_ERR(master))\n \t\treturn PTR_ERR(master);\ndiff --git a/fs/afs/vlclient.c b/fs/afs/vlclient.c\nindex dc9327332f06..882f0727c3cd 100644\n--- a/fs/afs/vlclient.c\n+++ b/fs/afs/vlclient.c\n@@ -670,7 +670,7 @@ static int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n \t\t\treturn ret;\n \n \t\tnamesz = ntohl(call->tmp);\n-\t\tif (namesz > AFS_MAXCELLNAME)\n+\t\tif (namesz > YFS_VL_MAXCELLNAME)\n \t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n \t\tpaddedsz = (namesz + 3) & ~3;\n \t\tcall->count = namesz;\n",
                            "downstream_file_content": {
                                "fs/afs/afs.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS common types\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_H\n#define AFS_H\n\n#include <linux/in.h>\n\n#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n#define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n#define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n#define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\n#define AFS_MAXTYPES\t\t3\t/* Maximum number of volume types */\n#define AFSNAMEMAX\t\t256 \t/* Maximum length of a filename plus NUL */\n#define AFSPATHMAX\t\t1024\t/* Maximum length of a pathname plus NUL */\n#define AFSOPAQUEMAX\t\t1024\t/* Maximum length of an opaque field */\n\n#define AFS_VL_MAX_LIFESPAN\t(120 * HZ)\n#define AFS_PROBE_MAX_LIFESPAN\t(30 * HZ)\n\ntypedef u64\t\t\tafs_volid_t;\ntypedef u64\t\t\tafs_vnodeid_t;\ntypedef u64\t\t\tafs_dataversion_t;\n\ntypedef enum {\n\tAFSVL_RWVOL,\t\t\t/* read/write volume */\n\tAFSVL_ROVOL,\t\t\t/* read-only volume */\n\tAFSVL_BACKVOL,\t\t\t/* backup volume */\n} __attribute__((packed)) afs_voltype_t;\n\ntypedef enum {\n\tAFS_FTYPE_INVALID\t= 0,\n\tAFS_FTYPE_FILE\t\t= 1,\n\tAFS_FTYPE_DIR\t\t= 2,\n\tAFS_FTYPE_SYMLINK\t= 3,\n} afs_file_type_t;\n\ntypedef enum {\n\tAFS_LOCK_READ\t\t= 0,\t/* read lock request */\n\tAFS_LOCK_WRITE\t\t= 1,\t/* write lock request */\n} afs_lock_type_t;\n\n#define AFS_LOCKWAIT\t\t(5 * 60) /* time until a lock times out (seconds) */\n\n/*\n * AFS file identifier\n */\nstruct afs_fid {\n\tafs_volid_t\tvid;\t\t/* volume ID */\n\tafs_vnodeid_t\tvnode;\t\t/* Lower 64-bits of file index within volume */\n\tu32\t\tvnode_hi;\t/* Upper 32-bits of file index */\n\tu32\t\tunique;\t\t/* unique ID number (file index version) */\n};\n\n/*\n * AFS callback notification\n */\ntypedef enum {\n\tAFSCM_CB_UNTYPED\t= 0,\t/* no type set on CB break */\n\tAFSCM_CB_EXCLUSIVE\t= 1,\t/* CB exclusive to CM [not implemented] */\n\tAFSCM_CB_SHARED\t\t= 2,\t/* CB shared by other CM's */\n\tAFSCM_CB_DROPPED\t= 3,\t/* CB promise cancelled by file server */\n} afs_callback_type_t;\n\nstruct afs_callback {\n\ttime64_t\t\texpires_at;\t/* Time at which expires */\n\t//unsigned\t\tversion;\t/* Callback version */\n\t//afs_callback_type_t\ttype;\t\t/* Type of callback */\n};\n\nstruct afs_callback_break {\n\tstruct afs_fid\t\tfid;\t\t/* File identifier */\n\t//struct afs_callback\tcb;\t\t/* Callback details */\n};\n\n#define AFSCBMAX 50\t/* maximum callbacks transferred per bulk op */\n\nstruct afs_uuid {\n\t__be32\t\ttime_low;\t\t\t/* low part of timestamp */\n\t__be16\t\ttime_mid;\t\t\t/* mid part of timestamp */\n\t__be16\t\ttime_hi_and_version;\t\t/* high part of timestamp and version  */\n\t__s8\t\tclock_seq_hi_and_reserved;\t/* clock seq hi and variant */\n\t__s8\t\tclock_seq_low;\t\t\t/* clock seq low */\n\t__s8\t\tnode[6];\t\t\t/* spatially unique node ID (MAC addr) */\n};\n\n/*\n * AFS volume information\n */\nstruct afs_volume_info {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_voltype_t\t\ttype;\t\t/* type of this volume */\n\tafs_volid_t\t\ttype_vids[5];\t/* volume ID's for possible types for this vol */\n\n\t/* list of fileservers serving this volume */\n\tsize_t\t\t\tnservers;\t/* number of entries used in servers[] */\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* fileserver address */\n\t} servers[8];\n};\n\n/*\n * AFS security ACE access mask\n */\ntypedef u32 afs_access_t;\n#define AFS_ACE_READ\t\t0x00000001U\t/* - permission to read a file/dir */\n#define AFS_ACE_WRITE\t\t0x00000002U\t/* - permission to write/chmod a file */\n#define AFS_ACE_INSERT\t\t0x00000004U\t/* - permission to create dirent in a dir */\n#define AFS_ACE_LOOKUP\t\t0x00000008U\t/* - permission to lookup a file/dir in a dir */\n#define AFS_ACE_DELETE\t\t0x00000010U\t/* - permission to delete a dirent from a dir */\n#define AFS_ACE_LOCK\t\t0x00000020U\t/* - permission to lock a file */\n#define AFS_ACE_ADMINISTER\t0x00000040U\t/* - permission to change ACL */\n#define AFS_ACE_USER_A\t\t0x01000000U\t/* - 'A' user-defined permission */\n#define AFS_ACE_USER_B\t\t0x02000000U\t/* - 'B' user-defined permission */\n#define AFS_ACE_USER_C\t\t0x04000000U\t/* - 'C' user-defined permission */\n#define AFS_ACE_USER_D\t\t0x08000000U\t/* - 'D' user-defined permission */\n#define AFS_ACE_USER_E\t\t0x10000000U\t/* - 'E' user-defined permission */\n#define AFS_ACE_USER_F\t\t0x20000000U\t/* - 'F' user-defined permission */\n#define AFS_ACE_USER_G\t\t0x40000000U\t/* - 'G' user-defined permission */\n#define AFS_ACE_USER_H\t\t0x80000000U\t/* - 'H' user-defined permission */\n\n/*\n * AFS file status information\n */\nstruct afs_file_status {\n\tu64\t\t\tsize;\t\t/* file size */\n\tafs_dataversion_t\tdata_version;\t/* current data version */\n\tstruct timespec64\tmtime_client;\t/* Last time client changed data */\n\tstruct timespec64\tmtime_server;\t/* Last time server changed data */\n\ts64\t\t\tauthor;\t\t/* author ID */\n\ts64\t\t\towner;\t\t/* owner ID */\n\ts64\t\t\tgroup;\t\t/* group ID */\n\tafs_access_t\t\tcaller_access;\t/* access rights for authenticated caller */\n\tafs_access_t\t\tanon_access;\t/* access rights for unauthenticated caller */\n\tumode_t\t\t\tmode;\t\t/* UNIX mode */\n\tafs_file_type_t\t\ttype;\t\t/* file type */\n\tu32\t\t\tnlink;\t\t/* link count */\n\ts32\t\t\tlock_count;\t/* file lock count (0=UNLK -1=WRLCK +ve=#RDLCK */\n\tu32\t\t\tabort_code;\t/* Abort if bulk-fetching this failed */\n};\n\nstruct afs_status_cb {\n\tstruct afs_file_status\tstatus;\n\tstruct afs_callback\tcallback;\n\tbool\t\t\thave_status;\t/* True if status record was retrieved */\n\tbool\t\t\thave_cb;\t/* True if cb record was retrieved */\n\tbool\t\t\thave_error;\t/* True if status.abort_code indicates an error */\n};\n\n/*\n * AFS file status change request\n */\n\n#define AFS_SET_MTIME\t\t0x01\t\t/* set the mtime */\n#define AFS_SET_OWNER\t\t0x02\t\t/* set the owner ID */\n#define AFS_SET_GROUP\t\t0x04\t\t/* set the group ID (unsupported?) */\n#define AFS_SET_MODE\t\t0x08\t\t/* set the UNIX mode */\n#define AFS_SET_SEG_SIZE\t0x10\t\t/* set the segment size (unsupported) */\n\n/*\n * AFS volume synchronisation information\n */\nstruct afs_volsync {\n\ttime64_t\t\tcreation;\t/* volume creation time */\n};\n\n/*\n * AFS volume status record\n */\nstruct afs_volume_status {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_volid_t\t\tparent_id;\t/* parent volume ID */\n\tu8\t\t\tonline;\t\t/* true if volume currently online and available */\n\tu8\t\t\tin_service;\t/* true if volume currently in service */\n\tu8\t\t\tblessed;\t/* same as in_service */\n\tu8\t\t\tneeds_salvage;\t/* true if consistency checking required */\n\tu32\t\t\ttype;\t\t/* volume type (afs_voltype_t) */\n\tu64\t\t\tmin_quota;\t/* minimum space set aside (blocks) */\n\tu64\t\t\tmax_quota;\t/* maximum space this volume may occupy (blocks) */\n\tu64\t\t\tblocks_in_use;\t/* space this volume currently occupies (blocks) */\n\tu64\t\t\tpart_blocks_avail; /* space available in volume's partition */\n\tu64\t\t\tpart_max_blocks; /* size of volume's partition */\n\ts64\t\t\tvol_copy_date;\n\ts64\t\t\tvol_backup_date;\n};\n\n#define AFS_BLOCK_SIZE\t1024\n\n/*\n * XDR encoding of UUID in AFS.\n */\nstruct afs_uuid__xdr {\n\t__be32\t\ttime_low;\n\t__be32\t\ttime_mid;\n\t__be32\t\ttime_hi_and_version;\n\t__be32\t\tclock_seq_hi_and_reserved;\n\t__be32\t\tclock_seq_low;\n\t__be32\t\tnode[6];\n};\n\n#endif /* AFS_H */\n",
                                "fs/afs/afs_vl.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS Volume Location Service client interface\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_VL_H\n#define AFS_VL_H\n\n#include \"afs.h\"\n\n#define AFS_VL_PORT\t\t7003\t/* volume location service port */\n#define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n#define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n\nenum AFSVL_Operations {\n\tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\n\tVLGETENTRYBYNAME\t= 504,\t/* AFS Get VLDB entry by name */\n\tVLPROBE\t\t\t= 514,\t/* AFS probe VL service */\n\tVLGETENTRYBYIDU\t\t= 526,\t/* AFS Get VLDB entry by ID (UUID-variant) */\n\tVLGETENTRYBYNAMEU\t= 527,\t/* AFS Get VLDB entry by name (UUID-variant) */\n\tVLGETADDRSU\t\t= 533,\t/* AFS Get addrs for fileserver */\n\tYVLGETENDPOINTS\t\t= 64002, /* YFS Get endpoints for file/volume server */\n\tYVLGETCELLNAME\t\t= 64014, /* YFS Get actual cell name */\n\tVLGETCAPABILITIES\t= 65537, /* AFS Get server capabilities */\n};\n\nenum AFSVL_Errors {\n\tAFSVL_IDEXIST \t\t= 363520,\t/* Volume Id entry exists in vl database */\n\tAFSVL_IO \t\t= 363521,\t/* I/O related error */\n\tAFSVL_NAMEEXIST \t= 363522,\t/* Volume name entry exists in vl database */\n\tAFSVL_CREATEFAIL \t= 363523,\t/* Internal creation failure */\n\tAFSVL_NOENT \t\t= 363524,\t/* No such entry */\n\tAFSVL_EMPTY \t\t= 363525,\t/* Vl database is empty */\n\tAFSVL_ENTDELETED \t= 363526,\t/* Entry is deleted (soft delete) */\n\tAFSVL_BADNAME \t\t= 363527,\t/* Volume name is illegal */\n\tAFSVL_BADINDEX \t\t= 363528,\t/* Index is out of range */\n\tAFSVL_BADVOLTYPE \t= 363529,\t/* Bad volume type */\n\tAFSVL_BADSERVER \t= 363530,\t/* Illegal server number (out of range) */\n\tAFSVL_BADPARTITION \t= 363531,\t/* Bad partition number */\n\tAFSVL_REPSFULL \t\t= 363532,\t/* Run out of space for Replication sites */\n\tAFSVL_NOREPSERVER \t= 363533,\t/* No such Replication server site exists */\n\tAFSVL_DUPREPSERVER \t= 363534,\t/* Replication site already exists */\n\tAFSVL_RWNOTFOUND \t= 363535,\t/* Parent R/W entry not found */\n\tAFSVL_BADREFCOUNT \t= 363536,\t/* Illegal Reference Count number */\n\tAFSVL_SIZEEXCEEDED \t= 363537,\t/* Vl size for attributes exceeded */\n\tAFSVL_BADENTRY \t\t= 363538,\t/* Bad incoming vl entry */\n\tAFSVL_BADVOLIDBUMP \t= 363539,\t/* Illegal max volid increment */\n\tAFSVL_IDALREADYHASHED \t= 363540,\t/* RO/BACK id already hashed */\n\tAFSVL_ENTRYLOCKED \t= 363541,\t/* Vl entry is already locked */\n\tAFSVL_BADVOLOPER \t= 363542,\t/* Bad volume operation code */\n\tAFSVL_BADRELLOCKTYPE \t= 363543,\t/* Bad release lock type */\n\tAFSVL_RERELEASE \t= 363544,\t/* Status report: last release was aborted */\n\tAFSVL_BADSERVERFLAG \t= 363545,\t/* Invalid replication site server flag */\n\tAFSVL_PERM \t\t= 363546,\t/* No permission access */\n\tAFSVL_NOMEM \t\t= 363547,\t/* malloc/realloc failed to alloc enough memory */\n};\n\nenum {\n\tYFS_SERVER_INDEX\t= 0,\n\tYFS_SERVER_UUID\t\t= 1,\n\tYFS_SERVER_ENDPOINT\t= 2,\n};\n\nenum {\n\tYFS_ENDPOINT_IPV4\t= 0,\n\tYFS_ENDPOINT_IPV6\t= 1,\n};\n\n#define YFS_MAXENDPOINTS\t16\n\n/*\n * maps to \"struct vldbentry\" in vvl-spec.pdf\n */\nstruct afs_vldbentry {\n\tchar\t\tname[65];\t\t/* name of volume (with NUL char) */\n\tafs_voltype_t\ttype;\t\t\t/* volume type */\n\tunsigned\tnum_servers;\t\t/* num servers that hold instances of this vol */\n\tunsigned\tclone_id;\t\t/* cloning ID */\n\n\tunsigned\tflags;\n#define AFS_VLF_RWEXISTS\t0x1000\t\t/* R/W volume exists */\n#define AFS_VLF_ROEXISTS\t0x2000\t\t/* R/O volume exists */\n#define AFS_VLF_BACKEXISTS\t0x4000\t\t/* backup volume exists */\n\n\tafs_volid_t\tvolume_ids[3];\t\t/* volume IDs */\n\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* server address */\n\t\tunsigned\tpartition;\t/* partition ID on this server */\n\t\tunsigned\tflags;\t\t/* server specific flags */\n#define AFS_VLSF_NEWREPSITE\t0x0001\t/* Ignore all 'non-new' servers */\n#define AFS_VLSF_ROVOL\t\t0x0002\t/* this server holds a R/O instance of the volume */\n#define AFS_VLSF_RWVOL\t\t0x0004\t/* this server holds a R/W instance of the volume */\n#define AFS_VLSF_BACKVOL\t0x0008\t/* this server holds a backup instance of the volume */\n#define AFS_VLSF_UUID\t\t0x0010\t/* This server is referred to by its UUID */\n#define AFS_VLSF_DONTUSE\t0x0020\t/* This server ref should be ignored */\n\t} servers[8];\n};\n\n#define AFS_VLDB_MAXNAMELEN 65\n\n\nstruct afs_ListAddrByAttributes__xdr {\n\t__be32\t\t\tMask;\n#define AFS_VLADDR_IPADDR\t0x1\t/* Match by ->ipaddr */\n#define AFS_VLADDR_INDEX\t0x2\t/* Match by ->index */\n#define AFS_VLADDR_UUID\t\t0x4\t/* Match by ->uuid */\n\t__be32\t\t\tipaddr;\n\t__be32\t\t\tindex;\n\t__be32\t\t\tspare;\n\tstruct afs_uuid__xdr\tuuid;\n};\n\nstruct afs_uvldbentry__xdr {\n\t__be32\t\t\tname[AFS_VLDB_MAXNAMELEN];\n\t__be32\t\t\tnServers;\n\tstruct afs_uuid__xdr\tserverNumber[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverUnique[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverPartition[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverFlags[AFS_NMAXNSERVERS];\n\t__be32\t\t\tvolumeId[AFS_MAXTYPES];\n\t__be32\t\t\tcloneId;\n\t__be32\t\t\tflags;\n\t__be32\t\t\tspares1;\n\t__be32\t\t\tspares2;\n\t__be32\t\t\tspares3;\n\t__be32\t\t\tspares4;\n\t__be32\t\t\tspares5;\n\t__be32\t\t\tspares6;\n\t__be32\t\t\tspares7;\n\t__be32\t\t\tspares8;\n\t__be32\t\t\tspares9;\n};\n\nstruct afs_address_list {\n\trefcount_t\t\tusage;\n\tunsigned int\t\tversion;\n\tunsigned int\t\tnr_addrs;\n\tstruct sockaddr_rxrpc\taddrs[];\n};\n\nextern void afs_put_address_list(struct afs_address_list *alist);\n\n#endif /* AFS_VL_H */\n",
                                "fs/afs/vl_alias.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS cell alias detection\n *\n * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/namei.h>\n#include <keys/rxrpc-type.h>\n#include \"internal.h\"\n\n/*\n * Sample a volume.\n */\nstatic struct afs_volume *afs_sample_volume(struct afs_cell *cell, struct key *key,\n\t\t\t\t\t    const char *name, unsigned int namelen)\n{\n\tstruct afs_volume *volume;\n\tstruct afs_fs_context fc = {\n\t\t.type\t\t= 0, /* Explicitly leave it to the VLDB */\n\t\t.volnamesz\t= namelen,\n\t\t.volname\t= name,\n\t\t.net\t\t= cell->net,\n\t\t.cell\t\t= cell,\n\t\t.key\t\t= key, /* This might need to be something */\n\t};\n\n\tvolume = afs_create_volume(&fc);\n\t_leave(\" = %p\", volume);\n\treturn volume;\n}\n\n/*\n * Compare two addresses.\n */\nstatic int afs_compare_addrs(const struct sockaddr_rxrpc *srx_a,\n\t\t\t     const struct sockaddr_rxrpc *srx_b)\n{\n\tshort port_a, port_b;\n\tint addr_a, addr_b, diff;\n\n\tdiff = (short)srx_a->transport_type - (short)srx_b->transport_type;\n\tif (diff)\n\t\tgoto out;\n\n\tswitch (srx_a->transport_type) {\n\tcase AF_INET: {\n\t\tconst struct sockaddr_in *a = &srx_a->transport.sin;\n\t\tconst struct sockaddr_in *b = &srx_b->transport.sin;\n\t\taddr_a = ntohl(a->sin_addr.s_addr);\n\t\taddr_b = ntohl(b->sin_addr.s_addr);\n\t\tdiff = addr_a - addr_b;\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin_port);\n\t\t\tport_b = ntohs(b->sin_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase AF_INET6: {\n\t\tconst struct sockaddr_in6 *a = &srx_a->transport.sin6;\n\t\tconst struct sockaddr_in6 *b = &srx_b->transport.sin6;\n\t\tdiff = memcmp(&a->sin6_addr, &b->sin6_addr, 16);\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin6_port);\n\t\t\tport_b = ntohs(b->sin6_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\tWARN_ON(1);\n\t\tdiff = 1;\n\t}\n\nout:\n\treturn diff;\n}\n\n/*\n * Compare the address lists of a pair of fileservers.\n */\nstatic int afs_compare_fs_alists(const struct afs_server *server_a,\n\t\t\t\t const struct afs_server *server_b)\n{\n\tconst struct afs_addr_list *la, *lb;\n\tint a = 0, b = 0, addr_matches = 0;\n\n\tla = rcu_dereference(server_a->addresses);\n\tlb = rcu_dereference(server_b->addresses);\n\n\twhile (a < la->nr_addrs && b < lb->nr_addrs) {\n\t\tconst struct sockaddr_rxrpc *srx_a = &la->addrs[a];\n\t\tconst struct sockaddr_rxrpc *srx_b = &lb->addrs[b];\n\t\tint diff = afs_compare_addrs(srx_a, srx_b);\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\taddr_matches++;\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\treturn addr_matches;\n}\n\n/*\n * Compare the fileserver lists of two volumes.  The server lists are sorted in\n * order of ascending UUID.\n */\nstatic int afs_compare_volume_slists(const struct afs_volume *vol_a,\n\t\t\t\t     const struct afs_volume *vol_b)\n{\n\tconst struct afs_server_list *la, *lb;\n\tint i, a = 0, b = 0, uuid_matches = 0, addr_matches = 0;\n\n\tla = rcu_dereference(vol_a->servers);\n\tlb = rcu_dereference(vol_b->servers);\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tif (la->vids[i] != lb->vids[i])\n\t\t\treturn 0;\n\n\twhile (a < la->nr_servers && b < lb->nr_servers) {\n\t\tconst struct afs_server *server_a = la->servers[a].server;\n\t\tconst struct afs_server *server_b = lb->servers[b].server;\n\t\tint diff = memcmp(&server_a->uuid, &server_b->uuid, sizeof(uuid_t));\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\tuuid_matches++;\n\t\t\taddr_matches += afs_compare_fs_alists(server_a, server_b);\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\t_leave(\" = %d [um %d]\", addr_matches, uuid_matches);\n\treturn addr_matches;\n}\n\n/*\n * Compare root.cell volumes.\n */\nstatic int afs_compare_cell_roots(struct afs_cell *cell)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"\");\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (!p->root_volume)\n\t\t\tcontinue; /* Ignore cells that don't have a root.cell volume. */\n\n\t\tif (afs_compare_volume_slists(cell->root_volume, p->root_volume) != 0)\n\t\t\tgoto is_alias;\n\t}\n\n\trcu_read_unlock();\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\trcu_read_unlock();\n\tcell->alias_of = afs_use_cell(p, afs_cell_trace_use_alias);\n\treturn 1;\n}\n\n/*\n * Query the new cell for a volume from a cell we're already using.\n */\nstatic int afs_query_for_alias_one(struct afs_cell *cell, struct key *key,\n\t\t\t\t   struct afs_cell *p)\n{\n\tstruct afs_volume *volume, *pvol = NULL;\n\tint ret;\n\n\t/* Arbitrarily pick a volume from the list. */\n\tread_seqlock_excl(&p->volume_lock);\n\tif (!RB_EMPTY_ROOT(&p->volumes))\n\t\tpvol = afs_get_volume(rb_entry(p->volumes.rb_node,\n\t\t\t\t\t       struct afs_volume, cell_node),\n\t\t\t\t      afs_volume_trace_get_query_alias);\n\tread_sequnlock_excl(&p->volume_lock);\n\tif (!pvol)\n\t\treturn 0;\n\n\t_enter(\"%s:%s\", cell->name, pvol->name);\n\n\t/* And see if it's in the new cell. */\n\tvolume = afs_sample_volume(cell, key, pvol->name, pvol->name_len);\n\tif (IS_ERR(volume)) {\n\t\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\t\tif (PTR_ERR(volume) != -ENOMEDIUM)\n\t\t\treturn PTR_ERR(volume);\n\t\t/* That volume is not in the new cell, so not an alias */\n\t\treturn 0;\n\t}\n\n\t/* The new cell has a like-named volume also - compare volume ID,\n\t * server and address lists.\n\t */\n\tret = 0;\n\tif (pvol->vid == volume->vid) {\n\t\trcu_read_lock();\n\t\tif (afs_compare_volume_slists(volume, pvol))\n\t\t\tret = 1;\n\t\trcu_read_unlock();\n\t}\n\n\tafs_put_volume(cell->net, volume, afs_volume_trace_put_query_alias);\n\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\treturn ret;\n}\n\n/*\n * Query the new cell for volumes we know exist in cells we're already using.\n */\nstatic int afs_query_for_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"%s\", cell->name);\n\n\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\thlist_for_each_entry(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (RB_EMPTY_ROOT(&p->volumes))\n\t\t\tcontinue;\n\t\tif (p->root_volume)\n\t\t\tcontinue; /* Ignore cells that have a root.cell volume. */\n\t\tafs_use_cell(p, afs_cell_trace_use_check_alias);\n\t\tmutex_unlock(&cell->net->proc_cells_lock);\n\n\t\tif (afs_query_for_alias_one(cell, key, p) != 0)\n\t\t\tgoto is_alias;\n\n\t\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0) {\n\t\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\n\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t}\n\n\tmutex_unlock(&cell->net->proc_cells_lock);\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\tcell->alias_of = p; /* Transfer our ref */\n\treturn 1;\n}\n\n/*\n * Look up a VLDB record for a volume.\n */\nstatic char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_vl_cursor vc;\n\tchar *cell_name = ERR_PTR(-EDESTADDRREQ);\n\tbool skipped = false, not_skipped = false;\n\tint ret;\n\n\tif (!afs_begin_vlserver_operation(&vc, cell, key))\n\t\treturn ERR_PTR(-ERESTARTSYS);\n\n\twhile (afs_select_vlserver(&vc)) {\n\t\tif (!test_bit(AFS_VLSERVER_FL_IS_YFS, &vc.server->flags)) {\n\t\t\tvc.ac.error = -EOPNOTSUPP;\n\t\t\tskipped = true;\n\t\t\tcontinue;\n\t\t}\n\t\tnot_skipped = true;\n\t\tcell_name = afs_yfsvl_get_cell_name(&vc);\n\t}\n\n\tret = afs_end_vlserver_operation(&vc);\n\tif (skipped && !not_skipped)\n\t\tret = -EOPNOTSUPP;\n\treturn ret < 0 ? ERR_PTR(ret) : cell_name;\n}\n\nstatic int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *master;\n\tchar *cell_name;\n\n\tcell_name = afs_vl_get_cell_name(cell, key);\n\tif (IS_ERR(cell_name))\n\t\treturn PTR_ERR(cell_name);\n\n\tif (strcmp(cell_name, cell->name) == 0) {\n\t\tkfree(cell_name);\n\t\treturn 0;\n\t}\n\n\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n\t\t\t\t NULL, false);\n\tkfree(cell_name);\n\tif (IS_ERR(master))\n\t\treturn PTR_ERR(master);\n\n\tcell->alias_of = master; /* Transfer our ref */\n\treturn 1;\n}\n\nstatic int afs_do_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_volume *root_volume;\n\tint ret;\n\n\t_enter(\"%s\", cell->name);\n\n\tret = yfs_check_canonical_cell_name(cell, key);\n\tif (ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\t/* Try and get the root.cell volume for comparison with other cells */\n\troot_volume = afs_sample_volume(cell, key, \"root.cell\", 9);\n\tif (!IS_ERR(root_volume)) {\n\t\tcell->root_volume = root_volume;\n\t\treturn afs_compare_cell_roots(cell);\n\t}\n\n\tif (PTR_ERR(root_volume) != -ENOMEDIUM)\n\t\treturn PTR_ERR(root_volume);\n\n\t/* Okay, this cell doesn't have an root.cell volume.  We need to\n\t * locate some other random volume and use that to check.\n\t */\n\treturn afs_query_for_alias(cell, key);\n}\n\n/*\n * Check to see if a new cell is an alias of a cell we already have.  At this\n * point we have the cell's volume server list.\n *\n * Returns 0 if we didn't detect an alias, 1 if we found an alias and an error\n * if we had problems gathering the data required.  In the case the we did\n * detect an alias, cell->alias_of is set to point to the assumed master.\n */\nint afs_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_net *net = cell->net;\n\tint ret;\n\n\tif (mutex_lock_interruptible(&net->cells_alias_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\tif (test_bit(AFS_CELL_FL_CHECK_ALIAS, &cell->flags)) {\n\t\tret = afs_do_cell_detect_alias(cell, key);\n\t\tif (ret >= 0)\n\t\t\tclear_bit_unlock(AFS_CELL_FL_CHECK_ALIAS, &cell->flags);\n\t} else {\n\t\tret = cell->alias_of ? 1 : 0;\n\t}\n\n\tmutex_unlock(&net->cells_alias_lock);\n\n\tif (ret == 1)\n\t\tpr_notice(\"kAFS: Cell %s is an alias of %s\\n\",\n\t\t\t  cell->name, cell->alias_of->name);\n\treturn ret;\n}\n",
                                "fs/afs/vlclient.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS Volume Location Service client\n *\n * Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include \"afs_fs.h\"\n#include \"internal.h\"\n\n/*\n * Deliver reply data to a VL.GetEntryByNameU call.\n */\nstatic int afs_deliver_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tstruct afs_uvldbentry__xdr *uvldb;\n\tstruct afs_vldb_entry *entry;\n\tbool new_only = false;\n\tu32 tmp, nr_servers, vlflags;\n\tint i, ret;\n\n\t_enter(\"\");\n\n\tret = afs_transfer_reply(call);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* unmarshall the reply once we've received all of it */\n\tuvldb = call->buffer;\n\tentry = call->ret_vldb;\n\n\tnr_servers = ntohl(uvldb->nServers);\n\tif (nr_servers > AFS_NMAXNSERVERS)\n\t\tnr_servers = AFS_NMAXNSERVERS;\n\n\tfor (i = 0; i < ARRAY_SIZE(uvldb->name) - 1; i++)\n\t\tentry->name[i] = (u8)ntohl(uvldb->name[i]);\n\tentry->name[i] = 0;\n\tentry->name_len = strlen(entry->name);\n\n\t/* If there is a new replication site that we can use, ignore all the\n\t * sites that aren't marked as new.\n\t */\n\tfor (i = 0; i < nr_servers; i++) {\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (!(tmp & AFS_VLSF_DONTUSE) &&\n\t\t    (tmp & AFS_VLSF_NEWREPSITE))\n\t\t\tnew_only = true;\n\t}\n\n\tvlflags = ntohl(uvldb->flags);\n\tfor (i = 0; i < nr_servers; i++) {\n\t\tstruct afs_uuid__xdr *xdr;\n\t\tstruct afs_uuid *uuid;\n\t\tint j;\n\t\tint n = entry->nr_servers;\n\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (tmp & AFS_VLSF_DONTUSE ||\n\t\t    (new_only && !(tmp & AFS_VLSF_NEWREPSITE)))\n\t\t\tcontinue;\n\t\tif (tmp & AFS_VLSF_RWVOL) {\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RW;\n\t\t\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_BAK;\n\t\t}\n\t\tif (tmp & AFS_VLSF_ROVOL)\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RO;\n\t\tif (!entry->fs_mask[n])\n\t\t\tcontinue;\n\n\t\txdr = &uvldb->serverNumber[i];\n\t\tuuid = (struct afs_uuid *)&entry->fs_server[n];\n\t\tuuid->time_low\t\t\t= xdr->time_low;\n\t\tuuid->time_mid\t\t\t= htons(ntohl(xdr->time_mid));\n\t\tuuid->time_hi_and_version\t= htons(ntohl(xdr->time_hi_and_version));\n\t\tuuid->clock_seq_hi_and_reserved\t= (u8)ntohl(xdr->clock_seq_hi_and_reserved);\n\t\tuuid->clock_seq_low\t\t= (u8)ntohl(xdr->clock_seq_low);\n\t\tfor (j = 0; j < 6; j++)\n\t\t\tuuid->node[j] = (u8)ntohl(xdr->node[j]);\n\n\t\tentry->addr_version[n] = ntohl(uvldb->serverUnique[i]);\n\t\tentry->nr_servers++;\n\t}\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tentry->vid[i] = ntohl(uvldb->volumeId[i]);\n\n\tif (vlflags & AFS_VLF_RWEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RW, &entry->flags);\n\tif (vlflags & AFS_VLF_ROEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RO, &entry->flags);\n\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_BAK, &entry->flags);\n\n\tif (!(vlflags & (AFS_VLF_RWEXISTS | AFS_VLF_ROEXISTS | AFS_VLF_BACKEXISTS))) {\n\t\tentry->error = -ENOMEDIUM;\n\t\t__set_bit(AFS_VLDB_QUERY_ERROR, &entry->flags);\n\t}\n\n\t__set_bit(AFS_VLDB_QUERY_VALID, &entry->flags);\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tkfree(call->ret_vldb);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetEntryByNameU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetEntryByNameU = {\n\t.name\t\t= \"VL.GetEntryByNameU\",\n\t.op\t\t= afs_VL_GetEntryByNameU,\n\t.deliver\t= afs_deliver_vl_get_entry_by_name_u,\n\t.destructor\t= afs_destroy_vl_get_entry_by_name_u,\n};\n\n/*\n * Dispatch a get volume entry by name or ID operation (uuid variant).  If the\n * volname is a decimal number then it's a volume ID not a volume name.\n */\nstruct afs_vldb_entry *afs_vl_get_entry_by_name_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t\t  const char *volname,\n\t\t\t\t\t\t  int volnamesz)\n{\n\tstruct afs_vldb_entry *entry;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\tsize_t reqsz, padsz;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tpadsz = (4 - (volnamesz & 3)) & 3;\n\treqsz = 8 + volnamesz + padsz;\n\n\tentry = kzalloc(sizeof(struct afs_vldb_entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetEntryByNameU, reqsz,\n\t\t\t\t   sizeof(struct afs_uvldbentry__xdr));\n\tif (!call) {\n\t\tkfree(entry);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tcall->key = vc->key;\n\tcall->ret_vldb = entry;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETENTRYBYNAMEU);\n\t*bp++ = htonl(volnamesz);\n\tmemcpy(bp, volname, volnamesz);\n\tif (padsz > 0)\n\t\tmemset((void *)bp + volnamesz, 0, padsz);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_vldb_entry *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a VL.GetAddrsU call.\n *\n *\tGetAddrsU(IN ListAddrByAttributes *inaddr,\n *\t\t  OUT afsUUID *uuidp1,\n *\t\t  OUT uint32_t *uniquifier,\n *\t\t  OUT uint32_t *nentries,\n *\t\t  OUT bulkaddrs *blkaddrs);\n */\nstatic int afs_deliver_vl_get_addrs_u(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, nentries, count;\n\tint i, ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call,\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\t\tcall->unmarshall++;\n\n\t\t/* Extract the returned uuid, uniquifier, nentries and\n\t\t * blkaddrs size */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(struct afs_uuid__xdr);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tnentries\t= ntohl(*bp++);\n\t\tcount\t\t= ntohl(*bp);\n\n\t\tnentries = min(nentries, count);\n\t\talist = afs_alloc_addrlist(nentries, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\t\tcall->count = count;\n\t\tcall->count2 = nentries;\n\t\tcall->unmarshall++;\n\n\tmore_entries:\n\t\tcount = min(call->count, 4U);\n\t\tafs_extract_to_buf(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, call->count > 4);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tcount = min(call->count, 4U);\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (alist->nr_addrs < call->count2)\n\t\t\t\tafs_merge_fs_addr4(alist, *bp++, AFS_FS_PORT);\n\n\t\tcall->count -= count;\n\t\tif (call->count > 0)\n\t\t\tgoto more_entries;\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_vl_get_addrs_u_destructor(struct afs_call *call)\n{\n\tafs_put_addrlist(call->ret_alist);\n\treturn afs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetAddrsU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetAddrsU = {\n\t.name\t\t= \"VL.GetAddrsU\",\n\t.op\t\t= afs_VL_GetAddrsU,\n\t.deliver\t= afs_deliver_vl_get_addrs_u,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_vl_get_addrs_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t const uuid_t *uuid)\n{\n\tstruct afs_ListAddrByAttributes__xdr *r;\n\tconst struct afs_uuid *u = (const struct afs_uuid *)uuid;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\tint i;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetAddrsU,\n\t\t\t\t   sizeof(__be32) + sizeof(struct afs_ListAddrByAttributes__xdr),\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETADDRSU);\n\tr = (struct afs_ListAddrByAttributes__xdr *)bp;\n\tr->Mask\t\t= htonl(AFS_VLADDR_UUID);\n\tr->ipaddr\t= 0;\n\tr->index\t= 0;\n\tr->spare\t= 0;\n\tr->uuid.time_low\t\t\t= u->time_low;\n\tr->uuid.time_mid\t\t\t= htonl(ntohs(u->time_mid));\n\tr->uuid.time_hi_and_version\t\t= htonl(ntohs(u->time_hi_and_version));\n\tr->uuid.clock_seq_hi_and_reserved \t= htonl(u->clock_seq_hi_and_reserved);\n\tr->uuid.clock_seq_low\t\t\t= htonl(u->clock_seq_low);\n\tfor (i = 0; i < 6; i++)\n\t\tr->uuid.node[i] = htonl(u->node[i]);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to an VL.GetCapabilities operation.\n */\nstatic int afs_deliver_vl_get_capabilities(struct afs_call *call)\n{\n\tu32 count;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the capabilities word count */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcount = ntohl(call->tmp);\n\t\tcall->count = count;\n\t\tcall->count2 = count;\n\n\t\tcall->unmarshall++;\n\t\tafs_extract_discard(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract capabilities words */\n\tcase 2:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\t/* TODO: Examine capabilities */\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_capabilities(struct afs_call *call)\n{\n\tafs_put_vlserver(call->net, call->vlserver);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_RXVLGetCapabilities = {\n\t.name\t\t= \"VL.GetCapabilities\",\n\t.op\t\t= afs_VL_GetCapabilities,\n\t.deliver\t= afs_deliver_vl_get_capabilities,\n\t.done\t\t= afs_vlserver_probe_result,\n\t.destructor\t= afs_destroy_vl_get_capabilities,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nstruct afs_call *afs_vl_get_capabilities(struct afs_net *net,\n\t\t\t\t\t struct afs_addr_cursor *ac,\n\t\t\t\t\t struct key *key,\n\t\t\t\t\t struct afs_vlserver *server,\n\t\t\t\t\t unsigned int server_index)\n{\n\tstruct afs_call *call;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetCapabilities, 1 * 4, 16 * 4);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = key;\n\tcall->vlserver = afs_get_vlserver(server);\n\tcall->server_index = server_index;\n\tcall->upgrade = true;\n\tcall->async = true;\n\tcall->max_lifespan = AFS_PROBE_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETCAPABILITIES);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(ac, call, GFP_KERNEL);\n\treturn call;\n}\n\n/*\n * Deliver reply data to a YFSVL.GetEndpoints call.\n *\n *\tGetEndpoints(IN yfsServerAttributes *attr,\n *\t\t     OUT opr_uuid *uuid,\n *\t\t     OUT afs_int32 *uniquifier,\n *\t\t     OUT endpoints *fsEndpoints,\n *\t\t     OUT endpoints *volEndpoints)\n */\nstatic int afs_deliver_yfsvl_get_endpoints(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, size;\n\tint ret;\n\n\t_enter(\"{%u,%zu,%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count2);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call, sizeof(uuid_t) + 3 * sizeof(__be32));\n\t\tcall->unmarshall = 1;\n\n\t\t/* Extract the returned uuid, uniquifier, fsEndpoints count and\n\t\t * either the first fsEndpoint type or the volEndpoints\n\t\t * count if there are no fsEndpoints. */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(uuid_t);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tcall->count\t= ntohl(*bp++);\n\t\tcall->count2\t= ntohl(*bp); /* Type or next count */\n\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_num);\n\n\t\talist = afs_alloc_addrlist(call->count, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\n\t\tif (call->count == 0)\n\t\t\tgoto extract_volendpoints;\n\n\tnext_fsendpoint:\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\tsize += sizeof(__be32);\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 2;\n\n\t\tfallthrough;\t/* and extract fsEndpoints[] entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt4_len);\n\t\t\tafs_merge_fs_addr4(alist, bp[1], ntohl(bp[2]));\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt6_len);\n\t\t\tafs_merge_fs_addr6(alist, bp + 1, ntohl(bp[5]));\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count2 = ntohl(*bp++);\n\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_fsendpoint;\n\n\textract_volendpoints:\n\t\t/* Extract the list of volEndpoints. */\n\t\tcall->count = call->count2;\n\t\tif (!call->count)\n\t\t\tgoto end;\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\n\t\tafs_extract_to_buf(call, 1 * sizeof(__be32));\n\t\tcall->unmarshall = 3;\n\n\t\t/* Extract the type of volEndpoints[0].  Normally we would\n\t\t * extract the type of the next endpoint when we extract the\n\t\t * data of the current one, but this is the first...\n\t\t */\n\t\tfallthrough;\n\tcase 3:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\n\tnext_volendpoint:\n\t\tcall->count2 = ntohl(*bp++);\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\tif (call->count > 1)\n\t\t\tsize += sizeof(__be32); /* Get next type too */\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 4;\n\n\t\tfallthrough;\t/* and extract volEndpoints[] entries */\n\tcase 4:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt4_len);\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt6_len);\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_volendpoint;\n\n\tend:\n\t\tafs_extract_discard(call, 0);\n\t\tcall->unmarshall = 5;\n\n\t\tfallthrough;\t/* Done */\n\tcase 5:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tcall->unmarshall = 6;\n\n\tcase 6:\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * YFSVL.GetEndpoints operation type.\n */\nstatic const struct afs_call_type afs_YFSVLGetEndpoints = {\n\t.name\t\t= \"YFSVL.GetEndpoints\",\n\t.op\t\t= afs_YFSVL_GetEndpoints,\n\t.deliver\t= afs_deliver_yfsvl_get_endpoints,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_yfsvl_get_endpoints(struct afs_vl_cursor *vc,\n\t\t\t\t\t      const uuid_t *uuid)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetEndpoints,\n\t\t\t\t   sizeof(__be32) * 2 + sizeof(*uuid),\n\t\t\t\t   sizeof(struct in6_addr) + sizeof(__be32) * 3);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETENDPOINTS);\n\t*bp++ = htonl(YFS_SERVER_UUID);\n\tmemcpy(bp, uuid, sizeof(*uuid)); /* Type opr_uuid */\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a YFSVL.GetCellName operation.\n */\nstatic int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tchar *cell_name;\n\tu32 namesz, paddedsz;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the cell name length */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnamesz = ntohl(call->tmp);\n\t\tif (namesz > AFS_MAXCELLNAME)\n\t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n\t\tpaddedsz = (namesz + 3) & ~3;\n\t\tcall->count = namesz;\n\t\tcall->count2 = paddedsz - namesz;\n\n\t\tcell_name = kmalloc(namesz + 1, GFP_KERNEL);\n\t\tif (!cell_name)\n\t\t\treturn -ENOMEM;\n\t\tcell_name[namesz] = 0;\n\t\tcall->ret_str = cell_name;\n\n\t\tafs_extract_begin(call, cell_name, namesz);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract cell name */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tafs_extract_discard(call, call->count2);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract padding */\n\tcase 3:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tkfree(call->ret_str);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_YFSVLGetCellName = {\n\t.name\t\t= \"YFSVL.GetCellName\",\n\t.op\t\t= afs_YFSVL_GetCellName,\n\t.deliver\t= afs_deliver_yfsvl_get_cell_name,\n\t.destructor\t= afs_destroy_yfsvl_get_cell_name,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nchar *afs_yfsvl_get_cell_name(struct afs_vl_cursor *vc)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetCellName, 1 * 4, 0);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_str = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETCELLNAME);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (char *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n"
                            }
                        }
                    ],
                    "upstream_patch_content": "From 8fd56ad6e7c90ac2bddb0741c6b248c8c5d56ac8 Mon Sep 17 00:00:00 2001\nFrom: David Howells <dhowells@redhat.com>\nDate: Mon, 6 Jan 2025 16:21:00 +0000\nSubject: [PATCH] afs: Fix the maximum cell name length\n\nThe kafs filesystem limits the maximum length of a cell to 256 bytes, but a\nproblem occurs if someone actually does that: kafs tries to create a\ndirectory under /proc/net/afs/ with the name of the cell, but that fails\nwith a warning:\n\n        WARNING: CPU: 0 PID: 9 at fs/proc/generic.c:405\n\nbecause procfs limits the maximum filename length to 255.\n\nHowever, the DNS limits the maximum lookup length and, by extension, the\nmaximum cell name, to 255 less two (length count and trailing NUL).\n\nFix this by limiting the maximum acceptable cellname length to 253.  This\nalso allows us to be sure we can create the \"/afs/.<cell>/\" mountpoint too.\n\nFurther, split the YFS VL record cell name maximum to be the 256 allowed by\nthe protocol and ignore the record retrieved by YFSVL.GetCellName if it\nexceeds 253.\n\nFixes: c3e9f888263b (\"afs: Implement client support for the YFSVL.GetCellName RPC op\")\nReported-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/r/6776d25d.050a0220.3a8527.0048.GAE@google.com/\nSigned-off-by: David Howells <dhowells@redhat.com>\nLink: https://lore.kernel.org/r/376236.1736180460@warthog.procyon.org.uk\nTested-by: syzbot+7848fee1f1e5c53f912b@syzkaller.appspotmail.com\ncc: Marc Dionne <marc.dionne@auristor.com>\ncc: linux-afs@lists.infradead.org\nSigned-off-by: Christian Brauner <brauner@kernel.org>\n---\n fs/afs/afs.h      | 2 +-\n fs/afs/afs_vl.h   | 1 +\n fs/afs/vl_alias.c | 8 ++++++--\n fs/afs/vlclient.c | 2 +-\n 4 files changed, 9 insertions(+), 4 deletions(-)\n\ndiff --git a/fs/afs/afs.h b/fs/afs/afs.h\nindex b488072aee87..ec3db00bd081 100644\n--- a/fs/afs/afs.h\n+++ b/fs/afs/afs.h\n@@ -10,7 +10,7 @@\n \n #include <linux/in.h>\n \n-#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n+#define AFS_MAXCELLNAME\t\t253  \t/* Maximum length of a cell name (DNS limited) */\n #define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n #define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n #define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\ndiff --git a/fs/afs/afs_vl.h b/fs/afs/afs_vl.h\nindex a06296c8827d..b835e25a2c02 100644\n--- a/fs/afs/afs_vl.h\n+++ b/fs/afs/afs_vl.h\n@@ -13,6 +13,7 @@\n #define AFS_VL_PORT\t\t7003\t/* volume location service port */\n #define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n #define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n+#define YFS_VL_MAXCELLNAME\t256  \t/* Maximum length of a cell name in YFS protocol */\n \n enum AFSVL_Operations {\n \tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\ndiff --git a/fs/afs/vl_alias.c b/fs/afs/vl_alias.c\nindex 9f36e14f1c2d..f9e76b604f31 100644\n--- a/fs/afs/vl_alias.c\n+++ b/fs/afs/vl_alias.c\n@@ -253,6 +253,7 @@ static char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n {\n \tstruct afs_cell *master;\n+\tsize_t name_len;\n \tchar *cell_name;\n \n \tcell_name = afs_vl_get_cell_name(cell, key);\n@@ -264,8 +265,11 @@ static int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n \t\treturn 0;\n \t}\n \n-\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n-\t\t\t\t NULL, false);\n+\tname_len = strlen(cell_name);\n+\tif (!name_len || name_len > AFS_MAXCELLNAME)\n+\t\tmaster = ERR_PTR(-EOPNOTSUPP);\n+\telse\n+\t\tmaster = afs_lookup_cell(cell->net, cell_name, name_len, NULL, false);\n \tkfree(cell_name);\n \tif (IS_ERR(master))\n \t\treturn PTR_ERR(master);\ndiff --git a/fs/afs/vlclient.c b/fs/afs/vlclient.c\nindex cac75f89b64a..55dd0fc5aad7 100644\n--- a/fs/afs/vlclient.c\n+++ b/fs/afs/vlclient.c\n@@ -697,7 +697,7 @@ static int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n \t\t\treturn ret;\n \n \t\tnamesz = ntohl(call->tmp);\n-\t\tif (namesz > AFS_MAXCELLNAME)\n+\t\tif (namesz > YFS_VL_MAXCELLNAME)\n \t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n \t\tpaddedsz = (namesz + 3) & ~3;\n \t\tcall->count = namesz;\n-- \n2.39.5 (Apple Git-154)\n\n",
                    "upstream_file_content": {
                        "fs/afs/afs.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS common types\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_H\n#define AFS_H\n\n#include <linux/in.h>\n\n#define AFS_MAXCELLNAME\t\t256  \t/* Maximum length of a cell name */\n#define AFS_MAXVOLNAME\t\t64  \t/* Maximum length of a volume name */\n#define AFS_MAXNSERVERS\t\t8   \t/* Maximum servers in a basic volume record */\n#define AFS_NMAXNSERVERS\t13  \t/* Maximum servers in a N/U-class volume record */\n#define AFS_MAXTYPES\t\t3\t/* Maximum number of volume types */\n#define AFSNAMEMAX\t\t256 \t/* Maximum length of a filename plus NUL */\n#define AFSPATHMAX\t\t1024\t/* Maximum length of a pathname plus NUL */\n#define AFSOPAQUEMAX\t\t1024\t/* Maximum length of an opaque field */\n\n#define AFS_VL_MAX_LIFESPAN\t(120 * HZ)\n#define AFS_PROBE_MAX_LIFESPAN\t(30 * HZ)\n\ntypedef u64\t\t\tafs_volid_t;\ntypedef u64\t\t\tafs_vnodeid_t;\ntypedef u64\t\t\tafs_dataversion_t;\n\ntypedef enum {\n\tAFSVL_RWVOL,\t\t\t/* read/write volume */\n\tAFSVL_ROVOL,\t\t\t/* read-only volume */\n\tAFSVL_BACKVOL,\t\t\t/* backup volume */\n} __attribute__((packed)) afs_voltype_t;\n\ntypedef enum {\n\tAFS_FTYPE_INVALID\t= 0,\n\tAFS_FTYPE_FILE\t\t= 1,\n\tAFS_FTYPE_DIR\t\t= 2,\n\tAFS_FTYPE_SYMLINK\t= 3,\n} afs_file_type_t;\n\ntypedef enum {\n\tAFS_LOCK_READ\t\t= 0,\t/* read lock request */\n\tAFS_LOCK_WRITE\t\t= 1,\t/* write lock request */\n} afs_lock_type_t;\n\n#define AFS_LOCKWAIT\t\t(5 * 60) /* time until a lock times out (seconds) */\n\n/*\n * AFS file identifier\n */\nstruct afs_fid {\n\tafs_volid_t\tvid;\t\t/* volume ID */\n\tafs_vnodeid_t\tvnode;\t\t/* Lower 64-bits of file index within volume */\n\tu32\t\tvnode_hi;\t/* Upper 32-bits of file index */\n\tu32\t\tunique;\t\t/* unique ID number (file index version) */\n};\n\n/*\n * AFS callback notification\n */\ntypedef enum {\n\tAFSCM_CB_UNTYPED\t= 0,\t/* no type set on CB break */\n\tAFSCM_CB_EXCLUSIVE\t= 1,\t/* CB exclusive to CM [not implemented] */\n\tAFSCM_CB_SHARED\t\t= 2,\t/* CB shared by other CM's */\n\tAFSCM_CB_DROPPED\t= 3,\t/* CB promise cancelled by file server */\n} afs_callback_type_t;\n\nstruct afs_callback {\n\ttime64_t\t\texpires_at;\t/* Time at which expires */\n\t//unsigned\t\tversion;\t/* Callback version */\n\t//afs_callback_type_t\ttype;\t\t/* Type of callback */\n};\n\nstruct afs_callback_break {\n\tstruct afs_fid\t\tfid;\t\t/* File identifier */\n\t//struct afs_callback\tcb;\t\t/* Callback details */\n};\n\n#define AFSCBMAX 50\t/* maximum callbacks transferred per bulk op */\n\nstruct afs_uuid {\n\t__be32\t\ttime_low;\t\t\t/* low part of timestamp */\n\t__be16\t\ttime_mid;\t\t\t/* mid part of timestamp */\n\t__be16\t\ttime_hi_and_version;\t\t/* high part of timestamp and version  */\n\t__s8\t\tclock_seq_hi_and_reserved;\t/* clock seq hi and variant */\n\t__s8\t\tclock_seq_low;\t\t\t/* clock seq low */\n\t__s8\t\tnode[6];\t\t\t/* spatially unique node ID (MAC addr) */\n};\n\n/*\n * AFS volume information\n */\nstruct afs_volume_info {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_voltype_t\t\ttype;\t\t/* type of this volume */\n\tafs_volid_t\t\ttype_vids[5];\t/* volume ID's for possible types for this vol */\n\n\t/* list of fileservers serving this volume */\n\tsize_t\t\t\tnservers;\t/* number of entries used in servers[] */\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* fileserver address */\n\t} servers[8];\n};\n\n/*\n * AFS security ACE access mask\n */\ntypedef u32 afs_access_t;\n#define AFS_ACE_READ\t\t0x00000001U\t/* - permission to read a file/dir */\n#define AFS_ACE_WRITE\t\t0x00000002U\t/* - permission to write/chmod a file */\n#define AFS_ACE_INSERT\t\t0x00000004U\t/* - permission to create dirent in a dir */\n#define AFS_ACE_LOOKUP\t\t0x00000008U\t/* - permission to lookup a file/dir in a dir */\n#define AFS_ACE_DELETE\t\t0x00000010U\t/* - permission to delete a dirent from a dir */\n#define AFS_ACE_LOCK\t\t0x00000020U\t/* - permission to lock a file */\n#define AFS_ACE_ADMINISTER\t0x00000040U\t/* - permission to change ACL */\n#define AFS_ACE_USER_A\t\t0x01000000U\t/* - 'A' user-defined permission */\n#define AFS_ACE_USER_B\t\t0x02000000U\t/* - 'B' user-defined permission */\n#define AFS_ACE_USER_C\t\t0x04000000U\t/* - 'C' user-defined permission */\n#define AFS_ACE_USER_D\t\t0x08000000U\t/* - 'D' user-defined permission */\n#define AFS_ACE_USER_E\t\t0x10000000U\t/* - 'E' user-defined permission */\n#define AFS_ACE_USER_F\t\t0x20000000U\t/* - 'F' user-defined permission */\n#define AFS_ACE_USER_G\t\t0x40000000U\t/* - 'G' user-defined permission */\n#define AFS_ACE_USER_H\t\t0x80000000U\t/* - 'H' user-defined permission */\n\n/*\n * AFS file status information\n */\nstruct afs_file_status {\n\tu64\t\t\tsize;\t\t/* file size */\n\tafs_dataversion_t\tdata_version;\t/* current data version */\n\tstruct timespec64\tmtime_client;\t/* Last time client changed data */\n\tstruct timespec64\tmtime_server;\t/* Last time server changed data */\n\ts64\t\t\tauthor;\t\t/* author ID */\n\ts64\t\t\towner;\t\t/* owner ID */\n\ts64\t\t\tgroup;\t\t/* group ID */\n\tafs_access_t\t\tcaller_access;\t/* access rights for authenticated caller */\n\tafs_access_t\t\tanon_access;\t/* access rights for unauthenticated caller */\n\tumode_t\t\t\tmode;\t\t/* UNIX mode */\n\tafs_file_type_t\t\ttype;\t\t/* file type */\n\tu32\t\t\tnlink;\t\t/* link count */\n\ts32\t\t\tlock_count;\t/* file lock count (0=UNLK -1=WRLCK +ve=#RDLCK */\n\tu32\t\t\tabort_code;\t/* Abort if bulk-fetching this failed */\n};\n\nstruct afs_status_cb {\n\tstruct afs_file_status\tstatus;\n\tstruct afs_callback\tcallback;\n\tbool\t\t\thave_status;\t/* True if status record was retrieved */\n\tbool\t\t\thave_cb;\t/* True if cb record was retrieved */\n\tbool\t\t\thave_error;\t/* True if status.abort_code indicates an error */\n};\n\n/*\n * AFS file status change request\n */\n\n#define AFS_SET_MTIME\t\t0x01\t\t/* set the mtime */\n#define AFS_SET_OWNER\t\t0x02\t\t/* set the owner ID */\n#define AFS_SET_GROUP\t\t0x04\t\t/* set the group ID (unsupported?) */\n#define AFS_SET_MODE\t\t0x08\t\t/* set the UNIX mode */\n#define AFS_SET_SEG_SIZE\t0x10\t\t/* set the segment size (unsupported) */\n\n/*\n * AFS volume synchronisation information\n */\nstruct afs_volsync {\n\ttime64_t\t\tcreation;\t/* volume creation time */\n};\n\n/*\n * AFS volume status record\n */\nstruct afs_volume_status {\n\tafs_volid_t\t\tvid;\t\t/* volume ID */\n\tafs_volid_t\t\tparent_id;\t/* parent volume ID */\n\tu8\t\t\tonline;\t\t/* true if volume currently online and available */\n\tu8\t\t\tin_service;\t/* true if volume currently in service */\n\tu8\t\t\tblessed;\t/* same as in_service */\n\tu8\t\t\tneeds_salvage;\t/* true if consistency checking required */\n\tu32\t\t\ttype;\t\t/* volume type (afs_voltype_t) */\n\tu64\t\t\tmin_quota;\t/* minimum space set aside (blocks) */\n\tu64\t\t\tmax_quota;\t/* maximum space this volume may occupy (blocks) */\n\tu64\t\t\tblocks_in_use;\t/* space this volume currently occupies (blocks) */\n\tu64\t\t\tpart_blocks_avail; /* space available in volume's partition */\n\tu64\t\t\tpart_max_blocks; /* size of volume's partition */\n\ts64\t\t\tvol_copy_date;\n\ts64\t\t\tvol_backup_date;\n};\n\n#define AFS_BLOCK_SIZE\t1024\n\n/*\n * XDR encoding of UUID in AFS.\n */\nstruct afs_uuid__xdr {\n\t__be32\t\ttime_low;\n\t__be32\t\ttime_mid;\n\t__be32\t\ttime_hi_and_version;\n\t__be32\t\tclock_seq_hi_and_reserved;\n\t__be32\t\tclock_seq_low;\n\t__be32\t\tnode[6];\n};\n\n#endif /* AFS_H */\n",
                        "fs/afs/afs_vl.h": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/* AFS Volume Location Service client interface\n *\n * Copyright (C) 2002, 2007 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#ifndef AFS_VL_H\n#define AFS_VL_H\n\n#include \"afs.h\"\n\n#define AFS_VL_PORT\t\t7003\t/* volume location service port */\n#define VL_SERVICE\t\t52\t/* RxRPC service ID for the Volume Location service */\n#define YFS_VL_SERVICE\t\t2503\t/* Service ID for AuriStor upgraded VL service */\n\nenum AFSVL_Operations {\n\tVLGETENTRYBYID\t\t= 503,\t/* AFS Get VLDB entry by ID */\n\tVLGETENTRYBYNAME\t= 504,\t/* AFS Get VLDB entry by name */\n\tVLPROBE\t\t\t= 514,\t/* AFS probe VL service */\n\tVLGETENTRYBYIDU\t\t= 526,\t/* AFS Get VLDB entry by ID (UUID-variant) */\n\tVLGETENTRYBYNAMEU\t= 527,\t/* AFS Get VLDB entry by name (UUID-variant) */\n\tVLGETADDRSU\t\t= 533,\t/* AFS Get addrs for fileserver */\n\tYVLGETENDPOINTS\t\t= 64002, /* YFS Get endpoints for file/volume server */\n\tYVLGETCELLNAME\t\t= 64014, /* YFS Get actual cell name */\n\tVLGETCAPABILITIES\t= 65537, /* AFS Get server capabilities */\n};\n\nenum AFSVL_Errors {\n\tAFSVL_IDEXIST \t\t= 363520,\t/* Volume Id entry exists in vl database */\n\tAFSVL_IO \t\t= 363521,\t/* I/O related error */\n\tAFSVL_NAMEEXIST \t= 363522,\t/* Volume name entry exists in vl database */\n\tAFSVL_CREATEFAIL \t= 363523,\t/* Internal creation failure */\n\tAFSVL_NOENT \t\t= 363524,\t/* No such entry */\n\tAFSVL_EMPTY \t\t= 363525,\t/* Vl database is empty */\n\tAFSVL_ENTDELETED \t= 363526,\t/* Entry is deleted (soft delete) */\n\tAFSVL_BADNAME \t\t= 363527,\t/* Volume name is illegal */\n\tAFSVL_BADINDEX \t\t= 363528,\t/* Index is out of range */\n\tAFSVL_BADVOLTYPE \t= 363529,\t/* Bad volume type */\n\tAFSVL_BADSERVER \t= 363530,\t/* Illegal server number (out of range) */\n\tAFSVL_BADPARTITION \t= 363531,\t/* Bad partition number */\n\tAFSVL_REPSFULL \t\t= 363532,\t/* Run out of space for Replication sites */\n\tAFSVL_NOREPSERVER \t= 363533,\t/* No such Replication server site exists */\n\tAFSVL_DUPREPSERVER \t= 363534,\t/* Replication site already exists */\n\tAFSVL_RWNOTFOUND \t= 363535,\t/* Parent R/W entry not found */\n\tAFSVL_BADREFCOUNT \t= 363536,\t/* Illegal Reference Count number */\n\tAFSVL_SIZEEXCEEDED \t= 363537,\t/* Vl size for attributes exceeded */\n\tAFSVL_BADENTRY \t\t= 363538,\t/* Bad incoming vl entry */\n\tAFSVL_BADVOLIDBUMP \t= 363539,\t/* Illegal max volid increment */\n\tAFSVL_IDALREADYHASHED \t= 363540,\t/* RO/BACK id already hashed */\n\tAFSVL_ENTRYLOCKED \t= 363541,\t/* Vl entry is already locked */\n\tAFSVL_BADVOLOPER \t= 363542,\t/* Bad volume operation code */\n\tAFSVL_BADRELLOCKTYPE \t= 363543,\t/* Bad release lock type */\n\tAFSVL_RERELEASE \t= 363544,\t/* Status report: last release was aborted */\n\tAFSVL_BADSERVERFLAG \t= 363545,\t/* Invalid replication site server flag */\n\tAFSVL_PERM \t\t= 363546,\t/* No permission access */\n\tAFSVL_NOMEM \t\t= 363547,\t/* malloc/realloc failed to alloc enough memory */\n};\n\nenum {\n\tYFS_SERVER_INDEX\t= 0,\n\tYFS_SERVER_UUID\t\t= 1,\n\tYFS_SERVER_ENDPOINT\t= 2,\n};\n\nenum {\n\tYFS_ENDPOINT_IPV4\t= 0,\n\tYFS_ENDPOINT_IPV6\t= 1,\n};\n\n#define YFS_MAXENDPOINTS\t16\n\n/*\n * maps to \"struct vldbentry\" in vvl-spec.pdf\n */\nstruct afs_vldbentry {\n\tchar\t\tname[65];\t\t/* name of volume (with NUL char) */\n\tafs_voltype_t\ttype;\t\t\t/* volume type */\n\tunsigned\tnum_servers;\t\t/* num servers that hold instances of this vol */\n\tunsigned\tclone_id;\t\t/* cloning ID */\n\n\tunsigned\tflags;\n#define AFS_VLF_RWEXISTS\t0x1000\t\t/* R/W volume exists */\n#define AFS_VLF_ROEXISTS\t0x2000\t\t/* R/O volume exists */\n#define AFS_VLF_BACKEXISTS\t0x4000\t\t/* backup volume exists */\n\n\tafs_volid_t\tvolume_ids[3];\t\t/* volume IDs */\n\n\tstruct {\n\t\tstruct in_addr\taddr;\t\t/* server address */\n\t\tunsigned\tpartition;\t/* partition ID on this server */\n\t\tunsigned\tflags;\t\t/* server specific flags */\n#define AFS_VLSF_NEWREPSITE\t0x0001\t/* Ignore all 'non-new' servers */\n#define AFS_VLSF_ROVOL\t\t0x0002\t/* this server holds a R/O instance of the volume */\n#define AFS_VLSF_RWVOL\t\t0x0004\t/* this server holds a R/W instance of the volume */\n#define AFS_VLSF_BACKVOL\t0x0008\t/* this server holds a backup instance of the volume */\n#define AFS_VLSF_UUID\t\t0x0010\t/* This server is referred to by its UUID */\n#define AFS_VLSF_DONTUSE\t0x0020\t/* This server ref should be ignored */\n\t} servers[8];\n};\n\n#define AFS_VLDB_MAXNAMELEN 65\n\n\nstruct afs_ListAddrByAttributes__xdr {\n\t__be32\t\t\tMask;\n#define AFS_VLADDR_IPADDR\t0x1\t/* Match by ->ipaddr */\n#define AFS_VLADDR_INDEX\t0x2\t/* Match by ->index */\n#define AFS_VLADDR_UUID\t\t0x4\t/* Match by ->uuid */\n\t__be32\t\t\tipaddr;\n\t__be32\t\t\tindex;\n\t__be32\t\t\tspare;\n\tstruct afs_uuid__xdr\tuuid;\n};\n\nstruct afs_uvldbentry__xdr {\n\t__be32\t\t\tname[AFS_VLDB_MAXNAMELEN];\n\t__be32\t\t\tnServers;\n\tstruct afs_uuid__xdr\tserverNumber[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverUnique[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverPartition[AFS_NMAXNSERVERS];\n\t__be32\t\t\tserverFlags[AFS_NMAXNSERVERS];\n\t__be32\t\t\tvolumeId[AFS_MAXTYPES];\n\t__be32\t\t\tcloneId;\n\t__be32\t\t\tflags;\n\t__be32\t\t\tspares1;\n\t__be32\t\t\tspares2;\n\t__be32\t\t\tspares3;\n\t__be32\t\t\tspares4;\n\t__be32\t\t\tspares5;\n\t__be32\t\t\tspares6;\n\t__be32\t\t\tspares7;\n\t__be32\t\t\tspares8;\n\t__be32\t\t\tspares9;\n};\n\nstruct afs_address_list {\n\trefcount_t\t\tusage;\n\tunsigned int\t\tversion;\n\tunsigned int\t\tnr_addrs;\n\tstruct sockaddr_rxrpc\taddrs[];\n};\n\nextern void afs_put_address_list(struct afs_address_list *alist);\n\n#endif /* AFS_VL_H */\n",
                        "fs/afs/vl_alias.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS cell alias detection\n *\n * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/namei.h>\n#include <keys/rxrpc-type.h>\n#include \"internal.h\"\n\n/*\n * Sample a volume.\n */\nstatic struct afs_volume *afs_sample_volume(struct afs_cell *cell, struct key *key,\n\t\t\t\t\t    const char *name, unsigned int namelen)\n{\n\tstruct afs_volume *volume;\n\tstruct afs_fs_context fc = {\n\t\t.type\t\t= 0, /* Explicitly leave it to the VLDB */\n\t\t.volnamesz\t= namelen,\n\t\t.volname\t= name,\n\t\t.net\t\t= cell->net,\n\t\t.cell\t\t= cell,\n\t\t.key\t\t= key, /* This might need to be something */\n\t};\n\n\tvolume = afs_create_volume(&fc);\n\t_leave(\" = %p\", volume);\n\treturn volume;\n}\n\n/*\n * Compare two addresses.\n */\nstatic int afs_compare_addrs(const struct sockaddr_rxrpc *srx_a,\n\t\t\t     const struct sockaddr_rxrpc *srx_b)\n{\n\tshort port_a, port_b;\n\tint addr_a, addr_b, diff;\n\n\tdiff = (short)srx_a->transport_type - (short)srx_b->transport_type;\n\tif (diff)\n\t\tgoto out;\n\n\tswitch (srx_a->transport_type) {\n\tcase AF_INET: {\n\t\tconst struct sockaddr_in *a = &srx_a->transport.sin;\n\t\tconst struct sockaddr_in *b = &srx_b->transport.sin;\n\t\taddr_a = ntohl(a->sin_addr.s_addr);\n\t\taddr_b = ntohl(b->sin_addr.s_addr);\n\t\tdiff = addr_a - addr_b;\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin_port);\n\t\t\tport_b = ntohs(b->sin_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase AF_INET6: {\n\t\tconst struct sockaddr_in6 *a = &srx_a->transport.sin6;\n\t\tconst struct sockaddr_in6 *b = &srx_b->transport.sin6;\n\t\tdiff = memcmp(&a->sin6_addr, &b->sin6_addr, 16);\n\t\tif (diff == 0) {\n\t\t\tport_a = ntohs(a->sin6_port);\n\t\t\tport_b = ntohs(b->sin6_port);\n\t\t\tdiff = port_a - port_b;\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\tWARN_ON(1);\n\t\tdiff = 1;\n\t}\n\nout:\n\treturn diff;\n}\n\n/*\n * Compare the address lists of a pair of fileservers.\n */\nstatic int afs_compare_fs_alists(const struct afs_server *server_a,\n\t\t\t\t const struct afs_server *server_b)\n{\n\tconst struct afs_addr_list *la, *lb;\n\tint a = 0, b = 0, addr_matches = 0;\n\n\tla = rcu_dereference(server_a->addresses);\n\tlb = rcu_dereference(server_b->addresses);\n\n\twhile (a < la->nr_addrs && b < lb->nr_addrs) {\n\t\tconst struct sockaddr_rxrpc *srx_a = &la->addrs[a];\n\t\tconst struct sockaddr_rxrpc *srx_b = &lb->addrs[b];\n\t\tint diff = afs_compare_addrs(srx_a, srx_b);\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\taddr_matches++;\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\treturn addr_matches;\n}\n\n/*\n * Compare the fileserver lists of two volumes.  The server lists are sorted in\n * order of ascending UUID.\n */\nstatic int afs_compare_volume_slists(const struct afs_volume *vol_a,\n\t\t\t\t     const struct afs_volume *vol_b)\n{\n\tconst struct afs_server_list *la, *lb;\n\tint i, a = 0, b = 0, uuid_matches = 0, addr_matches = 0;\n\n\tla = rcu_dereference(vol_a->servers);\n\tlb = rcu_dereference(vol_b->servers);\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tif (la->vids[i] != lb->vids[i])\n\t\t\treturn 0;\n\n\twhile (a < la->nr_servers && b < lb->nr_servers) {\n\t\tconst struct afs_server *server_a = la->servers[a].server;\n\t\tconst struct afs_server *server_b = lb->servers[b].server;\n\t\tint diff = memcmp(&server_a->uuid, &server_b->uuid, sizeof(uuid_t));\n\n\t\tif (diff < 0) {\n\t\t\ta++;\n\t\t} else if (diff > 0) {\n\t\t\tb++;\n\t\t} else {\n\t\t\tuuid_matches++;\n\t\t\taddr_matches += afs_compare_fs_alists(server_a, server_b);\n\t\t\ta++;\n\t\t\tb++;\n\t\t}\n\t}\n\n\t_leave(\" = %d [um %d]\", addr_matches, uuid_matches);\n\treturn addr_matches;\n}\n\n/*\n * Compare root.cell volumes.\n */\nstatic int afs_compare_cell_roots(struct afs_cell *cell)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"\");\n\n\trcu_read_lock();\n\n\thlist_for_each_entry_rcu(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (!p->root_volume)\n\t\t\tcontinue; /* Ignore cells that don't have a root.cell volume. */\n\n\t\tif (afs_compare_volume_slists(cell->root_volume, p->root_volume) != 0)\n\t\t\tgoto is_alias;\n\t}\n\n\trcu_read_unlock();\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\trcu_read_unlock();\n\tcell->alias_of = afs_use_cell(p, afs_cell_trace_use_alias);\n\treturn 1;\n}\n\n/*\n * Query the new cell for a volume from a cell we're already using.\n */\nstatic int afs_query_for_alias_one(struct afs_cell *cell, struct key *key,\n\t\t\t\t   struct afs_cell *p)\n{\n\tstruct afs_volume *volume, *pvol = NULL;\n\tint ret;\n\n\t/* Arbitrarily pick a volume from the list. */\n\tread_seqlock_excl(&p->volume_lock);\n\tif (!RB_EMPTY_ROOT(&p->volumes))\n\t\tpvol = afs_get_volume(rb_entry(p->volumes.rb_node,\n\t\t\t\t\t       struct afs_volume, cell_node),\n\t\t\t\t      afs_volume_trace_get_query_alias);\n\tread_sequnlock_excl(&p->volume_lock);\n\tif (!pvol)\n\t\treturn 0;\n\n\t_enter(\"%s:%s\", cell->name, pvol->name);\n\n\t/* And see if it's in the new cell. */\n\tvolume = afs_sample_volume(cell, key, pvol->name, pvol->name_len);\n\tif (IS_ERR(volume)) {\n\t\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\t\tif (PTR_ERR(volume) != -ENOMEDIUM)\n\t\t\treturn PTR_ERR(volume);\n\t\t/* That volume is not in the new cell, so not an alias */\n\t\treturn 0;\n\t}\n\n\t/* The new cell has a like-named volume also - compare volume ID,\n\t * server and address lists.\n\t */\n\tret = 0;\n\tif (pvol->vid == volume->vid) {\n\t\trcu_read_lock();\n\t\tif (afs_compare_volume_slists(volume, pvol))\n\t\t\tret = 1;\n\t\trcu_read_unlock();\n\t}\n\n\tafs_put_volume(cell->net, volume, afs_volume_trace_put_query_alias);\n\tafs_put_volume(cell->net, pvol, afs_volume_trace_put_query_alias);\n\treturn ret;\n}\n\n/*\n * Query the new cell for volumes we know exist in cells we're already using.\n */\nstatic int afs_query_for_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *p;\n\n\t_enter(\"%s\", cell->name);\n\n\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\thlist_for_each_entry(p, &cell->net->proc_cells, proc_link) {\n\t\tif (p == cell || p->alias_of)\n\t\t\tcontinue;\n\t\tif (RB_EMPTY_ROOT(&p->volumes))\n\t\t\tcontinue;\n\t\tif (p->root_volume)\n\t\t\tcontinue; /* Ignore cells that have a root.cell volume. */\n\t\tafs_use_cell(p, afs_cell_trace_use_check_alias);\n\t\tmutex_unlock(&cell->net->proc_cells_lock);\n\n\t\tif (afs_query_for_alias_one(cell, key, p) != 0)\n\t\t\tgoto is_alias;\n\n\t\tif (mutex_lock_interruptible(&cell->net->proc_cells_lock) < 0) {\n\t\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\n\t\tafs_unuse_cell(cell->net, p, afs_cell_trace_unuse_check_alias);\n\t}\n\n\tmutex_unlock(&cell->net->proc_cells_lock);\n\t_leave(\" = 0\");\n\treturn 0;\n\nis_alias:\n\tcell->alias_of = p; /* Transfer our ref */\n\treturn 1;\n}\n\n/*\n * Look up a VLDB record for a volume.\n */\nstatic char *afs_vl_get_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_vl_cursor vc;\n\tchar *cell_name = ERR_PTR(-EDESTADDRREQ);\n\tbool skipped = false, not_skipped = false;\n\tint ret;\n\n\tif (!afs_begin_vlserver_operation(&vc, cell, key))\n\t\treturn ERR_PTR(-ERESTARTSYS);\n\n\twhile (afs_select_vlserver(&vc)) {\n\t\tif (!test_bit(AFS_VLSERVER_FL_IS_YFS, &vc.server->flags)) {\n\t\t\tvc.ac.error = -EOPNOTSUPP;\n\t\t\tskipped = true;\n\t\t\tcontinue;\n\t\t}\n\t\tnot_skipped = true;\n\t\tcell_name = afs_yfsvl_get_cell_name(&vc);\n\t}\n\n\tret = afs_end_vlserver_operation(&vc);\n\tif (skipped && !not_skipped)\n\t\tret = -EOPNOTSUPP;\n\treturn ret < 0 ? ERR_PTR(ret) : cell_name;\n}\n\nstatic int yfs_check_canonical_cell_name(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_cell *master;\n\tchar *cell_name;\n\n\tcell_name = afs_vl_get_cell_name(cell, key);\n\tif (IS_ERR(cell_name))\n\t\treturn PTR_ERR(cell_name);\n\n\tif (strcmp(cell_name, cell->name) == 0) {\n\t\tkfree(cell_name);\n\t\treturn 0;\n\t}\n\n\tmaster = afs_lookup_cell(cell->net, cell_name, strlen(cell_name),\n\t\t\t\t NULL, false);\n\tkfree(cell_name);\n\tif (IS_ERR(master))\n\t\treturn PTR_ERR(master);\n\n\tcell->alias_of = master; /* Transfer our ref */\n\treturn 1;\n}\n\nstatic int afs_do_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_volume *root_volume;\n\tint ret;\n\n\t_enter(\"%s\", cell->name);\n\n\tret = yfs_check_canonical_cell_name(cell, key);\n\tif (ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\t/* Try and get the root.cell volume for comparison with other cells */\n\troot_volume = afs_sample_volume(cell, key, \"root.cell\", 9);\n\tif (!IS_ERR(root_volume)) {\n\t\tcell->root_volume = root_volume;\n\t\treturn afs_compare_cell_roots(cell);\n\t}\n\n\tif (PTR_ERR(root_volume) != -ENOMEDIUM)\n\t\treturn PTR_ERR(root_volume);\n\n\t/* Okay, this cell doesn't have an root.cell volume.  We need to\n\t * locate some other random volume and use that to check.\n\t */\n\treturn afs_query_for_alias(cell, key);\n}\n\n/*\n * Check to see if a new cell is an alias of a cell we already have.  At this\n * point we have the cell's volume server list.\n *\n * Returns 0 if we didn't detect an alias, 1 if we found an alias and an error\n * if we had problems gathering the data required.  In the case the we did\n * detect an alias, cell->alias_of is set to point to the assumed master.\n */\nint afs_cell_detect_alias(struct afs_cell *cell, struct key *key)\n{\n\tstruct afs_net *net = cell->net;\n\tint ret;\n\n\tif (mutex_lock_interruptible(&net->cells_alias_lock) < 0)\n\t\treturn -ERESTARTSYS;\n\n\tif (test_bit(AFS_CELL_FL_CHECK_ALIAS, &cell->flags)) {\n\t\tret = afs_do_cell_detect_alias(cell, key);\n\t\tif (ret >= 0)\n\t\t\tclear_bit_unlock(AFS_CELL_FL_CHECK_ALIAS, &cell->flags);\n\t} else {\n\t\tret = cell->alias_of ? 1 : 0;\n\t}\n\n\tmutex_unlock(&net->cells_alias_lock);\n\n\tif (ret == 1)\n\t\tpr_notice(\"kAFS: Cell %s is an alias of %s\\n\",\n\t\t\t  cell->name, cell->alias_of->name);\n\treturn ret;\n}\n",
                        "fs/afs/vlclient.c": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* AFS Volume Location Service client\n *\n * Copyright (C) 2002 Red Hat, Inc. All Rights Reserved.\n * Written by David Howells (dhowells@redhat.com)\n */\n\n#include <linux/gfp.h>\n#include <linux/init.h>\n#include <linux/sched.h>\n#include \"afs_fs.h\"\n#include \"internal.h\"\n\n/*\n * Deliver reply data to a VL.GetEntryByNameU call.\n */\nstatic int afs_deliver_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tstruct afs_uvldbentry__xdr *uvldb;\n\tstruct afs_vldb_entry *entry;\n\tbool new_only = false;\n\tu32 tmp, nr_servers, vlflags;\n\tint i, ret;\n\n\t_enter(\"\");\n\n\tret = afs_transfer_reply(call);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* unmarshall the reply once we've received all of it */\n\tuvldb = call->buffer;\n\tentry = call->ret_vldb;\n\n\tnr_servers = ntohl(uvldb->nServers);\n\tif (nr_servers > AFS_NMAXNSERVERS)\n\t\tnr_servers = AFS_NMAXNSERVERS;\n\n\tfor (i = 0; i < ARRAY_SIZE(uvldb->name) - 1; i++)\n\t\tentry->name[i] = (u8)ntohl(uvldb->name[i]);\n\tentry->name[i] = 0;\n\tentry->name_len = strlen(entry->name);\n\n\t/* If there is a new replication site that we can use, ignore all the\n\t * sites that aren't marked as new.\n\t */\n\tfor (i = 0; i < nr_servers; i++) {\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (!(tmp & AFS_VLSF_DONTUSE) &&\n\t\t    (tmp & AFS_VLSF_NEWREPSITE))\n\t\t\tnew_only = true;\n\t}\n\n\tvlflags = ntohl(uvldb->flags);\n\tfor (i = 0; i < nr_servers; i++) {\n\t\tstruct afs_uuid__xdr *xdr;\n\t\tstruct afs_uuid *uuid;\n\t\tint j;\n\t\tint n = entry->nr_servers;\n\n\t\ttmp = ntohl(uvldb->serverFlags[i]);\n\t\tif (tmp & AFS_VLSF_DONTUSE ||\n\t\t    (new_only && !(tmp & AFS_VLSF_NEWREPSITE)))\n\t\t\tcontinue;\n\t\tif (tmp & AFS_VLSF_RWVOL) {\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RW;\n\t\t\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_BAK;\n\t\t}\n\t\tif (tmp & AFS_VLSF_ROVOL)\n\t\t\tentry->fs_mask[n] |= AFS_VOL_VTM_RO;\n\t\tif (!entry->fs_mask[n])\n\t\t\tcontinue;\n\n\t\txdr = &uvldb->serverNumber[i];\n\t\tuuid = (struct afs_uuid *)&entry->fs_server[n];\n\t\tuuid->time_low\t\t\t= xdr->time_low;\n\t\tuuid->time_mid\t\t\t= htons(ntohl(xdr->time_mid));\n\t\tuuid->time_hi_and_version\t= htons(ntohl(xdr->time_hi_and_version));\n\t\tuuid->clock_seq_hi_and_reserved\t= (u8)ntohl(xdr->clock_seq_hi_and_reserved);\n\t\tuuid->clock_seq_low\t\t= (u8)ntohl(xdr->clock_seq_low);\n\t\tfor (j = 0; j < 6; j++)\n\t\t\tuuid->node[j] = (u8)ntohl(xdr->node[j]);\n\n\t\tentry->addr_version[n] = ntohl(uvldb->serverUnique[i]);\n\t\tentry->nr_servers++;\n\t}\n\n\tfor (i = 0; i < AFS_MAXTYPES; i++)\n\t\tentry->vid[i] = ntohl(uvldb->volumeId[i]);\n\n\tif (vlflags & AFS_VLF_RWEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RW, &entry->flags);\n\tif (vlflags & AFS_VLF_ROEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_RO, &entry->flags);\n\tif (vlflags & AFS_VLF_BACKEXISTS)\n\t\t__set_bit(AFS_VLDB_HAS_BAK, &entry->flags);\n\n\tif (!(vlflags & (AFS_VLF_RWEXISTS | AFS_VLF_ROEXISTS | AFS_VLF_BACKEXISTS))) {\n\t\tentry->error = -ENOMEDIUM;\n\t\t__set_bit(AFS_VLDB_QUERY_ERROR, &entry->flags);\n\t}\n\n\t__set_bit(AFS_VLDB_QUERY_VALID, &entry->flags);\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_entry_by_name_u(struct afs_call *call)\n{\n\tkfree(call->ret_vldb);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetEntryByNameU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetEntryByNameU = {\n\t.name\t\t= \"VL.GetEntryByNameU\",\n\t.op\t\t= afs_VL_GetEntryByNameU,\n\t.deliver\t= afs_deliver_vl_get_entry_by_name_u,\n\t.destructor\t= afs_destroy_vl_get_entry_by_name_u,\n};\n\n/*\n * Dispatch a get volume entry by name or ID operation (uuid variant).  If the\n * volname is a decimal number then it's a volume ID not a volume name.\n */\nstruct afs_vldb_entry *afs_vl_get_entry_by_name_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t\t  const char *volname,\n\t\t\t\t\t\t  int volnamesz)\n{\n\tstruct afs_vldb_entry *entry;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\tsize_t reqsz, padsz;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tpadsz = (4 - (volnamesz & 3)) & 3;\n\treqsz = 8 + volnamesz + padsz;\n\n\tentry = kzalloc(sizeof(struct afs_vldb_entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetEntryByNameU, reqsz,\n\t\t\t\t   sizeof(struct afs_uvldbentry__xdr));\n\tif (!call) {\n\t\tkfree(entry);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tcall->key = vc->key;\n\tcall->ret_vldb = entry;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETENTRYBYNAMEU);\n\t*bp++ = htonl(volnamesz);\n\tmemcpy(bp, volname, volnamesz);\n\tif (padsz > 0)\n\t\tmemset((void *)bp + volnamesz, 0, padsz);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_vldb_entry *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a VL.GetAddrsU call.\n *\n *\tGetAddrsU(IN ListAddrByAttributes *inaddr,\n *\t\t  OUT afsUUID *uuidp1,\n *\t\t  OUT uint32_t *uniquifier,\n *\t\t  OUT uint32_t *nentries,\n *\t\t  OUT bulkaddrs *blkaddrs);\n */\nstatic int afs_deliver_vl_get_addrs_u(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, nentries, count;\n\tint i, ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call,\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\t\tcall->unmarshall++;\n\n\t\t/* Extract the returned uuid, uniquifier, nentries and\n\t\t * blkaddrs size */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(struct afs_uuid__xdr);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tnentries\t= ntohl(*bp++);\n\t\tcount\t\t= ntohl(*bp);\n\n\t\tnentries = min(nentries, count);\n\t\talist = afs_alloc_addrlist(nentries, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\t\tcall->count = count;\n\t\tcall->count2 = nentries;\n\t\tcall->unmarshall++;\n\n\tmore_entries:\n\t\tcount = min(call->count, 4U);\n\t\tafs_extract_to_buf(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, call->count > 4);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tcount = min(call->count, 4U);\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (alist->nr_addrs < call->count2)\n\t\t\t\tafs_merge_fs_addr4(alist, *bp++, AFS_FS_PORT);\n\n\t\tcall->count -= count;\n\t\tif (call->count > 0)\n\t\t\tgoto more_entries;\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_vl_get_addrs_u_destructor(struct afs_call *call)\n{\n\tafs_put_addrlist(call->ret_alist);\n\treturn afs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetAddrsU operation type.\n */\nstatic const struct afs_call_type afs_RXVLGetAddrsU = {\n\t.name\t\t= \"VL.GetAddrsU\",\n\t.op\t\t= afs_VL_GetAddrsU,\n\t.deliver\t= afs_deliver_vl_get_addrs_u,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_vl_get_addrs_u(struct afs_vl_cursor *vc,\n\t\t\t\t\t const uuid_t *uuid)\n{\n\tstruct afs_ListAddrByAttributes__xdr *r;\n\tconst struct afs_uuid *u = (const struct afs_uuid *)uuid;\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\tint i;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetAddrsU,\n\t\t\t\t   sizeof(__be32) + sizeof(struct afs_ListAddrByAttributes__xdr),\n\t\t\t\t   sizeof(struct afs_uuid__xdr) + 3 * sizeof(__be32));\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETADDRSU);\n\tr = (struct afs_ListAddrByAttributes__xdr *)bp;\n\tr->Mask\t\t= htonl(AFS_VLADDR_UUID);\n\tr->ipaddr\t= 0;\n\tr->index\t= 0;\n\tr->spare\t= 0;\n\tr->uuid.time_low\t\t\t= u->time_low;\n\tr->uuid.time_mid\t\t\t= htonl(ntohs(u->time_mid));\n\tr->uuid.time_hi_and_version\t\t= htonl(ntohs(u->time_hi_and_version));\n\tr->uuid.clock_seq_hi_and_reserved \t= htonl(u->clock_seq_hi_and_reserved);\n\tr->uuid.clock_seq_low\t\t\t= htonl(u->clock_seq_low);\n\tfor (i = 0; i < 6; i++)\n\t\tr->uuid.node[i] = htonl(u->node[i]);\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to an VL.GetCapabilities operation.\n */\nstatic int afs_deliver_vl_get_capabilities(struct afs_call *call)\n{\n\tu32 count;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the capabilities word count */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcount = ntohl(call->tmp);\n\t\tcall->count = count;\n\t\tcall->count2 = count;\n\n\t\tcall->unmarshall++;\n\t\tafs_extract_discard(call, count * sizeof(__be32));\n\n\t\tfallthrough;\t/* and extract capabilities words */\n\tcase 2:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\t/* TODO: Examine capabilities */\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_vl_get_capabilities(struct afs_call *call)\n{\n\tafs_put_vlserver(call->net, call->vlserver);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_RXVLGetCapabilities = {\n\t.name\t\t= \"VL.GetCapabilities\",\n\t.op\t\t= afs_VL_GetCapabilities,\n\t.deliver\t= afs_deliver_vl_get_capabilities,\n\t.done\t\t= afs_vlserver_probe_result,\n\t.destructor\t= afs_destroy_vl_get_capabilities,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nstruct afs_call *afs_vl_get_capabilities(struct afs_net *net,\n\t\t\t\t\t struct afs_addr_cursor *ac,\n\t\t\t\t\t struct key *key,\n\t\t\t\t\t struct afs_vlserver *server,\n\t\t\t\t\t unsigned int server_index)\n{\n\tstruct afs_call *call;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_RXVLGetCapabilities, 1 * 4, 16 * 4);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = key;\n\tcall->vlserver = afs_get_vlserver(server);\n\tcall->server_index = server_index;\n\tcall->upgrade = true;\n\tcall->async = true;\n\tcall->max_lifespan = AFS_PROBE_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(VLGETCAPABILITIES);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(ac, call, GFP_KERNEL);\n\treturn call;\n}\n\n/*\n * Deliver reply data to a YFSVL.GetEndpoints call.\n *\n *\tGetEndpoints(IN yfsServerAttributes *attr,\n *\t\t     OUT opr_uuid *uuid,\n *\t\t     OUT afs_int32 *uniquifier,\n *\t\t     OUT endpoints *fsEndpoints,\n *\t\t     OUT endpoints *volEndpoints)\n */\nstatic int afs_deliver_yfsvl_get_endpoints(struct afs_call *call)\n{\n\tstruct afs_addr_list *alist;\n\t__be32 *bp;\n\tu32 uniquifier, size;\n\tint ret;\n\n\t_enter(\"{%u,%zu,%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count2);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_buf(call, sizeof(uuid_t) + 3 * sizeof(__be32));\n\t\tcall->unmarshall = 1;\n\n\t\t/* Extract the returned uuid, uniquifier, fsEndpoints count and\n\t\t * either the first fsEndpoint type or the volEndpoints\n\t\t * count if there are no fsEndpoints. */\n\t\tfallthrough;\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer + sizeof(uuid_t);\n\t\tuniquifier\t= ntohl(*bp++);\n\t\tcall->count\t= ntohl(*bp++);\n\t\tcall->count2\t= ntohl(*bp); /* Type or next count */\n\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_num);\n\n\t\talist = afs_alloc_addrlist(call->count, FS_SERVICE, AFS_FS_PORT);\n\t\tif (!alist)\n\t\t\treturn -ENOMEM;\n\t\talist->version = uniquifier;\n\t\tcall->ret_alist = alist;\n\n\t\tif (call->count == 0)\n\t\t\tgoto extract_volendpoints;\n\n\tnext_fsendpoint:\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\tsize += sizeof(__be32);\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 2;\n\n\t\tfallthrough;\t/* and extract fsEndpoints[] entries */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\talist = call->ret_alist;\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt4_len);\n\t\t\tafs_merge_fs_addr4(alist, bp[1], ntohl(bp[2]));\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_fsendpt6_len);\n\t\t\tafs_merge_fs_addr6(alist, bp + 1, ntohl(bp[5]));\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_fsendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count2 = ntohl(*bp++);\n\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_fsendpoint;\n\n\textract_volendpoints:\n\t\t/* Extract the list of volEndpoints. */\n\t\tcall->count = call->count2;\n\t\tif (!call->count)\n\t\t\tgoto end;\n\t\tif (call->count > YFS_MAXENDPOINTS)\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\n\t\tafs_extract_to_buf(call, 1 * sizeof(__be32));\n\t\tcall->unmarshall = 3;\n\n\t\t/* Extract the type of volEndpoints[0].  Normally we would\n\t\t * extract the type of the next endpoint when we extract the\n\t\t * data of the current one, but this is the first...\n\t\t */\n\t\tfallthrough;\n\tcase 3:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\n\tnext_volendpoint:\n\t\tcall->count2 = ntohl(*bp++);\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tsize = sizeof(__be32) * (1 + 1 + 1);\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tsize = sizeof(__be32) * (1 + 4 + 1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\tif (call->count > 1)\n\t\t\tsize += sizeof(__be32); /* Get next type too */\n\t\tafs_extract_to_buf(call, size);\n\t\tcall->unmarshall = 4;\n\n\t\tfallthrough;\t/* and extract volEndpoints[] entries */\n\tcase 4:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tbp = call->buffer;\n\t\tswitch (call->count2) {\n\t\tcase YFS_ENDPOINT_IPV4:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 2)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt4_len);\n\t\t\tbp += 3;\n\t\t\tbreak;\n\t\tcase YFS_ENDPOINT_IPV6:\n\t\t\tif (ntohl(bp[0]) != sizeof(__be32) * 5)\n\t\t\t\treturn afs_protocol_error(\n\t\t\t\t\tcall, afs_eproto_yvl_vlendpt6_len);\n\t\t\tbp += 6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn afs_protocol_error(call, afs_eproto_yvl_vlendpt_type);\n\t\t}\n\n\t\t/* Got either the type of the next entry or the count of\n\t\t * volEndpoints if no more fsEndpoints.\n\t\t */\n\t\tcall->count--;\n\t\tif (call->count > 0)\n\t\t\tgoto next_volendpoint;\n\n\tend:\n\t\tafs_extract_discard(call, 0);\n\t\tcall->unmarshall = 5;\n\n\t\tfallthrough;\t/* Done */\n\tcase 5:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tcall->unmarshall = 6;\n\t\tfallthrough;\n\n\tcase 6:\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\n/*\n * YFSVL.GetEndpoints operation type.\n */\nstatic const struct afs_call_type afs_YFSVLGetEndpoints = {\n\t.name\t\t= \"YFSVL.GetEndpoints\",\n\t.op\t\t= afs_YFSVL_GetEndpoints,\n\t.deliver\t= afs_deliver_yfsvl_get_endpoints,\n\t.destructor\t= afs_vl_get_addrs_u_destructor,\n};\n\n/*\n * Dispatch an operation to get the addresses for a server, where the server is\n * nominated by UUID.\n */\nstruct afs_addr_list *afs_yfsvl_get_endpoints(struct afs_vl_cursor *vc,\n\t\t\t\t\t      const uuid_t *uuid)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetEndpoints,\n\t\t\t\t   sizeof(__be32) * 2 + sizeof(*uuid),\n\t\t\t\t   sizeof(struct in6_addr) + sizeof(__be32) * 3);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_alist = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* Marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETENDPOINTS);\n\t*bp++ = htonl(YFS_SERVER_UUID);\n\tmemcpy(bp, uuid, sizeof(*uuid)); /* Type opr_uuid */\n\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (struct afs_addr_list *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n\n/*\n * Deliver reply data to a YFSVL.GetCellName operation.\n */\nstatic int afs_deliver_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tchar *cell_name;\n\tu32 namesz, paddedsz;\n\tint ret;\n\n\t_enter(\"{%u,%zu/%u}\",\n\t       call->unmarshall, iov_iter_count(call->iter), call->count);\n\n\tswitch (call->unmarshall) {\n\tcase 0:\n\t\tafs_extract_to_tmp(call);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract the cell name length */\n\tcase 1:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tnamesz = ntohl(call->tmp);\n\t\tif (namesz > AFS_MAXCELLNAME)\n\t\t\treturn afs_protocol_error(call, afs_eproto_cellname_len);\n\t\tpaddedsz = (namesz + 3) & ~3;\n\t\tcall->count = namesz;\n\t\tcall->count2 = paddedsz - namesz;\n\n\t\tcell_name = kmalloc(namesz + 1, GFP_KERNEL);\n\t\tif (!cell_name)\n\t\t\treturn -ENOMEM;\n\t\tcell_name[namesz] = 0;\n\t\tcall->ret_str = cell_name;\n\n\t\tafs_extract_begin(call, cell_name, namesz);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract cell name */\n\tcase 2:\n\t\tret = afs_extract_data(call, true);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tafs_extract_discard(call, call->count2);\n\t\tcall->unmarshall++;\n\n\t\tfallthrough;\t/* and extract padding */\n\tcase 3:\n\t\tret = afs_extract_data(call, false);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tcall->unmarshall++;\n\t\tbreak;\n\t}\n\n\t_leave(\" = 0 [done]\");\n\treturn 0;\n}\n\nstatic void afs_destroy_yfsvl_get_cell_name(struct afs_call *call)\n{\n\tkfree(call->ret_str);\n\tafs_flat_call_destructor(call);\n}\n\n/*\n * VL.GetCapabilities operation type\n */\nstatic const struct afs_call_type afs_YFSVLGetCellName = {\n\t.name\t\t= \"YFSVL.GetCellName\",\n\t.op\t\t= afs_YFSVL_GetCellName,\n\t.deliver\t= afs_deliver_yfsvl_get_cell_name,\n\t.destructor\t= afs_destroy_yfsvl_get_cell_name,\n};\n\n/*\n * Probe a volume server for the capabilities that it supports.  This can\n * return up to 196 words.\n *\n * We use this to probe for service upgrade to determine what the server at the\n * other end supports.\n */\nchar *afs_yfsvl_get_cell_name(struct afs_vl_cursor *vc)\n{\n\tstruct afs_call *call;\n\tstruct afs_net *net = vc->cell->net;\n\t__be32 *bp;\n\n\t_enter(\"\");\n\n\tcall = afs_alloc_flat_call(net, &afs_YFSVLGetCellName, 1 * 4, 0);\n\tif (!call)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcall->key = vc->key;\n\tcall->ret_str = NULL;\n\tcall->max_lifespan = AFS_VL_MAX_LIFESPAN;\n\n\t/* marshall the parameters */\n\tbp = call->request;\n\t*bp++ = htonl(YVLGETCELLNAME);\n\n\t/* Can't take a ref on server */\n\ttrace_afs_make_vl_call(call);\n\tafs_make_call(&vc->ac, call, GFP_KERNEL);\n\treturn (char *)afs_wait_for_call_to_complete(call, &vc->ac);\n}\n"
                    }
                }
            ]
        }
    ],
    "cves_skipped": []
}