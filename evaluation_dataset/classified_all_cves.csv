CVE,upstream_patch_SHA,upstream_patch_diff,downstream_patch_SHA,downstream_patch_diff,error_message,loc_diff_size,num_methods,num_classes,diff_line_count,has_merge_error,label,predicted_label
CVE-2025-21655,c9a40292a44e78f71258b8522655bffaf5753bdb,"From c9a40292a44e78f71258b8522655bffaf5753bdb Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 8 Jan 2025 10:28:05 -0700
Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another
 RCU period

io_eventfd_do_signal() is invoked from an RCU callback, but when
dropping the reference to the io_ev_fd, it calls io_eventfd_free()
directly if the refcount drops to zero. This isn't correct, as any
potential freeing of the io_ev_fd should be deferred another RCU grace
period.

Just call io_eventfd_put() rather than open-code the dec-and-test and
free, which will correctly defer it another RCU grace period.

Fixes: 21a091b970cd (""io_uring: signal registered eventfd to process deferred task work"")
Reported-by: Jann Horn <jannh@google.com>
Cc: stable@vger.kernel.org
Tested-by: Li Zetao <lizetao1@huawei.com>
Reviewed-by: Li Zetao<lizetao1@huawei.com>
Reviewed-by: Prasanna Kumar T S M <ptsm@linux.microsoft.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/eventfd.c | 16 +++++++---------
 1 file changed, 7 insertions(+), 9 deletions(-)

diff --git a/io_uring/eventfd.c b/io_uring/eventfd.c
index fab936d31ba8..100d5da94cb9 100644
--- a/io_uring/eventfd.c
+++ b/io_uring/eventfd.c
@@ -33,20 +33,18 @@ static void io_eventfd_free(struct rcu_head *rcu)
         kfree(ev_fd);
 }
 
-static void io_eventfd_do_signal(struct rcu_head *rcu)
+static void io_eventfd_put(struct io_ev_fd *ev_fd)
 {
-        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
-
-        eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
-
         if (refcount_dec_and_test(&ev_fd->refs))
-                io_eventfd_free(rcu);
+                call_rcu(&ev_fd->rcu, io_eventfd_free);
 }
 
-static void io_eventfd_put(struct io_ev_fd *ev_fd)
+static void io_eventfd_do_signal(struct rcu_head *rcu)
 {
-        if (refcount_dec_and_test(&ev_fd->refs))
-                call_rcu(&ev_fd->rcu, io_eventfd_free);
+        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
+
+        eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
+        io_eventfd_put(ev_fd);
 }
 
 static void io_eventfd_release(struct io_ev_fd *ev_fd, bool put_ref)
-- 
2.39.5 (Apple Git-154)

",6b63308c28987c6010b1180c72a6db4df6c68033,"From 6b63308c28987c6010b1180c72a6db4df6c68033 Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 8 Jan 2025 11:16:13 -0700
Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another
 RCU period

Commit c9a40292a44e78f71258b8522655bffaf5753bdb upstream.

io_eventfd_do_signal() is invoked from an RCU callback, but when
dropping the reference to the io_ev_fd, it calls io_eventfd_free()
directly if the refcount drops to zero. This isn't correct, as any
potential freeing of the io_ev_fd should be deferred another RCU grace
period.

Just call io_eventfd_put() rather than open-code the dec-and-test and
free, which will correctly defer it another RCU grace period.

Fixes: 21a091b970cd (""io_uring: signal registered eventfd to process deferred task work"")
Reported-by: Jann Horn <jannh@google.com>
Cc: stable@vger.kernel.org
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/io_uring.c | 13 +++++++++----
 1 file changed, 9 insertions(+), 4 deletions(-)

diff --git a/io_uring/io_uring.c b/io_uring/io_uring.c
index 9b58ba4616d4..480752fc3eb6 100644
--- a/io_uring/io_uring.c
+++ b/io_uring/io_uring.c
@@ -479,6 +479,13 @@ static __cold void io_queue_deferred(struct io_ring_ctx *ctx)
         }
 }
 
+static void io_eventfd_free(struct rcu_head *rcu)
+{
+        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
+
+        eventfd_ctx_put(ev_fd->cq_ev_fd);
+        kfree(ev_fd);
+}
 
 static void io_eventfd_ops(struct rcu_head *rcu)
 {
@@ -492,10 +499,8 @@ static void io_eventfd_ops(struct rcu_head *rcu)
          * ordering in a race but if references are 0 we know we have to free
          * it regardless.
          */
-        if (atomic_dec_and_test(&ev_fd->refs)) {
-                eventfd_ctx_put(ev_fd->cq_ev_fd);
-                kfree(ev_fd);
-        }
+        if (atomic_dec_and_test(&ev_fd->refs))
+                call_rcu(&ev_fd->rcu, io_eventfd_free);
 }
 
 static void io_eventfd_signal(struct io_ring_ctx *ctx)
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 31\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From c9a40292a44e78f71258b8522655bffaf5753bdb Mon Sep 17 00:00:00 2001\n|From: Jens Axboe <axboe@kernel.dk>\n|Date: Wed, 8 Jan 2025 10:28:05 -0700\n|Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another\n| RCU period\n|\n|io_eventfd_do_signal() is invoked from an RCU callback, but when\n|dropping the reference to the io_ev_fd, it calls io_eventfd_free()\n|directly if the refcount drops to zero. This isn't correct, as any\n|potential freeing of the io_ev_fd should be deferred another RCU grace\n|period.\n|\n|Just call io_eventfd_put() rather than open-code the dec-and-test and\n|free, which will correctly defer it another RCU grace period.\n|\n|Fixes: 21a091b970cd (\""io_uring: signal registered eventfd to process deferred task work\"")\n|Reported-by: Jann Horn <jannh@google.com>\n|Cc: stable@vger.kernel.org\n|Tested-by: Li Zetao <lizetao1@huawei.com>\n|Reviewed-by: Li Zetao<lizetao1@huawei.com>\n|Reviewed-by: Prasanna Kumar T S M <ptsm@linux.microsoft.com>\n|Signed-off-by: Jens Axboe <axboe@kernel.dk>\n|---\n| io_uring/eventfd.c | 16 +++++++---------\n| 1 file changed, 7 insertions(+), 9 deletions(-)\n|\n|diff --git a/io_uring/eventfd.c b/io_uring/eventfd.c\n|index fab936d31ba8..100d5da94cb9 100644\n|--- a/io_uring/eventfd.c\n|+++ b/io_uring/eventfd.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",17,2,10,61,1,COMMENTS,COMMENTS
CVE-2025-21655,c9a40292a44e78f71258b8522655bffaf5753bdb,"From c9a40292a44e78f71258b8522655bffaf5753bdb Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 8 Jan 2025 10:28:05 -0700
Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another
 RCU period

io_eventfd_do_signal() is invoked from an RCU callback, but when
dropping the reference to the io_ev_fd, it calls io_eventfd_free()
directly if the refcount drops to zero. This isn't correct, as any
potential freeing of the io_ev_fd should be deferred another RCU grace
period.

Just call io_eventfd_put() rather than open-code the dec-and-test and
free, which will correctly defer it another RCU grace period.

Fixes: 21a091b970cd (""io_uring: signal registered eventfd to process deferred task work"")
Reported-by: Jann Horn <jannh@google.com>
Cc: stable@vger.kernel.org
Tested-by: Li Zetao <lizetao1@huawei.com>
Reviewed-by: Li Zetao<lizetao1@huawei.com>
Reviewed-by: Prasanna Kumar T S M <ptsm@linux.microsoft.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/eventfd.c | 16 +++++++---------
 1 file changed, 7 insertions(+), 9 deletions(-)

diff --git a/io_uring/eventfd.c b/io_uring/eventfd.c
index fab936d31ba8..100d5da94cb9 100644
--- a/io_uring/eventfd.c
+++ b/io_uring/eventfd.c
@@ -33,20 +33,18 @@ static void io_eventfd_free(struct rcu_head *rcu)
         kfree(ev_fd);
 }
 
-static void io_eventfd_do_signal(struct rcu_head *rcu)
+static void io_eventfd_put(struct io_ev_fd *ev_fd)
 {
-        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
-
-        eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
-
         if (refcount_dec_and_test(&ev_fd->refs))
-                io_eventfd_free(rcu);
+                call_rcu(&ev_fd->rcu, io_eventfd_free);
 }
 
-static void io_eventfd_put(struct io_ev_fd *ev_fd)
+static void io_eventfd_do_signal(struct rcu_head *rcu)
 {
-        if (refcount_dec_and_test(&ev_fd->refs))
-                call_rcu(&ev_fd->rcu, io_eventfd_free);
+        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
+
+        eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
+        io_eventfd_put(ev_fd);
 }
 
 static void io_eventfd_release(struct io_ev_fd *ev_fd, bool put_ref)
-- 
2.39.5 (Apple Git-154)

",8efff2aa2d95dc437ab67c5b4a9f1d3f367baa10,"From 8efff2aa2d95dc437ab67c5b4a9f1d3f367baa10 Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 8 Jan 2025 11:16:13 -0700
Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another
 RCU period

Commit c9a40292a44e78f71258b8522655bffaf5753bdb upstream.

io_eventfd_do_signal() is invoked from an RCU callback, but when
dropping the reference to the io_ev_fd, it calls io_eventfd_free()
directly if the refcount drops to zero. This isn't correct, as any
potential freeing of the io_ev_fd should be deferred another RCU grace
period.

Just call io_eventfd_put() rather than open-code the dec-and-test and
free, which will correctly defer it another RCU grace period.

Fixes: 21a091b970cd (""io_uring: signal registered eventfd to process deferred task work"")
Reported-by: Jann Horn <jannh@google.com>
Cc: stable@vger.kernel.org
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/io_uring.c | 13 +++++++++----
 1 file changed, 9 insertions(+), 4 deletions(-)

diff --git a/io_uring/io_uring.c b/io_uring/io_uring.c
index 0122f220ef0d..c7198fbcf734 100644
--- a/io_uring/io_uring.c
+++ b/io_uring/io_uring.c
@@ -537,6 +537,13 @@ static __cold void io_queue_deferred(struct io_ring_ctx *ctx)
 	}
 }
 
+static void io_eventfd_free(struct rcu_head *rcu)
+{
+	struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
+
+	eventfd_ctx_put(ev_fd->cq_ev_fd);
+	kfree(ev_fd);
+}
 
 static void io_eventfd_ops(struct rcu_head *rcu)
 {
@@ -550,10 +557,8 @@ static void io_eventfd_ops(struct rcu_head *rcu)
 	 * ordering in a race but if references are 0 we know we have to free
 	 * it regardless.
 	 */
-	if (atomic_dec_and_test(&ev_fd->refs)) {
-		eventfd_ctx_put(ev_fd->cq_ev_fd);
-		kfree(ev_fd);
-	}
+	if (atomic_dec_and_test(&ev_fd->refs))
+		call_rcu(&ev_fd->rcu, io_eventfd_free);
 }
 
 static void io_eventfd_signal(struct io_ring_ctx *ctx)
-- 
2.39.5 (Apple Git-154)
","can't find file to patch at input line 31\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From c9a40292a44e78f71258b8522655bffaf5753bdb Mon Sep 17 00:00:00 2001\n|From: Jens Axboe <axboe@kernel.dk>\n|Date: Wed, 8 Jan 2025 10:28:05 -0700\n|Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another\n| RCU period\n|\n|io_eventfd_do_signal() is invoked from an RCU callback, but when\n|dropping the reference to the io_ev_fd, it calls io_eventfd_free()\n|directly if the refcount drops to zero. This isn't correct, as any\n|potential freeing of the io_ev_fd should be deferred another RCU grace\n|period.\n|\n|Just call io_eventfd_put() rather than open-code the dec-and-test and\n|free, which will correctly defer it another RCU grace period.\n|\n|Fixes: 21a091b970cd (\""io_uring: signal registered eventfd to process deferred task work\"")\n|Reported-by: Jann Horn <jannh@google.com>\n|Cc: stable@vger.kernel.org\n|Tested-by: Li Zetao <lizetao1@huawei.com>\n|Reviewed-by: Li Zetao<lizetao1@huawei.com>\n|Reviewed-by: Prasanna Kumar T S M <ptsm@linux.microsoft.com>\n|Signed-off-by: Jens Axboe <axboe@kernel.dk>\n|---\n| io_uring/eventfd.c | 16 +++++++---------\n| 1 file changed, 7 insertions(+), 9 deletions(-)\n|\n|diff --git a/io_uring/eventfd.c b/io_uring/eventfd.c\n|index fab936d31ba8..100d5da94cb9 100644\n|--- a/io_uring/eventfd.c\n|+++ b/io_uring/eventfd.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",17,2,10,61,1,DELETE,COMMENTS
CVE-2025-21655,c9a40292a44e78f71258b8522655bffaf5753bdb,"From c9a40292a44e78f71258b8522655bffaf5753bdb Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 8 Jan 2025 10:28:05 -0700
Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another
 RCU period

io_eventfd_do_signal() is invoked from an RCU callback, but when
dropping the reference to the io_ev_fd, it calls io_eventfd_free()
directly if the refcount drops to zero. This isn't correct, as any
potential freeing of the io_ev_fd should be deferred another RCU grace
period.

Just call io_eventfd_put() rather than open-code the dec-and-test and
free, which will correctly defer it another RCU grace period.

Fixes: 21a091b970cd (""io_uring: signal registered eventfd to process deferred task work"")
Reported-by: Jann Horn <jannh@google.com>
Cc: stable@vger.kernel.org
Tested-by: Li Zetao <lizetao1@huawei.com>
Reviewed-by: Li Zetao<lizetao1@huawei.com>
Reviewed-by: Prasanna Kumar T S M <ptsm@linux.microsoft.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/eventfd.c | 16 +++++++---------
 1 file changed, 7 insertions(+), 9 deletions(-)

diff --git a/io_uring/eventfd.c b/io_uring/eventfd.c
index fab936d31ba8..100d5da94cb9 100644
--- a/io_uring/eventfd.c
+++ b/io_uring/eventfd.c
@@ -33,20 +33,18 @@ static void io_eventfd_free(struct rcu_head *rcu)
         kfree(ev_fd);
 }
 
-static void io_eventfd_do_signal(struct rcu_head *rcu)
+static void io_eventfd_put(struct io_ev_fd *ev_fd)
 {
-        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
-
-        eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
-
         if (refcount_dec_and_test(&ev_fd->refs))
-                io_eventfd_free(rcu);
+                call_rcu(&ev_fd->rcu, io_eventfd_free);
 }
 
-static void io_eventfd_put(struct io_ev_fd *ev_fd)
+static void io_eventfd_do_signal(struct rcu_head *rcu)
 {
-        if (refcount_dec_and_test(&ev_fd->refs))
-                call_rcu(&ev_fd->rcu, io_eventfd_free);
+        struct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);
+
+        eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
+        io_eventfd_put(ev_fd);
 }
 
 static void io_eventfd_release(struct io_ev_fd *ev_fd, bool put_ref)
-- 
2.39.5 (Apple Git-154)

",a7085c3ae43b86d4b3d1b8275e6a67f14257e3b7,"From a7085c3ae43b86d4b3d1b8275e6a67f14257e3b7 Mon Sep 17 00:00:00 2001
From: Jens Axboe <axboe@kernel.dk>
Date: Wed, 8 Jan 2025 10:28:05 -0700
Subject: [PATCH] io_uring/eventfd: ensure io_eventfd_signal() defers another
 RCU period

Commit c9a40292a44e78f71258b8522655bffaf5753bdb upstream.

io_eventfd_do_signal() is invoked from an RCU callback, but when
dropping the reference to the io_ev_fd, it calls io_eventfd_free()
directly if the refcount drops to zero. This isn't correct, as any
potential freeing of the io_ev_fd should be deferred another RCU grace
period.

Just call io_eventfd_put() rather than open-code the dec-and-test and
free, which will correctly defer it another RCU grace period.

Fixes: 21a091b970cd (""io_uring: signal registered eventfd to process deferred task work"")
Reported-by: Jann Horn <jannh@google.com>
Cc: stable@vger.kernel.org
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/eventfd.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/io_uring/eventfd.c b/io_uring/eventfd.c
index e37fddd5d9ce..ffc4bd17d078 100644
--- a/io_uring/eventfd.c
+++ b/io_uring/eventfd.c
@@ -38,7 +38,7 @@ static void io_eventfd_do_signal(struct rcu_head *rcu)
         eventfd_signal_mask(ev_fd->cq_ev_fd, EPOLL_URING_WAKE);
 
         if (refcount_dec_and_test(&ev_fd->refs))
-                io_eventfd_free(rcu);
+                call_rcu(&ev_fd->rcu, io_eventfd_free);
 }
 
 void io_eventfd_signal(struct io_ring_ctx *ctx)
-- 
2.39.5 (Apple Git-154)
",patching file io_uring/eventfd.c\nHunk #1 FAILED at 33.\n1 out of 1 hunk FAILED -- saving rejects to file io_uring/eventfd.c.rej,17,2,10,61,1,DELETE,COMMENTS
CVE-2025-21659,d1cacd74776895f6435941f86a1130e58f6dd226,"From d1cacd74776895f6435941f86a1130e58f6dd226 Mon Sep 17 00:00:00 2001
From: Jakub Kicinski <kuba@kernel.org>
Date: Mon, 6 Jan 2025 10:01:36 -0800
Subject: [PATCH] netdev: prevent accessing NAPI instances from another
 namespace

The NAPI IDs were not fully exposed to user space prior to the netlink
API, so they were never namespaced. The netlink API must ensure that
at the very least NAPI instance belongs to the same netns as the owner
of the genl sock.

napi_by_id() can become static now, but it needs to move because of
dev_get_by_napi_id().

Cc: stable@vger.kernel.org
Fixes: 1287c1ae0fc2 (""netdev-genl: Support setting per-NAPI config values"")
Fixes: 27f91aaf49b3 (""netdev-genl: Add netlink framework functions for napi"")
Reviewed-by: Sridhar Samudrala <sridhar.samudrala@intel.com>
Reviewed-by: Joe Damato <jdamato@fastly.com>
Link: https://patch.msgid.link/20250106180137.1861472-1-kuba@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/core/dev.c         | 43 +++++++++++++++++++++++++++++-------------
 net/core/dev.h         |  3 ++-
 net/core/netdev-genl.c |  6 ++----
 3 files changed, 34 insertions(+), 18 deletions(-)

diff --git a/net/core/dev.c b/net/core/dev.c
index faa23042df38..a9f62f5aeb84 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -753,6 +753,36 @@ int dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,
 }
 EXPORT_SYMBOL_GPL(dev_fill_forward_path);
 
+/* must be called under rcu_read_lock(), as we dont take a reference */
+static struct napi_struct *napi_by_id(unsigned int napi_id)
+{
+        unsigned int hash = napi_id % HASH_SIZE(napi_hash);
+        struct napi_struct *napi;
+
+        hlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)
+                if (napi->napi_id == napi_id)
+                        return napi;
+
+        return NULL;
+}
+
+/* must be called under rcu_read_lock(), as we dont take a reference */
+struct napi_struct *netdev_napi_by_id(struct net *net, unsigned int napi_id)
+{
+        struct napi_struct *napi;
+
+        napi = napi_by_id(napi_id);
+        if (!napi)
+                return NULL;
+
+        if (WARN_ON_ONCE(!napi->dev))
+                return NULL;
+        if (!net_eq(net, dev_net(napi->dev)))
+                return NULL;
+
+        return napi;
+}
+
 /**
  *        __dev_get_by_name        - find a device by its name
  *        @net: the applicable net namespace
@@ -6293,19 +6323,6 @@ bool napi_complete_done(struct napi_struct *n, int work_done)
 }
 EXPORT_SYMBOL(napi_complete_done);
 
-/* must be called under rcu_read_lock(), as we dont take a reference */
-struct napi_struct *napi_by_id(unsigned int napi_id)
-{
-        unsigned int hash = napi_id % HASH_SIZE(napi_hash);
-        struct napi_struct *napi;
-
-        hlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)
-                if (napi->napi_id == napi_id)
-                        return napi;
-
-        return NULL;
-}
-
 static void skb_defer_free_flush(struct softnet_data *sd)
 {
         struct sk_buff *skb, *next;
diff --git a/net/core/dev.h b/net/core/dev.h
index d043dee25a68..deb5eae5749f 100644
--- a/net/core/dev.h
+++ b/net/core/dev.h
@@ -22,6 +22,8 @@ struct sd_flow_limit {
 
 extern int netdev_flow_limit_table_len;
 
+struct napi_struct *netdev_napi_by_id(struct net *net, unsigned int napi_id);
+
 #ifdef CONFIG_PROC_FS
 int __init dev_proc_init(void);
 #else
@@ -269,7 +271,6 @@ void xdp_do_check_flushed(struct napi_struct *napi);
 static inline void xdp_do_check_flushed(struct napi_struct *napi) { }
 #endif
 
-struct napi_struct *napi_by_id(unsigned int napi_id);
 void kick_defer_list_purge(struct softnet_data *sd, unsigned int cpu);
 
 #define XMIT_RECURSION_LIMIT        8
diff --git a/net/core/netdev-genl.c b/net/core/netdev-genl.c
index 125b660004d3..a3bdaf075b6b 100644
--- a/net/core/netdev-genl.c
+++ b/net/core/netdev-genl.c
@@ -167,8 +167,6 @@ netdev_nl_napi_fill_one(struct sk_buff *rsp, struct napi_struct *napi,
         void *hdr;
         pid_t pid;
 
-        if (WARN_ON_ONCE(!napi->dev))
-                return -EINVAL;
         if (!(napi->dev->flags & IFF_UP))
                 return 0;
 
@@ -234,7 +232,7 @@ int netdev_nl_napi_get_doit(struct sk_buff *skb, struct genl_info *info)
         rtnl_lock();
         rcu_read_lock();
 
-        napi = napi_by_id(napi_id);
+        napi = netdev_napi_by_id(genl_info_net(info), napi_id);
         if (napi) {
                 err = netdev_nl_napi_fill_one(rsp, napi, info);
         } else {
@@ -355,7 +353,7 @@ int netdev_nl_napi_set_doit(struct sk_buff *skb, struct genl_info *info)
         rtnl_lock();
         rcu_read_lock();
 
-        napi = napi_by_id(napi_id);
+        napi = netdev_napi_by_id(genl_info_net(info), napi_id);
         if (napi) {
                 err = netdev_nl_napi_set_config(napi, info);
         } else {
-- 
2.39.5 (Apple Git-154)

",b683ba0df11ff563cc237eb1b74d6adfa77226bf,"From b683ba0df11ff563cc237eb1b74d6adfa77226bf Mon Sep 17 00:00:00 2001
From: Jakub Kicinski <kuba@kernel.org>
Date: Mon, 6 Jan 2025 10:01:36 -0800
Subject: [PATCH] netdev: prevent accessing NAPI instances from another
 namespace

commit d1cacd74776895f6435941f86a1130e58f6dd226 upstream.

The NAPI IDs were not fully exposed to user space prior to the netlink
API, so they were never namespaced. The netlink API must ensure that
at the very least NAPI instance belongs to the same netns as the owner
of the genl sock.

napi_by_id() can become static now, but it needs to move because of
dev_get_by_napi_id().

Cc: stable@vger.kernel.org
Fixes: 1287c1ae0fc2 (""netdev-genl: Support setting per-NAPI config values"")
Fixes: 27f91aaf49b3 (""netdev-genl: Add netlink framework functions for napi"")
Reviewed-by: Sridhar Samudrala <sridhar.samudrala@intel.com>
Reviewed-by: Joe Damato <jdamato@fastly.com>
Link: https://patch.msgid.link/20250106180137.1861472-1-kuba@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/core/dev.c         | 43 +++++++++++++++++++++++++++++-------------
 net/core/dev.h         |  3 ++-
 net/core/netdev-genl.c |  4 +---
 3 files changed, 33 insertions(+), 17 deletions(-)

diff --git a/net/core/dev.c b/net/core/dev.c
index f3fa8353d262..1867a6a8d76d 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -753,6 +753,36 @@ int dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,
 }
 EXPORT_SYMBOL_GPL(dev_fill_forward_path);
 
+/* must be called under rcu_read_lock(), as we dont take a reference */
+static struct napi_struct *napi_by_id(unsigned int napi_id)
+{
+        unsigned int hash = napi_id % HASH_SIZE(napi_hash);
+        struct napi_struct *napi;
+
+        hlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)
+                if (napi->napi_id == napi_id)
+                        return napi;
+
+        return NULL;
+}
+
+/* must be called under rcu_read_lock(), as we dont take a reference */
+struct napi_struct *netdev_napi_by_id(struct net *net, unsigned int napi_id)
+{
+        struct napi_struct *napi;
+
+        napi = napi_by_id(napi_id);
+        if (!napi)
+                return NULL;
+
+        if (WARN_ON_ONCE(!napi->dev))
+                return NULL;
+        if (!net_eq(net, dev_net(napi->dev)))
+                return NULL;
+
+        return napi;
+}
+
 /**
  *        __dev_get_by_name        - find a device by its name
  *        @net: the applicable net namespace
@@ -6291,19 +6321,6 @@ bool napi_complete_done(struct napi_struct *n, int work_done)
 }
 EXPORT_SYMBOL(napi_complete_done);
 
-/* must be called under rcu_read_lock(), as we dont take a reference */
-struct napi_struct *napi_by_id(unsigned int napi_id)
-{
-        unsigned int hash = napi_id % HASH_SIZE(napi_hash);
-        struct napi_struct *napi;
-
-        hlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)
-                if (napi->napi_id == napi_id)
-                        return napi;
-
-        return NULL;
-}
-
 static void skb_defer_free_flush(struct softnet_data *sd)
 {
         struct sk_buff *skb, *next;
diff --git a/net/core/dev.h b/net/core/dev.h
index 5654325c5b71..2e3bb7669984 100644
--- a/net/core/dev.h
+++ b/net/core/dev.h
@@ -22,6 +22,8 @@ struct sd_flow_limit {
 
 extern int netdev_flow_limit_table_len;
 
+struct napi_struct *netdev_napi_by_id(struct net *net, unsigned int napi_id);
+
 #ifdef CONFIG_PROC_FS
 int __init dev_proc_init(void);
 #else
@@ -146,7 +148,6 @@ void xdp_do_check_flushed(struct napi_struct *napi);
 static inline void xdp_do_check_flushed(struct napi_struct *napi) { }
 #endif
 
-struct napi_struct *napi_by_id(unsigned int napi_id);
 void kick_defer_list_purge(struct softnet_data *sd, unsigned int cpu);
 
 #define XMIT_RECURSION_LIMIT        8
diff --git a/net/core/netdev-genl.c b/net/core/netdev-genl.c
index c639acb5abfd..ad426b3a03b5 100644
--- a/net/core/netdev-genl.c
+++ b/net/core/netdev-genl.c
@@ -164,8 +164,6 @@ netdev_nl_napi_fill_one(struct sk_buff *rsp, struct napi_struct *napi,
         void *hdr;
         pid_t pid;
 
-        if (WARN_ON_ONCE(!napi->dev))
-                return -EINVAL;
         if (!(napi->dev->flags & IFF_UP))
                 return 0;
 
@@ -216,7 +214,7 @@ int netdev_nl_napi_get_doit(struct sk_buff *skb, struct genl_info *info)
         rtnl_lock();
         rcu_read_lock();
 
-        napi = napi_by_id(napi_id);
+        napi = netdev_napi_by_id(genl_info_net(info), napi_id);
         if (napi) {
                 err = netdev_nl_napi_fill_one(rsp, napi, info);
         } else {
-- 
2.39.5 (Apple Git-154)

",patching file net/core/dev.c\nHunk #2 succeeded at 6321 (offset -2 lines).\npatching file net/core/dev.h\nHunk #2 succeeded at 148 (offset -123 lines).\npatching file net/core/netdev-genl.c\nHunk #1 succeeded at 164 (offset -3 lines).\nHunk #2 succeeded at 214 (offset -18 lines).\nHunk #3 FAILED at 353.\n1 out of 3 hunks FAILED -- saving rejects to file net/core/netdev-genl.c.rej,53,2,24,143,1,FORMATTING,FORMATTING
CVE-2025-21693,12dcb0ef540629a281533f9dedc1b6b8e14cfb65,"From 12dcb0ef540629a281533f9dedc1b6b8e14cfb65 Mon Sep 17 00:00:00 2001
From: Yosry Ahmed <yosryahmed@google.com>
Date: Wed, 8 Jan 2025 22:24:41 +0000
Subject: [PATCH] mm: zswap: properly synchronize freeing resources during CPU
 hotunplug

In zswap_compress() and zswap_decompress(), the per-CPU acomp_ctx of the
current CPU at the beginning of the operation is retrieved and used
throughout.  However, since neither preemption nor migration are disabled,
it is possible that the operation continues on a different CPU.

If the original CPU is hotunplugged while the acomp_ctx is still in use,
we run into a UAF bug as some of the resources attached to the acomp_ctx
are freed during hotunplug in zswap_cpu_comp_dead() (i.e.
acomp_ctx.buffer, acomp_ctx.req, or acomp_ctx.acomp).

The problem was introduced in commit 1ec3b5fe6eec (""mm/zswap: move to use
crypto_acomp API for hardware acceleration"") when the switch to the
crypto_acomp API was made.  Prior to that, the per-CPU crypto_comp was
retrieved using get_cpu_ptr() which disables preemption and makes sure the
CPU cannot go away from under us.  Preemption cannot be disabled with the
crypto_acomp API as a sleepable context is needed.

Use the acomp_ctx.mutex to synchronize CPU hotplug callbacks allocating
and freeing resources with compression/decompression paths.  Make sure
that acomp_ctx.req is NULL when the resources are freed.  In the
compression/decompression paths, check if acomp_ctx.req is NULL after
acquiring the mutex (meaning the CPU was offlined) and retry on the new
CPU.

The initialization of acomp_ctx.mutex is moved from the CPU hotplug
callback to the pool initialization where it belongs (where the mutex is
allocated).  In addition to adding clarity, this makes sure that CPU
hotplug cannot reinitialize a mutex that is already locked by
compression/decompression.

Previously a fix was attempted by holding cpus_read_lock() [1].  This
would have caused a potential deadlock as it is possible for code already
holding the lock to fall into reclaim and enter zswap (causing a
deadlock).  A fix was also attempted using SRCU for synchronization, but
Johannes pointed out that synchronize_srcu() cannot be used in CPU hotplug
notifiers [2].

Alternative fixes that were considered/attempted and could have worked:
- Refcounting the per-CPU acomp_ctx. This involves complexity in
  handling the race between the refcount dropping to zero in
  zswap_[de]compress() and the refcount being re-initialized when the
  CPU is onlined.
- Disabling migration before getting the per-CPU acomp_ctx [3], but
  that's discouraged and is a much bigger hammer than needed, and could
  result in subtle performance issues.

[1]https://lkml.kernel.org/20241219212437.2714151-1-yosryahmed@google.com/
[2]https://lkml.kernel.org/20250107074724.1756696-2-yosryahmed@google.com/
[3]https://lkml.kernel.org/20250107222236.2715883-2-yosryahmed@google.com/

[yosryahmed@google.com: remove comment]
  Link: https://lkml.kernel.org/r/CAJD7tkaxS1wjn+swugt8QCvQ-rVF5RZnjxwPGX17k8x9zSManA@mail.gmail.com
Link: https://lkml.kernel.org/r/20250108222441.3622031-1-yosryahmed@google.com
Fixes: 1ec3b5fe6eec (""mm/zswap: move to use crypto_acomp API for hardware acceleration"")
Signed-off-by: Yosry Ahmed <yosryahmed@google.com>
Reported-by: Johannes Weiner <hannes@cmpxchg.org>
Closes: https://lore.kernel.org/lkml/20241113213007.GB1564047@cmpxchg.org/
Reported-by: Sam Sun <samsun1006219@gmail.com>
Closes: https://lore.kernel.org/lkml/CAEkJfYMtSdM5HceNsXUDf5haghD5+o2e7Qv4OcuruL4tPg6OaQ@mail.gmail.com/
Cc: Barry Song <baohua@kernel.org>
Cc: Chengming Zhou <chengming.zhou@linux.dev>
Cc: Kanchana P Sridhar <kanchana.p.sridhar@intel.com>
Cc: Nhat Pham <nphamcs@gmail.com>
Cc: Vitaly Wool <vitalywool@gmail.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 mm/zswap.c | 58 +++++++++++++++++++++++++++++++++++++++++-------------
 1 file changed, 44 insertions(+), 14 deletions(-)

diff --git a/mm/zswap.c b/mm/zswap.c
index f6316b66fb23..30f5a27a6862 100644
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@ -251,7 +251,7 @@ static struct zswap_pool *zswap_pool_create(char *type, char *compressor)
         struct zswap_pool *pool;
         char name[38]; /* 'zswap' + 32 char (max) num + \0 */
         gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;
-        int ret;
+        int ret, cpu;
 
         if (!zswap_has_pool) {
                 /* if either are unset, pool initialization failed, and we
@@ -285,6 +285,9 @@ static struct zswap_pool *zswap_pool_create(char *type, char *compressor)
                 goto error;
         }
 
+        for_each_possible_cpu(cpu)
+                mutex_init(&per_cpu_ptr(pool->acomp_ctx, cpu)->mutex);
+
         ret = cpuhp_state_add_instance(CPUHP_MM_ZSWP_POOL_PREPARE,
                                        &pool->node);
         if (ret)
@@ -821,11 +824,12 @@ static int zswap_cpu_comp_prepare(unsigned int cpu, struct hlist_node *node)
         struct acomp_req *req;
         int ret;
 
-        mutex_init(&acomp_ctx->mutex);
-
+        mutex_lock(&acomp_ctx->mutex);
         acomp_ctx->buffer = kmalloc_node(PAGE_SIZE * 2, GFP_KERNEL, cpu_to_node(cpu));
-        if (!acomp_ctx->buffer)
-                return -ENOMEM;
+        if (!acomp_ctx->buffer) {
+                ret = -ENOMEM;
+                goto buffer_fail;
+        }
 
         acomp = crypto_alloc_acomp_node(pool->tfm_name, 0, 0, cpu_to_node(cpu));
         if (IS_ERR(acomp)) {
@@ -855,12 +859,15 @@ static int zswap_cpu_comp_prepare(unsigned int cpu, struct hlist_node *node)
         acomp_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,
                                    crypto_req_done, &acomp_ctx->wait);
 
+        mutex_unlock(&acomp_ctx->mutex);
         return 0;
 
 req_fail:
         crypto_free_acomp(acomp_ctx->acomp);
 acomp_fail:
         kfree(acomp_ctx->buffer);
+buffer_fail:
+        mutex_unlock(&acomp_ctx->mutex);
         return ret;
 }
 
@@ -869,17 +876,45 @@ static int zswap_cpu_comp_dead(unsigned int cpu, struct hlist_node *node)
         struct zswap_pool *pool = hlist_entry(node, struct zswap_pool, node);
         struct crypto_acomp_ctx *acomp_ctx = per_cpu_ptr(pool->acomp_ctx, cpu);
 
+        mutex_lock(&acomp_ctx->mutex);
         if (!IS_ERR_OR_NULL(acomp_ctx)) {
                 if (!IS_ERR_OR_NULL(acomp_ctx->req))
                         acomp_request_free(acomp_ctx->req);
+                acomp_ctx->req = NULL;
                 if (!IS_ERR_OR_NULL(acomp_ctx->acomp))
                         crypto_free_acomp(acomp_ctx->acomp);
                 kfree(acomp_ctx->buffer);
         }
+        mutex_unlock(&acomp_ctx->mutex);
 
         return 0;
 }
 
+static struct crypto_acomp_ctx *acomp_ctx_get_cpu_lock(struct zswap_pool *pool)
+{
+        struct crypto_acomp_ctx *acomp_ctx;
+
+        for (;;) {
+                acomp_ctx = raw_cpu_ptr(pool->acomp_ctx);
+                mutex_lock(&acomp_ctx->mutex);
+                if (likely(acomp_ctx->req))
+                        return acomp_ctx;
+                /*
+                 * It is possible that we were migrated to a different CPU after
+                 * getting the per-CPU ctx but before the mutex was acquired. If
+                 * the old CPU got offlined, zswap_cpu_comp_dead() could have
+                 * already freed ctx->req (among other things) and set it to
+                 * NULL. Just try again on the new CPU that we ended up on.
+                 */
+                mutex_unlock(&acomp_ctx->mutex);
+        }
+}
+
+static void acomp_ctx_put_unlock(struct crypto_acomp_ctx *acomp_ctx)
+{
+        mutex_unlock(&acomp_ctx->mutex);
+}
+
 static bool zswap_compress(struct page *page, struct zswap_entry *entry,
                            struct zswap_pool *pool)
 {
@@ -893,10 +928,7 @@ static bool zswap_compress(struct page *page, struct zswap_entry *entry,
         gfp_t gfp;
         u8 *dst;
 
-        acomp_ctx = raw_cpu_ptr(pool->acomp_ctx);
-
-        mutex_lock(&acomp_ctx->mutex);
-
+        acomp_ctx = acomp_ctx_get_cpu_lock(pool);
         dst = acomp_ctx->buffer;
         sg_init_table(&input, 1);
         sg_set_page(&input, page, PAGE_SIZE, 0);
@@ -949,7 +981,7 @@ static bool zswap_compress(struct page *page, struct zswap_entry *entry,
         else if (alloc_ret)
                 zswap_reject_alloc_fail++;
 
-        mutex_unlock(&acomp_ctx->mutex);
+        acomp_ctx_put_unlock(acomp_ctx);
         return comp_ret == 0 && alloc_ret == 0;
 }
 
@@ -960,9 +992,7 @@ static void zswap_decompress(struct zswap_entry *entry, struct folio *folio)
         struct crypto_acomp_ctx *acomp_ctx;
         u8 *src;
 
-        acomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);
-        mutex_lock(&acomp_ctx->mutex);
-
+        acomp_ctx = acomp_ctx_get_cpu_lock(entry->pool);
         src = zpool_map_handle(zpool, entry->handle, ZPOOL_MM_RO);
         /*
          * If zpool_map_handle is atomic, we cannot reliably utilize its mapped buffer
@@ -986,10 +1016,10 @@ static void zswap_decompress(struct zswap_entry *entry, struct folio *folio)
         acomp_request_set_params(acomp_ctx->req, &input, &output, entry->length, PAGE_SIZE);
         BUG_ON(crypto_wait_req(crypto_acomp_decompress(acomp_ctx->req), &acomp_ctx->wait));
         BUG_ON(acomp_ctx->req->dlen != PAGE_SIZE);
-        mutex_unlock(&acomp_ctx->mutex);
 
         if (src != acomp_ctx->buffer)
                 zpool_unmap_handle(zpool, entry->handle);
+        acomp_ctx_put_unlock(acomp_ctx);
 }
 
 /*********************************
-- 
2.39.5 (Apple Git-154)
",8d29ff5d50304daa41dc3cfdda4a9d1e46cf5be1,"From 8d29ff5d50304daa41dc3cfdda4a9d1e46cf5be1 Mon Sep 17 00:00:00 2001
From: Yosry Ahmed <yosryahmed@google.com>
Date: Wed, 8 Jan 2025 22:24:41 +0000
Subject: [PATCH] mm: zswap: properly synchronize freeing resources during CPU
 hotunplug

commit 12dcb0ef540629a281533f9dedc1b6b8e14cfb65 upstream.

In zswap_compress() and zswap_decompress(), the per-CPU acomp_ctx of the
current CPU at the beginning of the operation is retrieved and used
throughout.  However, since neither preemption nor migration are disabled,
it is possible that the operation continues on a different CPU.

If the original CPU is hotunplugged while the acomp_ctx is still in use,
we run into a UAF bug as some of the resources attached to the acomp_ctx
are freed during hotunplug in zswap_cpu_comp_dead() (i.e.
acomp_ctx.buffer, acomp_ctx.req, or acomp_ctx.acomp).

The problem was introduced in commit 1ec3b5fe6eec (""mm/zswap: move to use
crypto_acomp API for hardware acceleration"") when the switch to the
crypto_acomp API was made.  Prior to that, the per-CPU crypto_comp was
retrieved using get_cpu_ptr() which disables preemption and makes sure the
CPU cannot go away from under us.  Preemption cannot be disabled with the
crypto_acomp API as a sleepable context is needed.

Use the acomp_ctx.mutex to synchronize CPU hotplug callbacks allocating
and freeing resources with compression/decompression paths.  Make sure
that acomp_ctx.req is NULL when the resources are freed.  In the
compression/decompression paths, check if acomp_ctx.req is NULL after
acquiring the mutex (meaning the CPU was offlined) and retry on the new
CPU.

The initialization of acomp_ctx.mutex is moved from the CPU hotplug
callback to the pool initialization where it belongs (where the mutex is
allocated).  In addition to adding clarity, this makes sure that CPU
hotplug cannot reinitialize a mutex that is already locked by
compression/decompression.

Previously a fix was attempted by holding cpus_read_lock() [1].  This
would have caused a potential deadlock as it is possible for code already
holding the lock to fall into reclaim and enter zswap (causing a
deadlock).  A fix was also attempted using SRCU for synchronization, but
Johannes pointed out that synchronize_srcu() cannot be used in CPU hotplug
notifiers [2].

Alternative fixes that were considered/attempted and could have worked:
- Refcounting the per-CPU acomp_ctx. This involves complexity in
  handling the race between the refcount dropping to zero in
  zswap_[de]compress() and the refcount being re-initialized when the
  CPU is onlined.
- Disabling migration before getting the per-CPU acomp_ctx [3], but
  that's discouraged and is a much bigger hammer than needed, and could
  result in subtle performance issues.

[1]https://lkml.kernel.org/20241219212437.2714151-1-yosryahmed@google.com/
[2]https://lkml.kernel.org/20250107074724.1756696-2-yosryahmed@google.com/
[3]https://lkml.kernel.org/20250107222236.2715883-2-yosryahmed@google.com/

[yosryahmed@google.com: remove comment]
  Link: https://lkml.kernel.org/r/CAJD7tkaxS1wjn+swugt8QCvQ-rVF5RZnjxwPGX17k8x9zSManA@mail.gmail.com
Link: https://lkml.kernel.org/r/20250108222441.3622031-1-yosryahmed@google.com
Fixes: 1ec3b5fe6eec (""mm/zswap: move to use crypto_acomp API for hardware acceleration"")
Signed-off-by: Yosry Ahmed <yosryahmed@google.com>
Reported-by: Johannes Weiner <hannes@cmpxchg.org>
Closes: https://lore.kernel.org/lkml/20241113213007.GB1564047@cmpxchg.org/
Reported-by: Sam Sun <samsun1006219@gmail.com>
Closes: https://lore.kernel.org/lkml/CAEkJfYMtSdM5HceNsXUDf5haghD5+o2e7Qv4OcuruL4tPg6OaQ@mail.gmail.com/
Cc: Barry Song <baohua@kernel.org>
Cc: Chengming Zhou <chengming.zhou@linux.dev>
Cc: Kanchana P Sridhar <kanchana.p.sridhar@intel.com>
Cc: Nhat Pham <nphamcs@gmail.com>
Cc: Vitaly Wool <vitalywool@gmail.com>
Cc: <stable@vger.kernel.org>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 mm/zswap.c | 58 +++++++++++++++++++++++++++++++++++++++++-------------
 1 file changed, 44 insertions(+), 14 deletions(-)

diff --git a/mm/zswap.c b/mm/zswap.c
index 0030ce8fecfc..db81fd7c3f1b 100644
--- a/mm/zswap.c
+++ b/mm/zswap.c
@@ -251,7 +251,7 @@ static struct zswap_pool *zswap_pool_create(char *type, char *compressor)
 	struct zswap_pool *pool;
 	char name[38]; /* 'zswap' + 32 char (max) num + \0 */
 	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;
-	int ret;
+	int ret, cpu;
 
 	if (!zswap_has_pool) {
 		/* if either are unset, pool initialization failed, and we
@@ -285,6 +285,9 @@ static struct zswap_pool *zswap_pool_create(char *type, char *compressor)
 		goto error;
 	}
 
+	for_each_possible_cpu(cpu)
+		mutex_init(&per_cpu_ptr(pool->acomp_ctx, cpu)->mutex);
+
 	ret = cpuhp_state_add_instance(CPUHP_MM_ZSWP_POOL_PREPARE,
 				       &pool->node);
 	if (ret)
@@ -816,11 +819,12 @@ static int zswap_cpu_comp_prepare(unsigned int cpu, struct hlist_node *node)
 	struct acomp_req *req;
 	int ret;
 
-	mutex_init(&acomp_ctx->mutex);
-
+	mutex_lock(&acomp_ctx->mutex);
 	acomp_ctx->buffer = kmalloc_node(PAGE_SIZE * 2, GFP_KERNEL, cpu_to_node(cpu));
-	if (!acomp_ctx->buffer)
-		return -ENOMEM;
+	if (!acomp_ctx->buffer) {
+		ret = -ENOMEM;
+		goto buffer_fail;
+	}
 
 	acomp = crypto_alloc_acomp_node(pool->tfm_name, 0, 0, cpu_to_node(cpu));
 	if (IS_ERR(acomp)) {
@@ -850,12 +854,15 @@ static int zswap_cpu_comp_prepare(unsigned int cpu, struct hlist_node *node)
 	acomp_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,
 				   crypto_req_done, &acomp_ctx->wait);
 
+	mutex_unlock(&acomp_ctx->mutex);
 	return 0;
 
 req_fail:
 	crypto_free_acomp(acomp_ctx->acomp);
 acomp_fail:
 	kfree(acomp_ctx->buffer);
+buffer_fail:
+	mutex_unlock(&acomp_ctx->mutex);
 	return ret;
 }
 
@@ -864,17 +871,45 @@ static int zswap_cpu_comp_dead(unsigned int cpu, struct hlist_node *node)
 	struct zswap_pool *pool = hlist_entry(node, struct zswap_pool, node);
 	struct crypto_acomp_ctx *acomp_ctx = per_cpu_ptr(pool->acomp_ctx, cpu);
 
+	mutex_lock(&acomp_ctx->mutex);
 	if (!IS_ERR_OR_NULL(acomp_ctx)) {
 		if (!IS_ERR_OR_NULL(acomp_ctx->req))
 			acomp_request_free(acomp_ctx->req);
+		acomp_ctx->req = NULL;
 		if (!IS_ERR_OR_NULL(acomp_ctx->acomp))
 			crypto_free_acomp(acomp_ctx->acomp);
 		kfree(acomp_ctx->buffer);
 	}
+	mutex_unlock(&acomp_ctx->mutex);
 
 	return 0;
 }
 
+static struct crypto_acomp_ctx *acomp_ctx_get_cpu_lock(struct zswap_pool *pool)
+{
+	struct crypto_acomp_ctx *acomp_ctx;
+
+	for (;;) {
+		acomp_ctx = raw_cpu_ptr(pool->acomp_ctx);
+		mutex_lock(&acomp_ctx->mutex);
+		if (likely(acomp_ctx->req))
+			return acomp_ctx;
+		/*
+		 * It is possible that we were migrated to a different CPU after
+		 * getting the per-CPU ctx but before the mutex was acquired. If
+		 * the old CPU got offlined, zswap_cpu_comp_dead() could have
+		 * already freed ctx->req (among other things) and set it to
+		 * NULL. Just try again on the new CPU that we ended up on.
+		 */
+		mutex_unlock(&acomp_ctx->mutex);
+	}
+}
+
+static void acomp_ctx_put_unlock(struct crypto_acomp_ctx *acomp_ctx)
+{
+	mutex_unlock(&acomp_ctx->mutex);
+}
+
 static bool zswap_compress(struct folio *folio, struct zswap_entry *entry)
 {
 	struct crypto_acomp_ctx *acomp_ctx;
@@ -887,10 +922,7 @@ static bool zswap_compress(struct folio *folio, struct zswap_entry *entry)
 	gfp_t gfp;
 	u8 *dst;
 
-	acomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);
-
-	mutex_lock(&acomp_ctx->mutex);
-
+	acomp_ctx = acomp_ctx_get_cpu_lock(entry->pool);
 	dst = acomp_ctx->buffer;
 	sg_init_table(&input, 1);
 	sg_set_folio(&input, folio, PAGE_SIZE, 0);
@@ -943,7 +975,7 @@ static bool zswap_compress(struct folio *folio, struct zswap_entry *entry)
 	else if (alloc_ret)
 		zswap_reject_alloc_fail++;
 
-	mutex_unlock(&acomp_ctx->mutex);
+	acomp_ctx_put_unlock(acomp_ctx);
 	return comp_ret == 0 && alloc_ret == 0;
 }
 
@@ -954,9 +986,7 @@ static void zswap_decompress(struct zswap_entry *entry, struct folio *folio)
 	struct crypto_acomp_ctx *acomp_ctx;
 	u8 *src;
 
-	acomp_ctx = raw_cpu_ptr(entry->pool->acomp_ctx);
-	mutex_lock(&acomp_ctx->mutex);
-
+	acomp_ctx = acomp_ctx_get_cpu_lock(entry->pool);
 	src = zpool_map_handle(zpool, entry->handle, ZPOOL_MM_RO);
 	/*
 	 * If zpool_map_handle is atomic, we cannot reliably utilize its mapped buffer
@@ -980,10 +1010,10 @@ static void zswap_decompress(struct zswap_entry *entry, struct folio *folio)
 	acomp_request_set_params(acomp_ctx->req, &input, &output, entry->length, PAGE_SIZE);
 	BUG_ON(crypto_wait_req(crypto_acomp_decompress(acomp_ctx->req), &acomp_ctx->wait));
 	BUG_ON(acomp_ctx->req->dlen != PAGE_SIZE);
-	mutex_unlock(&acomp_ctx->mutex);
 
 	if (src != acomp_ctx->buffer)
 		zpool_unmap_handle(zpool, entry->handle);
+	acomp_ctx_put_unlock(acomp_ctx);
 }
 
 /*********************************
-- 
2.39.5 (Apple Git-154)

",patching file mm/zswap.c\nHunk #3 succeeded at 819 (offset -5 lines).\nHunk #4 succeeded at 854 (offset -5 lines).\nHunk #5 FAILED at 876.\nHunk #6 FAILED at 900.\nHunk #7 succeeded at 950 (offset -6 lines).\nHunk #8 succeeded at 961 (offset -6 lines).\nHunk #9 succeeded at 985 (offset -6 lines).\n2 out of 9 hunks FAILED -- saving rejects to file mm/zswap.c.rej,61,1,26,224,1,OTHER,OTHER
CVE-2025-21836,8802766324e1f5d414a81ac43365c20142e85603,"From 8802766324e1f5d414a81ac43365c20142e85603 Mon Sep 17 00:00:00 2001
From: Pavel Begunkov <asml.silence@gmail.com>
Date: Wed, 12 Feb 2025 13:46:46 +0000
Subject: [PATCH] io_uring/kbuf: reallocate buf lists on upgrade

IORING_REGISTER_PBUF_RING can reuse an old struct io_buffer_list if it
was created for legacy selected buffer and has been emptied. It violates
the requirement that most of the field should stay stable after publish.
Always reallocate it instead.

Cc: stable@vger.kernel.org
Reported-by: Pumpkin Chang <pumpkin@devco.re>
Fixes: 2fcabce2d7d34 (""io_uring: disallow mixed provided buffer group registrations"")
Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/kbuf.c | 16 ++++++++++++----
 1 file changed, 12 insertions(+), 4 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 04bf493eecae..8e72de7712ac 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -415,6 +415,13 @@ void io_destroy_buffers(struct io_ring_ctx *ctx)
         }
 }
 
+static void io_destroy_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
+{
+        scoped_guard(mutex, &ctx->mmap_lock)
+                WARN_ON_ONCE(xa_erase(&ctx->io_bl_xa, bl->bgid) != bl);
+        io_put_bl(ctx, bl);
+}
+
 int io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
         struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
@@ -636,12 +643,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
                 /* if mapped buffer ring OR classic exists, don't allow */
                 if (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))
                         return -EEXIST;
-        } else {
-                free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
-                if (!bl)
-                        return -ENOMEM;
+                io_destroy_bl(ctx, bl);
         }
 
+        free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
+        if (!bl)
+                return -ENOMEM;
+
         mmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;
         ring_size = flex_array_size(br, bufs, reg.ring_entries);
 
-- 
2.39.5 (Apple Git-154)
",146a185f6c05ee263db715f860620606303c4633,"From 146a185f6c05ee263db715f860620606303c4633 Mon Sep 17 00:00:00 2001
From: Pavel Begunkov <asml.silence@gmail.com>
Date: Wed, 12 Feb 2025 13:46:46 +0000
Subject: [PATCH] io_uring/kbuf: reallocate buf lists on upgrade

commit 8802766324e1f5d414a81ac43365c20142e85603 upstream.

IORING_REGISTER_PBUF_RING can reuse an old struct io_buffer_list if it
was created for legacy selected buffer and has been emptied. It violates
the requirement that most of the field should stay stable after publish.
Always reallocate it instead.

Cc: stable@vger.kernel.org
Reported-by: Pumpkin Chang <pumpkin@devco.re>
Fixes: 2fcabce2d7d34 (""io_uring: disallow mixed provided buffer group registrations"")
Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/kbuf.c | 15 +++++++++++----
 1 file changed, 11 insertions(+), 4 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 702c08c26cd4..b6fbae874f27 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -301,6 +301,12 @@ void io_destroy_buffers(struct io_ring_ctx *ctx)
         }
 }
 
+static void io_destroy_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
+{
+        xa_erase(&ctx->io_bl_xa, bl->bgid);
+        io_put_bl(ctx, bl);
+}
+
 int io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
         struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
@@ -642,12 +648,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
                 /* if mapped buffer ring OR classic exists, don't allow */
                 if (bl->is_mapped || !list_empty(&bl->buf_list))
                         return -EEXIST;
-        } else {
-                free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
-                if (!bl)
-                        return -ENOMEM;
+                io_destroy_bl(ctx, bl);
         }
 
+        free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
+        if (!bl)
+                return -ENOMEM;
+
         if (!(reg.flags & IOU_PBUF_RING_MMAP))
                 ret = io_pin_pbuf_ring(&reg, bl);
         else
-- 
2.39.5 (Apple Git-154)

",patching file io_uring/kbuf.c\nHunk #1 succeeded at 301 (offset -114 lines).\nHunk #2 FAILED at 643.\n1 out of 2 hunks FAILED -- saving rejects to file io_uring/kbuf.c.rej,17,1,9,57,1,SEMANTIC,COMMENTS
CVE-2025-21836,8802766324e1f5d414a81ac43365c20142e85603,"From 8802766324e1f5d414a81ac43365c20142e85603 Mon Sep 17 00:00:00 2001
From: Pavel Begunkov <asml.silence@gmail.com>
Date: Wed, 12 Feb 2025 13:46:46 +0000
Subject: [PATCH] io_uring/kbuf: reallocate buf lists on upgrade

IORING_REGISTER_PBUF_RING can reuse an old struct io_buffer_list if it
was created for legacy selected buffer and has been emptied. It violates
the requirement that most of the field should stay stable after publish.
Always reallocate it instead.

Cc: stable@vger.kernel.org
Reported-by: Pumpkin Chang <pumpkin@devco.re>
Fixes: 2fcabce2d7d34 (""io_uring: disallow mixed provided buffer group registrations"")
Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/kbuf.c | 16 ++++++++++++----
 1 file changed, 12 insertions(+), 4 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 04bf493eecae..8e72de7712ac 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -415,6 +415,13 @@ void io_destroy_buffers(struct io_ring_ctx *ctx)
 	}
 }
 
+static void io_destroy_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
+{
+	scoped_guard(mutex, &ctx->mmap_lock)
+		WARN_ON_ONCE(xa_erase(&ctx->io_bl_xa, bl->bgid) != bl);
+	io_put_bl(ctx, bl);
+}
+
 int io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
 	struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
@@ -636,12 +643,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 		/* if mapped buffer ring OR classic exists, don't allow */
 		if (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))
 			return -EEXIST;
-	} else {
-		free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
-		if (!bl)
-			return -ENOMEM;
+		io_destroy_bl(ctx, bl);
 	}
 
+	free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
+	if (!bl)
+		return -ENOMEM;
+
 	mmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;
 	ring_size = flex_array_size(br, bufs, reg.ring_entries);
 
-- 
2.39.5 (Apple Git-154)
",7d0dc28dae836caf7645fef62a10befc624dd17b,"From 7d0dc28dae836caf7645fef62a10befc624dd17b Mon Sep 17 00:00:00 2001
From: Pavel Begunkov <asml.silence@gmail.com>
Date: Wed, 12 Feb 2025 13:46:46 +0000
Subject: [PATCH] io_uring/kbuf: reallocate buf lists on upgrade

commit 8802766324e1f5d414a81ac43365c20142e85603 upstream.

IORING_REGISTER_PBUF_RING can reuse an old struct io_buffer_list if it
was created for legacy selected buffer and has been emptied. It violates
the requirement that most of the field should stay stable after publish.
Always reallocate it instead.

Cc: stable@vger.kernel.org
Reported-by: Pumpkin Chang <pumpkin@devco.re>
Fixes: 2fcabce2d7d34 (""io_uring: disallow mixed provided buffer group registrations"")
Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/kbuf.c | 15 +++++++++++----
 1 file changed, 11 insertions(+), 4 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index eec5eb7de843..e1895952066e 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -420,6 +420,12 @@ void io_destroy_buffers(struct io_ring_ctx *ctx)
         }
 }
 
+static void io_destroy_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
+{
+        xa_erase(&ctx->io_bl_xa, bl->bgid);
+        io_put_bl(ctx, bl);
+}
+
 int io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
         struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
@@ -717,12 +723,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
                 /* if mapped buffer ring OR classic exists, don't allow */
                 if (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))
                         return -EEXIST;
-        } else {
-                free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
-                if (!bl)
-                        return -ENOMEM;
+                io_destroy_bl(ctx, bl);
         }
 
+        free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
+        if (!bl)
+                return -ENOMEM;
+
         if (!(reg.flags & IOU_PBUF_RING_MMAP))
                 ret = io_pin_pbuf_ring(&reg, bl);
         else
-- 
2.39.5 (Apple Git-154)

",patching file io_uring/kbuf.c\nHunk #1 succeeded at 420 (offset 5 lines).\nHunk #2 FAILED at 643.\n1 out of 2 hunks FAILED -- saving rejects to file io_uring/kbuf.c.rej,17,1,9,57,1,FORMATTING,COMMENTS
CVE-2025-21836,8802766324e1f5d414a81ac43365c20142e85603,"From 8802766324e1f5d414a81ac43365c20142e85603 Mon Sep 17 00:00:00 2001
From: Pavel Begunkov <asml.silence@gmail.com>
Date: Wed, 12 Feb 2025 13:46:46 +0000
Subject: [PATCH] io_uring/kbuf: reallocate buf lists on upgrade

IORING_REGISTER_PBUF_RING can reuse an old struct io_buffer_list if it
was created for legacy selected buffer and has been emptied. It violates
the requirement that most of the field should stay stable after publish.
Always reallocate it instead.

Cc: stable@vger.kernel.org
Reported-by: Pumpkin Chang <pumpkin@devco.re>
Fixes: 2fcabce2d7d34 (""io_uring: disallow mixed provided buffer group registrations"")
Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/kbuf.c | 16 ++++++++++++----
 1 file changed, 12 insertions(+), 4 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 04bf493eecae..8e72de7712ac 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -415,6 +415,13 @@ void io_destroy_buffers(struct io_ring_ctx *ctx)
 	}
 }
 
+static void io_destroy_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
+{
+	scoped_guard(mutex, &ctx->mmap_lock)
+		WARN_ON_ONCE(xa_erase(&ctx->io_bl_xa, bl->bgid) != bl);
+	io_put_bl(ctx, bl);
+}
+
 int io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
 	struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
@@ -636,12 +643,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 		/* if mapped buffer ring OR classic exists, don't allow */
 		if (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))
 			return -EEXIST;
-	} else {
-		free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
-		if (!bl)
-			return -ENOMEM;
+		io_destroy_bl(ctx, bl);
 	}
 
+	free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
+	if (!bl)
+		return -ENOMEM;
+
 	mmap_offset = (unsigned long)reg.bgid << IORING_OFF_PBUF_SHIFT;
 	ring_size = flex_array_size(br, bufs, reg.ring_entries);
 
-- 
2.39.5 (Apple Git-154)
",2a5febbef40ce968e295a7aeaa5d5cbd9e3e5ad4,"From 2a5febbef40ce968e295a7aeaa5d5cbd9e3e5ad4 Mon Sep 17 00:00:00 2001
From: Pavel Begunkov <asml.silence@gmail.com>
Date: Wed, 12 Feb 2025 13:46:46 +0000
Subject: [PATCH] io_uring/kbuf: reallocate buf lists on upgrade

commit 8802766324e1f5d414a81ac43365c20142e85603 upstream.

IORING_REGISTER_PBUF_RING can reuse an old struct io_buffer_list if it
was created for legacy selected buffer and has been emptied. It violates
the requirement that most of the field should stay stable after publish.
Always reallocate it instead.

Cc: stable@vger.kernel.org
Reported-by: Pumpkin Chang <pumpkin@devco.re>
Fixes: 2fcabce2d7d34 (""io_uring: disallow mixed provided buffer group registrations"")
Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/kbuf.c | 15 +++++++++++----
 1 file changed, 11 insertions(+), 4 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index eec5eb7de843..e1895952066e 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -420,6 +420,12 @@ void io_destroy_buffers(struct io_ring_ctx *ctx)
         }
 }
 
+static void io_destroy_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
+{
+        xa_erase(&ctx->io_bl_xa, bl->bgid);
+        io_put_bl(ctx, bl);
+}
+
 int io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
         struct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);
@@ -717,12 +723,13 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
                 /* if mapped buffer ring OR classic exists, don't allow */
                 if (bl->flags & IOBL_BUF_RING || !list_empty(&bl->buf_list))
                         return -EEXIST;
-        } else {
-                free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
-                if (!bl)
-                        return -ENOMEM;
+                io_destroy_bl(ctx, bl);
         }
 
+        free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);
+        if (!bl)
+                return -ENOMEM;
+
         if (!(reg.flags & IOU_PBUF_RING_MMAP))
                 ret = io_pin_pbuf_ring(&reg, bl);
         else
-- 
2.39.5 (Apple Git-154)

",patching file io_uring/kbuf.c\nHunk #1 succeeded at 420 (offset 5 lines).\nHunk #2 FAILED at 643.\n1 out of 2 hunks FAILED -- saving rejects to file io_uring/kbuf.c.rej,17,1,9,57,1,DISJOINT,COMMENTS
CVE-2025-21639,9fc17b76fc70763780aa78b38fcf4742384044a5,"From 9fc17b76fc70763780aa78b38fcf4742384044a5 Mon Sep 17 00:00:00 2001
From: ""Matthieu Baerts (NGI0)"" <matttbe@kernel.org>
Date: Wed, 8 Jan 2025 16:34:33 +0100
Subject: [PATCH] sctp: sysctl: rto_min/max: avoid using current->nsproxy

As mentioned in a previous commit of this series, using the 'net'
structure via 'current' is not recommended for different reasons:

- Inconsistency: getting info from the reader's/writer's netns vs only
  from the opener's netns.

- current->nsproxy can be NULL in some cases, resulting in an 'Oops'
  (null-ptr-deref), e.g. when the current task is exiting, as spotted by
  syzbot [1] using acct(2).

The 'net' structure can be obtained from the table->data using
container_of().

Note that table->data could also be used directly, as this is the only
member needed from the 'net' structure, but that would increase the size
of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.

Fixes: 4f3fdf3bc59c (""sctp: add check rto_min and rto_max in sysctl"")
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]
Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sctp/sysctl.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c
index 9848d19630a4..a5285815264d 100644
--- a/net/sctp/sysctl.c
+++ b/net/sctp/sysctl.c
@@ -433,7 +433,7 @@ static int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,
 static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,
                                 void *buffer, size_t *lenp, loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_min);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
@@ -461,7 +461,7 @@ static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,
 static int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,
                                 void *buffer, size_t *lenp, loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_max);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
-- 
2.39.5 (Apple Git-154)
",c8d179f3b1c1d60bf4484f50aa67b4c70f91bff9,"From c8d179f3b1c1d60bf4484f50aa67b4c70f91bff9 Mon Sep 17 00:00:00 2001
From: ""Matthieu Baerts (NGI0)"" <matttbe@kernel.org>
Date: Wed, 8 Jan 2025 16:34:33 +0100
Subject: [PATCH] sctp: sysctl: rto_min/max: avoid using current->nsproxy

[ Upstream commit 9fc17b76fc70763780aa78b38fcf4742384044a5 ]

As mentioned in a previous commit of this series, using the 'net'
structure via 'current' is not recommended for different reasons:

- Inconsistency: getting info from the reader's/writer's netns vs only
  from the opener's netns.

- current->nsproxy can be NULL in some cases, resulting in an 'Oops'
  (null-ptr-deref), e.g. when the current task is exiting, as spotted by
  syzbot [1] using acct(2).

The 'net' structure can be obtained from the table->data using
container_of().

Note that table->data could also be used directly, as this is the only
member needed from the 'net' structure, but that would increase the size
of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.

Fixes: 4f3fdf3bc59c (""sctp: add check rto_min and rto_max in sysctl"")
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]
Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 net/sctp/sysctl.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c
index 4513d8d45e55..7777c0096a38 100644
--- a/net/sctp/sysctl.c
+++ b/net/sctp/sysctl.c
@@ -372,7 +372,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,
                                 void __user *buffer, size_t *lenp,
                                 loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_min);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
@@ -401,7 +401,7 @@ static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,
                                 void __user *buffer, size_t *lenp,
                                 loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_max);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
-- 
2.39.5 (Apple Git-154)

",patching file net/sctp/sysctl.c\nHunk #1 succeeded at 401 with fuzz 2 (offset -32 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej,7,2,12,57,1,COMMENTS,COMMENTS
CVE-2025-21640,9fc17b76fc70763780aa78b38fcf4742384044a6,"From 9fc17b76fc70763780aa78b38fcf4742384044a5 Mon Sep 17 00:00:00 2001
From: ""Matthieu Baerts (NGI0)"" <matttbe@kernel.org>
Date: Wed, 8 Jan 2025 16:34:33 +0100
Subject: [PATCH] sctp: sysctl: rto_min/max: avoid using current->nsproxy

As mentioned in a previous commit of this series, using the 'net'
structure via 'current' is not recommended for different reasons:

- Inconsistency: getting info from the reader's/writer's netns vs only
  from the opener's netns.

- current->nsproxy can be NULL in some cases, resulting in an 'Oops'
  (null-ptr-deref), e.g. when the current task is exiting, as spotted by
  syzbot [1] using acct(2).

The 'net' structure can be obtained from the table->data using
container_of().

Note that table->data could also be used directly, as this is the only
member needed from the 'net' structure, but that would increase the size
of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.

Fixes: 4f3fdf3bc59c (""sctp: add check rto_min and rto_max in sysctl"")
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]
Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sctp/sysctl.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c
index 9848d19630a4..a5285815264d 100644
--- a/net/sctp/sysctl.c
+++ b/net/sctp/sysctl.c
@@ -433,7 +433,7 @@ static int proc_sctp_do_hmac_alg(const struct ctl_table *ctl, int write,
 static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,
                                 void *buffer, size_t *lenp, loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_min);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
@@ -461,7 +461,7 @@ static int proc_sctp_do_rto_min(const struct ctl_table *ctl, int write,
 static int proc_sctp_do_rto_max(const struct ctl_table *ctl, int write,
                                 void *buffer, size_t *lenp, loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_max);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
-- 
2.39.5 (Apple Git-154)
",246428bfb9e7db15c5cd08e1d0eca41b65af2b06,"From 246428bfb9e7db15c5cd08e1d0eca41b65af2b06 Mon Sep 17 00:00:00 2001
From: ""Matthieu Baerts (NGI0)"" <matttbe@kernel.org>
Date: Wed, 8 Jan 2025 16:34:33 +0100
Subject: [PATCH] sctp: sysctl: rto_min/max: avoid using current->nsproxy

[ Upstream commit 9fc17b76fc70763780aa78b38fcf4742384044a5 ]

As mentioned in a previous commit of this series, using the 'net'
structure via 'current' is not recommended for different reasons:

- Inconsistency: getting info from the reader's/writer's netns vs only
  from the opener's netns.

- current->nsproxy can be NULL in some cases, resulting in an 'Oops'
  (null-ptr-deref), e.g. when the current task is exiting, as spotted by
  syzbot [1] using acct(2).

The 'net' structure can be obtained from the table->data using
container_of().

Note that table->data could also be used directly, as this is the only
member needed from the 'net' structure, but that would increase the size
of this fix, to use '*data' everywhere 'net->sctp.rto_min/max' is used.

Fixes: 4f3fdf3bc59c (""sctp: add check rto_min and rto_max in sysctl"")
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/67769ecb.050a0220.3a8527.003f.GAE@google.com [1]
Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Link: https://patch.msgid.link/20250108-net-sysctl-current-nsproxy-v1-5-5df34b2083e8@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 net/sctp/sysctl.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/net/sctp/sysctl.c b/net/sctp/sysctl.c
index 8be80096fbb6..82b736843c9d 100644
--- a/net/sctp/sysctl.c
+++ b/net/sctp/sysctl.c
@@ -396,7 +396,7 @@ static int proc_sctp_do_hmac_alg(struct ctl_table *ctl, int write,
 static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,
                                 void *buffer, size_t *lenp, loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_min);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
@@ -424,7 +424,7 @@ static int proc_sctp_do_rto_min(struct ctl_table *ctl, int write,
 static int proc_sctp_do_rto_max(struct ctl_table *ctl, int write,
                                 void *buffer, size_t *lenp, loff_t *ppos)
 {
-        struct net *net = current->nsproxy->net_ns;
+        struct net *net = container_of(ctl->data, struct net, sctp.rto_max);
         unsigned int min = *(unsigned int *) ctl->extra1;
         unsigned int max = *(unsigned int *) ctl->extra2;
         struct ctl_table tbl;
-- 
2.39.5 (Apple Git-154)

",patching file net/sctp/sysctl.c\nHunk #1 succeeded at 424 with fuzz 1 (offset -9 lines).\nHunk #2 FAILED at 461.\n1 out of 2 hunks FAILED -- saving rejects to file net/sctp/sysctl.c.rej,7,2,12,57,1,OTHER,COMMENTS
CVE-2025-21647,737d4d91d35b5f7fa5bb442651472277318b0bfd,"From 737d4d91d35b5f7fa5bb442651472277318b0bfd Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@redhat.com>
Date: Tue, 7 Jan 2025 13:01:05 +0100
Subject: [PATCH] sched: sch_cake: add bounds checks to host bulk flow fairness
 counts
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Even though we fixed a logic error in the commit cited below, syzbot
still managed to trigger an underflow of the per-host bulk flow
counters, leading to an out of bounds memory access.

To avoid any such logic errors causing out of bounds memory accesses,
this commit factors out all accesses to the per-host bulk flow counters
to a series of helpers that perform bounds-checking before any
increments and decrements. This also has the benefit of improving
readability by moving the conditional checks for the flow mode into
these helpers, instead of having them spread out throughout the
code (which was the cause of the original logic error).

As part of this change, the flow quantum calculation is consolidated
into a helper function, which means that the dithering applied to the
ost load scaling is now applied both in the DRR rotation and when a
sparse flow's quantum is first initiated. The only user-visible effect
of this is that the maximum packet size that can be sent while a flow
stays sparse will now vary with +/- one byte in some cases. This should
not make a noticeable difference in practice, and thus it's not worth
complicating the code to preserve the old behaviour.

Fixes: 546ea84d07e3 (""sched: sch_cake: fix bulk flow accounting logic for host fairness"")
Reported-by: syzbot+f63600d288bfb7057424@syzkaller.appspotmail.com
Signed-off-by: Toke Hiland-Jrgensen <toke@redhat.com>
Acked-by: Dave Taht <dave.taht@gmail.com>
Link: https://patch.msgid.link/20250107120105.70685-1-toke@redhat.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_cake.c | 140 +++++++++++++++++++++++--------------------
 1 file changed, 75 insertions(+), 65 deletions(-)

diff --git a/net/sched/sch_cake.c b/net/sched/sch_cake.c
index 8d8b2db4653c..2c2e2a67f3b2 100644
--- a/net/sched/sch_cake.c
+++ b/net/sched/sch_cake.c
@@ -627,6 +627,63 @@ static bool cake_ddst(int flow_mode)
         return (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;
 }
 
+static void cake_dec_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count))
+                q->hosts[flow->srchost].srchost_bulk_flow_count--;
+}
+
+static void cake_inc_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->srchost].srchost_bulk_flow_count++;
+}
+
+static void cake_dec_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count--;
+}
+
+static void cake_inc_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count++;
+}
+
+static u16 cake_get_flow_quantum(struct cake_tin_data *q,
+                                 struct cake_flow *flow,
+                                 int flow_mode)
+{
+        u16 host_load = 1;
+
+        if (cake_dsrc(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->srchost].srchost_bulk_flow_count);
+
+        if (cake_ddst(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->dsthost].dsthost_bulk_flow_count);
+
+        /* The get_random_u16() is a way to apply dithering to avoid
+         * accumulating roundoff errors
+         */
+        return (q->flow_quantum * quantum_div[host_load] +
+                get_random_u16()) >> 16;
+}
+
 static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                      int flow_mode, u16 flow_override, u16 host_override)
 {
@@ -773,10 +830,8 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                 allocate_dst = cake_ddst(flow_mode);
 
                 if (q->flows[outer_hash + k].set == CAKE_SET_BULK) {
-                        if (allocate_src)
-                                q->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;
-                        if (allocate_dst)
-                                q->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;
+                        cake_dec_srchost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
+                        cake_dec_dsthost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
                 }
 found:
                 /* reserve queue for future packets in same flow */
@@ -801,9 +856,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].srchost_tag = srchost_hash;
 found_src:
                         srchost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[srchost_idx].srchost_bulk_flow_count++;
                         q->flows[reduced_hash].srchost = srchost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_srchost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
 
                 if (allocate_dst) {
@@ -824,9 +880,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].dsthost_tag = dsthost_hash;
 found_dst:
                         dsthost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[dsthost_idx].dsthost_bulk_flow_count++;
                         q->flows[reduced_hash].dsthost = dsthost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_dsthost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
         }
 
@@ -1839,10 +1896,6 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 
         /* flowchain */
         if (!flow->set || flow->set == CAKE_SET_DECAYING) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-                u16 host_load = 1;
-
                 if (!flow->set) {
                         list_add_tail(&flow->flowchain, &b->new_flows);
                 } else {
@@ -1852,18 +1905,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 flow->set = CAKE_SET_SPARSE;
                 b->sparse_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                flow->deficit = (b->flow_quantum *
-                                 quantum_div[host_load]) >> 16;
+                flow->deficit = cake_get_flow_quantum(b, flow, q->flow_mode);
         } else if (flow->set == CAKE_SET_SPARSE_WAIT) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-
                 /* this flow was empty, accounted as a sparse flow, but actually
                  * in the bulk rotation.
                  */
@@ -1871,12 +1914,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 b->sparse_flow_count--;
                 b->bulk_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        srchost->srchost_bulk_flow_count++;
-
-                if (cake_ddst(q->flow_mode))
-                        dsthost->dsthost_bulk_flow_count++;
-
+                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
         }
 
         if (q->buffer_used > q->buffer_max_used)
@@ -1933,13 +1972,11 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 {
         struct cake_sched_data *q = qdisc_priv(sch);
         struct cake_tin_data *b = &q->tins[q->cur_tin];
-        struct cake_host *srchost, *dsthost;
         ktime_t now = ktime_get();
         struct cake_flow *flow;
         struct list_head *head;
         bool first_flow = true;
         struct sk_buff *skb;
-        u16 host_load;
         u64 delay;
         u32 len;
 
@@ -2039,11 +2076,6 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
         q->cur_flow = flow - b->flows;
         first_flow = false;
 
-        /* triple isolation (modified DRR++) */
-        srchost = &b->hosts[flow->srchost];
-        dsthost = &b->hosts[flow->dsthost];
-        host_load = 1;
-
         /* flow isolation (DRR++) */
         if (flow->deficit <= 0) {
                 /* Keep all flows with deficits out of the sparse and decaying
@@ -2055,11 +2087,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 b->sparse_flow_count--;
                                 b->bulk_flow_count++;
 
-                                if (cake_dsrc(q->flow_mode))
-                                        srchost->srchost_bulk_flow_count++;
-
-                                if (cake_ddst(q->flow_mode))
-                                        dsthost->dsthost_bulk_flow_count++;
+                                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                 flow->set = CAKE_SET_BULK;
                         } else {
@@ -2071,19 +2100,7 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                         }
                 }
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                WARN_ON(host_load > CAKE_QUEUES);
-
-                /* The get_random_u16() is a way to apply dithering to avoid
-                 * accumulating roundoff errors
-                 */
-                flow->deficit += (b->flow_quantum * quantum_div[host_load] +
-                                  get_random_u16()) >> 16;
+                flow->deficit += cake_get_flow_quantum(b, flow, q->flow_mode);
                 list_move_tail(&flow->flowchain, &b->old_flows);
 
                 goto retry;
@@ -2107,11 +2124,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                         b->decaying_flow_count++;
                                 } else if (flow->set == CAKE_SET_SPARSE ||
@@ -2129,12 +2143,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 else if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
-
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
                                 } else
                                         b->decaying_flow_count--;
 
-- 
2.39.5 (Apple Git-154)
",44fe1efb4961c1a5ccab16bb579dfc6b308ad58b,"From 44fe1efb4961c1a5ccab16bb579dfc6b308ad58b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@redhat.com>
Date: Tue, 7 Jan 2025 13:01:05 +0100
Subject: [PATCH] sched: sch_cake: add bounds checks to host bulk flow fairness
 counts
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

commit 737d4d91d35b5f7fa5bb442651472277318b0bfd upstream.

Even though we fixed a logic error in the commit cited below, syzbot
still managed to trigger an underflow of the per-host bulk flow
counters, leading to an out of bounds memory access.

To avoid any such logic errors causing out of bounds memory accesses,
this commit factors out all accesses to the per-host bulk flow counters
to a series of helpers that perform bounds-checking before any
increments and decrements. This also has the benefit of improving
readability by moving the conditional checks for the flow mode into
these helpers, instead of having them spread out throughout the
code (which was the cause of the original logic error).

As part of this change, the flow quantum calculation is consolidated
into a helper function, which means that the dithering applied to the
ost load scaling is now applied both in the DRR rotation and when a
sparse flow's quantum is first initiated. The only user-visible effect
of this is that the maximum packet size that can be sent while a flow
stays sparse will now vary with +/- one byte in some cases. This should
not make a noticeable difference in practice, and thus it's not worth
complicating the code to preserve the old behaviour.

Fixes: 546ea84d07e3 (""sched: sch_cake: fix bulk flow accounting logic for host fairness"")
Reported-by: syzbot+f63600d288bfb7057424@syzkaller.appspotmail.com
Signed-off-by: Toke Hiland-Jrgensen <toke@redhat.com>
Acked-by: Dave Taht <dave.taht@gmail.com>
Link: https://patch.msgid.link/20250107120105.70685-1-toke@redhat.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
[Hagar: needed contextual fixes due to missing commit 7e3cf0843fe5]
Signed-off-by: Hagar Hemdan <hagarhem@amazon.com>
Reviewed-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_cake.c | 140 +++++++++++++++++++++++--------------------
 1 file changed, 75 insertions(+), 65 deletions(-)

diff --git a/net/sched/sch_cake.c b/net/sched/sch_cake.c
index f2a49bccb5ef..fc96ec46e6f6 100644
--- a/net/sched/sch_cake.c
+++ b/net/sched/sch_cake.c
@@ -622,6 +622,63 @@ static bool cake_ddst(int flow_mode)
 	return (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;
 }
 
+static void cake_dec_srchost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_dsrc(flow_mode) &&
+		   q->hosts[flow->srchost].srchost_bulk_flow_count))
+		q->hosts[flow->srchost].srchost_bulk_flow_count--;
+}
+
+static void cake_inc_srchost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_dsrc(flow_mode) &&
+		   q->hosts[flow->srchost].srchost_bulk_flow_count < CAKE_QUEUES))
+		q->hosts[flow->srchost].srchost_bulk_flow_count++;
+}
+
+static void cake_dec_dsthost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_ddst(flow_mode) &&
+		   q->hosts[flow->dsthost].dsthost_bulk_flow_count))
+		q->hosts[flow->dsthost].dsthost_bulk_flow_count--;
+}
+
+static void cake_inc_dsthost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_ddst(flow_mode) &&
+		   q->hosts[flow->dsthost].dsthost_bulk_flow_count < CAKE_QUEUES))
+		q->hosts[flow->dsthost].dsthost_bulk_flow_count++;
+}
+
+static u16 cake_get_flow_quantum(struct cake_tin_data *q,
+				 struct cake_flow *flow,
+				 int flow_mode)
+{
+	u16 host_load = 1;
+
+	if (cake_dsrc(flow_mode))
+		host_load = max(host_load,
+				q->hosts[flow->srchost].srchost_bulk_flow_count);
+
+	if (cake_ddst(flow_mode))
+		host_load = max(host_load,
+				q->hosts[flow->dsthost].dsthost_bulk_flow_count);
+
+	/* The shifted prandom_u32() is a way to apply dithering to avoid
+	 * accumulating roundoff errors
+	 */
+	return (q->flow_quantum * quantum_div[host_load] +
+		(prandom_u32() >> 16)) >> 16;
+}
+
 static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 		     int flow_mode, u16 flow_override, u16 host_override)
 {
@@ -753,10 +810,8 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 		allocate_dst = cake_ddst(flow_mode);
 
 		if (q->flows[outer_hash + k].set == CAKE_SET_BULK) {
-			if (allocate_src)
-				q->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;
-			if (allocate_dst)
-				q->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;
+			cake_dec_srchost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
+			cake_dec_dsthost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
 		}
 found:
 		/* reserve queue for future packets in same flow */
@@ -781,9 +836,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 			q->hosts[outer_hash + k].srchost_tag = srchost_hash;
 found_src:
 			srchost_idx = outer_hash + k;
-			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-				q->hosts[srchost_idx].srchost_bulk_flow_count++;
 			q->flows[reduced_hash].srchost = srchost_idx;
+
+			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+				cake_inc_srchost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
 		}
 
 		if (allocate_dst) {
@@ -804,9 +860,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 			q->hosts[outer_hash + k].dsthost_tag = dsthost_hash;
 found_dst:
 			dsthost_idx = outer_hash + k;
-			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-				q->hosts[dsthost_idx].dsthost_bulk_flow_count++;
 			q->flows[reduced_hash].dsthost = dsthost_idx;
+
+			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+				cake_inc_dsthost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
 		}
 	}
 
@@ -1821,10 +1878,6 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 
 	/* flowchain */
 	if (!flow->set || flow->set == CAKE_SET_DECAYING) {
-		struct cake_host *srchost = &b->hosts[flow->srchost];
-		struct cake_host *dsthost = &b->hosts[flow->dsthost];
-		u16 host_load = 1;
-
 		if (!flow->set) {
 			list_add_tail(&flow->flowchain, &b->new_flows);
 		} else {
@@ -1834,18 +1887,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 		flow->set = CAKE_SET_SPARSE;
 		b->sparse_flow_count++;
 
-		if (cake_dsrc(q->flow_mode))
-			host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-		if (cake_ddst(q->flow_mode))
-			host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-		flow->deficit = (b->flow_quantum *
-				 quantum_div[host_load]) >> 16;
+		flow->deficit = cake_get_flow_quantum(b, flow, q->flow_mode);
 	} else if (flow->set == CAKE_SET_SPARSE_WAIT) {
-		struct cake_host *srchost = &b->hosts[flow->srchost];
-		struct cake_host *dsthost = &b->hosts[flow->dsthost];
-
 		/* this flow was empty, accounted as a sparse flow, but actually
 		 * in the bulk rotation.
 		 */
@@ -1853,12 +1896,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 		b->sparse_flow_count--;
 		b->bulk_flow_count++;
 
-		if (cake_dsrc(q->flow_mode))
-			srchost->srchost_bulk_flow_count++;
-
-		if (cake_ddst(q->flow_mode))
-			dsthost->dsthost_bulk_flow_count++;
-
+		cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+		cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 	}
 
 	if (q->buffer_used > q->buffer_max_used)
@@ -1915,13 +1954,11 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 {
 	struct cake_sched_data *q = qdisc_priv(sch);
 	struct cake_tin_data *b = &q->tins[q->cur_tin];
-	struct cake_host *srchost, *dsthost;
 	ktime_t now = ktime_get();
 	struct cake_flow *flow;
 	struct list_head *head;
 	bool first_flow = true;
 	struct sk_buff *skb;
-	u16 host_load;
 	u64 delay;
 	u32 len;
 
@@ -2021,11 +2058,6 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 	q->cur_flow = flow - b->flows;
 	first_flow = false;
 
-	/* triple isolation (modified DRR++) */
-	srchost = &b->hosts[flow->srchost];
-	dsthost = &b->hosts[flow->dsthost];
-	host_load = 1;
-
 	/* flow isolation (DRR++) */
 	if (flow->deficit <= 0) {
 		/* Keep all flows with deficits out of the sparse and decaying
@@ -2037,11 +2069,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 				b->sparse_flow_count--;
 				b->bulk_flow_count++;
 
-				if (cake_dsrc(q->flow_mode))
-					srchost->srchost_bulk_flow_count++;
-
-				if (cake_ddst(q->flow_mode))
-					dsthost->dsthost_bulk_flow_count++;
+				cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+				cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
 				flow->set = CAKE_SET_BULK;
 			} else {
@@ -2053,19 +2082,7 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 			}
 		}
 
-		if (cake_dsrc(q->flow_mode))
-			host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-		if (cake_ddst(q->flow_mode))
-			host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-		WARN_ON(host_load > CAKE_QUEUES);
-
-		/* The shifted prandom_u32() is a way to apply dithering to
-		 * avoid accumulating roundoff errors
-		 */
-		flow->deficit += (b->flow_quantum * quantum_div[host_load] +
-				  (prandom_u32() >> 16)) >> 16;
+		flow->deficit += cake_get_flow_quantum(b, flow, q->flow_mode);
 		list_move_tail(&flow->flowchain, &b->old_flows);
 
 		goto retry;
@@ -2089,11 +2106,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 				if (flow->set == CAKE_SET_BULK) {
 					b->bulk_flow_count--;
 
-					if (cake_dsrc(q->flow_mode))
-						srchost->srchost_bulk_flow_count--;
-
-					if (cake_ddst(q->flow_mode))
-						dsthost->dsthost_bulk_flow_count--;
+					cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+					cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
 					b->decaying_flow_count++;
 				} else if (flow->set == CAKE_SET_SPARSE ||
@@ -2111,12 +2125,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 				else if (flow->set == CAKE_SET_BULK) {
 					b->bulk_flow_count--;
 
-					if (cake_dsrc(q->flow_mode))
-						srchost->srchost_bulk_flow_count--;
-
-					if (cake_ddst(q->flow_mode))
-						dsthost->dsthost_bulk_flow_count--;
-
+					cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+					cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 				} else
 					b->decaying_flow_count--;
 
-- 
2.39.5 (Apple Git-154)
",patching file net/sched/sch_cake.c\nHunk #1 succeeded at 622 (offset -5 lines).\nHunk #2 succeeded at 810 (offset -20 lines).\nHunk #3 succeeded at 836 (offset -20 lines).\nHunk #4 succeeded at 860 (offset -20 lines).\nHunk #5 succeeded at 1878 (offset -18 lines).\nHunk #6 succeeded at 1887 (offset -18 lines).\nHunk #7 succeeded at 1896 (offset -18 lines).\nHunk #8 succeeded at 1954 (offset -18 lines).\nHunk #9 succeeded at 2058 (offset -18 lines).\nHunk #10 succeeded at 2069 (offset -18 lines).\nHunk #11 FAILED at 2100.\nHunk #12 succeeded at 2118 (offset -18 lines).\nHunk #13 succeeded at 2137 (offset -18 lines).\n1 out of 13 hunks FAILED -- saving rejects to file net/sched/sch_cake.c.rej,141,4,46,285,1,OTHER,DELETE
CVE-2025-21648,737d4d91d35b5f7fa5bb442651472277318b0bfd,"From 737d4d91d35b5f7fa5bb442651472277318b0bfd Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@redhat.com>
Date: Tue, 7 Jan 2025 13:01:05 +0100
Subject: [PATCH] sched: sch_cake: add bounds checks to host bulk flow fairness
 counts
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Even though we fixed a logic error in the commit cited below, syzbot
still managed to trigger an underflow of the per-host bulk flow
counters, leading to an out of bounds memory access.

To avoid any such logic errors causing out of bounds memory accesses,
this commit factors out all accesses to the per-host bulk flow counters
to a series of helpers that perform bounds-checking before any
increments and decrements. This also has the benefit of improving
readability by moving the conditional checks for the flow mode into
these helpers, instead of having them spread out throughout the
code (which was the cause of the original logic error).

As part of this change, the flow quantum calculation is consolidated
into a helper function, which means that the dithering applied to the
ost load scaling is now applied both in the DRR rotation and when a
sparse flow's quantum is first initiated. The only user-visible effect
of this is that the maximum packet size that can be sent while a flow
stays sparse will now vary with +/- one byte in some cases. This should
not make a noticeable difference in practice, and thus it's not worth
complicating the code to preserve the old behaviour.

Fixes: 546ea84d07e3 (""sched: sch_cake: fix bulk flow accounting logic for host fairness"")
Reported-by: syzbot+f63600d288bfb7057424@syzkaller.appspotmail.com
Signed-off-by: Toke Hiland-Jrgensen <toke@redhat.com>
Acked-by: Dave Taht <dave.taht@gmail.com>
Link: https://patch.msgid.link/20250107120105.70685-1-toke@redhat.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_cake.c | 140 +++++++++++++++++++++++--------------------
 1 file changed, 75 insertions(+), 65 deletions(-)

diff --git a/net/sched/sch_cake.c b/net/sched/sch_cake.c
index 8d8b2db4653c..2c2e2a67f3b2 100644
--- a/net/sched/sch_cake.c
+++ b/net/sched/sch_cake.c
@@ -627,6 +627,63 @@ static bool cake_ddst(int flow_mode)
         return (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;
 }
 
+static void cake_dec_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count))
+                q->hosts[flow->srchost].srchost_bulk_flow_count--;
+}
+
+static void cake_inc_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->srchost].srchost_bulk_flow_count++;
+}
+
+static void cake_dec_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count--;
+}
+
+static void cake_inc_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count++;
+}
+
+static u16 cake_get_flow_quantum(struct cake_tin_data *q,
+                                 struct cake_flow *flow,
+                                 int flow_mode)
+{
+        u16 host_load = 1;
+
+        if (cake_dsrc(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->srchost].srchost_bulk_flow_count);
+
+        if (cake_ddst(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->dsthost].dsthost_bulk_flow_count);
+
+        /* The get_random_u16() is a way to apply dithering to avoid
+         * accumulating roundoff errors
+         */
+        return (q->flow_quantum * quantum_div[host_load] +
+                get_random_u16()) >> 16;
+}
+
 static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                      int flow_mode, u16 flow_override, u16 host_override)
 {
@@ -773,10 +830,8 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                 allocate_dst = cake_ddst(flow_mode);
 
                 if (q->flows[outer_hash + k].set == CAKE_SET_BULK) {
-                        if (allocate_src)
-                                q->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;
-                        if (allocate_dst)
-                                q->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;
+                        cake_dec_srchost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
+                        cake_dec_dsthost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
                 }
 found:
                 /* reserve queue for future packets in same flow */
@@ -801,9 +856,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].srchost_tag = srchost_hash;
 found_src:
                         srchost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[srchost_idx].srchost_bulk_flow_count++;
                         q->flows[reduced_hash].srchost = srchost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_srchost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
 
                 if (allocate_dst) {
@@ -824,9 +880,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].dsthost_tag = dsthost_hash;
 found_dst:
                         dsthost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[dsthost_idx].dsthost_bulk_flow_count++;
                         q->flows[reduced_hash].dsthost = dsthost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_dsthost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
         }
 
@@ -1839,10 +1896,6 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 
         /* flowchain */
         if (!flow->set || flow->set == CAKE_SET_DECAYING) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-                u16 host_load = 1;
-
                 if (!flow->set) {
                         list_add_tail(&flow->flowchain, &b->new_flows);
                 } else {
@@ -1852,18 +1905,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 flow->set = CAKE_SET_SPARSE;
                 b->sparse_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                flow->deficit = (b->flow_quantum *
-                                 quantum_div[host_load]) >> 16;
+                flow->deficit = cake_get_flow_quantum(b, flow, q->flow_mode);
         } else if (flow->set == CAKE_SET_SPARSE_WAIT) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-
                 /* this flow was empty, accounted as a sparse flow, but actually
                  * in the bulk rotation.
                  */
@@ -1871,12 +1914,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 b->sparse_flow_count--;
                 b->bulk_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        srchost->srchost_bulk_flow_count++;
-
-                if (cake_ddst(q->flow_mode))
-                        dsthost->dsthost_bulk_flow_count++;
-
+                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
         }
 
         if (q->buffer_used > q->buffer_max_used)
@@ -1933,13 +1972,11 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 {
         struct cake_sched_data *q = qdisc_priv(sch);
         struct cake_tin_data *b = &q->tins[q->cur_tin];
-        struct cake_host *srchost, *dsthost;
         ktime_t now = ktime_get();
         struct cake_flow *flow;
         struct list_head *head;
         bool first_flow = true;
         struct sk_buff *skb;
-        u16 host_load;
         u64 delay;
         u32 len;
 
@@ -2039,11 +2076,6 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
         q->cur_flow = flow - b->flows;
         first_flow = false;
 
-        /* triple isolation (modified DRR++) */
-        srchost = &b->hosts[flow->srchost];
-        dsthost = &b->hosts[flow->dsthost];
-        host_load = 1;
-
         /* flow isolation (DRR++) */
         if (flow->deficit <= 0) {
                 /* Keep all flows with deficits out of the sparse and decaying
@@ -2055,11 +2087,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 b->sparse_flow_count--;
                                 b->bulk_flow_count++;
 
-                                if (cake_dsrc(q->flow_mode))
-                                        srchost->srchost_bulk_flow_count++;
-
-                                if (cake_ddst(q->flow_mode))
-                                        dsthost->dsthost_bulk_flow_count++;
+                                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                 flow->set = CAKE_SET_BULK;
                         } else {
@@ -2071,19 +2100,7 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                         }
                 }
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                WARN_ON(host_load > CAKE_QUEUES);
-
-                /* The get_random_u16() is a way to apply dithering to avoid
-                 * accumulating roundoff errors
-                 */
-                flow->deficit += (b->flow_quantum * quantum_div[host_load] +
-                                  get_random_u16()) >> 16;
+                flow->deficit += cake_get_flow_quantum(b, flow, q->flow_mode);
                 list_move_tail(&flow->flowchain, &b->old_flows);
 
                 goto retry;
@@ -2107,11 +2124,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                         b->decaying_flow_count++;
                                 } else if (flow->set == CAKE_SET_SPARSE ||
@@ -2129,12 +2143,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 else if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
-
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
                                 } else
                                         b->decaying_flow_count--;
 
-- 
2.39.5 (Apple Git-154)
",b1a1743aaa4906c41c426eda97e2e2586f79246d,"From b1a1743aaa4906c41c426eda97e2e2586f79246d Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@redhat.com>
Date: Tue, 7 Jan 2025 13:01:05 +0100
Subject: [PATCH] sched: sch_cake: add bounds checks to host bulk flow fairness
 counts
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

commit 737d4d91d35b5f7fa5bb442651472277318b0bfd upstream.

Even though we fixed a logic error in the commit cited below, syzbot
still managed to trigger an underflow of the per-host bulk flow
counters, leading to an out of bounds memory access.

To avoid any such logic errors causing out of bounds memory accesses,
this commit factors out all accesses to the per-host bulk flow counters
to a series of helpers that perform bounds-checking before any
increments and decrements. This also has the benefit of improving
readability by moving the conditional checks for the flow mode into
these helpers, instead of having them spread out throughout the
code (which was the cause of the original logic error).

As part of this change, the flow quantum calculation is consolidated
into a helper function, which means that the dithering applied to the
ost load scaling is now applied both in the DRR rotation and when a
sparse flow's quantum is first initiated. The only user-visible effect
of this is that the maximum packet size that can be sent while a flow
stays sparse will now vary with +/- one byte in some cases. This should
not make a noticeable difference in practice, and thus it's not worth
complicating the code to preserve the old behaviour.

Fixes: 546ea84d07e3 (""sched: sch_cake: fix bulk flow accounting logic for host fairness"")
Reported-by: syzbot+f63600d288bfb7057424@syzkaller.appspotmail.com
Signed-off-by: Toke Hiland-Jrgensen <toke@redhat.com>
Acked-by: Dave Taht <dave.taht@gmail.com>
Link: https://patch.msgid.link/20250107120105.70685-1-toke@redhat.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
[Hagar: needed contextual fixes due to missing commit 7e3cf0843fe5]
Signed-off-by: Hagar Hemdan <hagarhem@amazon.com>
Reviewed-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_cake.c | 140 +++++++++++++++++++++++--------------------
 1 file changed, 75 insertions(+), 65 deletions(-)

diff --git a/net/sched/sch_cake.c b/net/sched/sch_cake.c
index 8d9c0b98a747..d9535129f4e9 100644
--- a/net/sched/sch_cake.c
+++ b/net/sched/sch_cake.c
@@ -643,6 +643,63 @@ static bool cake_ddst(int flow_mode)
         return (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;
 }
 
+static void cake_dec_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count))
+                q->hosts[flow->srchost].srchost_bulk_flow_count--;
+}
+
+static void cake_inc_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->srchost].srchost_bulk_flow_count++;
+}
+
+static void cake_dec_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count--;
+}
+
+static void cake_inc_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count++;
+}
+
+static u16 cake_get_flow_quantum(struct cake_tin_data *q,
+                                 struct cake_flow *flow,
+                                 int flow_mode)
+{
+        u16 host_load = 1;
+
+        if (cake_dsrc(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->srchost].srchost_bulk_flow_count);
+
+        if (cake_ddst(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->dsthost].dsthost_bulk_flow_count);
+
+        /* The shifted prandom_u32() is a way to apply dithering to avoid
+         * accumulating roundoff errors
+         */
+        return (q->flow_quantum * quantum_div[host_load] +
+                (prandom_u32() >> 16)) >> 16;
+}
+
 static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                      int flow_mode, u16 flow_override, u16 host_override)
 {
@@ -789,10 +846,8 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                 allocate_dst = cake_ddst(flow_mode);
 
                 if (q->flows[outer_hash + k].set == CAKE_SET_BULK) {
-                        if (allocate_src)
-                                q->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;
-                        if (allocate_dst)
-                                q->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;
+                        cake_dec_srchost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
+                        cake_dec_dsthost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
                 }
 found:
                 /* reserve queue for future packets in same flow */
@@ -817,9 +872,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].srchost_tag = srchost_hash;
 found_src:
                         srchost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[srchost_idx].srchost_bulk_flow_count++;
                         q->flows[reduced_hash].srchost = srchost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_srchost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
 
                 if (allocate_dst) {
@@ -840,9 +896,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].dsthost_tag = dsthost_hash;
 found_dst:
                         dsthost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[dsthost_idx].dsthost_bulk_flow_count++;
                         q->flows[reduced_hash].dsthost = dsthost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_dsthost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
         }
 
@@ -1855,10 +1912,6 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 
         /* flowchain */
         if (!flow->set || flow->set == CAKE_SET_DECAYING) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-                u16 host_load = 1;
-
                 if (!flow->set) {
                         list_add_tail(&flow->flowchain, &b->new_flows);
                 } else {
@@ -1868,18 +1921,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 flow->set = CAKE_SET_SPARSE;
                 b->sparse_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                flow->deficit = (b->flow_quantum *
-                                 quantum_div[host_load]) >> 16;
+                flow->deficit = cake_get_flow_quantum(b, flow, q->flow_mode);
         } else if (flow->set == CAKE_SET_SPARSE_WAIT) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-
                 /* this flow was empty, accounted as a sparse flow, but actually
                  * in the bulk rotation.
                  */
@@ -1887,12 +1930,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 b->sparse_flow_count--;
                 b->bulk_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        srchost->srchost_bulk_flow_count++;
-
-                if (cake_ddst(q->flow_mode))
-                        dsthost->dsthost_bulk_flow_count++;
-
+                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
         }
 
         if (q->buffer_used > q->buffer_max_used)
@@ -1949,13 +1988,11 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 {
         struct cake_sched_data *q = qdisc_priv(sch);
         struct cake_tin_data *b = &q->tins[q->cur_tin];
-        struct cake_host *srchost, *dsthost;
         ktime_t now = ktime_get();
         struct cake_flow *flow;
         struct list_head *head;
         bool first_flow = true;
         struct sk_buff *skb;
-        u16 host_load;
         u64 delay;
         u32 len;
 
@@ -2055,11 +2092,6 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
         q->cur_flow = flow - b->flows;
         first_flow = false;
 
-        /* triple isolation (modified DRR++) */
-        srchost = &b->hosts[flow->srchost];
-        dsthost = &b->hosts[flow->dsthost];
-        host_load = 1;
-
         /* flow isolation (DRR++) */
         if (flow->deficit <= 0) {
                 /* Keep all flows with deficits out of the sparse and decaying
@@ -2071,11 +2103,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 b->sparse_flow_count--;
                                 b->bulk_flow_count++;
 
-                                if (cake_dsrc(q->flow_mode))
-                                        srchost->srchost_bulk_flow_count++;
-
-                                if (cake_ddst(q->flow_mode))
-                                        dsthost->dsthost_bulk_flow_count++;
+                                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                 flow->set = CAKE_SET_BULK;
                         } else {
@@ -2087,19 +2116,7 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                         }
                 }
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                WARN_ON(host_load > CAKE_QUEUES);
-
-                /* The shifted prandom_u32() is a way to apply dithering to
-                 * avoid accumulating roundoff errors
-                 */
-                flow->deficit += (b->flow_quantum * quantum_div[host_load] +
-                                  (prandom_u32() >> 16)) >> 16;
+                flow->deficit += cake_get_flow_quantum(b, flow, q->flow_mode);
                 list_move_tail(&flow->flowchain, &b->old_flows);
 
                 goto retry;
@@ -2123,11 +2140,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                         b->decaying_flow_count++;
                                 } else if (flow->set == CAKE_SET_SPARSE ||
@@ -2145,12 +2159,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 else if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
-
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
                                 } else
                                         b->decaying_flow_count--;
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_cake.c\nHunk #1 succeeded at 643 (offset 16 lines).\nHunk #2 succeeded at 846 (offset 16 lines).\nHunk #3 succeeded at 872 (offset 16 lines).\nHunk #4 succeeded at 896 (offset 16 lines).\nHunk #5 succeeded at 1912 (offset 16 lines).\nHunk #6 succeeded at 1921 (offset 16 lines).\nHunk #7 succeeded at 1930 (offset 16 lines).\nHunk #8 succeeded at 1988 (offset 16 lines).\nHunk #9 succeeded at 2092 (offset 16 lines).\nHunk #10 succeeded at 2103 (offset 16 lines).\nHunk #11 FAILED at 2100.\nHunk #12 succeeded at 2152 (offset 16 lines).\nHunk #13 succeeded at 2171 (offset 16 lines).\n1 out of 13 hunks FAILED -- saving rejects to file net/sched/sch_cake.c.rej,141,4,46,285,1,DELETE,DELETE
CVE-2025-21649,737d4d91d35b5f7fa5bb442651472277318b0bfd,"From 737d4d91d35b5f7fa5bb442651472277318b0bfd Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@redhat.com>
Date: Tue, 7 Jan 2025 13:01:05 +0100
Subject: [PATCH] sched: sch_cake: add bounds checks to host bulk flow fairness
 counts
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Even though we fixed a logic error in the commit cited below, syzbot
still managed to trigger an underflow of the per-host bulk flow
counters, leading to an out of bounds memory access.

To avoid any such logic errors causing out of bounds memory accesses,
this commit factors out all accesses to the per-host bulk flow counters
to a series of helpers that perform bounds-checking before any
increments and decrements. This also has the benefit of improving
readability by moving the conditional checks for the flow mode into
these helpers, instead of having them spread out throughout the
code (which was the cause of the original logic error).

As part of this change, the flow quantum calculation is consolidated
into a helper function, which means that the dithering applied to the
ost load scaling is now applied both in the DRR rotation and when a
sparse flow's quantum is first initiated. The only user-visible effect
of this is that the maximum packet size that can be sent while a flow
stays sparse will now vary with +/- one byte in some cases. This should
not make a noticeable difference in practice, and thus it's not worth
complicating the code to preserve the old behaviour.

Fixes: 546ea84d07e3 (""sched: sch_cake: fix bulk flow accounting logic for host fairness"")
Reported-by: syzbot+f63600d288bfb7057424@syzkaller.appspotmail.com
Signed-off-by: Toke Hiland-Jrgensen <toke@redhat.com>
Acked-by: Dave Taht <dave.taht@gmail.com>
Link: https://patch.msgid.link/20250107120105.70685-1-toke@redhat.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_cake.c | 140 +++++++++++++++++++++++--------------------
 1 file changed, 75 insertions(+), 65 deletions(-)

diff --git a/net/sched/sch_cake.c b/net/sched/sch_cake.c
index 8d8b2db4653c..2c2e2a67f3b2 100644
--- a/net/sched/sch_cake.c
+++ b/net/sched/sch_cake.c
@@ -627,6 +627,63 @@ static bool cake_ddst(int flow_mode)
 	return (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;
 }
 
+static void cake_dec_srchost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_dsrc(flow_mode) &&
+		   q->hosts[flow->srchost].srchost_bulk_flow_count))
+		q->hosts[flow->srchost].srchost_bulk_flow_count--;
+}
+
+static void cake_inc_srchost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_dsrc(flow_mode) &&
+		   q->hosts[flow->srchost].srchost_bulk_flow_count < CAKE_QUEUES))
+		q->hosts[flow->srchost].srchost_bulk_flow_count++;
+}
+
+static void cake_dec_dsthost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_ddst(flow_mode) &&
+		   q->hosts[flow->dsthost].dsthost_bulk_flow_count))
+		q->hosts[flow->dsthost].dsthost_bulk_flow_count--;
+}
+
+static void cake_inc_dsthost_bulk_flow_count(struct cake_tin_data *q,
+					     struct cake_flow *flow,
+					     int flow_mode)
+{
+	if (likely(cake_ddst(flow_mode) &&
+		   q->hosts[flow->dsthost].dsthost_bulk_flow_count < CAKE_QUEUES))
+		q->hosts[flow->dsthost].dsthost_bulk_flow_count++;
+}
+
+static u16 cake_get_flow_quantum(struct cake_tin_data *q,
+				 struct cake_flow *flow,
+				 int flow_mode)
+{
+	u16 host_load = 1;
+
+	if (cake_dsrc(flow_mode))
+		host_load = max(host_load,
+				q->hosts[flow->srchost].srchost_bulk_flow_count);
+
+	if (cake_ddst(flow_mode))
+		host_load = max(host_load,
+				q->hosts[flow->dsthost].dsthost_bulk_flow_count);
+
+	/* The get_random_u16() is a way to apply dithering to avoid
+	 * accumulating roundoff errors
+	 */
+	return (q->flow_quantum * quantum_div[host_load] +
+		get_random_u16()) >> 16;
+}
+
 static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 		     int flow_mode, u16 flow_override, u16 host_override)
 {
@@ -773,10 +830,8 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 		allocate_dst = cake_ddst(flow_mode);
 
 		if (q->flows[outer_hash + k].set == CAKE_SET_BULK) {
-			if (allocate_src)
-				q->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;
-			if (allocate_dst)
-				q->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;
+			cake_dec_srchost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
+			cake_dec_dsthost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
 		}
 found:
 		/* reserve queue for future packets in same flow */
@@ -801,9 +856,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 			q->hosts[outer_hash + k].srchost_tag = srchost_hash;
 found_src:
 			srchost_idx = outer_hash + k;
-			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-				q->hosts[srchost_idx].srchost_bulk_flow_count++;
 			q->flows[reduced_hash].srchost = srchost_idx;
+
+			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+				cake_inc_srchost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
 		}
 
 		if (allocate_dst) {
@@ -824,9 +880,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
 			q->hosts[outer_hash + k].dsthost_tag = dsthost_hash;
 found_dst:
 			dsthost_idx = outer_hash + k;
-			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-				q->hosts[dsthost_idx].dsthost_bulk_flow_count++;
 			q->flows[reduced_hash].dsthost = dsthost_idx;
+
+			if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+				cake_inc_dsthost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
 		}
 	}
 
@@ -1839,10 +1896,6 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 
 	/* flowchain */
 	if (!flow->set || flow->set == CAKE_SET_DECAYING) {
-		struct cake_host *srchost = &b->hosts[flow->srchost];
-		struct cake_host *dsthost = &b->hosts[flow->dsthost];
-		u16 host_load = 1;
-
 		if (!flow->set) {
 			list_add_tail(&flow->flowchain, &b->new_flows);
 		} else {
@@ -1852,18 +1905,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 		flow->set = CAKE_SET_SPARSE;
 		b->sparse_flow_count++;
 
-		if (cake_dsrc(q->flow_mode))
-			host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-		if (cake_ddst(q->flow_mode))
-			host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-		flow->deficit = (b->flow_quantum *
-				 quantum_div[host_load]) >> 16;
+		flow->deficit = cake_get_flow_quantum(b, flow, q->flow_mode);
 	} else if (flow->set == CAKE_SET_SPARSE_WAIT) {
-		struct cake_host *srchost = &b->hosts[flow->srchost];
-		struct cake_host *dsthost = &b->hosts[flow->dsthost];
-
 		/* this flow was empty, accounted as a sparse flow, but actually
 		 * in the bulk rotation.
 		 */
@@ -1871,12 +1914,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 		b->sparse_flow_count--;
 		b->bulk_flow_count++;
 
-		if (cake_dsrc(q->flow_mode))
-			srchost->srchost_bulk_flow_count++;
-
-		if (cake_ddst(q->flow_mode))
-			dsthost->dsthost_bulk_flow_count++;
-
+		cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+		cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 	}
 
 	if (q->buffer_used > q->buffer_max_used)
@@ -1933,13 +1972,11 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 {
 	struct cake_sched_data *q = qdisc_priv(sch);
 	struct cake_tin_data *b = &q->tins[q->cur_tin];
-	struct cake_host *srchost, *dsthost;
 	ktime_t now = ktime_get();
 	struct cake_flow *flow;
 	struct list_head *head;
 	bool first_flow = true;
 	struct sk_buff *skb;
-	u16 host_load;
 	u64 delay;
 	u32 len;
 
@@ -2039,11 +2076,6 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 	q->cur_flow = flow - b->flows;
 	first_flow = false;
 
-	/* triple isolation (modified DRR++) */
-	srchost = &b->hosts[flow->srchost];
-	dsthost = &b->hosts[flow->dsthost];
-	host_load = 1;
-
 	/* flow isolation (DRR++) */
 	if (flow->deficit <= 0) {
 		/* Keep all flows with deficits out of the sparse and decaying
@@ -2055,11 +2087,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 				b->sparse_flow_count--;
 				b->bulk_flow_count++;
 
-				if (cake_dsrc(q->flow_mode))
-					srchost->srchost_bulk_flow_count++;
-
-				if (cake_ddst(q->flow_mode))
-					dsthost->dsthost_bulk_flow_count++;
+				cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+				cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
 				flow->set = CAKE_SET_BULK;
 			} else {
@@ -2071,19 +2100,7 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 			}
 		}
 
-		if (cake_dsrc(q->flow_mode))
-			host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-		if (cake_ddst(q->flow_mode))
-			host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-		WARN_ON(host_load > CAKE_QUEUES);
-
-		/* The get_random_u16() is a way to apply dithering to avoid
-		 * accumulating roundoff errors
-		 */
-		flow->deficit += (b->flow_quantum * quantum_div[host_load] +
-				  get_random_u16()) >> 16;
+		flow->deficit += cake_get_flow_quantum(b, flow, q->flow_mode);
 		list_move_tail(&flow->flowchain, &b->old_flows);
 
 		goto retry;
@@ -2107,11 +2124,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 				if (flow->set == CAKE_SET_BULK) {
 					b->bulk_flow_count--;
 
-					if (cake_dsrc(q->flow_mode))
-						srchost->srchost_bulk_flow_count--;
-
-					if (cake_ddst(q->flow_mode))
-						dsthost->dsthost_bulk_flow_count--;
+					cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+					cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
 					b->decaying_flow_count++;
 				} else if (flow->set == CAKE_SET_SPARSE ||
@@ -2129,12 +2143,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 				else if (flow->set == CAKE_SET_BULK) {
 					b->bulk_flow_count--;
 
-					if (cake_dsrc(q->flow_mode))
-						srchost->srchost_bulk_flow_count--;
-
-					if (cake_ddst(q->flow_mode))
-						dsthost->dsthost_bulk_flow_count--;
-
+					cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+					cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 				} else
 					b->decaying_flow_count--;
 
-- 
2.39.5 (Apple Git-154)

",bb0245fa72b783cb23a9949c5048781341e91423,"From bb0245fa72b783cb23a9949c5048781341e91423 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@redhat.com>
Date: Tue, 7 Jan 2025 13:01:05 +0100
Subject: [PATCH] sched: sch_cake: add bounds checks to host bulk flow fairness
 counts
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

commit 737d4d91d35b5f7fa5bb442651472277318b0bfd upstream.

Even though we fixed a logic error in the commit cited below, syzbot
still managed to trigger an underflow of the per-host bulk flow
counters, leading to an out of bounds memory access.

To avoid any such logic errors causing out of bounds memory accesses,
this commit factors out all accesses to the per-host bulk flow counters
to a series of helpers that perform bounds-checking before any
increments and decrements. This also has the benefit of improving
readability by moving the conditional checks for the flow mode into
these helpers, instead of having them spread out throughout the
code (which was the cause of the original logic error).

As part of this change, the flow quantum calculation is consolidated
into a helper function, which means that the dithering applied to the
ost load scaling is now applied both in the DRR rotation and when a
sparse flow's quantum is first initiated. The only user-visible effect
of this is that the maximum packet size that can be sent while a flow
stays sparse will now vary with +/- one byte in some cases. This should
not make a noticeable difference in practice, and thus it's not worth
complicating the code to preserve the old behaviour.

Fixes: 546ea84d07e3 (""sched: sch_cake: fix bulk flow accounting logic for host fairness"")
Reported-by: syzbot+f63600d288bfb7057424@syzkaller.appspotmail.com
Signed-off-by: Toke Hiland-Jrgensen <toke@redhat.com>
Acked-by: Dave Taht <dave.taht@gmail.com>
Link: https://patch.msgid.link/20250107120105.70685-1-toke@redhat.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
[Hagar: needed contextual fixes due to missing commit 7e3cf0843fe5]
Signed-off-by: Hagar Hemdan <hagarhem@amazon.com>
Reviewed-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_cake.c | 140 +++++++++++++++++++++++--------------------
 1 file changed, 75 insertions(+), 65 deletions(-)

diff --git a/net/sched/sch_cake.c b/net/sched/sch_cake.c
index eeb418165755..8429d7f8aba4 100644
--- a/net/sched/sch_cake.c
+++ b/net/sched/sch_cake.c
@@ -643,6 +643,63 @@ static bool cake_ddst(int flow_mode)
         return (flow_mode & CAKE_FLOW_DUAL_DST) == CAKE_FLOW_DUAL_DST;
 }
 
+static void cake_dec_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count))
+                q->hosts[flow->srchost].srchost_bulk_flow_count--;
+}
+
+static void cake_inc_srchost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_dsrc(flow_mode) &&
+                   q->hosts[flow->srchost].srchost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->srchost].srchost_bulk_flow_count++;
+}
+
+static void cake_dec_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count--;
+}
+
+static void cake_inc_dsthost_bulk_flow_count(struct cake_tin_data *q,
+                                             struct cake_flow *flow,
+                                             int flow_mode)
+{
+        if (likely(cake_ddst(flow_mode) &&
+                   q->hosts[flow->dsthost].dsthost_bulk_flow_count < CAKE_QUEUES))
+                q->hosts[flow->dsthost].dsthost_bulk_flow_count++;
+}
+
+static u16 cake_get_flow_quantum(struct cake_tin_data *q,
+                                 struct cake_flow *flow,
+                                 int flow_mode)
+{
+        u16 host_load = 1;
+
+        if (cake_dsrc(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->srchost].srchost_bulk_flow_count);
+
+        if (cake_ddst(flow_mode))
+                host_load = max(host_load,
+                                q->hosts[flow->dsthost].dsthost_bulk_flow_count);
+
+        /* The shifted prandom_u32() is a way to apply dithering to avoid
+         * accumulating roundoff errors
+         */
+        return (q->flow_quantum * quantum_div[host_load] +
+                (prandom_u32() >> 16)) >> 16;
+}
+
 static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                      int flow_mode, u16 flow_override, u16 host_override)
 {
@@ -789,10 +846,8 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                 allocate_dst = cake_ddst(flow_mode);
 
                 if (q->flows[outer_hash + k].set == CAKE_SET_BULK) {
-                        if (allocate_src)
-                                q->hosts[q->flows[reduced_hash].srchost].srchost_bulk_flow_count--;
-                        if (allocate_dst)
-                                q->hosts[q->flows[reduced_hash].dsthost].dsthost_bulk_flow_count--;
+                        cake_dec_srchost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
+                        cake_dec_dsthost_bulk_flow_count(q, &q->flows[outer_hash + k], flow_mode);
                 }
 found:
                 /* reserve queue for future packets in same flow */
@@ -817,9 +872,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].srchost_tag = srchost_hash;
 found_src:
                         srchost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[srchost_idx].srchost_bulk_flow_count++;
                         q->flows[reduced_hash].srchost = srchost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_srchost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
 
                 if (allocate_dst) {
@@ -840,9 +896,10 @@ static u32 cake_hash(struct cake_tin_data *q, const struct sk_buff *skb,
                         q->hosts[outer_hash + k].dsthost_tag = dsthost_hash;
 found_dst:
                         dsthost_idx = outer_hash + k;
-                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
-                                q->hosts[dsthost_idx].dsthost_bulk_flow_count++;
                         q->flows[reduced_hash].dsthost = dsthost_idx;
+
+                        if (q->flows[reduced_hash].set == CAKE_SET_BULK)
+                                cake_inc_dsthost_bulk_flow_count(q, &q->flows[reduced_hash], flow_mode);
                 }
         }
 
@@ -1855,10 +1912,6 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 
         /* flowchain */
         if (!flow->set || flow->set == CAKE_SET_DECAYING) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-                u16 host_load = 1;
-
                 if (!flow->set) {
                         list_add_tail(&flow->flowchain, &b->new_flows);
                 } else {
@@ -1868,18 +1921,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 flow->set = CAKE_SET_SPARSE;
                 b->sparse_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                flow->deficit = (b->flow_quantum *
-                                 quantum_div[host_load]) >> 16;
+                flow->deficit = cake_get_flow_quantum(b, flow, q->flow_mode);
         } else if (flow->set == CAKE_SET_SPARSE_WAIT) {
-                struct cake_host *srchost = &b->hosts[flow->srchost];
-                struct cake_host *dsthost = &b->hosts[flow->dsthost];
-
                 /* this flow was empty, accounted as a sparse flow, but actually
                  * in the bulk rotation.
                  */
@@ -1887,12 +1930,8 @@ static s32 cake_enqueue(struct sk_buff *skb, struct Qdisc *sch,
                 b->sparse_flow_count--;
                 b->bulk_flow_count++;
 
-                if (cake_dsrc(q->flow_mode))
-                        srchost->srchost_bulk_flow_count++;
-
-                if (cake_ddst(q->flow_mode))
-                        dsthost->dsthost_bulk_flow_count++;
-
+                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
         }
 
         if (q->buffer_used > q->buffer_max_used)
@@ -1949,13 +1988,11 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
 {
         struct cake_sched_data *q = qdisc_priv(sch);
         struct cake_tin_data *b = &q->tins[q->cur_tin];
-        struct cake_host *srchost, *dsthost;
         ktime_t now = ktime_get();
         struct cake_flow *flow;
         struct list_head *head;
         bool first_flow = true;
         struct sk_buff *skb;
-        u16 host_load;
         u64 delay;
         u32 len;
 
@@ -2055,11 +2092,6 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
         q->cur_flow = flow - b->flows;
         first_flow = false;
 
-        /* triple isolation (modified DRR++) */
-        srchost = &b->hosts[flow->srchost];
-        dsthost = &b->hosts[flow->dsthost];
-        host_load = 1;
-
         /* flow isolation (DRR++) */
         if (flow->deficit <= 0) {
                 /* Keep all flows with deficits out of the sparse and decaying
@@ -2071,11 +2103,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 b->sparse_flow_count--;
                                 b->bulk_flow_count++;
 
-                                if (cake_dsrc(q->flow_mode))
-                                        srchost->srchost_bulk_flow_count++;
-
-                                if (cake_ddst(q->flow_mode))
-                                        dsthost->dsthost_bulk_flow_count++;
+                                cake_inc_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                cake_inc_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                 flow->set = CAKE_SET_BULK;
                         } else {
@@ -2087,19 +2116,7 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                         }
                 }
 
-                if (cake_dsrc(q->flow_mode))
-                        host_load = max(host_load, srchost->srchost_bulk_flow_count);
-
-                if (cake_ddst(q->flow_mode))
-                        host_load = max(host_load, dsthost->dsthost_bulk_flow_count);
-
-                WARN_ON(host_load > CAKE_QUEUES);
-
-                /* The shifted prandom_u32() is a way to apply dithering to
-                 * avoid accumulating roundoff errors
-                 */
-                flow->deficit += (b->flow_quantum * quantum_div[host_load] +
-                                  (prandom_u32() >> 16)) >> 16;
+                flow->deficit += cake_get_flow_quantum(b, flow, q->flow_mode);
                 list_move_tail(&flow->flowchain, &b->old_flows);
 
                 goto retry;
@@ -2123,11 +2140,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
 
                                         b->decaying_flow_count++;
                                 } else if (flow->set == CAKE_SET_SPARSE ||
@@ -2145,12 +2159,8 @@ static struct sk_buff *cake_dequeue(struct Qdisc *sch)
                                 else if (flow->set == CAKE_SET_BULK) {
                                         b->bulk_flow_count--;
 
-                                        if (cake_dsrc(q->flow_mode))
-                                                srchost->srchost_bulk_flow_count--;
-
-                                        if (cake_ddst(q->flow_mode))
-                                                dsthost->dsthost_bulk_flow_count--;
-
+                                        cake_dec_srchost_bulk_flow_count(b, flow, q->flow_mode);
+                                        cake_dec_dsthost_bulk_flow_count(b, flow, q->flow_mode);
                                 } else
                                         b->decaying_flow_count--;
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_cake.c\nHunk #1 succeeded at 643 (offset 16 lines).\nHunk #2 succeeded at 846 (offset 16 lines).\nHunk #3 succeeded at 872 (offset 16 lines).\nHunk #4 succeeded at 896 (offset 16 lines).\nHunk #5 succeeded at 1912 (offset 16 lines).\nHunk #6 succeeded at 1921 (offset 16 lines).\nHunk #7 succeeded at 1930 (offset 16 lines).\nHunk #8 succeeded at 1988 (offset 16 lines).\nHunk #9 succeeded at 2092 (offset 16 lines).\nHunk #10 succeeded at 2103 (offset 16 lines).\nHunk #11 FAILED at 2100.\nHunk #12 succeeded at 2152 (offset 16 lines).\nHunk #13 succeeded at 2171 (offset 16 lines).\n1 out of 13 hunks FAILED -- saving rejects to file net/sched/sch_cake.c.rej,141,4,46,286,1,DELETE,DELETE
CVE-2025-21666,91751e248256efc111e52e15115840c35d85abaf,"From 91751e248256efc111e52e15115840c35d85abaf Mon Sep 17 00:00:00 2001
From: Stefano Garzarella <sgarzare@redhat.com>
Date: Fri, 10 Jan 2025 09:35:11 +0100
Subject: [PATCH] vsock: prevent null-ptr-deref in vsock_*[has_data|has_space]

Recent reports have shown how we sometimes call vsock_*_has_data()
when a vsock socket has been de-assigned from a transport (see attached
links), but we shouldn't.

Previous commits should have solved the real problems, but we may have
more in the future, so to avoid null-ptr-deref, we can return 0
(no space, no data available) but with a warning.

This way the code should continue to run in a nearly consistent state
and have a warning that allows us to debug future problems.

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/netdev/Z2K%2FI4nlHdfMRTZC@v4bel-B760M-AORUS-ELITE-AX/
Link: https://lore.kernel.org/netdev/5ca20d4c-1017-49c2-9516-f6f75fd331e9@rbox.co/
Link: https://lore.kernel.org/netdev/677f84a8.050a0220.25a300.01b3.GAE@google.com/
Co-developed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Hyunwoo Kim <v4bel@theori.io>
Co-developed-by: Wongi Lee <qwerty@theori.io>
Signed-off-by: Wongi Lee <qwerty@theori.io>
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Reviewed-by: Luigi Leonardi <leonardi@redhat.com>
Reviewed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Paolo Abeni <pabeni@redhat.com>
---
 net/vmw_vsock/af_vsock.c | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/net/vmw_vsock/af_vsock.c b/net/vmw_vsock/af_vsock.c
index 74d35a871644..fa9d1b49599b 100644
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -879,6 +879,9 @@ EXPORT_SYMBOL_GPL(vsock_create_connected);
 
 s64 vsock_stream_has_data(struct vsock_sock *vsk)
 {
+        if (WARN_ON(!vsk->transport))
+                return 0;
+
         return vsk->transport->stream_has_data(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_stream_has_data);
@@ -887,6 +890,9 @@ s64 vsock_connectible_has_data(struct vsock_sock *vsk)
 {
         struct sock *sk = sk_vsock(vsk);
 
+        if (WARN_ON(!vsk->transport))
+                return 0;
+
         if (sk->sk_type == SOCK_SEQPACKET)
                 return vsk->transport->seqpacket_has_data(vsk);
         else
@@ -896,6 +902,9 @@ EXPORT_SYMBOL_GPL(vsock_connectible_has_data);
 
 s64 vsock_stream_has_space(struct vsock_sock *vsk)
 {
+        if (WARN_ON(!vsk->transport))
+                return 0;
+
         return vsk->transport->stream_has_space(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_stream_has_space);
-- 
2.39.5 (Apple Git-154)
",daeac89cdb03d30028186f5ff7dc26ec8fa843e7,"From daeac89cdb03d30028186f5ff7dc26ec8fa843e7 Mon Sep 17 00:00:00 2001
From: Stefano Garzarella <sgarzare@redhat.com>
Date: Fri, 10 Jan 2025 09:35:11 +0100
Subject: [PATCH] vsock: prevent null-ptr-deref in vsock_*[has_data|has_space]

commit 91751e248256efc111e52e15115840c35d85abaf upstream.

Recent reports have shown how we sometimes call vsock_*_has_data()
when a vsock socket has been de-assigned from a transport (see attached
links), but we shouldn't.

Previous commits should have solved the real problems, but we may have
more in the future, so to avoid null-ptr-deref, we can return 0
(no space, no data available) but with a warning.

This way the code should continue to run in a nearly consistent state
and have a warning that allows us to debug future problems.

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/netdev/Z2K%2FI4nlHdfMRTZC@v4bel-B760M-AORUS-ELITE-AX/
Link: https://lore.kernel.org/netdev/5ca20d4c-1017-49c2-9516-f6f75fd331e9@rbox.co/
Link: https://lore.kernel.org/netdev/677f84a8.050a0220.25a300.01b3.GAE@google.com/
Co-developed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Hyunwoo Kim <v4bel@theori.io>
Co-developed-by: Wongi Lee <qwerty@theori.io>
Signed-off-by: Wongi Lee <qwerty@theori.io>
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Reviewed-by: Luigi Leonardi <leonardi@redhat.com>
Reviewed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Paolo Abeni <pabeni@redhat.com>
[SG: fixed conflict since this tree is missing vsock_connectible_has_data()
 added by commit 0798e78b102b (""af_vsock: rest of SEQPACKET support"")]
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/vmw_vsock/af_vsock.c | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/net/vmw_vsock/af_vsock.c b/net/vmw_vsock/af_vsock.c
index 8da25707e1fb..53a9c0a73489 100644
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -837,12 +837,18 @@ EXPORT_SYMBOL_GPL(vsock_create_connected);
 
 s64 vsock_stream_has_data(struct vsock_sock *vsk)
 {
+        if (WARN_ON(!vsk->transport))
+                return 0;
+
         return vsk->transport->stream_has_data(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_stream_has_data);
 
 s64 vsock_stream_has_space(struct vsock_sock *vsk)
 {
+        if (WARN_ON(!vsk->transport))
+                return 0;
+
         return vsk->transport->stream_has_space(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_stream_has_space);
-- 
2.39.5 (Apple Git-154)

",patching file net/vmw_vsock/af_vsock.c\nHunk #1 succeeded at 837 (offset -42 lines).\nHunk #2 FAILED at 890.\nHunk #3 succeeded at 846 (offset -53 lines).\n1 out of 3 hunks FAILED -- saving rejects to file net/vmw_vsock/af_vsock.c.rej,10,3,4,69,1,DELETE,SEMANTIC
CVE-2025-21668,726efa92e02b460811e8bc6990dd742f03b645ea,"From 726efa92e02b460811e8bc6990dd742f03b645ea Mon Sep 17 00:00:00 2001
From: Xiaolei Wang <xiaolei.wang@windriver.com>
Date: Wed, 15 Jan 2025 09:41:18 +0800
Subject: [PATCH] pmdomain: imx8mp-blk-ctrl: add missing loop break condition

Currently imx8mp_blk_ctrl_remove() will continue the for loop
until an out-of-bounds exception occurs.

pstate: 60000005 (nZCv daif -PAN -UAO -TCO -DIT -SSBS BTYPE=--)
pc : dev_pm_domain_detach+0x8/0x48
lr : imx8mp_blk_ctrl_shutdown+0x58/0x90
sp : ffffffc084f8bbf0
x29: ffffffc084f8bbf0 x28: ffffff80daf32ac0 x27: 0000000000000000
x26: ffffffc081658d78 x25: 0000000000000001 x24: ffffffc08201b028
x23: ffffff80d0db9490 x22: ffffffc082340a78 x21: 00000000000005b0
x20: ffffff80d19bc180 x19: 000000000000000a x18: ffffffffffffffff
x17: ffffffc080a39e08 x16: ffffffc080a39c98 x15: 4f435f464f006c72
x14: 0000000000000004 x13: ffffff80d0172110 x12: 0000000000000000
x11: ffffff80d0537740 x10: ffffff80d05376c0 x9 : ffffffc0808ed2d8
x8 : ffffffc084f8bab0 x7 : 0000000000000000 x6 : 0000000000000000
x5 : ffffff80d19b9420 x4 : fffffffe03466e60 x3 : 0000000080800077
x2 : 0000000000000000 x1 : 0000000000000001 x0 : 0000000000000000
Call trace:
 dev_pm_domain_detach+0x8/0x48
 platform_shutdown+0x2c/0x48
 device_shutdown+0x158/0x268
 kernel_restart_prepare+0x40/0x58
 kernel_kexec+0x58/0xe8
 __do_sys_reboot+0x198/0x258
 __arm64_sys_reboot+0x2c/0x40
 invoke_syscall+0x5c/0x138
 el0_svc_common.constprop.0+0x48/0xf0
 do_el0_svc+0x24/0x38
 el0_svc+0x38/0xc8
 el0t_64_sync_handler+0x120/0x130
 el0t_64_sync+0x190/0x198
Code: 8128c2d0 ffffffc0 aa1e03e9 d503201f

Fixes: 556f5cf9568a (""soc: imx: add i.MX8MP HSIO blk-ctrl"")
Cc: stable@vger.kernel.org
Signed-off-by: Xiaolei Wang <xiaolei.wang@windriver.com>
Reviewed-by: Lucas Stach <l.stach@pengutronix.de>
Reviewed-by: Fabio Estevam <festevam@gmail.com>
Reviewed-by: Frank Li <Frank.Li@nxp.com>
Link: https://lore.kernel.org/r/20250115014118.4086729-1-xiaolei.wang@windriver.com
Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
---
 drivers/pmdomain/imx/imx8mp-blk-ctrl.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/pmdomain/imx/imx8mp-blk-ctrl.c b/drivers/pmdomain/imx/imx8mp-blk-ctrl.c
index e3a0f64c144c..3668fe66b22c 100644
--- a/drivers/pmdomain/imx/imx8mp-blk-ctrl.c
+++ b/drivers/pmdomain/imx/imx8mp-blk-ctrl.c
@@ -770,7 +770,7 @@ static void imx8mp_blk_ctrl_remove(struct platform_device *pdev)
 
         of_genpd_del_provider(pdev->dev.of_node);
 
-        for (i = 0; bc->onecell_data.num_domains; i++) {
+        for (i = 0; i < bc->onecell_data.num_domains; i++) {
                 struct imx8mp_blk_ctrl_domain *domain = &bc->domains[i];
 
                 pm_genpd_remove(&domain->genpd);
-- 
2.39.5 (Apple Git-154)
",699cc10cc3068f9097a506eae7fe178c860dca4e,"From 699cc10cc3068f9097a506eae7fe178c860dca4e Mon Sep 17 00:00:00 2001
From: Xiaolei Wang <xiaolei.wang@windriver.com>
Date: Wed, 15 Jan 2025 09:41:18 +0800
Subject: [PATCH] pmdomain: imx8mp-blk-ctrl: add missing loop break condition

commit 726efa92e02b460811e8bc6990dd742f03b645ea upstream.

Currently imx8mp_blk_ctrl_remove() will continue the for loop
until an out-of-bounds exception occurs.

pstate: 60000005 (nZCv daif -PAN -UAO -TCO -DIT -SSBS BTYPE=--)
pc : dev_pm_domain_detach+0x8/0x48
lr : imx8mp_blk_ctrl_shutdown+0x58/0x90
sp : ffffffc084f8bbf0
x29: ffffffc084f8bbf0 x28: ffffff80daf32ac0 x27: 0000000000000000
x26: ffffffc081658d78 x25: 0000000000000001 x24: ffffffc08201b028
x23: ffffff80d0db9490 x22: ffffffc082340a78 x21: 00000000000005b0
x20: ffffff80d19bc180 x19: 000000000000000a x18: ffffffffffffffff
x17: ffffffc080a39e08 x16: ffffffc080a39c98 x15: 4f435f464f006c72
x14: 0000000000000004 x13: ffffff80d0172110 x12: 0000000000000000
x11: ffffff80d0537740 x10: ffffff80d05376c0 x9 : ffffffc0808ed2d8
x8 : ffffffc084f8bab0 x7 : 0000000000000000 x6 : 0000000000000000
x5 : ffffff80d19b9420 x4 : fffffffe03466e60 x3 : 0000000080800077
x2 : 0000000000000000 x1 : 0000000000000001 x0 : 0000000000000000
Call trace:
 dev_pm_domain_detach+0x8/0x48
 platform_shutdown+0x2c/0x48
 device_shutdown+0x158/0x268
 kernel_restart_prepare+0x40/0x58
 kernel_kexec+0x58/0xe8
 __do_sys_reboot+0x198/0x258
 __arm64_sys_reboot+0x2c/0x40
 invoke_syscall+0x5c/0x138
 el0_svc_common.constprop.0+0x48/0xf0
 do_el0_svc+0x24/0x38
 el0_svc+0x38/0xc8
 el0t_64_sync_handler+0x120/0x130
 el0t_64_sync+0x190/0x198
Code: 8128c2d0 ffffffc0 aa1e03e9 d503201f

Fixes: 556f5cf9568a (""soc: imx: add i.MX8MP HSIO blk-ctrl"")
Cc: stable@vger.kernel.org
Signed-off-by: Xiaolei Wang <xiaolei.wang@windriver.com>
Reviewed-by: Lucas Stach <l.stach@pengutronix.de>
Reviewed-by: Fabio Estevam <festevam@gmail.com>
Reviewed-by: Frank Li <Frank.Li@nxp.com>
Link: https://lore.kernel.org/r/20250115014118.4086729-1-xiaolei.wang@windriver.com
Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/soc/imx/imx8mp-blk-ctrl.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/soc/imx/imx8mp-blk-ctrl.c b/drivers/soc/imx/imx8mp-blk-ctrl.c
index 0f13853901df..9bc536ee1395 100644
--- a/drivers/soc/imx/imx8mp-blk-ctrl.c
+++ b/drivers/soc/imx/imx8mp-blk-ctrl.c
@@ -659,7 +659,7 @@ static int imx8mp_blk_ctrl_remove(struct platform_device *pdev)
 
         of_genpd_del_provider(pdev->dev.of_node);
 
-        for (i = 0; bc->onecell_data.num_domains; i++) {
+        for (i = 0; i < bc->onecell_data.num_domains; i++) {
                 struct imx8mp_blk_ctrl_domain *domain = &bc->domains[i];
 
                 pm_genpd_remove(&domain->genpd);
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 55\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 726efa92e02b460811e8bc6990dd742f03b645ea Mon Sep 17 00:00:00 2001\n|From: Xiaolei Wang <xiaolei.wang@windriver.com>\n|Date: Wed, 15 Jan 2025 09:41:18 +0800\n|Subject: [PATCH] pmdomain: imx8mp-blk-ctrl: add missing loop break condition\n|\n|Currently imx8mp_blk_ctrl_remove() will continue the for loop\n|until an out-of-bounds exception occurs.\n|\n|pstate: 60000005 (nZCv daif -PAN -UAO -TCO -DIT -SSBS BTYPE=--)\n|pc : dev_pm_domain_detach+0x8/0x48\n|lr : imx8mp_blk_ctrl_shutdown+0x58/0x90\n|sp : ffffffc084f8bbf0\n|x29: ffffffc084f8bbf0 x28: ffffff80daf32ac0 x27: 0000000000000000\n|x26: ffffffc081658d78 x25: 0000000000000001 x24: ffffffc08201b028\n|x23: ffffff80d0db9490 x22: ffffffc082340a78 x21: 00000000000005b0\n|x20: ffffff80d19bc180 x19: 000000000000000a x18: ffffffffffffffff\n|x17: ffffffc080a39e08 x16: ffffffc080a39c98 x15: 4f435f464f006c72\n|x14: 0000000000000004 x13: ffffff80d0172110 x12: 0000000000000000\n|x11: ffffff80d0537740 x10: ffffff80d05376c0 x9 : ffffffc0808ed2d8\n|x8 : ffffffc084f8bab0 x7 : 0000000000000000 x6 : 0000000000000000\n|x5 : ffffff80d19b9420 x4 : fffffffe03466e60 x3 : 0000000080800077\n|x2 : 0000000000000000 x1 : 0000000000000001 x0 : 0000000000000000\n|Call trace:\n| dev_pm_domain_detach+0x8/0x48\n| platform_shutdown+0x2c/0x48\n| device_shutdown+0x158/0x268\n| kernel_restart_prepare+0x40/0x58\n| kernel_kexec+0x58/0xe8\n| __do_sys_reboot+0x198/0x258\n| __arm64_sys_reboot+0x2c/0x40\n| invoke_syscall+0x5c/0x138\n| el0_svc_common.constprop.0+0x48/0xf0\n| do_el0_svc+0x24/0x38\n| el0_svc+0x38/0xc8\n| el0t_64_sync_handler+0x120/0x130\n| el0t_64_sync+0x190/0x198\n|Code: 8128c2d0 ffffffc0 aa1e03e9 d503201f\n|\n|Fixes: 556f5cf9568a (\""soc: imx: add i.MX8MP HSIO blk-ctrl\"")\n|Cc: stable@vger.kernel.org\n|Signed-off-by: Xiaolei Wang <xiaolei.wang@windriver.com>\n|Reviewed-by: Lucas Stach <l.stach@pengutronix.de>\n|Reviewed-by: Fabio Estevam <festevam@gmail.com>\n|Reviewed-by: Frank Li <Frank.Li@nxp.com>\n|Link: https://lore.kernel.org/r/20250115014118.4086729-1-xiaolei.wang@windriver.com\n|Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>\n|---\n| drivers/pmdomain/imx/imx8mp-blk-ctrl.c | 2 +-\n| 1 file changed, 1 insertion(+), 1 deletion(-)\n|\n|diff --git a/drivers/pmdomain/imx/imx8mp-blk-ctrl.c b/drivers/pmdomain/imx/imx8mp-blk-ctrl.c\n|index e3a0f64c144c..3668fe66b22c 100644\n|--- a/drivers/pmdomain/imx/imx8mp-blk-ctrl.c\n|+++ b/drivers/pmdomain/imx/imx8mp-blk-ctrl.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",3,0,2,65,1,DELETE,DELETE
CVE-2025-21669,2cb7c756f605ec02ffe562fb26828e4bcc5fdfc1,"From 2cb7c756f605ec02ffe562fb26828e4bcc5fdfc1 Mon Sep 17 00:00:00 2001
From: Stefano Garzarella <sgarzare@redhat.com>
Date: Fri, 10 Jan 2025 09:35:07 +0100
Subject: [PATCH] vsock/virtio: discard packets if the transport changes

If the socket has been de-assigned or assigned to another transport,
we must discard any packets received because they are not expected
and would cause issues when we access vsk->transport.

A possible scenario is described by Hyunwoo Kim in the attached link,
where after a first connect() interrupted by a signal, and a second
connect() failed, we can find `vsk->transport` at NULL, leading to a
NULL pointer dereference.

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Cc: stable@vger.kernel.org
Reported-by: Hyunwoo Kim <v4bel@theori.io>
Reported-by: Wongi Lee <qwerty@theori.io>
Closes: https://lore.kernel.org/netdev/Z2LvdTTQR7dBmPb5@v4bel-B760M-AORUS-ELITE-AX/
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Reviewed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Paolo Abeni <pabeni@redhat.com>
---
 net/vmw_vsock/virtio_transport_common.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/net/vmw_vsock/virtio_transport_common.c b/net/vmw_vsock/virtio_transport_common.c
index 9acc13ab3f82..51a494b69be8 100644
--- a/net/vmw_vsock/virtio_transport_common.c
+++ b/net/vmw_vsock/virtio_transport_common.c
@@ -1628,8 +1628,11 @@ void virtio_transport_recv_pkt(struct virtio_transport *t,
 
         lock_sock(sk);
 
-        /* Check if sk has been closed before lock_sock */
-        if (sock_flag(sk, SOCK_DONE)) {
+        /* Check if sk has been closed or assigned to another transport before
+         * lock_sock (note: listener sockets are not assigned to any transport)
+         */
+        if (sock_flag(sk, SOCK_DONE) ||
+            (sk->sk_state != TCP_LISTEN && vsk->transport != &t->transport)) {
                 (void)virtio_transport_reset_no_sock(t, skb);
                 release_sock(sk);
                 sock_put(sk);
-- 
2.39.5 (Apple Git-154)
",6486915fa661584d70e8e7e4068c6c075c67dd6d,"From 6486915fa661584d70e8e7e4068c6c075c67dd6d Mon Sep 17 00:00:00 2001
From: Stefano Garzarella <sgarzare@redhat.com>
Date: Fri, 10 Jan 2025 09:35:07 +0100
Subject: [PATCH] vsock/virtio: discard packets if the transport changes

commit 2cb7c756f605ec02ffe562fb26828e4bcc5fdfc1 upstream.

If the socket has been de-assigned or assigned to another transport,
we must discard any packets received because they are not expected
and would cause issues when we access vsk->transport.

A possible scenario is described by Hyunwoo Kim in the attached link,
where after a first connect() interrupted by a signal, and a second
connect() failed, we can find `vsk->transport` at NULL, leading to a
NULL pointer dereference.

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Cc: stable@vger.kernel.org
Reported-by: Hyunwoo Kim <v4bel@theori.io>
Reported-by: Wongi Lee <qwerty@theori.io>
Closes: https://lore.kernel.org/netdev/Z2LvdTTQR7dBmPb5@v4bel-B760M-AORUS-ELITE-AX/
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Reviewed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Paolo Abeni <pabeni@redhat.com>
[SG: fixed context conflict since this tree is missing commit 71dc9ec9ac7d
 (""virtio/vsock: replace virtio_vsock_pkt with sk_buff"")]
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/vmw_vsock/virtio_transport_common.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/net/vmw_vsock/virtio_transport_common.c b/net/vmw_vsock/virtio_transport_common.c
index 20dac2c0aebd..ffd4db198bdf 100644
--- a/net/vmw_vsock/virtio_transport_common.c
+++ b/net/vmw_vsock/virtio_transport_common.c
@@ -1317,8 +1317,11 @@ void virtio_transport_recv_pkt(struct virtio_transport *t,
 
         lock_sock(sk);
 
-        /* Check if sk has been closed before lock_sock */
-        if (sock_flag(sk, SOCK_DONE)) {
+        /* Check if sk has been closed or assigned to another transport before
+         * lock_sock (note: listener sockets are not assigned to any transport)
+         */
+        if (sock_flag(sk, SOCK_DONE) ||
+            (sk->sk_state != TCP_LISTEN && vsk->transport != &t->transport)) {
                 (void)virtio_transport_reset_no_sock(t, pkt);
                 release_sock(sk);
                 sock_put(sk);
-- 
2.39.5 (Apple Git-154)

",patching file net/vmw_vsock/virtio_transport_common.c\nHunk #1 FAILED at 1628.\n1 out of 1 hunk FAILED -- saving rejects to file net/vmw_vsock/virtio_transport_common.c.rej,8,0,1,46,1,COMMENTS,COMMENTS
CVE-2025-21670,2cb7c756f605ec02ffe562fb26828e4bcc5fdfc2,"From 2cb7c756f605ec02ffe562fb26828e4bcc5fdfc1 Mon Sep 17 00:00:00 2001
From: Stefano Garzarella <sgarzare@redhat.com>
Date: Fri, 10 Jan 2025 09:35:07 +0100
Subject: [PATCH] vsock/virtio: discard packets if the transport changes

If the socket has been de-assigned or assigned to another transport,
we must discard any packets received because they are not expected
and would cause issues when we access vsk->transport.

A possible scenario is described by Hyunwoo Kim in the attached link,
where after a first connect() interrupted by a signal, and a second
connect() failed, we can find `vsk->transport` at NULL, leading to a
NULL pointer dereference.

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Cc: stable@vger.kernel.org
Reported-by: Hyunwoo Kim <v4bel@theori.io>
Reported-by: Wongi Lee <qwerty@theori.io>
Closes: https://lore.kernel.org/netdev/Z2LvdTTQR7dBmPb5@v4bel-B760M-AORUS-ELITE-AX/
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Reviewed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Paolo Abeni <pabeni@redhat.com>
---
 net/vmw_vsock/virtio_transport_common.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/net/vmw_vsock/virtio_transport_common.c b/net/vmw_vsock/virtio_transport_common.c
index 9acc13ab3f82..51a494b69be8 100644
--- a/net/vmw_vsock/virtio_transport_common.c
+++ b/net/vmw_vsock/virtio_transport_common.c
@@ -1628,8 +1628,11 @@ void virtio_transport_recv_pkt(struct virtio_transport *t,
 
         lock_sock(sk);
 
-        /* Check if sk has been closed before lock_sock */
-        if (sock_flag(sk, SOCK_DONE)) {
+        /* Check if sk has been closed or assigned to another transport before
+         * lock_sock (note: listener sockets are not assigned to any transport)
+         */
+        if (sock_flag(sk, SOCK_DONE) ||
+            (sk->sk_state != TCP_LISTEN && vsk->transport != &t->transport)) {
                 (void)virtio_transport_reset_no_sock(t, skb);
                 release_sock(sk);
                 sock_put(sk);
-- 
2.39.5 (Apple Git-154)
",18a7fc371d1dbf8deff16c2dd9292bcc73f43040,"From 18a7fc371d1dbf8deff16c2dd9292bcc73f43040 Mon Sep 17 00:00:00 2001
From: Stefano Garzarella <sgarzare@redhat.com>
Date: Fri, 10 Jan 2025 09:35:07 +0100
Subject: [PATCH] vsock/virtio: discard packets if the transport changes

commit 2cb7c756f605ec02ffe562fb26828e4bcc5fdfc1 upstream.

If the socket has been de-assigned or assigned to another transport,
we must discard any packets received because they are not expected
and would cause issues when we access vsk->transport.

A possible scenario is described by Hyunwoo Kim in the attached link,
where after a first connect() interrupted by a signal, and a second
connect() failed, we can find `vsk->transport` at NULL, leading to a
NULL pointer dereference.

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Cc: stable@vger.kernel.org
Reported-by: Hyunwoo Kim <v4bel@theori.io>
Reported-by: Wongi Lee <qwerty@theori.io>
Closes: https://lore.kernel.org/netdev/Z2LvdTTQR7dBmPb5@v4bel-B760M-AORUS-ELITE-AX/
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Reviewed-by: Hyunwoo Kim <v4bel@theori.io>
Signed-off-by: Paolo Abeni <pabeni@redhat.com>
[SG: fixed context conflict since this tree is missing commit 71dc9ec9ac7d
 (""virtio/vsock: replace virtio_vsock_pkt with sk_buff"")]
Signed-off-by: Stefano Garzarella <sgarzare@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/vmw_vsock/virtio_transport_common.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/net/vmw_vsock/virtio_transport_common.c b/net/vmw_vsock/virtio_transport_common.c
index b1c034fa1d6f..cbe8d777d511 100644
--- a/net/vmw_vsock/virtio_transport_common.c
+++ b/net/vmw_vsock/virtio_transport_common.c
@@ -1171,8 +1171,11 @@ void virtio_transport_recv_pkt(struct virtio_transport *t,
 
 	lock_sock(sk);
 
-	/* Check if sk has been closed before lock_sock */
-	if (sock_flag(sk, SOCK_DONE)) {
+	/* Check if sk has been closed or assigned to another transport before
+	 * lock_sock (note: listener sockets are not assigned to any transport)
+	 */
+	if (sock_flag(sk, SOCK_DONE) ||
+	    (sk->sk_state != TCP_LISTEN && vsk->transport != &t->transport)) {
 		(void)virtio_transport_reset_no_sock(t, pkt);
 		release_sock(sk);
 		sock_put(sk);
-- 
2.39.5 (Apple Git-154)

",patching file net/vmw_vsock/virtio_transport_common.c\nHunk #1 FAILED at 1628.\n1 out of 1 hunk FAILED -- saving rejects to file net/vmw_vsock/virtio_transport_common.c.rej,8,0,1,46,1,SEMANTIC,COMMENTS
CVE-2025-21684,9860370c2172704b6b4f0075a0c2a29fd84af96a,"From 9860370c2172704b6b4f0075a0c2a29fd84af96a Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 11:33:54 -0500
Subject: [PATCH] gpio: xilinx: Convert gpio_lock to raw spinlock

irq_chip functions may be called in raw spinlock context. Therefore, we
must also use a raw spinlock for our own internal locking.

This fixes the following lockdep splat:

[    5.349336] =============================
[    5.353349] [ BUG: Invalid wait context ]
[    5.357361] 6.13.0-rc5+ #69 Tainted: G        W
[    5.363031] -----------------------------
[    5.367045] kworker/u17:1/44 is trying to lock:
[    5.371587] ffffff88018b02c0 (&chip->gpio_lock){....}-{3:3}, at: xgpio_irq_unmask (drivers/gpio/gpio-xilinx.c:433 (discriminator 8))
[    5.380079] other info that might help us debug this:
[    5.385138] context-{5:5}
[    5.387762] 5 locks held by kworker/u17:1/44:
[    5.392123] #0: ffffff8800014958 ((wq_completion)events_unbound){+.+.}-{0:0}, at: process_one_work (kernel/workqueue.c:3204)
[    5.402260] #1: ffffffc082fcbdd8 (deferred_probe_work){+.+.}-{0:0}, at: process_one_work (kernel/workqueue.c:3205)
[    5.411528] #2: ffffff880172c900 (&dev->mutex){....}-{4:4}, at: __device_attach (drivers/base/dd.c:1006)
[    5.419929] #3: ffffff88039c8268 (request_class#2){+.+.}-{4:4}, at: __setup_irq (kernel/irq/internals.h:156 kernel/irq/manage.c:1596)
[    5.428331] #4: ffffff88039c80c8 (lock_class#2){....}-{2:2}, at: __setup_irq (kernel/irq/manage.c:1614)
[    5.436472] stack backtrace:
[    5.439359] CPU: 2 UID: 0 PID: 44 Comm: kworker/u17:1 Tainted: G        W          6.13.0-rc5+ #69
[    5.448690] Tainted: [W]=WARN
[    5.451656] Hardware name: xlnx,zynqmp (DT)
[    5.455845] Workqueue: events_unbound deferred_probe_work_func
[    5.461699] Call trace:
[    5.464147] show_stack+0x18/0x24 C
[    5.467821] dump_stack_lvl (lib/dump_stack.c:123)
[    5.471501] dump_stack (lib/dump_stack.c:130)
[    5.474824] __lock_acquire (kernel/locking/lockdep.c:4828 kernel/locking/lockdep.c:4898 kernel/locking/lockdep.c:5176)
[    5.478758] lock_acquire (arch/arm64/include/asm/percpu.h:40 kernel/locking/lockdep.c:467 kernel/locking/lockdep.c:5851 kernel/locking/lockdep.c:5814)
[    5.482429] _raw_spin_lock_irqsave (include/linux/spinlock_api_smp.h:111 kernel/locking/spinlock.c:162)
[    5.486797] xgpio_irq_unmask (drivers/gpio/gpio-xilinx.c:433 (discriminator 8))
[    5.490737] irq_enable (kernel/irq/internals.h:236 kernel/irq/chip.c:170 kernel/irq/chip.c:439 kernel/irq/chip.c:432 kernel/irq/chip.c:345)
[    5.494060] __irq_startup (kernel/irq/internals.h:241 kernel/irq/chip.c:180 kernel/irq/chip.c:250)
[    5.497645] irq_startup (kernel/irq/chip.c:270)
[    5.501143] __setup_irq (kernel/irq/manage.c:1807)
[    5.504728] request_threaded_irq (kernel/irq/manage.c:2208)

Fixes: a32c7caea292 (""gpio: gpio-xilinx: Add interrupt support"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/r/20250110163354.2012654-1-sean.anderson@linux.dev
Signed-off-by: Bartosz Golaszewski <bartosz.golaszewski@linaro.org>
---
 drivers/gpio/gpio-xilinx.c | 32 ++++++++++++++++----------------
 1 file changed, 16 insertions(+), 16 deletions(-)

diff --git a/drivers/gpio/gpio-xilinx.c b/drivers/gpio/gpio-xilinx.c
index c6a8f2c82680..792d94c49077 100644
--- a/drivers/gpio/gpio-xilinx.c
+++ b/drivers/gpio/gpio-xilinx.c
@@ -65,7 +65,7 @@ struct xgpio_instance {
         DECLARE_BITMAP(state, 64);
         DECLARE_BITMAP(last_irq_read, 64);
         DECLARE_BITMAP(dir, 64);
-        spinlock_t gpio_lock;        /* For serializing operations */
+        raw_spinlock_t gpio_lock;        /* For serializing operations */
         int irq;
         DECLARE_BITMAP(enable, 64);
         DECLARE_BITMAP(rising_edge, 64);
@@ -179,14 +179,14 @@ static void xgpio_set(struct gpio_chip *gc, unsigned int gpio, int val)
         struct xgpio_instance *chip = gpiochip_get_data(gc);
         int bit = xgpio_to_bit(chip, gpio);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         /* Write to GPIO signal and set its direction to output */
         __assign_bit(bit, chip->state, val);
 
         xgpio_write_ch(chip, XGPIO_DATA_OFFSET, bit, chip->state);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -210,7 +210,7 @@ static void xgpio_set_multiple(struct gpio_chip *gc, unsigned long *mask,
         bitmap_remap(hw_mask, mask, chip->sw_map, chip->hw_map, 64);
         bitmap_remap(hw_bits, bits, chip->sw_map, chip->hw_map, 64);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         bitmap_replace(state, chip->state, hw_bits, hw_mask, 64);
 
@@ -218,7 +218,7 @@ static void xgpio_set_multiple(struct gpio_chip *gc, unsigned long *mask,
 
         bitmap_copy(chip->state, state, 64);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -236,13 +236,13 @@ static int xgpio_dir_in(struct gpio_chip *gc, unsigned int gpio)
         struct xgpio_instance *chip = gpiochip_get_data(gc);
         int bit = xgpio_to_bit(chip, gpio);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         /* Set the GPIO bit in shadow register and set direction as input */
         __set_bit(bit, chip->dir);
         xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 
         return 0;
 }
@@ -265,7 +265,7 @@ static int xgpio_dir_out(struct gpio_chip *gc, unsigned int gpio, int val)
         struct xgpio_instance *chip = gpiochip_get_data(gc);
         int bit = xgpio_to_bit(chip, gpio);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         /* Write state of GPIO signal */
         __assign_bit(bit, chip->state, val);
@@ -275,7 +275,7 @@ static int xgpio_dir_out(struct gpio_chip *gc, unsigned int gpio, int val)
         __clear_bit(bit, chip->dir);
         xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 
         return 0;
 }
@@ -398,7 +398,7 @@ static void xgpio_irq_mask(struct irq_data *irq_data)
         int bit = xgpio_to_bit(chip, irq_offset);
         u32 mask = BIT(bit / 32), temp;
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         __clear_bit(bit, chip->enable);
 
@@ -408,7 +408,7 @@ static void xgpio_irq_mask(struct irq_data *irq_data)
                 temp &= ~mask;
                 xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, temp);
         }
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 
         gpiochip_disable_irq(&chip->gc, irq_offset);
 }
@@ -428,7 +428,7 @@ static void xgpio_irq_unmask(struct irq_data *irq_data)
 
         gpiochip_enable_irq(&chip->gc, irq_offset);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         __set_bit(bit, chip->enable);
 
@@ -447,7 +447,7 @@ static void xgpio_irq_unmask(struct irq_data *irq_data)
                 xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, val);
         }
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -512,7 +512,7 @@ static void xgpio_irqhandler(struct irq_desc *desc)
 
         chained_irq_enter(irqchip, desc);
 
-        spin_lock(&chip->gpio_lock);
+        raw_spin_lock(&chip->gpio_lock);
 
         xgpio_read_ch_all(chip, XGPIO_DATA_OFFSET, all);
 
@@ -529,7 +529,7 @@ static void xgpio_irqhandler(struct irq_desc *desc)
         bitmap_copy(chip->last_irq_read, all, 64);
         bitmap_or(all, rising, falling, 64);
 
-        spin_unlock(&chip->gpio_lock);
+        raw_spin_unlock(&chip->gpio_lock);
 
         dev_dbg(gc->parent, ""IRQ rising %*pb falling %*pb\n"", 64, rising, 64, falling);
 
@@ -620,7 +620,7 @@ static int xgpio_probe(struct platform_device *pdev)
         bitmap_set(chip->hw_map,  0, width[0]);
         bitmap_set(chip->hw_map, 32, width[1]);
 
-        spin_lock_init(&chip->gpio_lock);
+        raw_spin_lock_init(&chip->gpio_lock);
 
         chip->gc.base = -1;
         chip->gc.ngpio = bitmap_weight(chip->hw_map, 64);
-- 
2.39.5 (Apple Git-154)

",d25041d4a3b2af64c888cf762362b2528ba59294,"From d25041d4a3b2af64c888cf762362b2528ba59294 Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 11:33:54 -0500
Subject: [PATCH] gpio: xilinx: Convert gpio_lock to raw spinlock

[ Upstream commit 9860370c2172704b6b4f0075a0c2a29fd84af96a ]

irq_chip functions may be called in raw spinlock context. Therefore, we
must also use a raw spinlock for our own internal locking.

This fixes the following lockdep splat:

[    5.349336] =============================
[    5.353349] [ BUG: Invalid wait context ]
[    5.357361] 6.13.0-rc5+ #69 Tainted: G        W
[    5.363031] -----------------------------
[    5.367045] kworker/u17:1/44 is trying to lock:
[    5.371587] ffffff88018b02c0 (&chip->gpio_lock){....}-{3:3}, at: xgpio_irq_unmask (drivers/gpio/gpio-xilinx.c:433 (discriminator 8))
[    5.380079] other info that might help us debug this:
[    5.385138] context-{5:5}
[    5.387762] 5 locks held by kworker/u17:1/44:
[    5.392123] #0: ffffff8800014958 ((wq_completion)events_unbound){+.+.}-{0:0}, at: process_one_work (kernel/workqueue.c:3204)
[    5.402260] #1: ffffffc082fcbdd8 (deferred_probe_work){+.+.}-{0:0}, at: process_one_work (kernel/workqueue.c:3205)
[    5.411528] #2: ffffff880172c900 (&dev->mutex){....}-{4:4}, at: __device_attach (drivers/base/dd.c:1006)
[    5.419929] #3: ffffff88039c8268 (request_class#2){+.+.}-{4:4}, at: __setup_irq (kernel/irq/internals.h:156 kernel/irq/manage.c:1596)
[    5.428331] #4: ffffff88039c80c8 (lock_class#2){....}-{2:2}, at: __setup_irq (kernel/irq/manage.c:1614)
[    5.436472] stack backtrace:
[    5.439359] CPU: 2 UID: 0 PID: 44 Comm: kworker/u17:1 Tainted: G        W          6.13.0-rc5+ #69
[    5.448690] Tainted: [W]=WARN
[    5.451656] Hardware name: xlnx,zynqmp (DT)
[    5.455845] Workqueue: events_unbound deferred_probe_work_func
[    5.461699] Call trace:
[    5.464147] show_stack+0x18/0x24 C
[    5.467821] dump_stack_lvl (lib/dump_stack.c:123)
[    5.471501] dump_stack (lib/dump_stack.c:130)
[    5.474824] __lock_acquire (kernel/locking/lockdep.c:4828 kernel/locking/lockdep.c:4898 kernel/locking/lockdep.c:5176)
[    5.478758] lock_acquire (arch/arm64/include/asm/percpu.h:40 kernel/locking/lockdep.c:467 kernel/locking/lockdep.c:5851 kernel/locking/lockdep.c:5814)
[    5.482429] _raw_spin_lock_irqsave (include/linux/spinlock_api_smp.h:111 kernel/locking/spinlock.c:162)
[    5.486797] xgpio_irq_unmask (drivers/gpio/gpio-xilinx.c:433 (discriminator 8))
[    5.490737] irq_enable (kernel/irq/internals.h:236 kernel/irq/chip.c:170 kernel/irq/chip.c:439 kernel/irq/chip.c:432 kernel/irq/chip.c:345)
[    5.494060] __irq_startup (kernel/irq/internals.h:241 kernel/irq/chip.c:180 kernel/irq/chip.c:250)
[    5.497645] irq_startup (kernel/irq/chip.c:270)
[    5.501143] __setup_irq (kernel/irq/manage.c:1807)
[    5.504728] request_threaded_irq (kernel/irq/manage.c:2208)

Fixes: a32c7caea292 (""gpio: gpio-xilinx: Add interrupt support"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org
Link: https://lore.kernel.org/r/20250110163354.2012654-1-sean.anderson@linux.dev
Signed-off-by: Bartosz Golaszewski <bartosz.golaszewski@linaro.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/gpio/gpio-xilinx.c | 32 ++++++++++++++++----------------
 1 file changed, 16 insertions(+), 16 deletions(-)

diff --git a/drivers/gpio/gpio-xilinx.c b/drivers/gpio/gpio-xilinx.c
index db616ae560a3..2082a3db2a9e 100644
--- a/drivers/gpio/gpio-xilinx.c
+++ b/drivers/gpio/gpio-xilinx.c
@@ -66,7 +66,7 @@ struct xgpio_instance {
         DECLARE_BITMAP(state, 64);
         DECLARE_BITMAP(last_irq_read, 64);
         DECLARE_BITMAP(dir, 64);
-        spinlock_t gpio_lock;        /* For serializing operations */
+        raw_spinlock_t gpio_lock;        /* For serializing operations */
         int irq;
         struct irq_chip irqchip;
         DECLARE_BITMAP(enable, 64);
@@ -179,14 +179,14 @@ static void xgpio_set(struct gpio_chip *gc, unsigned int gpio, int val)
         struct xgpio_instance *chip = gpiochip_get_data(gc);
         int bit = xgpio_to_bit(chip, gpio);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         /* Write to GPIO signal and set its direction to output */
         __assign_bit(bit, chip->state, val);
 
         xgpio_write_ch(chip, XGPIO_DATA_OFFSET, bit, chip->state);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -210,7 +210,7 @@ static void xgpio_set_multiple(struct gpio_chip *gc, unsigned long *mask,
         bitmap_remap(hw_mask, mask, chip->sw_map, chip->hw_map, 64);
         bitmap_remap(hw_bits, bits, chip->sw_map, chip->hw_map, 64);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         bitmap_replace(state, chip->state, hw_bits, hw_mask, 64);
 
@@ -218,7 +218,7 @@ static void xgpio_set_multiple(struct gpio_chip *gc, unsigned long *mask,
 
         bitmap_copy(chip->state, state, 64);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -236,13 +236,13 @@ static int xgpio_dir_in(struct gpio_chip *gc, unsigned int gpio)
         struct xgpio_instance *chip = gpiochip_get_data(gc);
         int bit = xgpio_to_bit(chip, gpio);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         /* Set the GPIO bit in shadow register and set direction as input */
         __set_bit(bit, chip->dir);
         xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 
         return 0;
 }
@@ -265,7 +265,7 @@ static int xgpio_dir_out(struct gpio_chip *gc, unsigned int gpio, int val)
         struct xgpio_instance *chip = gpiochip_get_data(gc);
         int bit = xgpio_to_bit(chip, gpio);
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         /* Write state of GPIO signal */
         __assign_bit(bit, chip->state, val);
@@ -275,7 +275,7 @@ static int xgpio_dir_out(struct gpio_chip *gc, unsigned int gpio, int val)
         __clear_bit(bit, chip->dir);
         xgpio_write_ch(chip, XGPIO_TRI_OFFSET, bit, chip->dir);
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 
         return 0;
 }
@@ -405,7 +405,7 @@ static void xgpio_irq_mask(struct irq_data *irq_data)
         int bit = xgpio_to_bit(chip, irq_offset);
         u32 mask = BIT(bit / 32), temp;
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         __clear_bit(bit, chip->enable);
 
@@ -415,7 +415,7 @@ static void xgpio_irq_mask(struct irq_data *irq_data)
                 temp &= ~mask;
                 xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, temp);
         }
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -431,7 +431,7 @@ static void xgpio_irq_unmask(struct irq_data *irq_data)
         u32 old_enable = xgpio_get_value32(chip->enable, bit);
         u32 mask = BIT(bit / 32), val;
 
-        spin_lock_irqsave(&chip->gpio_lock, flags);
+        raw_spin_lock_irqsave(&chip->gpio_lock, flags);
 
         __set_bit(bit, chip->enable);
 
@@ -450,7 +450,7 @@ static void xgpio_irq_unmask(struct irq_data *irq_data)
                 xgpio_writereg(chip->regs + XGPIO_IPIER_OFFSET, val);
         }
 
-        spin_unlock_irqrestore(&chip->gpio_lock, flags);
+        raw_spin_unlock_irqrestore(&chip->gpio_lock, flags);
 }
 
 /**
@@ -515,7 +515,7 @@ static void xgpio_irqhandler(struct irq_desc *desc)
 
         chained_irq_enter(irqchip, desc);
 
-        spin_lock(&chip->gpio_lock);
+        raw_spin_lock(&chip->gpio_lock);
 
         xgpio_read_ch_all(chip, XGPIO_DATA_OFFSET, all);
 
@@ -532,7 +532,7 @@ static void xgpio_irqhandler(struct irq_desc *desc)
         bitmap_copy(chip->last_irq_read, all, 64);
         bitmap_or(all, rising, falling, 64);
 
-        spin_unlock(&chip->gpio_lock);
+        raw_spin_unlock(&chip->gpio_lock);
 
         dev_dbg(gc->parent, ""IRQ rising %*pb falling %*pb\n"", 64, rising, 64, falling);
 
@@ -623,7 +623,7 @@ static int xgpio_probe(struct platform_device *pdev)
         bitmap_set(chip->hw_map,  0, width[0]);
         bitmap_set(chip->hw_map, 32, width[1]);
 
-        spin_lock_init(&chip->gpio_lock);
+        raw_spin_lock_init(&chip->gpio_lock);
 
         chip->gc.base = -1;
         chip->gc.ngpio = bitmap_weight(chip->hw_map, 64);
-- 
2.39.5 (Apple Git-154)

",patching file drivers/gpio/gpio-xilinx.c\nHunk #1 succeeded at 66 with fuzz 2 (offset 1 line).\nHunk #8 succeeded at 405 (offset 7 lines).\nHunk #9 FAILED at 408.\nHunk #10 succeeded at 431 with fuzz 2 (offset 3 lines).\nHunk #11 succeeded at 450 (offset 3 lines).\nHunk #12 succeeded at 515 (offset 3 lines).\nHunk #13 succeeded at 532 (offset 3 lines).\nHunk #14 succeeded at 623 (offset 3 lines).\n1 out of 14 hunks FAILED -- saving rejects to file drivers/gpio/gpio-xilinx.c.rej,33,0,17,200,1,COMMENTS,OTHER
CVE-2025-21686,19d340a2988d4f3e673cded9dde405d727d7e248,"From 19d340a2988d4f3e673cded9dde405d727d7e248 Mon Sep 17 00:00:00 2001
From: Jann Horn <jannh@google.com>
Date: Tue, 14 Jan 2025 18:49:00 +0100
Subject: [PATCH] io_uring/rsrc: require cloned buffers to share accounting
 contexts

When IORING_REGISTER_CLONE_BUFFERS is used to clone buffers from uring
instance A to uring instance B, where A and B use different MMs for
accounting, the accounting can go wrong:
If uring instance A is closed before uring instance B, the pinned memory
counters for uring instance B will be decremented, even though the pinned
memory was originally accounted through uring instance A; so the MM of
uring instance B can end up with negative locked memory.

Cc: stable@vger.kernel.org
Closes: https://lore.kernel.org/r/CAG48ez1zez4bdhmeGLEFxtbFADY4Czn3CV0u9d_TMcbvRA01bg@mail.gmail.com
Fixes: 7cc2a6eadcd7 (""io_uring: add IORING_REGISTER_COPY_BUFFERS method"")
Signed-off-by: Jann Horn <jannh@google.com>
Link: https://lore.kernel.org/r/20250114-uring-check-accounting-v1-1-42e4145aa743@google.com
Signed-off-by: Jens Axboe <axboe@kernel.dk>
---
 io_uring/rsrc.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/io_uring/rsrc.c b/io_uring/rsrc.c
index 964a47c8d85e..688e277f0335 100644
--- a/io_uring/rsrc.c
+++ b/io_uring/rsrc.c
@@ -928,6 +928,13 @@ static int io_clone_buffers(struct io_ring_ctx *ctx, struct io_ring_ctx *src_ctx
         int i, ret, off, nr;
         unsigned int nbufs;
 
+        /*
+         * Accounting state is shared between the two rings; that only works if
+         * both rings are accounted towards the same counters.
+         */
+        if (ctx->user != src_ctx->user || ctx->mm_account != src_ctx->mm_account)
+                return -EINVAL;
+
         /* if offsets are given, must have nr specified too */
         if (!arg->nr && (arg->dst_off || arg->src_off))
                 return -EINVAL;
-- 
2.39.5 (Apple Git-154)
",efd96fbe23fa87de39116f632401f67b93be21ab,"From efd96fbe23fa87de39116f632401f67b93be21ab Mon Sep 17 00:00:00 2001
From: Jann Horn <jannh@google.com>
Date: Tue, 14 Jan 2025 18:49:00 +0100
Subject: [PATCH] io_uring/rsrc: require cloned buffers to share accounting
 contexts

Commit 19d340a2988d4f3e673cded9dde405d727d7e248 upstream.

When IORING_REGISTER_CLONE_BUFFERS is used to clone buffers from uring
instance A to uring instance B, where A and B use different MMs for
accounting, the accounting can go wrong:
If uring instance A is closed before uring instance B, the pinned memory
counters for uring instance B will be decremented, even though the pinned
memory was originally accounted through uring instance A; so the MM of
uring instance B can end up with negative locked memory.

Cc: stable@vger.kernel.org
Closes: https://lore.kernel.org/r/CAG48ez1zez4bdhmeGLEFxtbFADY4Czn3CV0u9d_TMcbvRA01bg@mail.gmail.com
Fixes: 7cc2a6eadcd7 (""io_uring: add IORING_REGISTER_COPY_BUFFERS method"")
Signed-off-by: Jann Horn <jannh@google.com>
Link: https://lore.kernel.org/r/20250114-uring-check-accounting-v1-1-42e4145aa743@google.com
Signed-off-by: Jens Axboe <axboe@kernel.dk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 io_uring/rsrc.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/io_uring/rsrc.c b/io_uring/rsrc.c
index 6f3b6de230bd..a67bae350416 100644
--- a/io_uring/rsrc.c
+++ b/io_uring/rsrc.c
@@ -1153,6 +1153,13 @@ static int io_clone_buffers(struct io_ring_ctx *ctx, struct io_ring_ctx *src_ctx
         struct io_rsrc_data *data;
         int i, ret, nbufs;
 
+        /*
+         * Accounting state is shared between the two rings; that only works if
+         * both rings are accounted towards the same counters.
+         */
+        if (ctx->user != src_ctx->user || ctx->mm_account != src_ctx->mm_account)
+                return -EINVAL;
+
         /*
          * Drop our own lock here. We'll setup the data we need and reference
          * the source buffers, then re-grab, check, and assign at the end.
-- 
2.39.5 (Apple Git-154)

",patching file io_uring/rsrc.c\nHunk #1 FAILED at 928.\n1 out of 1 hunk FAILED -- saving rejects to file io_uring/rsrc.c.rej,8,0,2,44,1,DISJOINT,COMMENTS
CVE-2025-2169,5f537664e705b0bf8b7e329861f20128534f6a83,"From 5f537664e705b0bf8b7e329861f20128534f6a83 Mon Sep 17 00:00:00 2001
From: Linus Torvalds <torvalds@linux-foundation.org>
Date: Tue, 21 Jan 2025 09:27:22 -0800
Subject: [PATCH] cachestat: fix page cache statistics permission checking

When the 'cachestat()' system call was added in commit cf264e1329fb
(""cachestat: implement cachestat syscall""), it was meant to be a much
more convenient (and performant) version of mincore() that didn't need
mapping things into the user virtual address space in order to work.

But it ended up missing the ""check for writability or ownership"" fix for
mincore(), done in commit 134fca9063ad (""mm/mincore.c: make mincore()
more conservative"").

This just adds equivalent logic to 'cachestat()', modified for the file
context (rather than vma).

Reported-by: Sudheendra Raghav Neela <sneela@tugraz.at>
Fixes: cf264e1329fb (""cachestat: implement cachestat syscall"")
Tested-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Nhat Pham <nphamcs@gmail.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
---
 mm/filemap.c | 17 +++++++++++++++++
 1 file changed, 17 insertions(+)

diff --git a/mm/filemap.c b/mm/filemap.c
index 4f476411a9a2..440922a7d8f1 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -4375,6 +4375,20 @@ static void filemap_cachestat(struct address_space *mapping,
         rcu_read_unlock();
 }
 
+/*
+ * See mincore: reveal pagecache information only for files
+ * that the calling process has write access to, or could (if
+ * tried) open for writing.
+ */
+static inline bool can_do_cachestat(struct file *f)
+{
+        if (f->f_mode & FMODE_WRITE)
+                return true;
+        if (inode_owner_or_capable(file_mnt_idmap(f), file_inode(f)))
+                return true;
+        return file_permission(f, MAY_WRITE) == 0;
+}
+
 /*
  * The cachestat(2) system call.
  *
@@ -4430,6 +4444,9 @@ SYSCALL_DEFINE4(cachestat, unsigned int, fd,
         if (is_file_hugepages(fd_file(f)))
                 return -EOPNOTSUPP;
 
+        if (!can_do_cachestat(fd_file(f)))
+                return -EPERM;
+
         if (flags != 0)
                 return -EINVAL;
 
-- 
2.39.5 (Apple Git-154)

",7d6405c13b0d8a8367cd8df63f118b619a3f0dd2,"From 7d6405c13b0d8a8367cd8df63f118b619a3f0dd2 Mon Sep 17 00:00:00 2001
From: Linus Torvalds <torvalds@linux-foundation.org>
Date: Tue, 21 Jan 2025 09:27:22 -0800
Subject: [PATCH] cachestat: fix page cache statistics permission checking

commit 5f537664e705b0bf8b7e329861f20128534f6a83 upstream.

When the 'cachestat()' system call was added in commit cf264e1329fb
(""cachestat: implement cachestat syscall""), it was meant to be a much
more convenient (and performant) version of mincore() that didn't need
mapping things into the user virtual address space in order to work.

But it ended up missing the ""check for writability or ownership"" fix for
mincore(), done in commit 134fca9063ad (""mm/mincore.c: make mincore()
more conservative"").

This just adds equivalent logic to 'cachestat()', modified for the file
context (rather than vma).

Reported-by: Sudheendra Raghav Neela <sneela@tugraz.at>
Fixes: cf264e1329fb (""cachestat: implement cachestat syscall"")
Tested-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Nhat Pham <nphamcs@gmail.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 mm/filemap.c | 19 +++++++++++++++++++
 1 file changed, 19 insertions(+)

diff --git a/mm/filemap.c b/mm/filemap.c
index 6a3d62de1cca..056422e6a0be 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -4270,6 +4270,20 @@ static void filemap_cachestat(struct address_space *mapping,
         rcu_read_unlock();
 }
 
+/*
+ * See mincore: reveal pagecache information only for files
+ * that the calling process has write access to, or could (if
+ * tried) open for writing.
+ */
+static inline bool can_do_cachestat(struct file *f)
+{
+        if (f->f_mode & FMODE_WRITE)
+                return true;
+        if (inode_owner_or_capable(file_mnt_idmap(f), file_inode(f)))
+                return true;
+        return file_permission(f, MAY_WRITE) == 0;
+}
+
 /*
  * The cachestat(2) system call.
  *
@@ -4329,6 +4343,11 @@ SYSCALL_DEFINE4(cachestat, unsigned int, fd,
                 return -EOPNOTSUPP;
         }
 
+        if (!can_do_cachestat(f.file)) {
+                fdput(f);
+                return -EPERM;
+        }
+
         if (flags != 0) {
                 fdput(f);
                 return -EINVAL;
-- 
2.39.5 (Apple Git-154)

",patching file mm/filemap.c\nHunk #1 succeeded at 4270 (offset -105 lines).\nHunk #2 FAILED at 4444.\n1 out of 2 hunks FAILED -- saving rejects to file mm/filemap.c.rej,18,0,2,65,1,COMMENTS,COMMENTS
CVE-2025-2169,5f537664e705b0bf8b7e329861f20128534f6a84,"From 5f537664e705b0bf8b7e329861f20128534f6a83 Mon Sep 17 00:00:00 2001
From: Linus Torvalds <torvalds@linux-foundation.org>
Date: Tue, 21 Jan 2025 09:27:22 -0800
Subject: [PATCH] cachestat: fix page cache statistics permission checking

When the 'cachestat()' system call was added in commit cf264e1329fb
(""cachestat: implement cachestat syscall""), it was meant to be a much
more convenient (and performant) version of mincore() that didn't need
mapping things into the user virtual address space in order to work.

But it ended up missing the ""check for writability or ownership"" fix for
mincore(), done in commit 134fca9063ad (""mm/mincore.c: make mincore()
more conservative"").

This just adds equivalent logic to 'cachestat()', modified for the file
context (rather than vma).

Reported-by: Sudheendra Raghav Neela <sneela@tugraz.at>
Fixes: cf264e1329fb (""cachestat: implement cachestat syscall"")
Tested-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Nhat Pham <nphamcs@gmail.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
---
 mm/filemap.c | 17 +++++++++++++++++
 1 file changed, 17 insertions(+)

diff --git a/mm/filemap.c b/mm/filemap.c
index 4f476411a9a2..440922a7d8f1 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -4375,6 +4375,20 @@ static void filemap_cachestat(struct address_space *mapping,
         rcu_read_unlock();
 }
 
+/*
+ * See mincore: reveal pagecache information only for files
+ * that the calling process has write access to, or could (if
+ * tried) open for writing.
+ */
+static inline bool can_do_cachestat(struct file *f)
+{
+        if (f->f_mode & FMODE_WRITE)
+                return true;
+        if (inode_owner_or_capable(file_mnt_idmap(f), file_inode(f)))
+                return true;
+        return file_permission(f, MAY_WRITE) == 0;
+}
+
 /*
  * The cachestat(2) system call.
  *
@@ -4430,6 +4444,9 @@ SYSCALL_DEFINE4(cachestat, unsigned int, fd,
         if (is_file_hugepages(fd_file(f)))
                 return -EOPNOTSUPP;
 
+        if (!can_do_cachestat(fd_file(f)))
+                return -EPERM;
+
         if (flags != 0)
                 return -EINVAL;
 
-- 
2.39.5 (Apple Git-154)

",780ab8329672464984cf1344bd5c3993af0226c7,"From 780ab8329672464984cf1344bd5c3993af0226c7 Mon Sep 17 00:00:00 2001
From: Linus Torvalds <torvalds@linux-foundation.org>
Date: Tue, 21 Jan 2025 09:27:22 -0800
Subject: [PATCH] cachestat: fix page cache statistics permission checking

commit 5f537664e705b0bf8b7e329861f20128534f6a83 upstream.

When the 'cachestat()' system call was added in commit cf264e1329fb
(""cachestat: implement cachestat syscall""), it was meant to be a much
more convenient (and performant) version of mincore() that didn't need
mapping things into the user virtual address space in order to work.

But it ended up missing the ""check for writability or ownership"" fix for
mincore(), done in commit 134fca9063ad (""mm/mincore.c: make mincore()
more conservative"").

This just adds equivalent logic to 'cachestat()', modified for the file
context (rather than vma).

Reported-by: Sudheendra Raghav Neela <sneela@tugraz.at>
Fixes: cf264e1329fb (""cachestat: implement cachestat syscall"")
Tested-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Acked-by: Nhat Pham <nphamcs@gmail.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 mm/filemap.c | 19 +++++++++++++++++++
 1 file changed, 19 insertions(+)

diff --git a/mm/filemap.c b/mm/filemap.c
index dc83baab85a1..05adf0392625 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -4383,6 +4383,20 @@ static void filemap_cachestat(struct address_space *mapping,
         rcu_read_unlock();
 }
 
+/*
+ * See mincore: reveal pagecache information only for files
+ * that the calling process has write access to, or could (if
+ * tried) open for writing.
+ */
+static inline bool can_do_cachestat(struct file *f)
+{
+        if (f->f_mode & FMODE_WRITE)
+                return true;
+        if (inode_owner_or_capable(file_mnt_idmap(f), file_inode(f)))
+                return true;
+        return file_permission(f, MAY_WRITE) == 0;
+}
+
 /*
  * The cachestat(2) system call.
  *
@@ -4442,6 +4456,11 @@ SYSCALL_DEFINE4(cachestat, unsigned int, fd,
                 return -EOPNOTSUPP;
         }
 
+        if (!can_do_cachestat(fd_file(f))) {
+                fdput(f);
+                return -EPERM;
+        }
+
         if (flags != 0) {
                 fdput(f);
                 return -EINVAL;
-- 
2.39.5 (Apple Git-154)

",patching file mm/filemap.c\nHunk #1 succeeded at 4383 (offset 8 lines).\nHunk #2 FAILED at 4444.\n1 out of 2 hunks FAILED -- saving rejects to file mm/filemap.c.rej,18,0,2,65,1,COMMENTS,COMMENTS
CVE-2025-21702,647cef20e649c576dff271e018d5d15d998b629d,"From 647cef20e649c576dff271e018d5d15d998b629d Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index b50b2c2cc09b..e6bfd39ff339 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -40,6 +40,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
         unsigned int prev_backlog;
 
+        if (unlikely(READ_ONCE(sch->limit) == 0))
+                return qdisc_drop(skb, sch, to_free);
+
         if (likely(sch->q.qlen < READ_ONCE(sch->limit)))
                 return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)
",020ecb76812a0526f4130ab5aeb6dc7c773e7ab9,"From 020ecb76812a0526f4130ab5aeb6dc7c773e7ab9 Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

commit 647cef20e649c576dff271e018d5d15d998b629d upstream.

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Lee Jones <lee@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index e1040421b797..af5f2ab69b8d 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -39,6 +39,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
         unsigned int prev_backlog;
 
+        if (unlikely(READ_ONCE(sch->limit) == 0))
+                return qdisc_drop(skb, sch, to_free);
+
         if (likely(sch->q.qlen < sch->limit))
                 return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_fifo.c\nHunk #1 FAILED at 40.\n1 out of 1 hunk FAILED -- saving rejects to file net/sched/sch_fifo.c.rej,4,0,2,66,1,FORMATTING,COMMENTS
CVE-2025-21702,647cef20e649c576dff271e018d5d15d998b629d,"From 647cef20e649c576dff271e018d5d15d998b629d Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index b50b2c2cc09b..e6bfd39ff339 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -40,6 +40,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
         unsigned int prev_backlog;
 
+        if (unlikely(READ_ONCE(sch->limit) == 0))
+                return qdisc_drop(skb, sch, to_free);
+
         if (likely(sch->q.qlen < READ_ONCE(sch->limit)))
                 return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",78285b53266d6d51fa4ff504a23df03852eba84e,"From 78285b53266d6d51fa4ff504a23df03852eba84e Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

commit 647cef20e649c576dff271e018d5d15d998b629d upstream.

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Lee Jones <lee@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index 56f4c1621e44..b76c3d199ca1 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -38,6 +38,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(READ_ONCE(sch->limit) == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < sch->limit))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_fifo.c\nHunk #1 FAILED at 40.\n1 out of 1 hunk FAILED -- saving rejects to file net/sched/sch_fifo.c.rej,4,0,2,67,1,COMMENTS,SEMANTIC
CVE-2025-21702,647cef20e649c576dff271e018d5d15d998b629d,"From 647cef20e649c576dff271e018d5d15d998b629d Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index b50b2c2cc09b..e6bfd39ff339 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -40,6 +40,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(READ_ONCE(sch->limit) == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < READ_ONCE(sch->limit)))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",7a9723ec27aff5674f1fd4934608937f1d650980,"From 7a9723ec27aff5674f1fd4934608937f1d650980 Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

commit 647cef20e649c576dff271e018d5d15d998b629d upstream.

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Lee Jones <lee@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index e1040421b797..af5f2ab69b8d 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -39,6 +39,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(READ_ONCE(sch->limit) == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < sch->limit))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_fifo.c\nHunk #1 FAILED at 40.\n1 out of 1 hunk FAILED -- saving rejects to file net/sched/sch_fifo.c.rej,4,0,2,67,1,DELETE,SEMANTIC
CVE-2025-21702,647cef20e649c576dff271e018d5d15d998b629d,"From 647cef20e649c576dff271e018d5d15d998b629d Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index b50b2c2cc09b..e6bfd39ff339 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -40,6 +40,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(READ_ONCE(sch->limit) == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < READ_ONCE(sch->limit)))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",a56a6e8589a9b98d8171611fbcc1e45a15fd2455,"From a56a6e8589a9b98d8171611fbcc1e45a15fd2455 Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

commit 647cef20e649c576dff271e018d5d15d998b629d upstream.

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Lee Jones <lee@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index e1040421b797..af5f2ab69b8d 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -39,6 +39,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(READ_ONCE(sch->limit) == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < sch->limit))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_fifo.c\nHunk #1 FAILED at 40.\n1 out of 1 hunk FAILED -- saving rejects to file net/sched/sch_fifo.c.rej,4,0,2,67,1,SEMANTIC,SEMANTIC
CVE-2025-21702,647cef20e649c576dff271e018d5d15d998b629d,"From 647cef20e649c576dff271e018d5d15d998b629d Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index b50b2c2cc09b..e6bfd39ff339 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -40,6 +40,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(READ_ONCE(sch->limit) == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < READ_ONCE(sch->limit)))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",79a955ea4a2e5ddf4a36328959de0de496419888,"From 79a955ea4a2e5ddf4a36328959de0de496419888 Mon Sep 17 00:00:00 2001
From: Quang Le <quanglex97@gmail.com>
Date: Mon, 3 Feb 2025 16:58:38 -0800
Subject: [PATCH] pfifo_tail_enqueue: Drop new packet when sch->limit == 0

commit 647cef20e649c576dff271e018d5d15d998b629d upstream.

Expected behaviour:
In case we reach scheduler's limit, pfifo_tail_enqueue() will drop a
packet in scheduler's queue and decrease scheduler's qlen by one.
Then, pfifo_tail_enqueue() enqueue new packet and increase
scheduler's qlen by one. Finally, pfifo_tail_enqueue() return
`NET_XMIT_CN` status code.

Weird behaviour:
In case we set `sch->limit == 0` and trigger pfifo_tail_enqueue() on a
scheduler that has no packet, the 'drop a packet' step will do nothing.
This means the scheduler's qlen still has value equal 0.
Then, we continue to enqueue new packet and increase scheduler's qlen by
one. In summary, we can leverage pfifo_tail_enqueue() to increase qlen by
one and return `NET_XMIT_CN` status code.

The problem is:
Let's say we have two qdiscs: Qdisc_A and Qdisc_B.
 - Qdisc_A's type must have '->graft()' function to create parent/child relationship.
   Let's say Qdisc_A's type is `hfsc`. Enqueue packet to this qdisc will trigger `hfsc_enqueue`.
 - Qdisc_B's type is pfifo_head_drop. Enqueue packet to this qdisc will trigger `pfifo_tail_enqueue`.
 - Qdisc_B is configured to have `sch->limit == 0`.
 - Qdisc_A is configured to route the enqueued's packet to Qdisc_B.

Enqueue packet through Qdisc_A will lead to:
 - hfsc_enqueue(Qdisc_A) -> pfifo_tail_enqueue(Qdisc_B)
 - Qdisc_B->q.qlen += 1
 - pfifo_tail_enqueue() return `NET_XMIT_CN`
 - hfsc_enqueue() check for `NET_XMIT_SUCCESS` and see `NET_XMIT_CN` => hfsc_enqueue() don't increase qlen of Qdisc_A.

The whole process lead to a situation where Qdisc_A->q.qlen == 0 and Qdisc_B->q.qlen == 1.
Replace 'hfsc' with other type (for example: 'drr') still lead to the same problem.
This violate the design where parent's qlen should equal to the sum of its childrens'qlen.

Bug impact: This issue can be used for user->kernel privilege escalation when it is reachable.

Fixes: 57dbb2d83d10 (""sched: add head drop fifo queue"")
Reported-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Quang Le <quanglex97@gmail.com>
Signed-off-by: Cong Wang <cong.wang@bytedance.com>
Link: https://patch.msgid.link/20250204005841.223511-2-xiyou.wangcong@gmail.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/sched/sch_fifo.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/net/sched/sch_fifo.c b/net/sched/sch_fifo.c
index e1040421b797..1080d89f9178 100644
--- a/net/sched/sch_fifo.c
+++ b/net/sched/sch_fifo.c
@@ -39,6 +39,9 @@ static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch,
 {
 	unsigned int prev_backlog;
 
+	if (unlikely(sch->limit == 0))
+		return qdisc_drop(skb, sch, to_free);
+
 	if (likely(sch->q.qlen < sch->limit))
 		return qdisc_enqueue_tail(skb, sch);
 
-- 
2.39.5 (Apple Git-154)

",patching file net/sched/sch_fifo.c\nHunk #1 FAILED at 40.\n1 out of 1 hunk FAILED -- saving rejects to file net/sched/sch_fifo.c.rej,4,0,2,67,1,SEMANTIC,SEMANTIC
CVE-2025-21706,1bb0d1348546ad059f55c93def34e67cb2a034a6,"From 1bb0d1348546ad059f55c93def34e67cb2a034a6 Mon Sep 17 00:00:00 2001
From: ""Matthieu Baerts (NGI0)"" <matttbe@kernel.org>
Date: Thu, 23 Jan 2025 19:05:55 +0100
Subject: [PATCH] mptcp: pm: only set fullmesh for subflow endp

With the in-kernel path-manager, it is possible to change the 'fullmesh'
flag. The code in mptcp_pm_nl_fullmesh() expects to change it only on
'subflow' endpoints, to recreate more or less subflows using the linked
address.

Unfortunately, the set_flags() hook was a bit more permissive, and
allowed 'implicit' endpoints to get the 'fullmesh' flag while it is not
allowed before.

That's what syzbot found, triggering the following warning:

  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 __mark_subflow_endp_available net/mptcp/pm_netlink.c:1496 [inline]
  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 mptcp_pm_nl_fullmesh net/mptcp/pm_netlink.c:1980 [inline]
  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 mptcp_nl_set_flags net/mptcp/pm_netlink.c:2003 [inline]
  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 mptcp_pm_nl_set_flags+0x974/0xdc0 net/mptcp/pm_netlink.c:2064
  Modules linked in:
  CPU: 0 UID: 0 PID: 6499 Comm: syz.1.413 Not tainted 6.13.0-rc5-syzkaller-00172-gd1bf27c4e176 #0
  Hardware name: Google Compute Engine/Google Compute Engine, BIOS Google 09/13/2024
  RIP: 0010:__mark_subflow_endp_available net/mptcp/pm_netlink.c:1496 [inline]
  RIP: 0010:mptcp_pm_nl_fullmesh net/mptcp/pm_netlink.c:1980 [inline]
  RIP: 0010:mptcp_nl_set_flags net/mptcp/pm_netlink.c:2003 [inline]
  RIP: 0010:mptcp_pm_nl_set_flags+0x974/0xdc0 net/mptcp/pm_netlink.c:2064
  Code: 01 00 00 49 89 c5 e8 fb 45 e8 f5 e9 b8 fc ff ff e8 f1 45 e8 f5 4c 89 f7 be 03 00 00 00 e8 44 1d 0b f9 eb a0 e8 dd 45 e8 f5 90 <0f> 0b 90 e9 17 ff ff ff 89 d9 80 e1 07 38 c1 0f 8c c9 fc ff ff 48
  RSP: 0018:ffffc9000d307240 EFLAGS: 00010293
  RAX: ffffffff8bb72e03 RBX: 0000000000000000 RCX: ffff88807da88000
  RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
  RBP: ffffc9000d307430 R08: ffffffff8bb72cf0 R09: 1ffff1100b842a5e
  R10: dffffc0000000000 R11: ffffed100b842a5f R12: ffff88801e2e5ac0
  R13: ffff88805c214800 R14: ffff88805c2152e8 R15: 1ffff1100b842a5d
  FS:  00005555619f6500(0000) GS:ffff8880b8600000(0000) knlGS:0000000000000000
  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
  CR2: 0000000020002840 CR3: 00000000247e6000 CR4: 00000000003526f0
  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
  Call Trace:
   <TASK>
   genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
   genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
   genl_rcv_msg+0xb14/0xec0 net/netlink/genetlink.c:1210
   netlink_rcv_skb+0x1e3/0x430 net/netlink/af_netlink.c:2542
   genl_rcv+0x28/0x40 net/netlink/genetlink.c:1219
   netlink_unicast_kernel net/netlink/af_netlink.c:1321 [inline]
   netlink_unicast+0x7f6/0x990 net/netlink/af_netlink.c:1347
   netlink_sendmsg+0x8e4/0xcb0 net/netlink/af_netlink.c:1891
   sock_sendmsg_nosec net/socket.c:711 [inline]
   __sock_sendmsg+0x221/0x270 net/socket.c:726
   ____sys_sendmsg+0x52a/0x7e0 net/socket.c:2583
   ___sys_sendmsg net/socket.c:2637 [inline]
   __sys_sendmsg+0x269/0x350 net/socket.c:2669
   do_syscall_x64 arch/x86/entry/common.c:52 [inline]
   do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
   entry_SYSCALL_64_after_hwframe+0x77/0x7f
  RIP: 0033:0x7f5fe8785d29
  Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 a8 ff ff ff f7 d8 64 89 01 48
  RSP: 002b:00007fff571f5558 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
  RAX: ffffffffffffffda RBX: 00007f5fe8975fa0 RCX: 00007f5fe8785d29
  RDX: 0000000000000000 RSI: 0000000020000480 RDI: 0000000000000007
  RBP: 00007f5fe8801b08 R08: 0000000000000000 R09: 0000000000000000
  R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
  R13: 00007f5fe8975fa0 R14: 00007f5fe8975fa0 R15: 00000000000011f4
   </TASK>

Here, syzbot managed to set the 'fullmesh' flag on an 'implicit' and
used -- according to 'id_avail_bitmap' -- endpoint, causing the PM to
try decrement the local_addr_used counter which is only incremented for
the 'subflow' endpoint.

Note that 'no type' endpoints -- not 'subflow', 'signal', 'implicit' --
are fine, because their ID will not be marked as used in the 'id_avail'
bitmap, and setting 'fullmesh' can help forcing the creation of subflow
when receiving an ADD_ADDR.

Fixes: 73c762c1f07d (""mptcp: set fullmesh flag in pm_netlink"")
Cc: stable@vger.kernel.org
Reported-by: syzbot+cd16e79c1e45f3fe0377@syzkaller.appspotmail.com
Closes: https://lore.kernel.org/6786ac51.050a0220.216c54.00a6.GAE@google.com
Closes: https://github.com/multipath-tcp/mptcp_net-next/issues/540
Reviewed-by: Mat Martineau <martineau@kernel.org>
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Link: https://patch.msgid.link/20250123-net-mptcp-syzbot-issues-v1-2-af73258a726f@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/mptcp/pm_netlink.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/net/mptcp/pm_netlink.c b/net/mptcp/pm_netlink.c
index 98ac73938bd8..572d160edca3 100644
--- a/net/mptcp/pm_netlink.c
+++ b/net/mptcp/pm_netlink.c
@@ -2020,7 +2020,8 @@ int mptcp_pm_nl_set_flags(struct sk_buff *skb, struct genl_info *info)
                 return -EINVAL;
         }
         if ((addr.flags & MPTCP_PM_ADDR_FLAG_FULLMESH) &&
-            (entry->flags & MPTCP_PM_ADDR_FLAG_SIGNAL)) {
+            (entry->flags & (MPTCP_PM_ADDR_FLAG_SIGNAL |
+                             MPTCP_PM_ADDR_FLAG_IMPLICIT))) {
                 spin_unlock_bh(&pernet->lock);
                 GENL_SET_ERR_MSG(info, ""invalid addr flags"");
                 return -EINVAL;
-- 
2.39.5 (Apple Git-154)

",de3b8d41d2547452c4cafb146d003fa4689fbaf2,"From de3b8d41d2547452c4cafb146d003fa4689fbaf2 Mon Sep 17 00:00:00 2001
From: ""Matthieu Baerts (NGI0)"" <matttbe@kernel.org>
Date: Sun, 9 Feb 2025 18:41:55 +0100
Subject: [PATCH] mptcp: pm: only set fullmesh for subflow endp

commit 1bb0d1348546ad059f55c93def34e67cb2a034a6 upstream.

With the in-kernel path-manager, it is possible to change the 'fullmesh'
flag. The code in mptcp_pm_nl_fullmesh() expects to change it only on
'subflow' endpoints, to recreate more or less subflows using the linked
address.

Unfortunately, the set_flags() hook was a bit more permissive, and
allowed 'implicit' endpoints to get the 'fullmesh' flag while it is not
allowed before.

That's what syzbot found, triggering the following warning:

  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 __mark_subflow_endp_available net/mptcp/pm_netlink.c:1496 [inline]
  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 mptcp_pm_nl_fullmesh net/mptcp/pm_netlink.c:1980 [inline]
  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 mptcp_nl_set_flags net/mptcp/pm_netlink.c:2003 [inline]
  WARNING: CPU: 0 PID: 6499 at net/mptcp/pm_netlink.c:1496 mptcp_pm_nl_set_flags+0x974/0xdc0 net/mptcp/pm_netlink.c:2064
  Modules linked in:
  CPU: 0 UID: 0 PID: 6499 Comm: syz.1.413 Not tainted 6.13.0-rc5-syzkaller-00172-gd1bf27c4e176 #0
  Hardware name: Google Compute Engine/Google Compute Engine, BIOS Google 09/13/2024
  RIP: 0010:__mark_subflow_endp_available net/mptcp/pm_netlink.c:1496 [inline]
  RIP: 0010:mptcp_pm_nl_fullmesh net/mptcp/pm_netlink.c:1980 [inline]
  RIP: 0010:mptcp_nl_set_flags net/mptcp/pm_netlink.c:2003 [inline]
  RIP: 0010:mptcp_pm_nl_set_flags+0x974/0xdc0 net/mptcp/pm_netlink.c:2064
  Code: 01 00 00 49 89 c5 e8 fb 45 e8 f5 e9 b8 fc ff ff e8 f1 45 e8 f5 4c 89 f7 be 03 00 00 00 e8 44 1d 0b f9 eb a0 e8 dd 45 e8 f5 90 <0f> 0b 90 e9 17 ff ff ff 89 d9 80 e1 07 38 c1 0f 8c c9 fc ff ff 48
  RSP: 0018:ffffc9000d307240 EFLAGS: 00010293
  RAX: ffffffff8bb72e03 RBX: 0000000000000000 RCX: ffff88807da88000
  RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
  RBP: ffffc9000d307430 R08: ffffffff8bb72cf0 R09: 1ffff1100b842a5e
  R10: dffffc0000000000 R11: ffffed100b842a5f R12: ffff88801e2e5ac0
  R13: ffff88805c214800 R14: ffff88805c2152e8 R15: 1ffff1100b842a5d
  FS:  00005555619f6500(0000) GS:ffff8880b8600000(0000) knlGS:0000000000000000
  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
  CR2: 0000000020002840 CR3: 00000000247e6000 CR4: 00000000003526f0
  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
  Call Trace:
   <TASK>
   genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
   genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
   genl_rcv_msg+0xb14/0xec0 net/netlink/genetlink.c:1210
   netlink_rcv_skb+0x1e3/0x430 net/netlink/af_netlink.c:2542
   genl_rcv+0x28/0x40 net/netlink/genetlink.c:1219
   netlink_unicast_kernel net/netlink/af_netlink.c:1321 [inline]
   netlink_unicast+0x7f6/0x990 net/netlink/af_netlink.c:1347
   netlink_sendmsg+0x8e4/0xcb0 net/netlink/af_netlink.c:1891
   sock_sendmsg_nosec net/socket.c:711 [inline]
   __sock_sendmsg+0x221/0x270 net/socket.c:726
   ____sys_sendmsg+0x52a/0x7e0 net/socket.c:2583
   ___sys_sendmsg net/socket.c:2637 [inline]
   __sys_sendmsg+0x269/0x350 net/socket.c:2669
   do_syscall_x64 arch/x86/entry/common.c:52 [inline]
   do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
   entry_SYSCALL_64_after_hwframe+0x77/0x7f
  RIP: 0033:0x7f5fe8785d29
  Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 a8 ff ff ff f7 d8 64 89 01 48
  RSP: 002b:00007fff571f5558 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
  RAX: ffffffffffffffda RBX: 00007f5fe8975fa0 RCX: 00007f5fe8785d29
  RDX: 0000000000000000 RSI: 0000000020000480 RDI: 0000000000000007
  RBP: 00007f5fe8801b08 R08: 0000000000000000 R09: 0000000000000000
  R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
  R13: 00007f5fe8975fa0 R14: 00007f5fe8975fa0 R15: 00000000000011f4
   </TASK>

Here, syzbot managed to set the 'fullmesh' flag on an 'implicit' and
used -- according to 'id_avail_bitmap' -- endpoint, causing the PM to
try decrement the local_addr_used counter which is only incremented for
the 'subflow' endpoint.

Note that 'no type' endpoints -- not 'subflow', 'signal', 'implicit' --
are fine, because their ID will not be marked as used in the 'id_avail'
bitmap, and setting 'fullmesh' can help forcing the creation of subflow
when receiving an ADD_ADDR.

Fixes: 73c762c1f07d (""mptcp: set fullmesh flag in pm_netlink"")
Cc: stable@vger.kernel.org
Reported-by: syzbot+cd16e79c1e45f3fe0377@syzkaller.appspotmail.com
Closes: https://lore.kernel.org/6786ac51.050a0220.216c54.00a6.GAE@google.com
Closes: https://github.com/multipath-tcp/mptcp_net-next/issues/540
Reviewed-by: Mat Martineau <martineau@kernel.org>
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Link: https://patch.msgid.link/20250123-net-mptcp-syzbot-issues-v1-2-af73258a726f@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
[ Conflicts in pm_netlink.c, because the code has been moved around in
  commit 6a42477fe449 (""mptcp: update set_flags interfaces""), but the
  same fix can still be applied at the original place. ]
Signed-off-by: Matthieu Baerts (NGI0) <matttbe@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/mptcp/pm_netlink.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/net/mptcp/pm_netlink.c b/net/mptcp/pm_netlink.c
index 2cf4393e48dc..2b63c5492eed 100644
--- a/net/mptcp/pm_netlink.c
+++ b/net/mptcp/pm_netlink.c
@@ -2069,7 +2069,8 @@ int mptcp_pm_nl_set_flags(struct net *net, struct mptcp_pm_addr_entry *addr, u8
                 return -EINVAL;
         }
         if ((addr->flags & MPTCP_PM_ADDR_FLAG_FULLMESH) &&
-            (entry->flags & MPTCP_PM_ADDR_FLAG_SIGNAL)) {
+            (entry->flags & (MPTCP_PM_ADDR_FLAG_SIGNAL |
+                             MPTCP_PM_ADDR_FLAG_IMPLICIT))) {
                 spin_unlock_bh(&pernet->lock);
                 return -EINVAL;
         }
-- 
2.39.5 (Apple Git-154)

",patching file net/mptcp/pm_netlink.c\nHunk #1 FAILED at 2020.\n1 out of 1 hunk FAILED -- saving rejects to file net/mptcp/pm_netlink.c.rej,4,0,2,107,1,DISJOINT,DISJOINT
CVE-2025-21712,8d28d0ddb986f56920ac97ae704cc3340a699a30,"From 8d28d0ddb986f56920ac97ae704cc3340a699a30 Mon Sep 17 00:00:00 2001
From: Yu Kuai <yukuai3@huawei.com>
Date: Fri, 24 Jan 2025 17:20:55 +0800
Subject: [PATCH] md/md-bitmap: Synchronize bitmap_get_stats() with bitmap
 lifetime

After commit ec6bb299c7c3 (""md/md-bitmap: add 'sync_size' into struct
md_bitmap_stats""), following panic is reported:

Oops: general protection fault, probably for non-canonical address
RIP: 0010:bitmap_get_stats+0x2b/0xa0
Call Trace:
 <TASK>
 md_seq_show+0x2d2/0x5b0
 seq_read_iter+0x2b9/0x470
 seq_read+0x12f/0x180
 proc_reg_read+0x57/0xb0
 vfs_read+0xf6/0x380
 ksys_read+0x6c/0xf0
 do_syscall_64+0x82/0x170
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Root cause is that bitmap_get_stats() can be called at anytime if mddev
is still there, even if bitmap is destroyed, or not fully initialized.
Deferenceing bitmap in this case can crash the kernel. Meanwhile, the
above commit start to deferencing bitmap->storage, make the problem
easier to trigger.

Fix the problem by protecting bitmap_get_stats() with bitmap_info.mutex.

Cc: stable@vger.kernel.org # v6.12+
Fixes: 32a7627cf3a3 (""[PATCH] md: optimised resync using Bitmap based intent logging"")
Reported-and-tested-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>
Closes: https://lore.kernel.org/linux-raid/ca3a91a2-50ae-4f68-b317-abd9889f3907@oracle.com/T/#m6e5086c95201135e4941fe38f9efa76daf9666c5
Signed-off-by: Yu Kuai <yukuai3@huawei.com>
Link: https://lore.kernel.org/r/20250124092055.4050195-1-yukuai1@huaweicloud.com
Signed-off-by: Song Liu <song@kernel.org>
---
 drivers/md/md-bitmap.c | 5 ++++-
 drivers/md/md.c        | 5 +++++
 2 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/drivers/md/md-bitmap.c b/drivers/md/md-bitmap.c
index ec4ecd96e6b1..23c09d22fcdb 100644
--- a/drivers/md/md-bitmap.c
+++ b/drivers/md/md-bitmap.c
@@ -2355,7 +2355,10 @@ static int bitmap_get_stats(void *data, struct md_bitmap_stats *stats)
 
         if (!bitmap)
                 return -ENOENT;
-
+        if (bitmap->mddev->bitmap_info.external)
+                return -ENOENT;
+        if (!bitmap->storage.sb_page) /* no superblock */
+                return -EINVAL;
         sb = kmap_local_page(bitmap->storage.sb_page);
         stats->sync_size = le64_to_cpu(sb->sync_size);
         kunmap_local(sb);
diff --git a/drivers/md/md.c b/drivers/md/md.c
index 866015b681af..465ca2af1e6e 100644
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@ -8376,6 +8376,10 @@ static int md_seq_show(struct seq_file *seq, void *v)
                 return 0;
 
         spin_unlock(&all_mddevs_lock);
+
+        /* prevent bitmap to be freed after checking */
+        mutex_lock(&mddev->bitmap_info.mutex);
+
         spin_lock(&mddev->lock);
         if (mddev->pers || mddev->raid_disks || !list_empty(&mddev->disks)) {
                 seq_printf(seq, ""%s : "", mdname(mddev));
@@ -8451,6 +8455,7 @@ static int md_seq_show(struct seq_file *seq, void *v)
                 seq_printf(seq, ""\n"");
         }
         spin_unlock(&mddev->lock);
+        mutex_unlock(&mddev->bitmap_info.mutex);
         spin_lock(&all_mddevs_lock);
 
         if (mddev == list_last_entry(&all_mddevs, struct mddev, all_mddevs))
-- 
2.39.5 (Apple Git-154)

",032fa54f486eac5507976e7e31f079a767bc13a8,"From 032fa54f486eac5507976e7e31f079a767bc13a8 Mon Sep 17 00:00:00 2001
From: Yu Kuai <yukuai3@huawei.com>
Date: Fri, 24 Jan 2025 17:20:55 +0800
Subject: [PATCH] md/md-bitmap: Synchronize bitmap_get_stats() with bitmap
 lifetime

[ Upstream commit 8d28d0ddb986f56920ac97ae704cc3340a699a30 ]

After commit ec6bb299c7c3 (""md/md-bitmap: add 'sync_size' into struct
md_bitmap_stats""), following panic is reported:

Oops: general protection fault, probably for non-canonical address
RIP: 0010:bitmap_get_stats+0x2b/0xa0
Call Trace:
 <TASK>
 md_seq_show+0x2d2/0x5b0
 seq_read_iter+0x2b9/0x470
 seq_read+0x12f/0x180
 proc_reg_read+0x57/0xb0
 vfs_read+0xf6/0x380
 ksys_read+0x6c/0xf0
 do_syscall_64+0x82/0x170
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Root cause is that bitmap_get_stats() can be called at anytime if mddev
is still there, even if bitmap is destroyed, or not fully initialized.
Deferenceing bitmap in this case can crash the kernel. Meanwhile, the
above commit start to deferencing bitmap->storage, make the problem
easier to trigger.

Fix the problem by protecting bitmap_get_stats() with bitmap_info.mutex.

Cc: stable@vger.kernel.org # v6.12+
Fixes: 32a7627cf3a3 (""[PATCH] md: optimised resync using Bitmap based intent logging"")
Reported-and-tested-by: Harshit Mogalapalli <harshit.m.mogalapalli@oracle.com>
Closes: https://lore.kernel.org/linux-raid/ca3a91a2-50ae-4f68-b317-abd9889f3907@oracle.com/T/#m6e5086c95201135e4941fe38f9efa76daf9666c5
Signed-off-by: Yu Kuai <yukuai3@huawei.com>
Link: https://lore.kernel.org/r/20250124092055.4050195-1-yukuai1@huaweicloud.com
Signed-off-by: Song Liu <song@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/md/md-bitmap.c | 5 ++++-
 drivers/md/md.c        | 4 ++++
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/drivers/md/md-bitmap.c b/drivers/md/md-bitmap.c
index bddf4f3d27a7..e18e21b24210 100644
--- a/drivers/md/md-bitmap.c
+++ b/drivers/md/md-bitmap.c
@@ -2029,7 +2029,10 @@ int md_bitmap_get_stats(struct bitmap *bitmap, struct md_bitmap_stats *stats)
 
         if (!bitmap)
                 return -ENOENT;
-
+        if (bitmap->mddev->bitmap_info.external)
+                return -ENOENT;
+        if (!bitmap->storage.sb_page) /* no superblock */
+                return -EINVAL;
         sb = kmap_local_page(bitmap->storage.sb_page);
         stats->sync_size = le64_to_cpu(sb->sync_size);
         kunmap_local(sb);
diff --git a/drivers/md/md.c b/drivers/md/md.c
index a52843826a95..5e2751d42f64 100644
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@ -8368,6 +8368,9 @@ static int md_seq_show(struct seq_file *seq, void *v)
                 return 0;
         }
 
+        /* prevent bitmap to be freed after checking */
+        mutex_lock(&mddev->bitmap_info.mutex);
+
         spin_lock(&mddev->lock);
         if (mddev->pers || mddev->raid_disks || !list_empty(&mddev->disks)) {
                 seq_printf(seq, ""%s : %sactive"", mdname(mddev),
@@ -8438,6 +8441,7 @@ static int md_seq_show(struct seq_file *seq, void *v)
                 seq_printf(seq, ""\n"");
         }
         spin_unlock(&mddev->lock);
+        mutex_unlock(&mddev->bitmap_info.mutex);
 
         return 0;
 }
-- 
2.39.5 (Apple Git-154)

",patching file drivers/md/md-bitmap.c\nHunk #1 succeeded at 2029 (offset -326 lines).\npatching file drivers/md/md.c\nHunk #1 FAILED at 8376.\nHunk #2 FAILED at 8451.\n2 out of 2 hunks FAILED -- saving rejects to file drivers/md/md.c.rej,11,0,5,84,1,SEMANTIC,COMMENTS
CVE-2025-21721,ee70999a988b8abc3490609142f50ebaa8344432,"From ee70999a988b8abc3490609142f50ebaa8344432 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 0a3aea6c416b..9b7f8e9655a2 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -400,7 +400,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
         return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
                     struct folio *folio, struct inode *inode)
 {
         size_t from = offset_in_folio(folio, de);
@@ -410,11 +410,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
         folio_lock(folio);
         err = nilfs_prepare_chunk(folio, from, to);
-        BUG_ON(err);
+        if (unlikely(err)) {
+                folio_unlock(folio);
+                return err;
+        }
         de->inode = cpu_to_le64(inode->i_ino);
         de->file_type = fs_umode_to_ftype(inode->i_mode);
         nilfs_commit_chunk(folio, mapping, from, to);
         inode_set_mtime_to_ts(dir, inode_set_ctime_current(dir));
+        return 0;
 }
 
 /*
@@ -543,7 +547,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct folio *folio)
                 from = (char *)pde - kaddr;
         folio_lock(folio);
         err = nilfs_prepare_chunk(folio, from, to);
-        BUG_ON(err);
+        if (unlikely(err)) {
+                folio_unlock(folio);
+                goto out;
+        }
         if (pde)
                 pde->rec_len = nilfs_rec_len_to_disk(to - from);
         dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 1d836a5540f3..e02fae6757f1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct mnt_idmap *idmap,
                         err = PTR_ERR(new_de);
                         goto out_dir;
                 }
-                nilfs_set_link(new_dir, new_de, new_folio, old_inode);
+                err = nilfs_set_link(new_dir, new_de, new_folio, old_inode);
                 folio_release_kmap(new_folio, new_de);
+                if (unlikely(err))
+                        goto out_dir;
                 nilfs_mark_inode_dirty(new_dir);
                 inode_set_ctime_current(new_inode);
                 if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct mnt_idmap *idmap,
          */
         inode_set_ctime_current(old_inode);
 
-        nilfs_delete_entry(old_de, old_folio);
-
-        if (dir_de) {
-                nilfs_set_link(old_inode, dir_de, dir_folio, new_dir);
-                folio_release_kmap(dir_folio, dir_de);
-                drop_nlink(old_dir);
+        err = nilfs_delete_entry(old_de, old_folio);
+        if (likely(!err)) {
+                if (dir_de) {
+                        err = nilfs_set_link(old_inode, dir_de, dir_folio,
+                                             new_dir);
+                        drop_nlink(old_dir);
+                }
+                nilfs_mark_inode_dirty(old_dir);
         }
-        folio_release_kmap(old_folio, old_de);
-
-        nilfs_mark_inode_dirty(old_dir);
         nilfs_mark_inode_dirty(old_inode);
 
-        err = nilfs_transaction_commit(old_dir->i_sb);
-        return err;
-
 out_dir:
         if (dir_de)
                 folio_release_kmap(dir_folio, dir_de);
 out_old:
         folio_release_kmap(old_folio, old_de);
 out:
-        nilfs_transaction_abort(old_dir->i_sb);
+        if (likely(!err))
+                err = nilfs_transaction_commit(old_dir->i_sb);
+        else
+                nilfs_transaction_abort(old_dir->i_sb);
         return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index dff241c53fc5..cb6ed54accd7 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -261,8 +261,8 @@ struct nilfs_dir_entry *nilfs_find_entry(struct inode *, const struct qstr *,
 int nilfs_delete_entry(struct nilfs_dir_entry *, struct folio *);
 int nilfs_empty_dir(struct inode *);
 struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct folio **);
-void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-                           struct folio *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+                   struct folio *folio, struct inode *inode);
 
 /* file.c */
 extern int nilfs_sync_file(struct file *, loff_t, loff_t, int);
-- 
2.39.5 (Apple Git-154)
",7891ac3b0a5c56f7148af507306308ab841cdc31,"From 7891ac3b0a5c56f7148af507306308ab841cdc31 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 21 Feb 2025 22:37:55 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

commit ee70999a988b8abc3490609142f50ebaa8344432 upstream.

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.

This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index c6dd8ef7d284..49ca762baa8f 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -444,7 +444,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
         return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
                     struct page *page, struct inode *inode)
 {
         unsigned int from = (char *)de - (char *)page_address(page);
@@ -454,11 +454,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
         lock_page(page);
         err = nilfs_prepare_chunk(page, from, to);
-        BUG_ON(err);
+        if (unlikely(err)) {
+                unlock_page(page);
+                return err;
+        }
         de->inode = cpu_to_le64(inode->i_ino);
         nilfs_set_de_type(de, inode);
         nilfs_commit_chunk(page, mapping, from, to);
         dir->i_mtime = inode_set_ctime_current(dir);
+        return 0;
 }
 
 /*
@@ -590,7 +594,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct page *page)
                 from = (char *)pde - (char *)page_address(page);
         lock_page(page);
         err = nilfs_prepare_chunk(page, from, to);
-        BUG_ON(err);
+        if (unlikely(err)) {
+                unlock_page(page);
+                goto out;
+        }
         if (pde)
                 pde->rec_len = nilfs_rec_len_to_disk(to - from);
         dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 4d60ccdd85f3..43f01fe556fe 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct mnt_idmap *idmap,
                         err = PTR_ERR(new_de);
                         goto out_dir;
                 }
-                nilfs_set_link(new_dir, new_de, new_page, old_inode);
+                err = nilfs_set_link(new_dir, new_de, new_page, old_inode);
                 nilfs_put_page(new_page);
+                if (unlikely(err))
+                        goto out_dir;
                 nilfs_mark_inode_dirty(new_dir);
                 inode_set_ctime_current(new_inode);
                 if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct mnt_idmap *idmap,
          */
         inode_set_ctime_current(old_inode);
 
-        nilfs_delete_entry(old_de, old_page);
-
-        if (dir_de) {
-                nilfs_set_link(old_inode, dir_de, dir_page, new_dir);
-                nilfs_put_page(dir_page);
-                drop_nlink(old_dir);
+        err = nilfs_delete_entry(old_de, old_page);
+        if (likely(!err)) {
+                if (dir_de) {
+                        err = nilfs_set_link(old_inode, dir_de, dir_page,
+                                             new_dir);
+                        drop_nlink(old_dir);
+                }
+                nilfs_mark_inode_dirty(old_dir);
         }
-        nilfs_put_page(old_page);
-
-        nilfs_mark_inode_dirty(old_dir);
         nilfs_mark_inode_dirty(old_inode);
 
-        err = nilfs_transaction_commit(old_dir->i_sb);
-        return err;
-
 out_dir:
         if (dir_de)
                 nilfs_put_page(dir_page);
 out_old:
         nilfs_put_page(old_page);
 out:
-        nilfs_transaction_abort(old_dir->i_sb);
+        if (likely(!err))
+                err = nilfs_transaction_commit(old_dir->i_sb);
+        else
+                nilfs_transaction_abort(old_dir->i_sb);
         return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index fbf74c1cfd1d..4c4b76865484 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -240,8 +240,8 @@ nilfs_find_entry(struct inode *, const struct qstr *, struct page **);
 extern int nilfs_delete_entry(struct nilfs_dir_entry *, struct page *);
 extern int nilfs_empty_dir(struct inode *);
 extern struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct page **);
-extern void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-                           struct page *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+                   struct page *page, struct inode *inode);
 
 static inline void nilfs_put_page(struct page *page)
 {
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/dir.c\nHunk #1 FAILED at 400.\nHunk #2 FAILED at 410.\nHunk #3 FAILED at 543.\n3 out of 3 hunks FAILED -- saving rejects to file fs/nilfs2/dir.c.rej\npatching file fs/nilfs2/namei.c\nHunk #1 FAILED at 406.\nHunk #2 FAILED at 430.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/namei.c.rej\npatching file fs/nilfs2/nilfs.h\nHunk #1 FAILED at 261.\n1 out of 1 hunk FAILED -- saving rejects to file fs/nilfs2/nilfs.h.rej,47,1,32,161,1,OTHER,OTHER
CVE-2025-21721,ee70999a988b8abc3490609142f50ebaa8344432,"From ee70999a988b8abc3490609142f50ebaa8344432 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 0a3aea6c416b..9b7f8e9655a2 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -400,7 +400,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct folio *folio, struct inode *inode)
 {
 	size_t from = offset_in_folio(folio, de);
@@ -410,11 +410,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	de->file_type = fs_umode_to_ftype(inode->i_mode);
 	nilfs_commit_chunk(folio, mapping, from, to);
 	inode_set_mtime_to_ts(dir, inode_set_ctime_current(dir));
+	return 0;
 }
 
 /*
@@ -543,7 +547,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct folio *folio)
 		from = (char *)pde - kaddr;
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 1d836a5540f3..e02fae6757f1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_folio, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_folio, old_inode);
 		folio_release_kmap(new_folio, new_de);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		inode_set_ctime_current(new_inode);
 		if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 	 */
 	inode_set_ctime_current(old_inode);
 
-	nilfs_delete_entry(old_de, old_folio);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_folio, new_dir);
-		folio_release_kmap(dir_folio, dir_de);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_folio);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_folio,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	folio_release_kmap(old_folio, old_de);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		folio_release_kmap(dir_folio, dir_de);
 out_old:
 	folio_release_kmap(old_folio, old_de);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index dff241c53fc5..cb6ed54accd7 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -261,8 +261,8 @@ struct nilfs_dir_entry *nilfs_find_entry(struct inode *, const struct qstr *,
 int nilfs_delete_entry(struct nilfs_dir_entry *, struct folio *);
 int nilfs_empty_dir(struct inode *);
 struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct folio **);
-void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct folio *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct folio *folio, struct inode *inode);
 
 /* file.c */
 extern int nilfs_sync_file(struct file *, loff_t, loff_t, int);
-- 
2.39.5 (Apple Git-154)

",b38c6c260c2415c7f0968871305e7a093daabb4c,"From b38c6c260c2415c7f0968871305e7a093daabb4c Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

commit ee70999a988b8abc3490609142f50ebaa8344432 upstream.

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 3a8ff6d4a1b0..3d7e692f3e7f 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -444,7 +444,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct page *page, struct inode *inode)
 {
 	unsigned int from = (char *)de - (char *)page_address(page);
@@ -454,11 +454,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	lock_page(page);
 	err = nilfs_prepare_chunk(page, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		unlock_page(page);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	nilfs_set_de_type(de, inode);
 	nilfs_commit_chunk(page, mapping, from, to);
 	dir->i_mtime = dir->i_ctime = current_time(dir);
+	return 0;
 }
 
 /*
@@ -590,7 +594,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct page *page)
 		from = (char *)pde - (char *)page_address(page);
 	lock_page(page);
 	err = nilfs_prepare_chunk(page, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		unlock_page(page);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 4f1109e40002..380af65e9ea1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -403,8 +403,10 @@ static int nilfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_page, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_page, old_inode);
 		nilfs_put_page(new_page);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		new_inode->i_ctime = current_time(new_inode);
 		if (dir_de)
@@ -427,28 +429,27 @@ static int nilfs_rename(struct inode *old_dir, struct dentry *old_dentry,
 	 */
 	old_inode->i_ctime = current_time(old_inode);
 
-	nilfs_delete_entry(old_de, old_page);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_page, new_dir);
-		nilfs_put_page(dir_page);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_page);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_page,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	nilfs_put_page(old_page);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		nilfs_put_page(dir_page);
 out_old:
 	nilfs_put_page(old_page);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index 22d9cb156292..df67e013de24 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -240,8 +240,8 @@ nilfs_find_entry(struct inode *, const struct qstr *, struct page **);
 extern int nilfs_delete_entry(struct nilfs_dir_entry *, struct page *);
 extern int nilfs_empty_dir(struct inode *);
 extern struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct page **);
-extern void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct page *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct page *page, struct inode *inode);
 
 static inline void nilfs_put_page(struct page *page)
 {
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/dir.c\nHunk #1 FAILED at 400.\nHunk #2 FAILED at 410.\nHunk #3 FAILED at 543.\n3 out of 3 hunks FAILED -- saving rejects to file fs/nilfs2/dir.c.rej\npatching file fs/nilfs2/namei.c\nHunk #1 FAILED at 406.\nHunk #2 FAILED at 430.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/namei.c.rej\npatching file fs/nilfs2/nilfs.h\nHunk #1 FAILED at 261.\n1 out of 1 hunk FAILED -- saving rejects to file fs/nilfs2/nilfs.h.rej,47,1,32,162,1,FORMATTING,OTHER
CVE-2025-21721,ee70999a988b8abc3490609142f50ebaa8344434,,,,,0,0,0,0,0,DELETE,DELETE
CVE-2025-21721,ee70999a988b8abc3490609142f50ebaa8344432,"From ee70999a988b8abc3490609142f50ebaa8344432 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 0a3aea6c416b..9b7f8e9655a2 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -400,7 +400,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct folio *folio, struct inode *inode)
 {
 	size_t from = offset_in_folio(folio, de);
@@ -410,11 +410,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	de->file_type = fs_umode_to_ftype(inode->i_mode);
 	nilfs_commit_chunk(folio, mapping, from, to);
 	inode_set_mtime_to_ts(dir, inode_set_ctime_current(dir));
+	return 0;
 }
 
 /*
@@ -543,7 +547,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct folio *folio)
 		from = (char *)pde - kaddr;
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 1d836a5540f3..e02fae6757f1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_folio, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_folio, old_inode);
 		folio_release_kmap(new_folio, new_de);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		inode_set_ctime_current(new_inode);
 		if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 	 */
 	inode_set_ctime_current(old_inode);
 
-	nilfs_delete_entry(old_de, old_folio);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_folio, new_dir);
-		folio_release_kmap(dir_folio, dir_de);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_folio);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_folio,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	folio_release_kmap(old_folio, old_de);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		folio_release_kmap(dir_folio, dir_de);
 out_old:
 	folio_release_kmap(old_folio, old_de);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index dff241c53fc5..cb6ed54accd7 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -261,8 +261,8 @@ struct nilfs_dir_entry *nilfs_find_entry(struct inode *, const struct qstr *,
 int nilfs_delete_entry(struct nilfs_dir_entry *, struct folio *);
 int nilfs_empty_dir(struct inode *);
 struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct folio **);
-void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct folio *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct folio *folio, struct inode *inode);
 
 /* file.c */
 extern int nilfs_sync_file(struct file *, loff_t, loff_t, int);
-- 
2.39.5 (Apple Git-154)

",f70bd2d8ca454e0ed78970f72147ca321dbaa015,"From f70bd2d8ca454e0ed78970f72147ca321dbaa015 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

commit ee70999a988b8abc3490609142f50ebaa8344432 upstream.

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 3a8ff6d4a1b0..3d7e692f3e7f 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -444,7 +444,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
         return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
                     struct page *page, struct inode *inode)
 {
         unsigned int from = (char *)de - (char *)page_address(page);
@@ -454,11 +454,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
         lock_page(page);
         err = nilfs_prepare_chunk(page, from, to);
-        BUG_ON(err);
+        if (unlikely(err)) {
+                unlock_page(page);
+                return err;
+        }
         de->inode = cpu_to_le64(inode->i_ino);
         nilfs_set_de_type(de, inode);
         nilfs_commit_chunk(page, mapping, from, to);
         dir->i_mtime = dir->i_ctime = current_time(dir);
+        return 0;
 }
 
 /*
@@ -590,7 +594,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct page *page)
                 from = (char *)pde - (char *)page_address(page);
         lock_page(page);
         err = nilfs_prepare_chunk(page, from, to);
-        BUG_ON(err);
+        if (unlikely(err)) {
+                unlock_page(page);
+                goto out;
+        }
         if (pde)
                 pde->rec_len = nilfs_rec_len_to_disk(to - from);
         dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 4f1109e40002..380af65e9ea1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -403,8 +403,10 @@ static int nilfs_rename(struct inode *old_dir, struct dentry *old_dentry,
                         err = PTR_ERR(new_de);
                         goto out_dir;
                 }
-                nilfs_set_link(new_dir, new_de, new_page, old_inode);
+                err = nilfs_set_link(new_dir, new_de, new_page, old_inode);
                 nilfs_put_page(new_page);
+                if (unlikely(err))
+                        goto out_dir;
                 nilfs_mark_inode_dirty(new_dir);
                 new_inode->i_ctime = current_time(new_inode);
                 if (dir_de)
@@ -427,28 +429,27 @@ static int nilfs_rename(struct inode *old_dir, struct dentry *old_dentry,
          */
         old_inode->i_ctime = current_time(old_inode);
 
-        nilfs_delete_entry(old_de, old_page);
-
-        if (dir_de) {
-                nilfs_set_link(old_inode, dir_de, dir_page, new_dir);
-                nilfs_put_page(dir_page);
-                drop_nlink(old_dir);
+        err = nilfs_delete_entry(old_de, old_page);
+        if (likely(!err)) {
+                if (dir_de) {
+                        err = nilfs_set_link(old_inode, dir_de, dir_page,
+                                             new_dir);
+                        drop_nlink(old_dir);
+                }
+                nilfs_mark_inode_dirty(old_dir);
         }
-        nilfs_put_page(old_page);
-
-        nilfs_mark_inode_dirty(old_dir);
         nilfs_mark_inode_dirty(old_inode);
 
-        err = nilfs_transaction_commit(old_dir->i_sb);
-        return err;
-
 out_dir:
         if (dir_de)
                 nilfs_put_page(dir_page);
 out_old:
         nilfs_put_page(old_page);
 out:
-        nilfs_transaction_abort(old_dir->i_sb);
+        if (likely(!err))
+                err = nilfs_transaction_commit(old_dir->i_sb);
+        else
+                nilfs_transaction_abort(old_dir->i_sb);
         return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index 169c6c5e0672..9bc8fdac408d 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -240,8 +240,8 @@ nilfs_find_entry(struct inode *, const struct qstr *, struct page **);
 extern int nilfs_delete_entry(struct nilfs_dir_entry *, struct page *);
 extern int nilfs_empty_dir(struct inode *);
 extern struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct page **);
-extern void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-                           struct page *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+                   struct page *page, struct inode *inode);
 
 static inline void nilfs_put_page(struct page *page)
 {
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/dir.c\nHunk #1 FAILED at 400.\nHunk #2 FAILED at 410.\nHunk #3 FAILED at 543.\n3 out of 3 hunks FAILED -- saving rejects to file fs/nilfs2/dir.c.rej\npatching file fs/nilfs2/namei.c\nHunk #1 FAILED at 406.\nHunk #2 FAILED at 430.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/namei.c.rej\npatching file fs/nilfs2/nilfs.h\nHunk #1 FAILED at 261.\n1 out of 1 hunk FAILED -- saving rejects to file fs/nilfs2/nilfs.h.rej,47,1,32,162,1,OTHER,OTHER
CVE-2025-21721,ee70999a988b8abc3490609142f50ebaa8344432,"From ee70999a988b8abc3490609142f50ebaa8344432 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 0a3aea6c416b..9b7f8e9655a2 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -400,7 +400,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct folio *folio, struct inode *inode)
 {
 	size_t from = offset_in_folio(folio, de);
@@ -410,11 +410,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	de->file_type = fs_umode_to_ftype(inode->i_mode);
 	nilfs_commit_chunk(folio, mapping, from, to);
 	inode_set_mtime_to_ts(dir, inode_set_ctime_current(dir));
+	return 0;
 }
 
 /*
@@ -543,7 +547,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct folio *folio)
 		from = (char *)pde - kaddr;
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 1d836a5540f3..e02fae6757f1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_folio, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_folio, old_inode);
 		folio_release_kmap(new_folio, new_de);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		inode_set_ctime_current(new_inode);
 		if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 	 */
 	inode_set_ctime_current(old_inode);
 
-	nilfs_delete_entry(old_de, old_folio);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_folio, new_dir);
-		folio_release_kmap(dir_folio, dir_de);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_folio);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_folio,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	folio_release_kmap(old_folio, old_de);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		folio_release_kmap(dir_folio, dir_de);
 out_old:
 	folio_release_kmap(old_folio, old_de);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index dff241c53fc5..cb6ed54accd7 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -261,8 +261,8 @@ struct nilfs_dir_entry *nilfs_find_entry(struct inode *, const struct qstr *,
 int nilfs_delete_entry(struct nilfs_dir_entry *, struct folio *);
 int nilfs_empty_dir(struct inode *);
 struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct folio **);
-void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct folio *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct folio *folio, struct inode *inode);
 
 /* file.c */
 extern int nilfs_sync_file(struct file *, loff_t, loff_t, int);
-- 
2.39.5 (Apple Git-154)
",607dc724b162f4452dc768865e578c1a509a1c8c,"From 607dc724b162f4452dc768865e578c1a509a1c8c Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

commit ee70999a988b8abc3490609142f50ebaa8344432 upstream.

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 3a8ff6d4a1b0..3d7e692f3e7f 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -444,7 +444,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct page *page, struct inode *inode)
 {
 	unsigned int from = (char *)de - (char *)page_address(page);
@@ -454,11 +454,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	lock_page(page);
 	err = nilfs_prepare_chunk(page, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		unlock_page(page);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	nilfs_set_de_type(de, inode);
 	nilfs_commit_chunk(page, mapping, from, to);
 	dir->i_mtime = dir->i_ctime = current_time(dir);
+	return 0;
 }
 
 /*
@@ -590,7 +594,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct page *page)
 		from = (char *)pde - (char *)page_address(page);
 	lock_page(page);
 	err = nilfs_prepare_chunk(page, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		unlock_page(page);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 9dd4527d8f69..a81c24630e26 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct user_namespace *mnt_userns,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_page, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_page, old_inode);
 		nilfs_put_page(new_page);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		new_inode->i_ctime = current_time(new_inode);
 		if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct user_namespace *mnt_userns,
 	 */
 	old_inode->i_ctime = current_time(old_inode);
 
-	nilfs_delete_entry(old_de, old_page);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_page, new_dir);
-		nilfs_put_page(dir_page);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_page);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_page,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	nilfs_put_page(old_page);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		nilfs_put_page(dir_page);
 out_old:
 	nilfs_put_page(old_page);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index 760f785c2b6d..14ae1e3df83f 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -240,8 +240,8 @@ nilfs_find_entry(struct inode *, const struct qstr *, struct page **);
 extern int nilfs_delete_entry(struct nilfs_dir_entry *, struct page *);
 extern int nilfs_empty_dir(struct inode *);
 extern struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct page **);
-extern void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct page *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct page *page, struct inode *inode);
 
 static inline void nilfs_put_page(struct page *page)
 {
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/dir.c\nHunk #1 FAILED at 400.\nHunk #2 FAILED at 410.\nHunk #3 FAILED at 543.\n3 out of 3 hunks FAILED -- saving rejects to file fs/nilfs2/dir.c.rej\npatching file fs/nilfs2/namei.c\nHunk #1 FAILED at 406.\nHunk #2 FAILED at 430.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/namei.c.rej\npatching file fs/nilfs2/nilfs.h\nHunk #1 FAILED at 261.\n1 out of 1 hunk FAILED -- saving rejects to file fs/nilfs2/nilfs.h.rej,47,1,32,161,1,COMMENTS,OTHER
CVE-2025-21721,ee70999a988b8abc3490609142f50ebaa8344432,"From ee70999a988b8abc3490609142f50ebaa8344432 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index 0a3aea6c416b..9b7f8e9655a2 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -400,7 +400,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct folio *folio, struct inode *inode)
 {
 	size_t from = offset_in_folio(folio, de);
@@ -410,11 +410,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	de->file_type = fs_umode_to_ftype(inode->i_mode);
 	nilfs_commit_chunk(folio, mapping, from, to);
 	inode_set_mtime_to_ts(dir, inode_set_ctime_current(dir));
+	return 0;
 }
 
 /*
@@ -543,7 +547,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct folio *folio)
 		from = (char *)pde - kaddr;
 	folio_lock(folio);
 	err = nilfs_prepare_chunk(folio, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		folio_unlock(folio);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index 1d836a5540f3..e02fae6757f1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_folio, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_folio, old_inode);
 		folio_release_kmap(new_folio, new_de);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		inode_set_ctime_current(new_inode);
 		if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct mnt_idmap *idmap,
 	 */
 	inode_set_ctime_current(old_inode);
 
-	nilfs_delete_entry(old_de, old_folio);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_folio, new_dir);
-		folio_release_kmap(dir_folio, dir_de);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_folio);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_folio,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	folio_release_kmap(old_folio, old_de);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		folio_release_kmap(dir_folio, dir_de);
 out_old:
 	folio_release_kmap(old_folio, old_de);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index dff241c53fc5..cb6ed54accd7 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -261,8 +261,8 @@ struct nilfs_dir_entry *nilfs_find_entry(struct inode *, const struct qstr *,
 int nilfs_delete_entry(struct nilfs_dir_entry *, struct folio *);
 int nilfs_empty_dir(struct inode *);
 struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct folio **);
-void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct folio *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct folio *folio, struct inode *inode);
 
 /* file.c */
 extern int nilfs_sync_file(struct file *, loff_t, loff_t, int);
-- 
2.39.5 (Apple Git-154)

",1ee2d454baa361d2964e3e2f2cca9ee3f769d93c,"From 1ee2d454baa361d2964e3e2f2cca9ee3f769d93c Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Sat, 11 Jan 2025 23:26:35 +0900
Subject: [PATCH] nilfs2: handle errors that nilfs_prepare_chunk() may return

commit ee70999a988b8abc3490609142f50ebaa8344432 upstream.

Patch series ""nilfs2: fix issues with rename operations"".

This series fixes BUG_ON check failures reported by syzbot around rename
operations, and a minor behavioral issue where the mtime of a child
directory changes when it is renamed instead of moved.


This patch (of 2):

The directory manipulation routines nilfs_set_link() and
nilfs_delete_entry() rewrite the directory entry in the folio/page
previously read by nilfs_find_entry(), so error handling is omitted on the
assumption that nilfs_prepare_chunk(), which prepares the buffer for
rewriting, will always succeed for these.  And if an error is returned, it
triggers the legacy BUG_ON() checks in each routine.

This assumption is wrong, as proven by syzbot: the buffer layer called by
nilfs_prepare_chunk() may call nilfs_get_block() if necessary, which may
fail due to metadata corruption or other reasons.  This has been there all
along, but improved sanity checks and error handling may have made it more
reproducible in fuzzing tests.

Fix this issue by adding missing error paths in nilfs_set_link(),
nilfs_delete_entry(), and their caller nilfs_rename().

Link: https://lkml.kernel.org/r/20250111143518.7901-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250111143518.7901-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+32c3706ebf5d95046ea1@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=32c3706ebf5d95046ea1
Reported-by: syzbot+1097e95f134f37d9395c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1097e95f134f37d9395c
Fixes: 2ba466d74ed7 (""nilfs2: directory entry operations"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/dir.c   | 13 ++++++++++---
 fs/nilfs2/namei.c | 29 +++++++++++++++--------------
 fs/nilfs2/nilfs.h |  4 ++--
 3 files changed, 27 insertions(+), 19 deletions(-)

diff --git a/fs/nilfs2/dir.c b/fs/nilfs2/dir.c
index de040ab05d37..0f3753af1674 100644
--- a/fs/nilfs2/dir.c
+++ b/fs/nilfs2/dir.c
@@ -444,7 +444,7 @@ int nilfs_inode_by_name(struct inode *dir, const struct qstr *qstr, ino_t *ino)
 	return 0;
 }
 
-void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 		    struct page *page, struct inode *inode)
 {
 	unsigned int from = (char *)de - (char *)page_address(page);
@@ -454,11 +454,15 @@ void nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
 
 	lock_page(page);
 	err = nilfs_prepare_chunk(page, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		unlock_page(page);
+		return err;
+	}
 	de->inode = cpu_to_le64(inode->i_ino);
 	nilfs_set_de_type(de, inode);
 	nilfs_commit_chunk(page, mapping, from, to);
 	dir->i_mtime = dir->i_ctime = current_time(dir);
+	return 0;
 }
 
 /*
@@ -590,7 +594,10 @@ int nilfs_delete_entry(struct nilfs_dir_entry *dir, struct page *page)
 		from = (char *)pde - (char *)page_address(page);
 	lock_page(page);
 	err = nilfs_prepare_chunk(page, from, to);
-	BUG_ON(err);
+	if (unlikely(err)) {
+		unlock_page(page);
+		goto out;
+	}
 	if (pde)
 		pde->rec_len = nilfs_rec_len_to_disk(to - from);
 	dir->inode = 0;
diff --git a/fs/nilfs2/namei.c b/fs/nilfs2/namei.c
index bbd27238b0e6..67d66207fae1 100644
--- a/fs/nilfs2/namei.c
+++ b/fs/nilfs2/namei.c
@@ -406,8 +406,10 @@ static int nilfs_rename(struct user_namespace *mnt_userns,
 			err = PTR_ERR(new_de);
 			goto out_dir;
 		}
-		nilfs_set_link(new_dir, new_de, new_page, old_inode);
+		err = nilfs_set_link(new_dir, new_de, new_page, old_inode);
 		nilfs_put_page(new_page);
+		if (unlikely(err))
+			goto out_dir;
 		nilfs_mark_inode_dirty(new_dir);
 		new_inode->i_ctime = current_time(new_inode);
 		if (dir_de)
@@ -430,28 +432,27 @@ static int nilfs_rename(struct user_namespace *mnt_userns,
 	 */
 	old_inode->i_ctime = current_time(old_inode);
 
-	nilfs_delete_entry(old_de, old_page);
-
-	if (dir_de) {
-		nilfs_set_link(old_inode, dir_de, dir_page, new_dir);
-		nilfs_put_page(dir_page);
-		drop_nlink(old_dir);
+	err = nilfs_delete_entry(old_de, old_page);
+	if (likely(!err)) {
+		if (dir_de) {
+			err = nilfs_set_link(old_inode, dir_de, dir_page,
+					     new_dir);
+			drop_nlink(old_dir);
+		}
+		nilfs_mark_inode_dirty(old_dir);
 	}
-	nilfs_put_page(old_page);
-
-	nilfs_mark_inode_dirty(old_dir);
 	nilfs_mark_inode_dirty(old_inode);
 
-	err = nilfs_transaction_commit(old_dir->i_sb);
-	return err;
-
 out_dir:
 	if (dir_de)
 		nilfs_put_page(dir_page);
 out_old:
 	nilfs_put_page(old_page);
 out:
-	nilfs_transaction_abort(old_dir->i_sb);
+	if (likely(!err))
+		err = nilfs_transaction_commit(old_dir->i_sb);
+	else
+		nilfs_transaction_abort(old_dir->i_sb);
 	return err;
 }
 
diff --git a/fs/nilfs2/nilfs.h b/fs/nilfs2/nilfs.h
index b577ca0575d7..dadafad2fae7 100644
--- a/fs/nilfs2/nilfs.h
+++ b/fs/nilfs2/nilfs.h
@@ -240,8 +240,8 @@ nilfs_find_entry(struct inode *, const struct qstr *, struct page **);
 extern int nilfs_delete_entry(struct nilfs_dir_entry *, struct page *);
 extern int nilfs_empty_dir(struct inode *);
 extern struct nilfs_dir_entry *nilfs_dotdot(struct inode *, struct page **);
-extern void nilfs_set_link(struct inode *, struct nilfs_dir_entry *,
-			   struct page *, struct inode *);
+int nilfs_set_link(struct inode *dir, struct nilfs_dir_entry *de,
+		   struct page *page, struct inode *inode);
 
 static inline void nilfs_put_page(struct page *page)
 {
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/dir.c\nHunk #1 FAILED at 400.\nHunk #2 FAILED at 410.\nHunk #3 FAILED at 543.\n3 out of 3 hunks FAILED -- saving rejects to file fs/nilfs2/dir.c.rej\npatching file fs/nilfs2/namei.c\nHunk #1 FAILED at 406.\nHunk #2 FAILED at 430.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/namei.c.rej\npatching file fs/nilfs2/nilfs.h\nHunk #1 FAILED at 261.\n1 out of 1 hunk FAILED -- saving rejects to file fs/nilfs2/nilfs.h.rej,47,1,32,162,1,FORMATTING,OTHER
CVE-2025-21722,ca76bb226bf47ff04c782cacbd299f12ddee1ec1,"From ca76bb226bf47ff04c782cacbd299f12ddee1ec1 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:46 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.


This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/page.c | 31 +++++++++++++++++++++++++++----
 1 file changed, 27 insertions(+), 4 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 9de2a494a069..899686d2e5f7 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -392,6 +392,11 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_folio_dirty - discard dirty folio
  * @folio: dirty folio that will be discarded
+ *
+ * nilfs_clear_folio_dirty() clears working states including dirty state for
+ * the folio and its buffers.  If the folio has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_folio_dirty(struct folio *folio)
 {
@@ -399,10 +404,6 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 
         BUG_ON(!folio_test_locked(folio));
 
-        folio_clear_uptodate(folio);
-        folio_clear_mappedtodisk(folio);
-        folio_clear_checked(folio);
-
         head = folio_buffers(folio);
         if (head) {
                 const unsigned long clear_bits =
@@ -410,6 +411,25 @@ void nilfs_clear_folio_dirty(struct folio *folio)
                          BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
                          BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
                          BIT(BH_Delay));
+                bool busy, invalidated = false;
+
+recheck_buffers:
+                busy = false;
+                bh = head;
+                do {
+                        if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+                                busy = true;
+                                break;
+                        }
+                } while (bh = bh->b_this_page, bh != head);
+
+                if (busy) {
+                        if (invalidated)
+                                return;
+                        invalidate_bh_lrus();
+                        invalidated = true;
+                        goto recheck_buffers;
+                }
 
                 bh = head;
                 do {
@@ -419,6 +439,9 @@ void nilfs_clear_folio_dirty(struct folio *folio)
                 } while (bh = bh->b_this_page, bh != head);
         }
 
+        folio_clear_uptodate(folio);
+        folio_clear_mappedtodisk(folio);
+        folio_clear_checked(folio);
         __nilfs_clear_folio_dirty(folio);
 }
 
-- 
2.39.5 (Apple Git-154)
",19296737024cd220a1d6590bf4c092bca8c99497,"From 19296737024cd220a1d6590bf4c092bca8c99497 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 03:14:31 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

commit ca76bb226bf47ff04c782cacbd299f12ddee1ec1 upstream.

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.

This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/page.c | 35 +++++++++++++++++++++++++++++------
 1 file changed, 29 insertions(+), 6 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index a5ff408f2e43..bf090240f6ed 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -388,24 +388,44 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_dirty_page - discard dirty page
  * @page: dirty page that will be discarded
+ *
+ * nilfs_clear_dirty_page() clears working states including dirty state for
+ * the page and its buffers.  If the page has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_dirty_page(struct page *page)
 {
         BUG_ON(!PageLocked(page));
 
-        ClearPageUptodate(page);
-        ClearPageMappedToDisk(page);
-        ClearPageChecked(page);
-
         if (page_has_buffers(page)) {
-                struct buffer_head *bh, *head;
+                struct buffer_head *bh, *head = page_buffers(page);
                 const unsigned long clear_bits =
                         (BIT(BH_Uptodate) | BIT(BH_Dirty) | BIT(BH_Mapped) |
                          BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
                          BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
                          BIT(BH_Delay));
+                bool busy, invalidated = false;
 
-                bh = head = page_buffers(page);
+recheck_buffers:
+                busy = false;
+                bh = head;
+                do {
+                        if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+                                busy = true;
+                                break;
+                        }
+                } while (bh = bh->b_this_page, bh != head);
+
+                if (busy) {
+                        if (invalidated)
+                                return;
+                        invalidate_bh_lrus();
+                        invalidated = true;
+                        goto recheck_buffers;
+                }
+
+                bh = head;
                 do {
                         lock_buffer(bh);
                         set_mask_bits(&bh->b_state, clear_bits, 0);
@@ -413,6 +433,9 @@ void nilfs_clear_dirty_page(struct page *page)
                 } while (bh = bh->b_this_page, bh != head);
         }
 
+        ClearPageUptodate(page);
+        ClearPageMappedToDisk(page);
+        ClearPageChecked(page);
         __nilfs_clear_page_dirty(page);
 }
 
-- 
2.39.5 (Apple Git-154)
",patching file fs/nilfs2/page.c\nHunk #1 FAILED at 392.\nHunk #2 FAILED at 399.\nHunk #3 succeeded at 404 with fuzz 2 (offset -6 lines).\nHunk #4 FAILED at 438.\n3 out of 4 hunks FAILED -- saving rejects to file fs/nilfs2/page.c.rej,32,1,5,150,1,OTHER,FORMATTING
CVE-2025-21722,ca76bb226bf47ff04c782cacbd299f12ddee1ec1,"From ca76bb226bf47ff04c782cacbd299f12ddee1ec1 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:46 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.


This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/page.c | 31 +++++++++++++++++++++++++++----
 1 file changed, 27 insertions(+), 4 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 9de2a494a069..899686d2e5f7 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -392,6 +392,11 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_folio_dirty - discard dirty folio
  * @folio: dirty folio that will be discarded
+ *
+ * nilfs_clear_folio_dirty() clears working states including dirty state for
+ * the folio and its buffers.  If the folio has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_folio_dirty(struct folio *folio)
 {
@@ -399,10 +404,6 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 
 	BUG_ON(!folio_test_locked(folio));
 
-	folio_clear_uptodate(folio);
-	folio_clear_mappedtodisk(folio);
-	folio_clear_checked(folio);
-
 	head = folio_buffers(folio);
 	if (head) {
 		const unsigned long clear_bits =
@@ -410,6 +411,25 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 			 BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
 			 BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
 			 BIT(BH_Delay));
+		bool busy, invalidated = false;
+
+recheck_buffers:
+		busy = false;
+		bh = head;
+		do {
+			if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+				busy = true;
+				break;
+			}
+		} while (bh = bh->b_this_page, bh != head);
+
+		if (busy) {
+			if (invalidated)
+				return;
+			invalidate_bh_lrus();
+			invalidated = true;
+			goto recheck_buffers;
+		}
 
 		bh = head;
 		do {
@@ -419,6 +439,9 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 		} while (bh = bh->b_this_page, bh != head);
 	}
 
+	folio_clear_uptodate(folio);
+	folio_clear_mappedtodisk(folio);
+	folio_clear_checked(folio);
 	__nilfs_clear_folio_dirty(folio);
 }
 
-- 
2.39.5 (Apple Git-154)
",7d0544bacc11d6aa26ecd7debf9353193c7a3328,"From 7d0544bacc11d6aa26ecd7debf9353193c7a3328 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 23:23:48 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

commit ca76bb226bf47ff04c782cacbd299f12ddee1ec1 upstream.

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.

This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/page.c | 35 +++++++++++++++++++++++++++++------
 1 file changed, 29 insertions(+), 6 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 6ff75d12adf8..79925342900e 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -389,24 +389,44 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_dirty_page - discard dirty page
  * @page: dirty page that will be discarded
+ *
+ * nilfs_clear_dirty_page() clears working states including dirty state for
+ * the page and its buffers.  If the page has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_dirty_page(struct page *page)
 {
 	BUG_ON(!PageLocked(page));
 
-	ClearPageUptodate(page);
-	ClearPageMappedToDisk(page);
-	ClearPageChecked(page);
-
 	if (page_has_buffers(page)) {
-		struct buffer_head *bh, *head;
+		struct buffer_head *bh, *head = page_buffers(page);
 		const unsigned long clear_bits =
 			(BIT(BH_Uptodate) | BIT(BH_Dirty) | BIT(BH_Mapped) |
 			 BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
 			 BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
 			 BIT(BH_Delay));
+		bool busy, invalidated = false;
 
-		bh = head = page_buffers(page);
+recheck_buffers:
+		busy = false;
+		bh = head;
+		do {
+			if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+				busy = true;
+				break;
+			}
+		} while (bh = bh->b_this_page, bh != head);
+
+		if (busy) {
+			if (invalidated)
+				return;
+			invalidate_bh_lrus();
+			invalidated = true;
+			goto recheck_buffers;
+		}
+
+		bh = head;
 		do {
 			lock_buffer(bh);
 			set_mask_bits(&bh->b_state, clear_bits, 0);
@@ -414,6 +434,9 @@ void nilfs_clear_dirty_page(struct page *page)
 		} while (bh = bh->b_this_page, bh != head);
 	}
 
+	ClearPageUptodate(page);
+	ClearPageMappedToDisk(page);
+	ClearPageChecked(page);
 	__nilfs_clear_page_dirty(page);
 }
 
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/page.c\nHunk #1 FAILED at 392.\nHunk #2 FAILED at 399.\nHunk #3 succeeded at 405 with fuzz 2 (offset -5 lines).\nHunk #4 FAILED at 438.\n3 out of 4 hunks FAILED -- saving rejects to file fs/nilfs2/page.c.rej,32,1,5,150,1,FORMATTING,FORMATTING
CVE-2025-21722,ca76bb226bf47ff04c782cacbd299f12ddee1ec1,"From ca76bb226bf47ff04c782cacbd299f12ddee1ec1 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:46 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.


This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/page.c | 31 +++++++++++++++++++++++++++----
 1 file changed, 27 insertions(+), 4 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 9de2a494a069..899686d2e5f7 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -392,6 +392,11 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_folio_dirty - discard dirty folio
  * @folio: dirty folio that will be discarded
+ *
+ * nilfs_clear_folio_dirty() clears working states including dirty state for
+ * the folio and its buffers.  If the folio has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_folio_dirty(struct folio *folio)
 {
@@ -399,10 +404,6 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 
 	BUG_ON(!folio_test_locked(folio));
 
-	folio_clear_uptodate(folio);
-	folio_clear_mappedtodisk(folio);
-	folio_clear_checked(folio);
-
 	head = folio_buffers(folio);
 	if (head) {
 		const unsigned long clear_bits =
@@ -410,6 +411,25 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 			 BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
 			 BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
 			 BIT(BH_Delay));
+		bool busy, invalidated = false;
+
+recheck_buffers:
+		busy = false;
+		bh = head;
+		do {
+			if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+				busy = true;
+				break;
+			}
+		} while (bh = bh->b_this_page, bh != head);
+
+		if (busy) {
+			if (invalidated)
+				return;
+			invalidate_bh_lrus();
+			invalidated = true;
+			goto recheck_buffers;
+		}
 
 		bh = head;
 		do {
@@ -419,6 +439,9 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 		} while (bh = bh->b_this_page, bh != head);
 	}
 
+	folio_clear_uptodate(folio);
+	folio_clear_mappedtodisk(folio);
+	folio_clear_checked(folio);
 	__nilfs_clear_folio_dirty(folio);
 }
 
-- 
2.39.5 (Apple Git-154)

",4d042811c72f71be7c14726db2c72b67025a7cb5,"From 4d042811c72f71be7c14726db2c72b67025a7cb5 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 23:23:48 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

commit ca76bb226bf47ff04c782cacbd299f12ddee1ec1 upstream.

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.

This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/page.c | 35 +++++++++++++++++++++++++++++------
 1 file changed, 29 insertions(+), 6 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 93f24fa3ab10..ce5947cf4bd5 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -388,24 +388,44 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_dirty_page - discard dirty page
  * @page: dirty page that will be discarded
+ *
+ * nilfs_clear_dirty_page() clears working states including dirty state for
+ * the page and its buffers.  If the page has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_dirty_page(struct page *page)
 {
 	BUG_ON(!PageLocked(page));
 
-	ClearPageUptodate(page);
-	ClearPageMappedToDisk(page);
-	ClearPageChecked(page);
-
 	if (page_has_buffers(page)) {
-		struct buffer_head *bh, *head;
+		struct buffer_head *bh, *head = page_buffers(page);
 		const unsigned long clear_bits =
 			(BIT(BH_Uptodate) | BIT(BH_Dirty) | BIT(BH_Mapped) |
 			 BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
 			 BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
 			 BIT(BH_Delay));
+		bool busy, invalidated = false;
 
-		bh = head = page_buffers(page);
+recheck_buffers:
+		busy = false;
+		bh = head;
+		do {
+			if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+				busy = true;
+				break;
+			}
+		} while (bh = bh->b_this_page, bh != head);
+
+		if (busy) {
+			if (invalidated)
+				return;
+			invalidate_bh_lrus();
+			invalidated = true;
+			goto recheck_buffers;
+		}
+
+		bh = head;
 		do {
 			lock_buffer(bh);
 			set_mask_bits(&bh->b_state, clear_bits, 0);
@@ -413,6 +433,9 @@ void nilfs_clear_dirty_page(struct page *page)
 		} while (bh = bh->b_this_page, bh != head);
 	}
 
+	ClearPageUptodate(page);
+	ClearPageMappedToDisk(page);
+	ClearPageChecked(page);
 	__nilfs_clear_page_dirty(page);
 }
 
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/page.c\nHunk #1 FAILED at 392.\nHunk #2 FAILED at 399.\nHunk #3 succeeded at 404 with fuzz 2 (offset -6 lines).\nHunk #4 FAILED at 438.\n3 out of 4 hunks FAILED -- saving rejects to file fs/nilfs2/page.c.rej,32,1,5,151,1,FORMATTING,FORMATTING
CVE-2025-21722,ca76bb226bf47ff04c782cacbd299f12ddee1ec1,"From ca76bb226bf47ff04c782cacbd299f12ddee1ec1 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:46 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.


This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/page.c | 31 +++++++++++++++++++++++++++----
 1 file changed, 27 insertions(+), 4 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 9de2a494a069..899686d2e5f7 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -392,6 +392,11 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_folio_dirty - discard dirty folio
  * @folio: dirty folio that will be discarded
+ *
+ * nilfs_clear_folio_dirty() clears working states including dirty state for
+ * the folio and its buffers.  If the folio has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_folio_dirty(struct folio *folio)
 {
@@ -399,10 +404,6 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 
 	BUG_ON(!folio_test_locked(folio));
 
-	folio_clear_uptodate(folio);
-	folio_clear_mappedtodisk(folio);
-	folio_clear_checked(folio);
-
 	head = folio_buffers(folio);
 	if (head) {
 		const unsigned long clear_bits =
@@ -410,6 +411,25 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 			 BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
 			 BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
 			 BIT(BH_Delay));
+		bool busy, invalidated = false;
+
+recheck_buffers:
+		busy = false;
+		bh = head;
+		do {
+			if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+				busy = true;
+				break;
+			}
+		} while (bh = bh->b_this_page, bh != head);
+
+		if (busy) {
+			if (invalidated)
+				return;
+			invalidate_bh_lrus();
+			invalidated = true;
+			goto recheck_buffers;
+		}
 
 		bh = head;
 		do {
@@ -419,6 +439,9 @@ void nilfs_clear_folio_dirty(struct folio *folio)
 		} while (bh = bh->b_this_page, bh != head);
 	}
 
+	folio_clear_uptodate(folio);
+	folio_clear_mappedtodisk(folio);
+	folio_clear_checked(folio);
 	__nilfs_clear_folio_dirty(folio);
 }
 
-- 
2.39.5 (Apple Git-154)

",f51ff43c4c5a6c8e72d0aca89e4d5e688938412f,"From f51ff43c4c5a6c8e72d0aca89e4d5e688938412f Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 23:23:48 +0900
Subject: [PATCH] nilfs2: do not force clear folio if buffer is referenced

commit ca76bb226bf47ff04c782cacbd299f12ddee1ec1 upstream.

Patch series ""nilfs2: protect busy buffer heads from being force-cleared"".

This series fixes the buffer head state inconsistency issues reported by
syzbot that occurs when the filesystem is corrupted and falls back to
read-only, and the associated buffer head use-after-free issue.

This patch (of 2):

Syzbot has reported that after nilfs2 detects filesystem corruption and
falls back to read-only, inconsistencies in the buffer state may occur.

One of the inconsistencies is that when nilfs2 calls mark_buffer_dirty()
to set a data or metadata buffer as dirty, but it detects that the buffer
is not in the uptodate state:

 WARNING: CPU: 0 PID: 6049 at fs/buffer.c:1177 mark_buffer_dirty+0x2e5/0x520
  fs/buffer.c:1177
 ...
 Call Trace:
  <TASK>
  nilfs_palloc_commit_alloc_entry+0x4b/0x160 fs/nilfs2/alloc.c:598
  nilfs_ifile_create_inode+0x1dd/0x3a0 fs/nilfs2/ifile.c:73
  nilfs_new_inode+0x254/0x830 fs/nilfs2/inode.c:344
  nilfs_mkdir+0x10d/0x340 fs/nilfs2/namei.c:218
  vfs_mkdir+0x2f9/0x4f0 fs/namei.c:4257
  do_mkdirat+0x264/0x3a0 fs/namei.c:4280
  __do_sys_mkdirat fs/namei.c:4295 [inline]
  __se_sys_mkdirat fs/namei.c:4293 [inline]
  __x64_sys_mkdirat+0x87/0xa0 fs/namei.c:4293
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xf3/0x230 arch/x86/entry/common.c:83
  entry_SYSCALL_64_after_hwframe+0x77/0x7f

The other is when nilfs_btree_propagate(), which propagates the dirty
state to the ancestor nodes of a b-tree that point to a dirty buffer,
detects that the origin buffer is not dirty, even though it should be:

 WARNING: CPU: 0 PID: 5245 at fs/nilfs2/btree.c:2089
  nilfs_btree_propagate+0xc79/0xdf0 fs/nilfs2/btree.c:2089
 ...
 Call Trace:
  <TASK>
  nilfs_bmap_propagate+0x75/0x120 fs/nilfs2/bmap.c:345
  nilfs_collect_file_data+0x4d/0xd0 fs/nilfs2/segment.c:587
  nilfs_segctor_apply_buffers+0x184/0x340 fs/nilfs2/segment.c:1006
  nilfs_segctor_scan_file+0x28c/0xa50 fs/nilfs2/segment.c:1045
  nilfs_segctor_collect_blocks fs/nilfs2/segment.c:1216 [inline]
  nilfs_segctor_collect fs/nilfs2/segment.c:1540 [inline]
  nilfs_segctor_do_construct+0x1c28/0x6b90 fs/nilfs2/segment.c:2115
  nilfs_segctor_construct+0x181/0x6b0 fs/nilfs2/segment.c:2479
  nilfs_segctor_thread_construct fs/nilfs2/segment.c:2587 [inline]
  nilfs_segctor_thread+0x69e/0xe80 fs/nilfs2/segment.c:2701
  kthread+0x2f0/0x390 kernel/kthread.c:389
  ret_from_fork+0x4b/0x80 arch/x86/kernel/process.c:147
  ret_from_fork_asm+0x1a/0x30 arch/x86/entry/entry_64.S:244
  </TASK>

Both of these issues are caused by the callbacks that handle the
page/folio write requests, forcibly clear various states, including the
working state of the buffers they hold, at unexpected times when they
detect read-only fallback.

Fix these issues by checking if the buffer is referenced before clearing
the page/folio state, and skipping the clear if it is.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-1-konishi.ryusuke@gmail.com
Link: https://lkml.kernel.org/r/20250107200202.6432-2-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Reported-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=b2b14916b77acf8626d7
Reported-by: syzbot+d98fd19acd08b36ff422@syzkaller.appspotmail.com
Link: https://syzkaller.appspot.com/bug?extid=d98fd19acd08b36ff422
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Tested-by: syzbot+b2b14916b77acf8626d7@syzkaller.appspotmail.com
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/page.c | 35 +++++++++++++++++++++++++++++------
 1 file changed, 29 insertions(+), 6 deletions(-)

diff --git a/fs/nilfs2/page.c b/fs/nilfs2/page.c
index 93f24fa3ab10..ce5947cf4bd5 100644
--- a/fs/nilfs2/page.c
+++ b/fs/nilfs2/page.c
@@ -388,24 +388,44 @@ void nilfs_clear_dirty_pages(struct address_space *mapping)
 /**
  * nilfs_clear_dirty_page - discard dirty page
  * @page: dirty page that will be discarded
+ *
+ * nilfs_clear_dirty_page() clears working states including dirty state for
+ * the page and its buffers.  If the page has buffers, clear only if it is
+ * confirmed that none of the buffer heads are busy (none have valid
+ * references and none are locked).
  */
 void nilfs_clear_dirty_page(struct page *page)
 {
 	BUG_ON(!PageLocked(page));
 
-	ClearPageUptodate(page);
-	ClearPageMappedToDisk(page);
-	ClearPageChecked(page);
-
 	if (page_has_buffers(page)) {
-		struct buffer_head *bh, *head;
+		struct buffer_head *bh, *head = page_buffers(page);
 		const unsigned long clear_bits =
 			(BIT(BH_Uptodate) | BIT(BH_Dirty) | BIT(BH_Mapped) |
 			 BIT(BH_Async_Write) | BIT(BH_NILFS_Volatile) |
 			 BIT(BH_NILFS_Checked) | BIT(BH_NILFS_Redirected) |
 			 BIT(BH_Delay));
+		bool busy, invalidated = false;
 
-		bh = head = page_buffers(page);
+recheck_buffers:
+		busy = false;
+		bh = head;
+		do {
+			if (atomic_read(&bh->b_count) | buffer_locked(bh)) {
+				busy = true;
+				break;
+			}
+		} while (bh = bh->b_this_page, bh != head);
+
+		if (busy) {
+			if (invalidated)
+				return;
+			invalidate_bh_lrus();
+			invalidated = true;
+			goto recheck_buffers;
+		}
+
+		bh = head;
 		do {
 			lock_buffer(bh);
 			set_mask_bits(&bh->b_state, clear_bits, 0);
@@ -413,6 +433,9 @@ void nilfs_clear_dirty_page(struct page *page)
 		} while (bh = bh->b_this_page, bh != head);
 	}
 
+	ClearPageUptodate(page);
+	ClearPageMappedToDisk(page);
+	ClearPageChecked(page);
 	__nilfs_clear_page_dirty(page);
 }
 
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/page.c\nHunk #1 FAILED at 392.\nHunk #2 FAILED at 399.\nHunk #3 succeeded at 404 with fuzz 2 (offset -6 lines).\nHunk #4 FAILED at 438.\n3 out of 4 hunks FAILED -- saving rejects to file fs/nilfs2/page.c.rej,32,1,5,151,1,FORMATTING,FORMATTING
CVE-2025-21724,e24c1551059268b37f6f40639883eafb281b8b9c,"From e24c1551059268b37f6f40639883eafb281b8b9c Mon Sep 17 00:00:00 2001
From: Qasim Ijaz <qasdev00@gmail.com>
Date: Mon, 13 Jan 2025 22:38:20 +0000
Subject: [PATCH] iommufd/iova_bitmap: Fix shift-out-of-bounds in
 iova_bitmap_offset_to_index()

Resolve a UBSAN shift-out-of-bounds issue in iova_bitmap_offset_to_index()
where shifting the constant ""1"" (of type int) by bitmap->mapped.pgshift
(an unsigned long value) could result in undefined behavior.

The constant ""1"" defaults to a 32-bit ""int"", and when ""pgshift"" exceeds
31 (e.g., pgshift = 63) the shift operation overflows, as the result
cannot be represented in a 32-bit type.

To resolve this, the constant is updated to ""1UL"", promoting it to an
unsigned long type to match the operand's type.

Fixes: 58ccf0190d19 (""vfio: Add an IOVA bitmap support"")
Link: https://patch.msgid.link/r/20250113223820.10713-1-qasdev00@gmail.com
Reported-by: syzbot <syzbot+85992ace37d5b7b51635@syzkaller.appspotmail.com>
Closes: https://syzkaller.appspot.com/bug?extid=85992ace37d5b7b51635
Signed-off-by: Qasim Ijaz <qasdev00@gmail.com>
Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
---
 drivers/iommu/iommufd/iova_bitmap.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/iommu/iommufd/iova_bitmap.c b/drivers/iommu/iommufd/iova_bitmap.c
index ab665cf38ef4..39a86a4a1d3a 100644
--- a/drivers/iommu/iommufd/iova_bitmap.c
+++ b/drivers/iommu/iommufd/iova_bitmap.c
@@ -130,7 +130,7 @@ struct iova_bitmap {
 static unsigned long iova_bitmap_offset_to_index(struct iova_bitmap *bitmap,
                                                  unsigned long iova)
 {
-        unsigned long pgsize = 1 << bitmap->mapped.pgshift;
+        unsigned long pgsize = 1UL << bitmap->mapped.pgshift;
 
         return iova / (BITS_PER_TYPE(*bitmap->bitmap) * pgsize);
 }
-- 
2.39.5 (Apple Git-154)

",38ac76fc06bc6826a3e4b12a98efbe98432380a9,"From 38ac76fc06bc6826a3e4b12a98efbe98432380a9 Mon Sep 17 00:00:00 2001
From: Qasim Ijaz <qasdev00@gmail.com>
Date: Mon, 13 Jan 2025 22:38:20 +0000
Subject: [PATCH] iommufd/iova_bitmap: Fix shift-out-of-bounds in
 iova_bitmap_offset_to_index()

[ Upstream commit e24c1551059268b37f6f40639883eafb281b8b9c ]

Resolve a UBSAN shift-out-of-bounds issue in iova_bitmap_offset_to_index()
where shifting the constant ""1"" (of type int) by bitmap->mapped.pgshift
(an unsigned long value) could result in undefined behavior.

The constant ""1"" defaults to a 32-bit ""int"", and when ""pgshift"" exceeds
31 (e.g., pgshift = 63) the shift operation overflows, as the result
cannot be represented in a 32-bit type.

To resolve this, the constant is updated to ""1UL"", promoting it to an
unsigned long type to match the operand's type.

Fixes: 58ccf0190d19 (""vfio: Add an IOVA bitmap support"")
Link: https://patch.msgid.link/r/20250113223820.10713-1-qasdev00@gmail.com
Reported-by: syzbot <syzbot+85992ace37d5b7b51635@syzkaller.appspotmail.com>
Closes: https://syzkaller.appspot.com/bug?extid=85992ace37d5b7b51635
Signed-off-by: Qasim Ijaz <qasdev00@gmail.com>
Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/vfio/iova_bitmap.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/vfio/iova_bitmap.c b/drivers/vfio/iova_bitmap.c
index 7af5b204990b..38b51613ecca 100644
--- a/drivers/vfio/iova_bitmap.c
+++ b/drivers/vfio/iova_bitmap.c
@@ -127,7 +127,7 @@ struct iova_bitmap {
 static unsigned long iova_bitmap_offset_to_index(struct iova_bitmap *bitmap,
 						 unsigned long iova)
 {
-	unsigned long pgsize = 1 << bitmap->mapped.pgshift;
+	unsigned long pgsize = 1UL << bitmap->mapped.pgshift;
 
 	return iova / (BITS_PER_TYPE(*bitmap->bitmap) * pgsize);
 }
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 33\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From e24c1551059268b37f6f40639883eafb281b8b9c Mon Sep 17 00:00:00 2001\n|From: Qasim Ijaz <qasdev00@gmail.com>\n|Date: Mon, 13 Jan 2025 22:38:20 +0000\n|Subject: [PATCH] iommufd/iova_bitmap: Fix shift-out-of-bounds in\n| iova_bitmap_offset_to_index()\n|\n|Resolve a UBSAN shift-out-of-bounds issue in iova_bitmap_offset_to_index()\n|where shifting the constant \""1\"" (of type int) by bitmap->mapped.pgshift\n|(an unsigned long value) could result in undefined behavior.\n|\n|The constant \""1\"" defaults to a 32-bit \""int\"", and when \""pgshift\"" exceeds\n|31 (e.g., pgshift = 63) the shift operation overflows, as the result\n|cannot be represented in a 32-bit type.\n|\n|To resolve this, the constant is updated to \""1UL\"", promoting it to an\n|unsigned long type to match the operand's type.\n|\n|Fixes: 58ccf0190d19 (\""vfio: Add an IOVA bitmap support\"")\n|Link: https://patch.msgid.link/r/20250113223820.10713-1-qasdev00@gmail.com\n|Reported-by: syzbot <syzbot+85992ace37d5b7b51635@syzkaller.appspotmail.com>\n|Closes: https://syzkaller.appspot.com/bug?extid=85992ace37d5b7b51635\n|Signed-off-by: Qasim Ijaz <qasdev00@gmail.com>\n|Reviewed-by: Joao Martins <joao.m.martins@oracle.com>\n|Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>\n|---\n| drivers/iommu/iommufd/iova_bitmap.c | 2 +-\n| 1 file changed, 1 insertion(+), 1 deletion(-)\n|\n|diff --git a/drivers/iommu/iommufd/iova_bitmap.c b/drivers/iommu/iommufd/iova_bitmap.c\n|index ab665cf38ef4..39a86a4a1d3a 100644\n|--- a/drivers/iommu/iommufd/iova_bitmap.c\n|+++ b/drivers/iommu/iommufd/iova_bitmap.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",3,1,2,44,1,COMMENTS,DELETE
CVE-2025-21724,e24c1551059268b37f6f40639883eafb281b8b9c,"From e24c1551059268b37f6f40639883eafb281b8b9c Mon Sep 17 00:00:00 2001
From: Qasim Ijaz <qasdev00@gmail.com>
Date: Mon, 13 Jan 2025 22:38:20 +0000
Subject: [PATCH] iommufd/iova_bitmap: Fix shift-out-of-bounds in
 iova_bitmap_offset_to_index()

Resolve a UBSAN shift-out-of-bounds issue in iova_bitmap_offset_to_index()
where shifting the constant ""1"" (of type int) by bitmap->mapped.pgshift
(an unsigned long value) could result in undefined behavior.

The constant ""1"" defaults to a 32-bit ""int"", and when ""pgshift"" exceeds
31 (e.g., pgshift = 63) the shift operation overflows, as the result
cannot be represented in a 32-bit type.

To resolve this, the constant is updated to ""1UL"", promoting it to an
unsigned long type to match the operand's type.

Fixes: 58ccf0190d19 (""vfio: Add an IOVA bitmap support"")
Link: https://patch.msgid.link/r/20250113223820.10713-1-qasdev00@gmail.com
Reported-by: syzbot <syzbot+85992ace37d5b7b51635@syzkaller.appspotmail.com>
Closes: https://syzkaller.appspot.com/bug?extid=85992ace37d5b7b51635
Signed-off-by: Qasim Ijaz <qasdev00@gmail.com>
Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
---
 drivers/iommu/iommufd/iova_bitmap.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/iommu/iommufd/iova_bitmap.c b/drivers/iommu/iommufd/iova_bitmap.c
index ab665cf38ef4..39a86a4a1d3a 100644
--- a/drivers/iommu/iommufd/iova_bitmap.c
+++ b/drivers/iommu/iommufd/iova_bitmap.c
@@ -130,7 +130,7 @@ struct iova_bitmap {
 static unsigned long iova_bitmap_offset_to_index(struct iova_bitmap *bitmap,
 						 unsigned long iova)
 {
-	unsigned long pgsize = 1 << bitmap->mapped.pgshift;
+	unsigned long pgsize = 1UL << bitmap->mapped.pgshift;
 
 	return iova / (BITS_PER_TYPE(*bitmap->bitmap) * pgsize);
 }
-- 
2.39.5 (Apple Git-154)

",d5d33f01b86af44b23eea61ee309e4ef22c0cdfe,"From d5d33f01b86af44b23eea61ee309e4ef22c0cdfe Mon Sep 17 00:00:00 2001
From: Qasim Ijaz <qasdev00@gmail.com>
Date: Mon, 13 Jan 2025 22:38:20 +0000
Subject: [PATCH] iommufd/iova_bitmap: Fix shift-out-of-bounds in
 iova_bitmap_offset_to_index()

[ Upstream commit e24c1551059268b37f6f40639883eafb281b8b9c ]

Resolve a UBSAN shift-out-of-bounds issue in iova_bitmap_offset_to_index()
where shifting the constant ""1"" (of type int) by bitmap->mapped.pgshift
(an unsigned long value) could result in undefined behavior.

The constant ""1"" defaults to a 32-bit ""int"", and when ""pgshift"" exceeds
31 (e.g., pgshift = 63) the shift operation overflows, as the result
cannot be represented in a 32-bit type.

To resolve this, the constant is updated to ""1UL"", promoting it to an
unsigned long type to match the operand's type.

Fixes: 58ccf0190d19 (""vfio: Add an IOVA bitmap support"")
Link: https://patch.msgid.link/r/20250113223820.10713-1-qasdev00@gmail.com
Reported-by: syzbot <syzbot+85992ace37d5b7b51635@syzkaller.appspotmail.com>
Closes: https://syzkaller.appspot.com/bug?extid=85992ace37d5b7b51635
Signed-off-by: Qasim Ijaz <qasdev00@gmail.com>
Reviewed-by: Joao Martins <joao.m.martins@oracle.com>
Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/iommu/iommufd/iova_bitmap.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/iommu/iommufd/iova_bitmap.c b/drivers/iommu/iommufd/iova_bitmap.c
index d90b9e253412..2cdc4f542df4 100644
--- a/drivers/iommu/iommufd/iova_bitmap.c
+++ b/drivers/iommu/iommufd/iova_bitmap.c
@@ -130,7 +130,7 @@ struct iova_bitmap {
 static unsigned long iova_bitmap_offset_to_index(struct iova_bitmap *bitmap,
 						 unsigned long iova)
 {
-	unsigned long pgsize = 1 << bitmap->mapped.pgshift;
+	unsigned long pgsize = 1UL << bitmap->mapped.pgshift;
 
 	return iova / (BITS_PER_TYPE(*bitmap->bitmap) * pgsize);
 }
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 33\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From e24c1551059268b37f6f40639883eafb281b8b9c Mon Sep 17 00:00:00 2001\n|From: Qasim Ijaz <qasdev00@gmail.com>\n|Date: Mon, 13 Jan 2025 22:38:20 +0000\n|Subject: [PATCH] iommufd/iova_bitmap: Fix shift-out-of-bounds in\n| iova_bitmap_offset_to_index()\n|\n|Resolve a UBSAN shift-out-of-bounds issue in iova_bitmap_offset_to_index()\n|where shifting the constant \""1\"" (of type int) by bitmap->mapped.pgshift\n|(an unsigned long value) could result in undefined behavior.\n|\n|The constant \""1\"" defaults to a 32-bit \""int\"", and when \""pgshift\"" exceeds\n|31 (e.g., pgshift = 63) the shift operation overflows, as the result\n|cannot be represented in a 32-bit type.\n|\n|To resolve this, the constant is updated to \""1UL\"", promoting it to an\n|unsigned long type to match the operand's type.\n|\n|Fixes: 58ccf0190d19 (\""vfio: Add an IOVA bitmap support\"")\n|Link: https://patch.msgid.link/r/20250113223820.10713-1-qasdev00@gmail.com\n|Reported-by: syzbot <syzbot+85992ace37d5b7b51635@syzkaller.appspotmail.com>\n|Closes: https://syzkaller.appspot.com/bug?extid=85992ace37d5b7b51635\n|Signed-off-by: Qasim Ijaz <qasdev00@gmail.com>\n|Reviewed-by: Joao Martins <joao.m.martins@oracle.com>\n|Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>\n|---\n| drivers/iommu/iommufd/iova_bitmap.c | 2 +-\n| 1 file changed, 1 insertion(+), 1 deletion(-)\n|\n|diff --git a/drivers/iommu/iommufd/iova_bitmap.c b/drivers/iommu/iommufd/iova_bitmap.c\n|index ab665cf38ef4..39a86a4a1d3a 100644\n|--- a/drivers/iommu/iommufd/iova_bitmap.c\n|+++ b/drivers/iommu/iommufd/iova_bitmap.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",3,1,2,44,1,DELETE,DELETE
CVE-2025-21748,aab98e2dbd648510f8f51b83fbf4721206ccae45,"From aab98e2dbd648510f8f51b83fbf4721206ccae45 Mon Sep 17 00:00:00 2001
From: Dan Carpenter <dan.carpenter@linaro.org>
Date: Wed, 15 Jan 2025 09:28:35 +0900
Subject: [PATCH] ksmbd: fix integer overflows on 32 bit systems

On 32bit systems the addition operations in ipc_msg_alloc() can
potentially overflow leading to memory corruption.
Add bounds checking using KSMBD_IPC_MAX_PAYLOAD to avoid overflow.

Fixes: 0626e6641f6b (""cifsd: add server handler for central processing and tranport layers"")
Cc: stable@vger.kernel.org
Signed-off-by: Dan Carpenter <dan.carpenter@linaro.org>
Signed-off-by: Namjae Jeon <linkinjeon@kernel.org>
Signed-off-by: Steve French <stfrench@microsoft.com>
---
 fs/smb/server/transport_ipc.c | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/fs/smb/server/transport_ipc.c b/fs/smb/server/transport_ipc.c
index c0bb8c7722d7..0460ebea6ff0 100644
--- a/fs/smb/server/transport_ipc.c
+++ b/fs/smb/server/transport_ipc.c
@@ -627,6 +627,9 @@ ksmbd_ipc_spnego_authen_request(const char *spnego_blob, int blob_len)
 	struct ksmbd_spnego_authen_request *req;
 	struct ksmbd_spnego_authen_response *resp;
 
+	if (blob_len > KSMBD_IPC_MAX_PAYLOAD)
+		return NULL;
+
 	msg = ipc_msg_alloc(sizeof(struct ksmbd_spnego_authen_request) +
 			blob_len + 1);
 	if (!msg)
@@ -806,6 +809,9 @@ struct ksmbd_rpc_command *ksmbd_rpc_write(struct ksmbd_session *sess, int handle
 	struct ksmbd_rpc_command *req;
 	struct ksmbd_rpc_command *resp;
 
+	if (payload_sz > KSMBD_IPC_MAX_PAYLOAD)
+		return NULL;
+
 	msg = ipc_msg_alloc(sizeof(struct ksmbd_rpc_command) + payload_sz + 1);
 	if (!msg)
 		return NULL;
@@ -854,6 +860,9 @@ struct ksmbd_rpc_command *ksmbd_rpc_ioctl(struct ksmbd_session *sess, int handle
 	struct ksmbd_rpc_command *req;
 	struct ksmbd_rpc_command *resp;
 
+	if (payload_sz > KSMBD_IPC_MAX_PAYLOAD)
+		return NULL;
+
 	msg = ipc_msg_alloc(sizeof(struct ksmbd_rpc_command) + payload_sz + 1);
 	if (!msg)
 		return NULL;
-- 
2.39.5 (Apple Git-154)
",f3b9fb2764591d792d160f375851013665a9e820,"From f3b9fb2764591d792d160f375851013665a9e820 Mon Sep 17 00:00:00 2001
From: Dan Carpenter <dan.carpenter@linaro.org>
Date: Wed, 15 Jan 2025 09:28:35 +0900
Subject: [PATCH] ksmbd: fix integer overflows on 32 bit systems

[ Upstream commit aab98e2dbd648510f8f51b83fbf4721206ccae45 ]

On 32bit systems the addition operations in ipc_msg_alloc() can
potentially overflow leading to memory corruption.
Add bounds checking using KSMBD_IPC_MAX_PAYLOAD to avoid overflow.

Fixes: 0626e6641f6b (""cifsd: add server handler for central processing and tranport layers"")
Cc: stable@vger.kernel.org
Signed-off-by: Dan Carpenter <dan.carpenter@linaro.org>
Signed-off-by: Namjae Jeon <linkinjeon@kernel.org>
Signed-off-by: Steve French <stfrench@microsoft.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 fs/ksmbd/transport_ipc.c | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/fs/ksmbd/transport_ipc.c b/fs/ksmbd/transport_ipc.c
index d62ebbff1e0f..0d096a11ba30 100644
--- a/fs/ksmbd/transport_ipc.c
+++ b/fs/ksmbd/transport_ipc.c
@@ -566,6 +566,9 @@ ksmbd_ipc_spnego_authen_request(const char *spnego_blob, int blob_len)
         struct ksmbd_spnego_authen_request *req;
         struct ksmbd_spnego_authen_response *resp;
 
+        if (blob_len > KSMBD_IPC_MAX_PAYLOAD)
+                return NULL;
+
         msg = ipc_msg_alloc(sizeof(struct ksmbd_spnego_authen_request) +
                         blob_len + 1);
         if (!msg)
@@ -745,6 +748,9 @@ struct ksmbd_rpc_command *ksmbd_rpc_write(struct ksmbd_session *sess, int handle
         struct ksmbd_rpc_command *req;
         struct ksmbd_rpc_command *resp;
 
+        if (payload_sz > KSMBD_IPC_MAX_PAYLOAD)
+                return NULL;
+
         msg = ipc_msg_alloc(sizeof(struct ksmbd_rpc_command) + payload_sz + 1);
         if (!msg)
                 return NULL;
@@ -793,6 +799,9 @@ struct ksmbd_rpc_command *ksmbd_rpc_ioctl(struct ksmbd_session *sess, int handle
         struct ksmbd_rpc_command *req;
         struct ksmbd_rpc_command *resp;
 
+        if (payload_sz > KSMBD_IPC_MAX_PAYLOAD)
+                return NULL;
+
         msg = ipc_msg_alloc(sizeof(struct ksmbd_rpc_command) + payload_sz + 1);
         if (!msg)
                 return NULL;
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 23\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From aab98e2dbd648510f8f51b83fbf4721206ccae45 Mon Sep 17 00:00:00 2001\n|From: Dan Carpenter <dan.carpenter@linaro.org>\n|Date: Wed, 15 Jan 2025 09:28:35 +0900\n|Subject: [PATCH] ksmbd: fix integer overflows on 32 bit systems\n|\n|On 32bit systems the addition operations in ipc_msg_alloc() can\n|potentially overflow leading to memory corruption.\n|Add bounds checking using KSMBD_IPC_MAX_PAYLOAD to avoid overflow.\n|\n|Fixes: 0626e6641f6b (\""cifsd: add server handler for central processing and tranport layers\"")\n|Cc: stable@vger.kernel.org\n|Signed-off-by: Dan Carpenter <dan.carpenter@linaro.org>\n|Signed-off-by: Namjae Jeon <linkinjeon@kernel.org>\n|Signed-off-by: Steve French <stfrench@microsoft.com>\n|---\n| fs/smb/server/transport_ipc.c | 9 +++++++++\n| 1 file changed, 9 insertions(+)\n|\n|diff --git a/fs/smb/server/transport_ipc.c b/fs/smb/server/transport_ipc.c\n|index c0bb8c7722d7..0460ebea6ff0 100644\n|--- a/fs/smb/server/transport_ipc.c\n|+++ b/fs/smb/server/transport_ipc.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n3 out of 3 hunks ignored",10,0,13,54,1,OTHER,OTHER
CVE-2025-21755,78dafe1cf3afa02ed71084b350713b07e72a18fb,"From 78dafe1cf3afa02ed71084b350713b07e72a18fb Mon Sep 17 00:00:00 2001
From: Michal Luczaj <mhal@rbox.co>
Date: Mon, 10 Feb 2025 13:15:00 +0100
Subject: [PATCH] vsock: Orphan socket after transport release

During socket release, sock_orphan() is called without considering that it
sets sk->sk_wq to NULL. Later, if SO_LINGER is enabled, this leads to a
null pointer dereferenced in virtio_transport_wait_close().

Orphan the socket only after transport release.

Partially reverts the 'Fixes:' commit.

KASAN: null-ptr-deref in range [0x0000000000000018-0x000000000000001f]
 lock_acquire+0x19e/0x500
 _raw_spin_lock_irqsave+0x47/0x70
 add_wait_queue+0x46/0x230
 virtio_transport_release+0x4e7/0x7f0
 __vsock_release+0xfd/0x490
 vsock_release+0x90/0x120
 __sock_release+0xa3/0x250
 sock_close+0x14/0x20
 __fput+0x35e/0xa90
 __x64_sys_close+0x78/0xd0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Reported-by: syzbot+9d55b199192a4be7d02c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=9d55b199192a4be7d02c
Fixes: fcdd2242c023 (""vsock: Keep the binding until socket destruction"")
Tested-by: Luigi Leonardi <leonardi@redhat.com>
Reviewed-by: Luigi Leonardi <leonardi@redhat.com>
Signed-off-by: Michal Luczaj <mhal@rbox.co>
Link: https://patch.msgid.link/20250210-vsock-linger-nullderef-v3-1-ef6244d02b54@rbox.co
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/vmw_vsock/af_vsock.c | 8 +++++++-
 1 file changed, 7 insertions(+), 1 deletion(-)

diff --git a/net/vmw_vsock/af_vsock.c b/net/vmw_vsock/af_vsock.c
index 075695173648..53a081d49d28 100644
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -824,13 +824,19 @@ static void __vsock_release(struct sock *sk, int level)
 	 */
 	lock_sock_nested(sk, level);
 
-	sock_orphan(sk);
+	/* Indicate to vsock_remove_sock() that the socket is being released and
+	 * can be removed from the bound_table. Unlike transport reassignment
+	 * case, where the socket must remain bound despite vsock_remove_sock()
+	 * being called from the transport release() callback.
+	 */
+	sock_set_flag(sk, SOCK_DEAD);
 
 	if (vsk->transport)
 		vsk->transport->release(vsk);
 	else if (sock_type_connectible(sk->sk_type))
 		vsock_remove_sock(vsk);
 
+	sock_orphan(sk);
 	sk->sk_shutdown = SHUTDOWN_MASK;
 
 	skb_queue_purge(&sk->sk_receive_queue);
-- 
2.39.5 (Apple Git-154)

",c6acb650a73d5705a93b9c5a2cd5e9c8161f0be3,"From c6acb650a73d5705a93b9c5a2cd5e9c8161f0be3 Mon Sep 17 00:00:00 2001
From: Michal Luczaj <mhal@rbox.co>
Date: Mon, 10 Feb 2025 13:15:00 +0100
Subject: [PATCH] vsock: Orphan socket after transport release

commit 78dafe1cf3afa02ed71084b350713b07e72a18fb upstream.

During socket release, sock_orphan() is called without considering that it
sets sk->sk_wq to NULL. Later, if SO_LINGER is enabled, this leads to a
null pointer dereferenced in virtio_transport_wait_close().

Orphan the socket only after transport release.

Partially reverts the 'Fixes:' commit.

KASAN: null-ptr-deref in range [0x0000000000000018-0x000000000000001f]
 lock_acquire+0x19e/0x500
 _raw_spin_lock_irqsave+0x47/0x70
 add_wait_queue+0x46/0x230
 virtio_transport_release+0x4e7/0x7f0
 __vsock_release+0xfd/0x490
 vsock_release+0x90/0x120
 __sock_release+0xa3/0x250
 sock_close+0x14/0x20
 __fput+0x35e/0xa90
 __x64_sys_close+0x78/0xd0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Reported-by: syzbot+9d55b199192a4be7d02c@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=9d55b199192a4be7d02c
Fixes: fcdd2242c023 (""vsock: Keep the binding until socket destruction"")
Tested-by: Luigi Leonardi <leonardi@redhat.com>
Reviewed-by: Luigi Leonardi <leonardi@redhat.com>
Signed-off-by: Michal Luczaj <mhal@rbox.co>
Link: https://patch.msgid.link/20250210-vsock-linger-nullderef-v3-1-ef6244d02b54@rbox.co
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Luigi Leonardi <leonardi@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/vmw_vsock/af_vsock.c | 8 +++++++-
 1 file changed, 7 insertions(+), 1 deletion(-)

diff --git a/net/vmw_vsock/af_vsock.c b/net/vmw_vsock/af_vsock.c
index 42e750215b04..d7395601a0e3 100644
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -785,13 +785,19 @@ static void __vsock_release(struct sock *sk, int level)
          */
         lock_sock_nested(sk, level);
 
-        sock_orphan(sk);
+        /* Indicate to vsock_remove_sock() that the socket is being released and
+         * can be removed from the bound_table. Unlike transport reassignment
+         * case, where the socket must remain bound despite vsock_remove_sock()
+         * being called from the transport release() callback.
+         */
+        sock_set_flag(sk, SOCK_DEAD);
 
         if (vsk->transport)
                 vsk->transport->release(vsk);
         else if (sk->sk_type == SOCK_STREAM)
                 vsock_remove_sock(vsk);
 
+        sock_orphan(sk);
         sk->sk_shutdown = SHUTDOWN_MASK;
 
         skb_queue_purge(&sk->sk_receive_queue);
-- 
2.39.5 (Apple Git-154)

",patching file net/vmw_vsock/af_vsock.c\nHunk #1 FAILED at 824.\n1 out of 1 hunk FAILED -- saving rejects to file net/vmw_vsock/af_vsock.c.rej,9,0,1,67,1,COMMENTS,COMMENTS
CVE-2025-21756,fcdd2242c0231032fc84e1404315c245ae56322a,"From fcdd2242c0231032fc84e1404315c245ae56322a Mon Sep 17 00:00:00 2001
From: Michal Luczaj <mhal@rbox.co>
Date: Tue, 28 Jan 2025 14:15:27 +0100
Subject: [PATCH] vsock: Keep the binding until socket destruction

Preserve sockets bindings; this includes both resulting from an explicit
bind() and those implicitly bound through autobind during connect().

Prevents socket unbinding during a transport reassignment, which fixes a
use-after-free:

    1. vsock_create() (refcnt=1) calls vsock_insert_unbound() (refcnt=2)
    2. transport->release() calls vsock_remove_bound() without checking if
       sk was bound and moved to bound list (refcnt=1)
    3. vsock_bind() assumes sk is in unbound list and before
       __vsock_insert_bound(vsock_bound_sockets()) calls
       __vsock_remove_bound() which does:
           list_del_init(&vsk->bound_table); // nop
           sock_put(&vsk->sk);               // refcnt=0

BUG: KASAN: slab-use-after-free in __vsock_bind+0x62e/0x730
Read of size 4 at addr ffff88816b46a74c by task a.out/2057
 dump_stack_lvl+0x68/0x90
 print_report+0x174/0x4f6
 kasan_report+0xb9/0x190
 __vsock_bind+0x62e/0x730
 vsock_bind+0x97/0xe0
 __sys_bind+0x154/0x1f0
 __x64_sys_bind+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Allocated by task 2057:
 kasan_save_stack+0x1e/0x40
 kasan_save_track+0x10/0x30
 __kasan_slab_alloc+0x85/0x90
 kmem_cache_alloc_noprof+0x131/0x450
 sk_prot_alloc+0x5b/0x220
 sk_alloc+0x2c/0x870
 __vsock_create.constprop.0+0x2e/0xb60
 vsock_create+0xe4/0x420
 __sock_create+0x241/0x650
 __sys_socket+0xf2/0x1a0
 __x64_sys_socket+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Freed by task 2057:
 kasan_save_stack+0x1e/0x40
 kasan_save_track+0x10/0x30
 kasan_save_free_info+0x37/0x60
 __kasan_slab_free+0x4b/0x70
 kmem_cache_free+0x1a1/0x590
 __sk_destruct+0x388/0x5a0
 __vsock_bind+0x5e1/0x730
 vsock_bind+0x97/0xe0
 __sys_bind+0x154/0x1f0
 __x64_sys_bind+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

refcount_t: addition on 0; use-after-free.
WARNING: CPU: 7 PID: 2057 at lib/refcount.c:25 refcount_warn_saturate+0xce/0x150
RIP: 0010:refcount_warn_saturate+0xce/0x150
 __vsock_bind+0x66d/0x730
 vsock_bind+0x97/0xe0
 __sys_bind+0x154/0x1f0
 __x64_sys_bind+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

refcount_t: underflow; use-after-free.
WARNING: CPU: 7 PID: 2057 at lib/refcount.c:28 refcount_warn_saturate+0xee/0x150
RIP: 0010:refcount_warn_saturate+0xee/0x150
 vsock_remove_bound+0x187/0x1e0
 __vsock_release+0x383/0x4a0
 vsock_release+0x90/0x120
 __sock_release+0xa3/0x250
 sock_close+0x14/0x20
 __fput+0x359/0xa80
 task_work_run+0x107/0x1d0
 do_exit+0x847/0x2560
 do_group_exit+0xb8/0x250
 __x64_sys_exit_group+0x3a/0x50
 x64_sys_call+0xfec/0x14f0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Reviewed-by: Stefano Garzarella <sgarzare@redhat.com>
Signed-off-by: Michal Luczaj <mhal@rbox.co>
Link: https://patch.msgid.link/20250128-vsock-transport-vs-autobind-v3-1-1cf57065b770@rbox.co
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/vmw_vsock/af_vsock.c | 8 ++++++--
 1 file changed, 6 insertions(+), 2 deletions(-)

diff --git a/net/vmw_vsock/af_vsock.c b/net/vmw_vsock/af_vsock.c
index fa9d1b49599b..cfe18bc8fdbe 100644
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -337,7 +337,10 @@ EXPORT_SYMBOL_GPL(vsock_find_connected_socket);
 
 void vsock_remove_sock(struct vsock_sock *vsk)
 {
-	vsock_remove_bound(vsk);
+	/* Transport reassignment must not remove the binding. */
+	if (sock_flag(sk_vsock(vsk), SOCK_DEAD))
+		vsock_remove_bound(vsk);
+
 	vsock_remove_connected(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_remove_sock);
@@ -821,12 +824,13 @@ static void __vsock_release(struct sock *sk, int level)
 	 */
 	lock_sock_nested(sk, level);
 
+	sock_orphan(sk);
+
 	if (vsk->transport)
 		vsk->transport->release(vsk);
 	else if (sock_type_connectible(sk->sk_type))
 		vsock_remove_sock(vsk);
 
-	sock_orphan(sk);
 	sk->sk_shutdown = SHUTDOWN_MASK;
 
 	skb_queue_purge(&sk->sk_receive_queue);
-- 
2.39.5 (Apple Git-154)
",e7754d564579a5db9c5c9f74228df5d6dd6f1173,"From e7754d564579a5db9c5c9f74228df5d6dd6f1173 Mon Sep 17 00:00:00 2001
From: Michal Luczaj <mhal@rbox.co>
Date: Tue, 28 Jan 2025 14:15:27 +0100
Subject: [PATCH] vsock: Keep the binding until socket destruction

commit fcdd2242c0231032fc84e1404315c245ae56322a upstream.

Preserve sockets bindings; this includes both resulting from an explicit
bind() and those implicitly bound through autobind during connect().

Prevents socket unbinding during a transport reassignment, which fixes a
use-after-free:

    1. vsock_create() (refcnt=1) calls vsock_insert_unbound() (refcnt=2)
    2. transport->release() calls vsock_remove_bound() without checking if
       sk was bound and moved to bound list (refcnt=1)
    3. vsock_bind() assumes sk is in unbound list and before
       __vsock_insert_bound(vsock_bound_sockets()) calls
       __vsock_remove_bound() which does:
           list_del_init(&vsk->bound_table); // nop
           sock_put(&vsk->sk);               // refcnt=0

BUG: KASAN: slab-use-after-free in __vsock_bind+0x62e/0x730
Read of size 4 at addr ffff88816b46a74c by task a.out/2057
 dump_stack_lvl+0x68/0x90
 print_report+0x174/0x4f6
 kasan_report+0xb9/0x190
 __vsock_bind+0x62e/0x730
 vsock_bind+0x97/0xe0
 __sys_bind+0x154/0x1f0
 __x64_sys_bind+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Allocated by task 2057:
 kasan_save_stack+0x1e/0x40
 kasan_save_track+0x10/0x30
 __kasan_slab_alloc+0x85/0x90
 kmem_cache_alloc_noprof+0x131/0x450
 sk_prot_alloc+0x5b/0x220
 sk_alloc+0x2c/0x870
 __vsock_create.constprop.0+0x2e/0xb60
 vsock_create+0xe4/0x420
 __sock_create+0x241/0x650
 __sys_socket+0xf2/0x1a0
 __x64_sys_socket+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Freed by task 2057:
 kasan_save_stack+0x1e/0x40
 kasan_save_track+0x10/0x30
 kasan_save_free_info+0x37/0x60
 __kasan_slab_free+0x4b/0x70
 kmem_cache_free+0x1a1/0x590
 __sk_destruct+0x388/0x5a0
 __vsock_bind+0x5e1/0x730
 vsock_bind+0x97/0xe0
 __sys_bind+0x154/0x1f0
 __x64_sys_bind+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

refcount_t: addition on 0; use-after-free.
WARNING: CPU: 7 PID: 2057 at lib/refcount.c:25 refcount_warn_saturate+0xce/0x150
RIP: 0010:refcount_warn_saturate+0xce/0x150
 __vsock_bind+0x66d/0x730
 vsock_bind+0x97/0xe0
 __sys_bind+0x154/0x1f0
 __x64_sys_bind+0x6e/0xb0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

refcount_t: underflow; use-after-free.
WARNING: CPU: 7 PID: 2057 at lib/refcount.c:28 refcount_warn_saturate+0xee/0x150
RIP: 0010:refcount_warn_saturate+0xee/0x150
 vsock_remove_bound+0x187/0x1e0
 __vsock_release+0x383/0x4a0
 vsock_release+0x90/0x120
 __sock_release+0xa3/0x250
 sock_close+0x14/0x20
 __fput+0x359/0xa80
 task_work_run+0x107/0x1d0
 do_exit+0x847/0x2560
 do_group_exit+0xb8/0x250
 __x64_sys_exit_group+0x3a/0x50
 x64_sys_call+0xfec/0x14f0
 do_syscall_64+0x93/0x1b0
 entry_SYSCALL_64_after_hwframe+0x76/0x7e

Fixes: c0cfa2d8a788 (""vsock: add multi-transports support"")
Reviewed-by: Stefano Garzarella <sgarzare@redhat.com>
Signed-off-by: Michal Luczaj <mhal@rbox.co>
Link: https://patch.msgid.link/20250128-vsock-transport-vs-autobind-v3-1-1cf57065b770@rbox.co
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Luigi Leonardi <leonardi@redhat.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/vmw_vsock/af_vsock.c | 8 ++++++--
 1 file changed, 6 insertions(+), 2 deletions(-)

diff --git a/net/vmw_vsock/af_vsock.c b/net/vmw_vsock/af_vsock.c
index 38e682c17ad4..42e750215b04 100644
--- a/net/vmw_vsock/af_vsock.c
+++ b/net/vmw_vsock/af_vsock.c
@@ -330,7 +330,10 @@ EXPORT_SYMBOL_GPL(vsock_find_connected_socket);
 
 void vsock_remove_sock(struct vsock_sock *vsk)
 {
-	vsock_remove_bound(vsk);
+	/* Transport reassignment must not remove the binding. */
+	if (sock_flag(sk_vsock(vsk), SOCK_DEAD))
+		vsock_remove_bound(vsk);
+
 	vsock_remove_connected(vsk);
 }
 EXPORT_SYMBOL_GPL(vsock_remove_sock);
@@ -782,12 +785,13 @@ static void __vsock_release(struct sock *sk, int level)
 	 */
 	lock_sock_nested(sk, level);
 
+	sock_orphan(sk);
+
 	if (vsk->transport)
 		vsk->transport->release(vsk);
 	else if (sk->sk_type == SOCK_STREAM)
 		vsock_remove_sock(vsk);
 
-	sock_orphan(sk);
 	sk->sk_shutdown = SHUTDOWN_MASK;
 
 	skb_queue_purge(&sk->sk_receive_queue);
-- 
2.39.5 (Apple Git-154)

",patching file net/vmw_vsock/af_vsock.c\nHunk #1 succeeded at 330 (offset -7 lines).\nHunk #2 FAILED at 824.\n1 out of 2 hunks FAILED -- saving rejects to file net/vmw_vsock/af_vsock.c.rej,9,1,2,130,1,SEMANTIC,DISJOINT
CVE-2025-21757,c71a192976ded2f2f416d03c4f595cdd4478b825,"From c71a192976ded2f2f416d03c4f595cdd4478b825 Mon Sep 17 00:00:00 2001
From: Jakub Kicinski <kuba@kernel.org>
Date: Wed, 29 Jan 2025 19:15:18 -0800
Subject: [PATCH] net: ipv6: fix dst refleaks in rpl, seg6 and ioam6 lwtunnels

dst_cache_get() gives us a reference, we need to release it.

Discovered by the ioam6.sh test, kmemleak was recently fixed
to catch per-cpu memory leaks.

Fixes: 985ec6f5e623 (""net: ipv6: rpl_iptunnel: mitigate 2-realloc issue"")
Fixes: 40475b63761a (""net: ipv6: seg6_iptunnel: mitigate 2-realloc issue"")
Fixes: dce525185bc9 (""net: ipv6: ioam6_iptunnel: mitigate 2-realloc issue"")
Reviewed-by: Justin Iurman <justin.iurman@uliege.be>
Reviewed-by: Simon Horman <horms@kernel.org>
Link: https://patch.msgid.link/20250130031519.2716843-1-kuba@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/ipv6/ioam6_iptunnel.c | 5 +++--
 net/ipv6/rpl_iptunnel.c   | 6 ++++--
 net/ipv6/seg6_iptunnel.c  | 6 ++++--
 3 files changed, 11 insertions(+), 6 deletions(-)

diff --git a/net/ipv6/ioam6_iptunnel.c b/net/ipv6/ioam6_iptunnel.c
index 28e5a89dc255..3936c137a572 100644
--- a/net/ipv6/ioam6_iptunnel.c
+++ b/net/ipv6/ioam6_iptunnel.c
@@ -336,7 +336,7 @@ static int ioam6_do_encap(struct net *net, struct sk_buff *skb,
 
 static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 {
-        struct dst_entry *dst = skb_dst(skb), *cache_dst;
+        struct dst_entry *dst = skb_dst(skb), *cache_dst = NULL;
         struct in6_addr orig_daddr;
         struct ioam6_lwt *ilwt;
         int err = -EINVAL;
@@ -407,7 +407,6 @@ static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 cache_dst = ip6_route_output(net, NULL, &fl6);
                 if (cache_dst->error) {
                         err = cache_dst->error;
-                        dst_release(cache_dst);
                         goto drop;
                 }
 
@@ -426,8 +425,10 @@ static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 return dst_output(net, sk, skb);
         }
 out:
+        dst_release(cache_dst);
         return dst->lwtstate->orig_output(net, sk, skb);
 drop:
+        dst_release(cache_dst);
         kfree_skb(skb);
         return err;
 }
diff --git a/net/ipv6/rpl_iptunnel.c b/net/ipv6/rpl_iptunnel.c
index 7ba22d2f2bfe..9b7d03563115 100644
--- a/net/ipv6/rpl_iptunnel.c
+++ b/net/ipv6/rpl_iptunnel.c
@@ -232,7 +232,6 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 dst = ip6_route_output(net, NULL, &fl6);
                 if (dst->error) {
                         err = dst->error;
-                        dst_release(dst);
                         goto drop;
                 }
 
@@ -251,6 +250,7 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
         return dst_output(net, sk, skb);
 
 drop:
+        dst_release(dst);
         kfree_skb(skb);
         return err;
 }
@@ -269,8 +269,10 @@ static int rpl_input(struct sk_buff *skb)
         local_bh_enable();
 
         err = rpl_do_srh(skb, rlwt, dst);
-        if (unlikely(err))
+        if (unlikely(err)) {
+                dst_release(dst);
                 goto drop;
+        }
 
         if (!dst) {
                 ip6_route_input(skb);
diff --git a/net/ipv6/seg6_iptunnel.c b/net/ipv6/seg6_iptunnel.c
index 4bf937bfc263..eacc4e91b48e 100644
--- a/net/ipv6/seg6_iptunnel.c
+++ b/net/ipv6/seg6_iptunnel.c
@@ -482,8 +482,10 @@ static int seg6_input_core(struct net *net, struct sock *sk,
         local_bh_enable();
 
         err = seg6_do_srh(skb, dst);
-        if (unlikely(err))
+        if (unlikely(err)) {
+                dst_release(dst);
                 goto drop;
+        }
 
         if (!dst) {
                 ip6_route_input(skb);
@@ -571,7 +573,6 @@ static int seg6_output_core(struct net *net, struct sock *sk,
                 dst = ip6_route_output(net, NULL, &fl6);
                 if (dst->error) {
                         err = dst->error;
-                        dst_release(dst);
                         goto drop;
                 }
 
@@ -593,6 +594,7 @@ static int seg6_output_core(struct net *net, struct sock *sk,
 
         return dst_output(net, sk, skb);
 drop:
+        dst_release(dst);
         kfree_skb(skb);
         return err;
 }
-- 
2.39.5 (Apple Git-154)
",cb9950eaaf9cc76dfe490c06aa11f185b3c7f22b,"From cb9950eaaf9cc76dfe490c06aa11f185b3c7f22b Mon Sep 17 00:00:00 2001
From: Jakub Kicinski <kuba@kernel.org>
Date: Wed, 29 Jan 2025 19:15:18 -0800
Subject: [PATCH] net: ipv6: fix dst refleaks in rpl, seg6 and ioam6 lwtunnels

commit c71a192976ded2f2f416d03c4f595cdd4478b825 upstream.

dst_cache_get() gives us a reference, we need to release it.

Discovered by the ioam6.sh test, kmemleak was recently fixed
to catch per-cpu memory leaks.

Fixes: 985ec6f5e623 (""net: ipv6: rpl_iptunnel: mitigate 2-realloc issue"")
Fixes: 40475b63761a (""net: ipv6: seg6_iptunnel: mitigate 2-realloc issue"")
Fixes: dce525185bc9 (""net: ipv6: ioam6_iptunnel: mitigate 2-realloc issue"")
Reviewed-by: Justin Iurman <justin.iurman@uliege.be>
Reviewed-by: Simon Horman <horms@kernel.org>
Link: https://patch.msgid.link/20250130031519.2716843-1-kuba@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/ipv6/rpl_iptunnel.c  | 6 ++++--
 net/ipv6/seg6_iptunnel.c | 2 +-
 2 files changed, 5 insertions(+), 3 deletions(-)

diff --git a/net/ipv6/rpl_iptunnel.c b/net/ipv6/rpl_iptunnel.c
index 95e84ec03520..5d47948c0364 100644
--- a/net/ipv6/rpl_iptunnel.c
+++ b/net/ipv6/rpl_iptunnel.c
@@ -232,7 +232,6 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 		dst = ip6_route_output(net, NULL, &fl6);
 		if (dst->error) {
 			err = dst->error;
-			dst_release(dst);
 			goto drop;
 		}
 
@@ -251,6 +250,7 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 	return dst_output(net, sk, skb);
 
 drop:
+	dst_release(dst);
 	kfree_skb(skb);
 	return err;
 }
@@ -277,8 +277,10 @@ static int rpl_input(struct sk_buff *skb)
 	local_bh_enable();
 
 	err = rpl_do_srh(skb, rlwt, dst);
-	if (unlikely(err))
+	if (unlikely(err)) {
+		dst_release(dst);
 		goto drop;
+	}
 
 	skb_dst_drop(skb);
 
diff --git a/net/ipv6/seg6_iptunnel.c b/net/ipv6/seg6_iptunnel.c
index a73840da34ed..986459a85fbd 100644
--- a/net/ipv6/seg6_iptunnel.c
+++ b/net/ipv6/seg6_iptunnel.c
@@ -380,7 +380,6 @@ static int seg6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 		dst = ip6_route_output(net, NULL, &fl6);
 		if (dst->error) {
 			err = dst->error;
-			dst_release(dst);
 			goto drop;
 		}
 
@@ -398,6 +397,7 @@ static int seg6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 
 	return dst_output(net, sk, skb);
 drop:
+	dst_release(dst);
 	kfree_skb(skb);
 	return err;
 }
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 28\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From c71a192976ded2f2f416d03c4f595cdd4478b825 Mon Sep 17 00:00:00 2001\n|From: Jakub Kicinski <kuba@kernel.org>\n|Date: Wed, 29 Jan 2025 19:15:18 -0800\n|Subject: [PATCH] net: ipv6: fix dst refleaks in rpl, seg6 and ioam6 lwtunnels\n|\n|dst_cache_get() gives us a reference, we need to release it.\n|\n|Discovered by the ioam6.sh test, kmemleak was recently fixed\n|to catch per-cpu memory leaks.\n|\n|Fixes: 985ec6f5e623 (\""net: ipv6: rpl_iptunnel: mitigate 2-realloc issue\"")\n|Fixes: 40475b63761a (\""net: ipv6: seg6_iptunnel: mitigate 2-realloc issue\"")\n|Fixes: dce525185bc9 (\""net: ipv6: ioam6_iptunnel: mitigate 2-realloc issue\"")\n|Reviewed-by: Justin Iurman <justin.iurman@uliege.be>\n|Reviewed-by: Simon Horman <horms@kernel.org>\n|Link: https://patch.msgid.link/20250130031519.2716843-1-kuba@kernel.org\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| net/ipv6/ioam6_iptunnel.c | 5 +++--\n| net/ipv6/rpl_iptunnel.c   | 6 ++++--\n| net/ipv6/seg6_iptunnel.c  | 6 ++++--\n| 3 files changed, 11 insertions(+), 6 deletions(-)\n|\n|diff --git a/net/ipv6/ioam6_iptunnel.c b/net/ipv6/ioam6_iptunnel.c\n|index 28e5a89dc255..3936c137a572 100644\n|--- a/net/ipv6/ioam6_iptunnel.c\n|+++ b/net/ipv6/ioam6_iptunnel.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n3 out of 3 hunks ignored\npatching file net/ipv6/rpl_iptunnel.c\nHunk #3 succeeded at 277 with fuzz 2 (offset 8 lines).\npatching file net/ipv6/seg6_iptunnel.c\nHunk #1 FAILED at 482.\nHunk #2 succeeded at 380 (offset -191 lines).\nHunk #3 succeeded at 397 (offset -195 lines).\n1 out of 3 hunks FAILED -- saving rejects to file net/ipv6/seg6_iptunnel.c.rej",18,1,28,121,1,OTHER,OTHER
CVE-2025-21757,c71a192976ded2f2f416d03c4f595cdd4478b825,"From c71a192976ded2f2f416d03c4f595cdd4478b825 Mon Sep 17 00:00:00 2001
From: Jakub Kicinski <kuba@kernel.org>
Date: Wed, 29 Jan 2025 19:15:18 -0800
Subject: [PATCH] net: ipv6: fix dst refleaks in rpl, seg6 and ioam6 lwtunnels

dst_cache_get() gives us a reference, we need to release it.

Discovered by the ioam6.sh test, kmemleak was recently fixed
to catch per-cpu memory leaks.

Fixes: 985ec6f5e623 (""net: ipv6: rpl_iptunnel: mitigate 2-realloc issue"")
Fixes: 40475b63761a (""net: ipv6: seg6_iptunnel: mitigate 2-realloc issue"")
Fixes: dce525185bc9 (""net: ipv6: ioam6_iptunnel: mitigate 2-realloc issue"")
Reviewed-by: Justin Iurman <justin.iurman@uliege.be>
Reviewed-by: Simon Horman <horms@kernel.org>
Link: https://patch.msgid.link/20250130031519.2716843-1-kuba@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/ipv6/ioam6_iptunnel.c | 5 +++--
 net/ipv6/rpl_iptunnel.c   | 6 ++++--
 net/ipv6/seg6_iptunnel.c  | 6 ++++--
 3 files changed, 11 insertions(+), 6 deletions(-)

diff --git a/net/ipv6/ioam6_iptunnel.c b/net/ipv6/ioam6_iptunnel.c
index 28e5a89dc255..3936c137a572 100644
--- a/net/ipv6/ioam6_iptunnel.c
+++ b/net/ipv6/ioam6_iptunnel.c
@@ -336,7 +336,7 @@ static int ioam6_do_encap(struct net *net, struct sk_buff *skb,
 
 static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 {
-        struct dst_entry *dst = skb_dst(skb), *cache_dst;
+        struct dst_entry *dst = skb_dst(skb), *cache_dst = NULL;
         struct in6_addr orig_daddr;
         struct ioam6_lwt *ilwt;
         int err = -EINVAL;
@@ -407,7 +407,6 @@ static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 cache_dst = ip6_route_output(net, NULL, &fl6);
                 if (cache_dst->error) {
                         err = cache_dst->error;
-                        dst_release(cache_dst);
                         goto drop;
                 }
 
@@ -426,8 +425,10 @@ static int ioam6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 return dst_output(net, sk, skb);
         }
 out:
+        dst_release(cache_dst);
         return dst->lwtstate->orig_output(net, sk, skb);
 drop:
+        dst_release(cache_dst);
         kfree_skb(skb);
         return err;
 }
diff --git a/net/ipv6/rpl_iptunnel.c b/net/ipv6/rpl_iptunnel.c
index 7ba22d2f2bfe..9b7d03563115 100644
--- a/net/ipv6/rpl_iptunnel.c
+++ b/net/ipv6/rpl_iptunnel.c
@@ -232,7 +232,6 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 dst = ip6_route_output(net, NULL, &fl6);
                 if (dst->error) {
                         err = dst->error;
-                        dst_release(dst);
                         goto drop;
                 }
 
@@ -251,6 +250,7 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
         return dst_output(net, sk, skb);
 
 drop:
+        dst_release(dst);
         kfree_skb(skb);
         return err;
 }
@@ -269,8 +269,10 @@ static int rpl_input(struct sk_buff *skb)
         local_bh_enable();
 
         err = rpl_do_srh(skb, rlwt, dst);
-        if (unlikely(err))
+        if (unlikely(err)) {
+                dst_release(dst);
                 goto drop;
+        }
 
         if (!dst) {
                 ip6_route_input(skb);
diff --git a/net/ipv6/seg6_iptunnel.c b/net/ipv6/seg6_iptunnel.c
index 4bf937bfc263..eacc4e91b48e 100644
--- a/net/ipv6/seg6_iptunnel.c
+++ b/net/ipv6/seg6_iptunnel.c
@@ -482,8 +482,10 @@ static int seg6_input_core(struct net *net, struct sock *sk,
         local_bh_enable();
 
         err = seg6_do_srh(skb, dst);
-        if (unlikely(err))
+        if (unlikely(err)) {
+                dst_release(dst);
                 goto drop;
+        }
 
         if (!dst) {
                 ip6_route_input(skb);
@@ -571,7 +573,6 @@ static int seg6_output_core(struct net *net, struct sock *sk,
                 dst = ip6_route_output(net, NULL, &fl6);
                 if (dst->error) {
                         err = dst->error;
-                        dst_release(dst);
                         goto drop;
                 }
 
@@ -593,6 +594,7 @@ static int seg6_output_core(struct net *net, struct sock *sk,
 
         return dst_output(net, sk, skb);
 drop:
+        dst_release(dst);
         kfree_skb(skb);
         return err;
 }
-- 
2.39.5 (Apple Git-154)
",bf500b0d0cfe92ee62dfd4c2ace7f6353cf3a4c8,"From bf500b0d0cfe92ee62dfd4c2ace7f6353cf3a4c8 Mon Sep 17 00:00:00 2001
From: Jakub Kicinski <kuba@kernel.org>
Date: Wed, 29 Jan 2025 19:15:18 -0800
Subject: [PATCH] net: ipv6: fix dst refleaks in rpl, seg6 and ioam6 lwtunnels

commit c71a192976ded2f2f416d03c4f595cdd4478b825 upstream.

dst_cache_get() gives us a reference, we need to release it.

Discovered by the ioam6.sh test, kmemleak was recently fixed
to catch per-cpu memory leaks.

Fixes: 985ec6f5e623 (""net: ipv6: rpl_iptunnel: mitigate 2-realloc issue"")
Fixes: 40475b63761a (""net: ipv6: seg6_iptunnel: mitigate 2-realloc issue"")
Fixes: dce525185bc9 (""net: ipv6: ioam6_iptunnel: mitigate 2-realloc issue"")
Reviewed-by: Justin Iurman <justin.iurman@uliege.be>
Reviewed-by: Simon Horman <horms@kernel.org>
Link: https://patch.msgid.link/20250130031519.2716843-1-kuba@kernel.org
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/ipv6/rpl_iptunnel.c  | 6 ++++--
 net/ipv6/seg6_iptunnel.c | 6 ++++--
 2 files changed, 8 insertions(+), 4 deletions(-)

diff --git a/net/ipv6/rpl_iptunnel.c b/net/ipv6/rpl_iptunnel.c
index 862ac1e2e191..6bf95aba0efc 100644
--- a/net/ipv6/rpl_iptunnel.c
+++ b/net/ipv6/rpl_iptunnel.c
@@ -232,7 +232,6 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
                 dst = ip6_route_output(net, NULL, &fl6);
                 if (dst->error) {
                         err = dst->error;
-                        dst_release(dst);
                         goto drop;
                 }
 
@@ -251,6 +250,7 @@ static int rpl_output(struct net *net, struct sock *sk, struct sk_buff *skb)
         return dst_output(net, sk, skb);
 
 drop:
+        dst_release(dst);
         kfree_skb(skb);
         return err;
 }
@@ -277,8 +277,10 @@ static int rpl_input(struct sk_buff *skb)
         local_bh_enable();
 
         err = rpl_do_srh(skb, rlwt, dst);
-        if (unlikely(err))
+        if (unlikely(err)) {
+                dst_release(dst);
                 goto drop;
+        }
 
         skb_dst_drop(skb);
 
diff --git a/net/ipv6/seg6_iptunnel.c b/net/ipv6/seg6_iptunnel.c
index b186d85ec5b3..4188c1675483 100644
--- a/net/ipv6/seg6_iptunnel.c
+++ b/net/ipv6/seg6_iptunnel.c
@@ -490,8 +490,10 @@ static int seg6_input_core(struct net *net, struct sock *sk,
         local_bh_enable();
 
         err = seg6_do_srh(skb, dst);
-        if (unlikely(err))
+        if (unlikely(err)) {
+                dst_release(dst);
                 goto drop;
+        }
 
         skb_dst_drop(skb);
 
@@ -582,7 +584,6 @@ static int seg6_output_core(struct net *net, struct sock *sk,
                 dst = ip6_route_output(net, NULL, &fl6);
                 if (dst->error) {
                         err = dst->error;
-                        dst_release(dst);
                         goto drop;
                 }
 
@@ -604,6 +605,7 @@ static int seg6_output_core(struct net *net, struct sock *sk,
 
         return dst_output(net, sk, skb);
 drop:
+        dst_release(dst);
         kfree_skb(skb);
         return err;
 }
-- 
2.39.5 (Apple Git-154)
",patching file net/ipv6/ioam6_iptunnel.c\nHunk #1 FAILED at 336.\nHunk #2 FAILED at 407.\nHunk #3 FAILED at 426.\n3 out of 3 hunks FAILED -- saving rejects to file net/ipv6/ioam6_iptunnel.c.rej\npatching file net/ipv6/rpl_iptunnel.c\nHunk #3 succeeded at 277 with fuzz 2 (offset 8 lines).\npatching file net/ipv6/seg6_iptunnel.c\nHunk #1 succeeded at 490 with fuzz 2 (offset 8 lines).\nHunk #2 succeeded at 584 (offset 11 lines).\nHunk #3 succeeded at 605 (offset 11 lines).,18,1,28,121,1,COMMENTS,OTHER
CVE-2025-21787,5bef3ac184b5626ea62385d6b82a1992b89d7940,"From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 drivers/net/team/team_core.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c
index dc7cbd6a9798..f4019815f473 100644
--- a/drivers/net/team/team_core.c
+++ b/drivers/net/team/team_core.c
@@ -2639,7 +2639,9 @@ int team_nl_options_set_doit(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

",4512482e4805dd30bc77dec511f2a2edba5cb868,"From 4512482e4805dd30bc77dec511f2a2edba5cb868 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

[ Upstream commit 5bef3ac184b5626ea62385d6b82a1992b89d7940 ]

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/net/team/team.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team.c b/drivers/net/team/team.c
index b23aa3c8bdf8..c2327fa10747 100644
--- a/drivers/net/team/team.c
+++ b/drivers/net/team/team.c
@@ -2670,7 +2670,9 @@ static int team_nl_cmd_options_set(struct sk_buff *skb, struct genl_info *info)
                                 ctx.data.u32_val = nla_get_u32(attr_data);
                                 break;
                         case TEAM_OPTION_TYPE_STRING:
-                                if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+                                if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+                                    !memchr(nla_data(attr_data), '\0',
+                                            nla_len(attr_data))) {
                                         err = -EINVAL;
                                         goto team_put;
                                 }
-- 
2.39.5 (Apple Git-154)

","an't find file to patch at input line 58\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001\n|From: Eric Dumazet <edumazet@google.com>\n|Date: Wed, 12 Feb 2025 13:49:28 +0000\n|Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation\n|\n|syzbot reported following splat [1]\n|\n|Make sure user-provided data contains one nul byte.\n|\n|[1]\n| BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]\n| BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  string_nocheck lib/vsprintf.c:633 [inline]\n|  string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843\n|  __request_module+0x252/0x9f0 kernel/module/kmod.c:149\n|  team_mode_get drivers/net/team/team_core.c:480 [inline]\n|  team_change_mode drivers/net/team/team_core.c:607 [inline]\n|  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401\n|  team_option_set drivers/net/team/team_core.c:375 [inline]\n|  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662\n|  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]\n|  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]\n|  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210\n|  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543\n|  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219\n|  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]\n|  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348\n|  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892\n|  sock_sendmsg_nosec net/socket.c:718 [inline]\n|  __sock_sendmsg+0x30f/0x380 net/socket.c:733\n|  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573\n|  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627\n|  __sys_sendmsg net/socket.c:2659 [inline]\n|  __do_sys_sendmsg net/socket.c:2664 [inline]\n|  __se_sys_sendmsg net/socket.c:2662 [inline]\n|  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662\n|  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47\n|  do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n|  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83\n| entry_SYSCALL_64_after_hwframe+0x77/0x7f\n|\n|Fixes: 3d249d4ca7d0 (\""net: introduce ethernet teaming device\"")\n|Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com\n|Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d\n|Signed-off-by: Eric Dumazet <edumazet@google.com>\n|Reviewed-by: Jiri Pirko <jiri@nvidia.com>\n|Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| drivers/net/team/team_core.c | 4 +++-\n| 1 file changed, 3 insertions(+), 1 deletion(-)\n|\n|diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c\n|index dc7cbd6a9798..f4019815f473 100644\n|--- a/drivers/net/team/team_core.c\n|+++ b/drivers/net/team/team_core.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,2,71,1,DELETE,COMMENTS
CVE-2025-21787,5bef3ac184b5626ea62385d6b82a1992b89d7940,"From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 drivers/net/team/team_core.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c
index dc7cbd6a9798..f4019815f473 100644
--- a/drivers/net/team/team_core.c
+++ b/drivers/net/team/team_core.c
@@ -2639,7 +2639,9 @@ int team_nl_options_set_doit(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

",d071a91fa614ecdf760c29f61f6a7bfb7df796d6,"From d071a91fa614ecdf760c29f61f6a7bfb7df796d6 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

[ Upstream commit 5bef3ac184b5626ea62385d6b82a1992b89d7940 ]

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/net/team/team.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team.c b/drivers/net/team/team.c
index 46a7c9fb6300..1ce3bccd4ebd 100644
--- a/drivers/net/team/team.c
+++ b/drivers/net/team/team.c
@@ -2657,7 +2657,9 @@ static int team_nl_cmd_options_set(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 58\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001\n|From: Eric Dumazet <edumazet@google.com>\n|Date: Wed, 12 Feb 2025 13:49:28 +0000\n|Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation\n|\n|syzbot reported following splat [1]\n|\n|Make sure user-provided data contains one nul byte.\n|\n|[1]\n| BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]\n| BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  string_nocheck lib/vsprintf.c:633 [inline]\n|  string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843\n|  __request_module+0x252/0x9f0 kernel/module/kmod.c:149\n|  team_mode_get drivers/net/team/team_core.c:480 [inline]\n|  team_change_mode drivers/net/team/team_core.c:607 [inline]\n|  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401\n|  team_option_set drivers/net/team/team_core.c:375 [inline]\n|  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662\n|  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]\n|  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]\n|  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210\n|  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543\n|  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219\n|  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]\n|  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348\n|  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892\n|  sock_sendmsg_nosec net/socket.c:718 [inline]\n|  __sock_sendmsg+0x30f/0x380 net/socket.c:733\n|  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573\n|  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627\n|  __sys_sendmsg net/socket.c:2659 [inline]\n|  __do_sys_sendmsg net/socket.c:2664 [inline]\n|  __se_sys_sendmsg net/socket.c:2662 [inline]\n|  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662\n|  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47\n|  do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n|  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83\n| entry_SYSCALL_64_after_hwframe+0x77/0x7f\n|\n|Fixes: 3d249d4ca7d0 (\""net: introduce ethernet teaming device\"")\n|Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com\n|Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d\n|Signed-off-by: Eric Dumazet <edumazet@google.com>\n|Reviewed-by: Jiri Pirko <jiri@nvidia.com>\n|Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| drivers/net/team/team_core.c | 4 +++-\n| 1 file changed, 3 insertions(+), 1 deletion(-)\n|\n|diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c\n|index dc7cbd6a9798..f4019815f473 100644\n|--- a/drivers/net/team/team_core.c\n|+++ b/drivers/net/team/team_core.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,2,71,1,OTHER,COMMENTS
CVE-2025-21787,5bef3ac184b5626ea62385d6b82a1992b89d7940,"From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 drivers/net/team/team_core.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c
index dc7cbd6a9798..f4019815f473 100644
--- a/drivers/net/team/team_core.c
+++ b/drivers/net/team/team_core.c
@@ -2639,7 +2639,9 @@ int team_nl_options_set_doit(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

",7c30483d0f6bdb2230e10e3e4be5167927eac7a0,"From 7c30483d0f6bdb2230e10e3e4be5167927eac7a0 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

[ Upstream commit 5bef3ac184b5626ea62385d6b82a1992b89d7940 ]

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/net/team/team.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team.c b/drivers/net/team/team.c
index b0a9dd33a019..197aea66b30d 100644
--- a/drivers/net/team/team.c
+++ b/drivers/net/team/team.c
@@ -2663,7 +2663,9 @@ static int team_nl_cmd_options_set(struct sk_buff *skb, struct genl_info *info)
                                 ctx.data.u32_val = nla_get_u32(attr_data);
                                 break;
                         case TEAM_OPTION_TYPE_STRING:
-                                if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+                                if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+                                    !memchr(nla_data(attr_data), '\0',
+                                            nla_len(attr_data))) {
                                         err = -EINVAL;
                                         goto team_put;
                                 }
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 58\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001\n|From: Eric Dumazet <edumazet@google.com>\n|Date: Wed, 12 Feb 2025 13:49:28 +0000\n|Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation\n|\n|syzbot reported following splat [1]\n|\n|Make sure user-provided data contains one nul byte.\n|\n|[1]\n| BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]\n| BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  string_nocheck lib/vsprintf.c:633 [inline]\n|  string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843\n|  __request_module+0x252/0x9f0 kernel/module/kmod.c:149\n|  team_mode_get drivers/net/team/team_core.c:480 [inline]\n|  team_change_mode drivers/net/team/team_core.c:607 [inline]\n|  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401\n|  team_option_set drivers/net/team/team_core.c:375 [inline]\n|  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662\n|  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]\n|  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]\n|  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210\n|  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543\n|  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219\n|  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]\n|  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348\n|  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892\n|  sock_sendmsg_nosec net/socket.c:718 [inline]\n|  __sock_sendmsg+0x30f/0x380 net/socket.c:733\n|  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573\n|  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627\n|  __sys_sendmsg net/socket.c:2659 [inline]\n|  __do_sys_sendmsg net/socket.c:2664 [inline]\n|  __se_sys_sendmsg net/socket.c:2662 [inline]\n|  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662\n|  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47\n|  do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n|  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83\n| entry_SYSCALL_64_after_hwframe+0x77/0x7f\n|\n|Fixes: 3d249d4ca7d0 (\""net: introduce ethernet teaming device\"")\n|Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com\n|Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d\n|Signed-off-by: Eric Dumazet <edumazet@google.com>\n|Reviewed-by: Jiri Pirko <jiri@nvidia.com>\n|Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| drivers/net/team/team_core.c | 4 +++-\n| 1 file changed, 3 insertions(+), 1 deletion(-)\n|\n|diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c\n|index dc7cbd6a9798..f4019815f473 100644\n|--- a/drivers/net/team/team_core.c\n|+++ b/drivers/net/team/team_core.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,2,71,1,COMMENTS,COMMENTS
CVE-2025-21787,5bef3ac184b5626ea62385d6b82a1992b89d7940,"From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 drivers/net/team/team_core.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c
index dc7cbd6a9798..f4019815f473 100644
--- a/drivers/net/team/team_core.c
+++ b/drivers/net/team/team_core.c
@@ -2639,7 +2639,9 @@ int team_nl_options_set_doit(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

",7f5af50f3aa0af8cbef9fb76fffeed69e8143f59,"From 7f5af50f3aa0af8cbef9fb76fffeed69e8143f59 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

[ Upstream commit 5bef3ac184b5626ea62385d6b82a1992b89d7940 ]

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/net/team/team.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team.c b/drivers/net/team/team.c
index 699076fbfb4d..c05a60f23677 100644
--- a/drivers/net/team/team.c
+++ b/drivers/net/team/team.c
@@ -2664,7 +2664,9 @@ static int team_nl_cmd_options_set(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 58\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001\n|From: Eric Dumazet <edumazet@google.com>\n|Date: Wed, 12 Feb 2025 13:49:28 +0000\n|Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation\n|\n|syzbot reported following splat [1]\n|\n|Make sure user-provided data contains one nul byte.\n|\n|[1]\n| BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]\n| BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  string_nocheck lib/vsprintf.c:633 [inline]\n|  string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843\n|  __request_module+0x252/0x9f0 kernel/module/kmod.c:149\n|  team_mode_get drivers/net/team/team_core.c:480 [inline]\n|  team_change_mode drivers/net/team/team_core.c:607 [inline]\n|  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401\n|  team_option_set drivers/net/team/team_core.c:375 [inline]\n|  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662\n|  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]\n|  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]\n|  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210\n|  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543\n|  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219\n|  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]\n|  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348\n|  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892\n|  sock_sendmsg_nosec net/socket.c:718 [inline]\n|  __sock_sendmsg+0x30f/0x380 net/socket.c:733\n|  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573\n|  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627\n|  __sys_sendmsg net/socket.c:2659 [inline]\n|  __do_sys_sendmsg net/socket.c:2664 [inline]\n|  __se_sys_sendmsg net/socket.c:2662 [inline]\n|  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662\n|  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47\n|  do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n|  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83\n| entry_SYSCALL_64_after_hwframe+0x77/0x7f\n|\n|Fixes: 3d249d4ca7d0 (\""net: introduce ethernet teaming device\"")\n|Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com\n|Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d\n|Signed-off-by: Eric Dumazet <edumazet@google.com>\n|Reviewed-by: Jiri Pirko <jiri@nvidia.com>\n|Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| drivers/net/team/team_core.c | 4 +++-\n| 1 file changed, 3 insertions(+), 1 deletion(-)\n|\n|diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c\n|index dc7cbd6a9798..f4019815f473 100644\n|--- a/drivers/net/team/team_core.c\n|+++ b/drivers/net/team/team_core.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,2,71,1,DELETE,COMMENTS
CVE-2025-21787,5bef3ac184b5626ea62385d6b82a1992b89d7940,"From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 drivers/net/team/team_core.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c
index dc7cbd6a9798..f4019815f473 100644
--- a/drivers/net/team/team_core.c
+++ b/drivers/net/team/team_core.c
@@ -2639,7 +2639,9 @@ int team_nl_options_set_doit(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

",f443687ad20c70320d1248f35f57bf46cac8df0a,"From f443687ad20c70320d1248f35f57bf46cac8df0a Mon Sep 17 00:00:00 2001
From: Eric Dumazet <edumazet@google.com>
Date: Wed, 12 Feb 2025 13:49:28 +0000
Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation

[ Upstream commit 5bef3ac184b5626ea62385d6b82a1992b89d7940 ]

syzbot reported following splat [1]

Make sure user-provided data contains one nul byte.

[1]
 BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]
 BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714
  string_nocheck lib/vsprintf.c:633 [inline]
  string+0x3ec/0x5f0 lib/vsprintf.c:714
  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843
  __request_module+0x252/0x9f0 kernel/module/kmod.c:149
  team_mode_get drivers/net/team/team_core.c:480 [inline]
  team_change_mode drivers/net/team/team_core.c:607 [inline]
  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401
  team_option_set drivers/net/team/team_core.c:375 [inline]
  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662
  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]
  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]
  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210
  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543
  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219
  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348
  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892
  sock_sendmsg_nosec net/socket.c:718 [inline]
  __sock_sendmsg+0x30f/0x380 net/socket.c:733
  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573
  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627
  __sys_sendmsg net/socket.c:2659 [inline]
  __do_sys_sendmsg net/socket.c:2664 [inline]
  __se_sys_sendmsg net/socket.c:2662 [inline]
  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662
  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47
  do_syscall_x64 arch/x86/entry/common.c:52 [inline]
  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83
 entry_SYSCALL_64_after_hwframe+0x77/0x7f

Fixes: 3d249d4ca7d0 (""net: introduce ethernet teaming device"")
Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com
Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d
Signed-off-by: Eric Dumazet <edumazet@google.com>
Reviewed-by: Jiri Pirko <jiri@nvidia.com>
Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 drivers/net/team/team.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/drivers/net/team/team.c b/drivers/net/team/team.c
index 015151cd2222..1e0adeb5e177 100644
--- a/drivers/net/team/team.c
+++ b/drivers/net/team/team.c
@@ -2665,7 +2665,9 @@ static int team_nl_cmd_options_set(struct sk_buff *skb, struct genl_info *info)
 				ctx.data.u32_val = nla_get_u32(attr_data);
 				break;
 			case TEAM_OPTION_TYPE_STRING:
-				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN) {
+				if (nla_len(attr_data) > TEAM_STRING_MAX_LEN ||
+				    !memchr(nla_data(attr_data), '\0',
+					    nla_len(attr_data))) {
 					err = -EINVAL;
 					goto team_put;
 				}
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 58\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 5bef3ac184b5626ea62385d6b82a1992b89d7940 Mon Sep 17 00:00:00 2001\n|From: Eric Dumazet <edumazet@google.com>\n|Date: Wed, 12 Feb 2025 13:49:28 +0000\n|Subject: [PATCH] team: better TEAM_OPTION_TYPE_STRING validation\n|\n|syzbot reported following splat [1]\n|\n|Make sure user-provided data contains one nul byte.\n|\n|[1]\n| BUG: KMSAN: uninit-value in string_nocheck lib/vsprintf.c:633 [inline]\n| BUG: KMSAN: uninit-value in string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  string_nocheck lib/vsprintf.c:633 [inline]\n|  string+0x3ec/0x5f0 lib/vsprintf.c:714\n|  vsnprintf+0xa5d/0x1960 lib/vsprintf.c:2843\n|  __request_module+0x252/0x9f0 kernel/module/kmod.c:149\n|  team_mode_get drivers/net/team/team_core.c:480 [inline]\n|  team_change_mode drivers/net/team/team_core.c:607 [inline]\n|  team_mode_option_set+0x437/0x970 drivers/net/team/team_core.c:1401\n|  team_option_set drivers/net/team/team_core.c:375 [inline]\n|  team_nl_options_set_doit+0x1339/0x1f90 drivers/net/team/team_core.c:2662\n|  genl_family_rcv_msg_doit net/netlink/genetlink.c:1115 [inline]\n|  genl_family_rcv_msg net/netlink/genetlink.c:1195 [inline]\n|  genl_rcv_msg+0x1214/0x12c0 net/netlink/genetlink.c:1210\n|  netlink_rcv_skb+0x375/0x650 net/netlink/af_netlink.c:2543\n|  genl_rcv+0x40/0x60 net/netlink/genetlink.c:1219\n|  netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]\n|  netlink_unicast+0xf52/0x1260 net/netlink/af_netlink.c:1348\n|  netlink_sendmsg+0x10da/0x11e0 net/netlink/af_netlink.c:1892\n|  sock_sendmsg_nosec net/socket.c:718 [inline]\n|  __sock_sendmsg+0x30f/0x380 net/socket.c:733\n|  ____sys_sendmsg+0x877/0xb60 net/socket.c:2573\n|  ___sys_sendmsg+0x28d/0x3c0 net/socket.c:2627\n|  __sys_sendmsg net/socket.c:2659 [inline]\n|  __do_sys_sendmsg net/socket.c:2664 [inline]\n|  __se_sys_sendmsg net/socket.c:2662 [inline]\n|  __x64_sys_sendmsg+0x212/0x3c0 net/socket.c:2662\n|  x64_sys_call+0x2ed6/0x3c30 arch/x86/include/generated/asm/syscalls_64.h:47\n|  do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n|  do_syscall_64+0xcd/0x1e0 arch/x86/entry/common.c:83\n| entry_SYSCALL_64_after_hwframe+0x77/0x7f\n|\n|Fixes: 3d249d4ca7d0 (\""net: introduce ethernet teaming device\"")\n|Reported-by: syzbot+1fcd957a82e3a1baa94d@syzkaller.appspotmail.com\n|Closes: https://syzkaller.appspot.com/bug?extid=1fcd957a82e3a1baa94d\n|Signed-off-by: Eric Dumazet <edumazet@google.com>\n|Reviewed-by: Jiri Pirko <jiri@nvidia.com>\n|Link: https://patch.msgid.link/20250212134928.1541609-1-edumazet@google.com\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| drivers/net/team/team_core.c | 4 +++-\n| 1 file changed, 3 insertions(+), 1 deletion(-)\n|\n|diff --git a/drivers/net/team/team_core.c b/drivers/net/team/team_core.c\n|index dc7cbd6a9798..f4019815f473 100644\n|--- a/drivers/net/team/team_core.c\n|+++ b/drivers/net/team/team_core.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,2,71,1,DISJOINT,COMMENTS
CVE-2025-21800,be482f1d10da781db9445d2753c1e3f1fd82babf,"From be482f1d10da781db9445d2753c1e3f1fd82babf Mon Sep 17 00:00:00 2001
From: Yevgeny Kliteynik <kliteyn@nvidia.com>
Date: Thu, 2 Jan 2025 20:14:10 +0200
Subject: [PATCH] net/mlx5: HWS, fix definer's HWS_SET32 macro for negative
 offset

When bit offset for HWS_SET32 macro is negative,
UBSAN complains about the shift-out-of-bounds:

  UBSAN: shift-out-of-bounds in
  drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c:177:2
  shift exponent -8 is negative

Fixes: 74a778b4a63f (""net/mlx5: HWS, added definers handling"")
Signed-off-by: Yevgeny Kliteynik <kliteyn@nvidia.com>
Reviewed-by: Erez Shitrit <erezsh@nvidia.com>
Reviewed-by: Mark Bloch <mbloch@nvidia.com>
Signed-off-by: Tariq Toukan <tariqt@nvidia.com>
Link: https://patch.msgid.link/20250102181415.1477316-12-tariqt@nvidia.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c
index 8fe96eb76baf..10ece7df1cfa 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c
@@ -70,7 +70,7 @@
 			u32 second_dw_mask = (mask) & ((1 << _bit_off) - 1); \
 			_HWS_SET32(p, (v) >> _bit_off, byte_off, 0, (mask) >> _bit_off); \
 			_HWS_SET32(p, (v) & second_dw_mask, (byte_off) + DW_SIZE, \
-				    (bit_off) % BITS_IN_DW, second_dw_mask); \
+				    (bit_off + BITS_IN_DW) % BITS_IN_DW, second_dw_mask); \
 		} else { \
 			_HWS_SET32(p, v, byte_off, (bit_off), (mask)); \
 		} \
-- 
2.39.5 (Apple Git-154)

",92cff996624c4757d5bbace3dfa3f1567ba94143,"From 92cff996624c4757d5bbace3dfa3f1567ba94143 Mon Sep 17 00:00:00 2001
From: Yevgeny Kliteynik <kliteyn@nvidia.com>
Date: Thu, 2 Jan 2025 20:14:10 +0200
Subject: [PATCH] net/mlx5: HWS, fix definer's HWS_SET32 macro for negative
 offset

[ Upstream commit be482f1d10da781db9445d2753c1e3f1fd82babf ]

When bit offset for HWS_SET32 macro is negative,
UBSAN complains about the shift-out-of-bounds:

  UBSAN: shift-out-of-bounds in
  drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c:177:2
  shift exponent -8 is negative

Fixes: 74a778b4a63f (""net/mlx5: HWS, added definers handling"")
Signed-off-by: Yevgeny Kliteynik <kliteyn@nvidia.com>
Reviewed-by: Erez Shitrit <erezsh@nvidia.com>
Reviewed-by: Mark Bloch <mbloch@nvidia.com>
Signed-off-by: Tariq Toukan <tariqt@nvidia.com>
Link: https://patch.msgid.link/20250102181415.1477316-12-tariqt@nvidia.com
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 .../ethernet/mellanox/mlx5/core/steering/hws/mlx5hws_definer.c  | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/mlx5hws_definer.c b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/mlx5hws_definer.c
index 3f4c58bada37..ab5f8f07f1f7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/mlx5hws_definer.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/mlx5hws_definer.c
@@ -70,7 +70,7 @@
 			u32 second_dw_mask = (mask) & ((1 << _bit_off) - 1); \
 			_HWS_SET32(p, (v) >> _bit_off, byte_off, 0, (mask) >> _bit_off); \
 			_HWS_SET32(p, (v) & second_dw_mask, (byte_off) + DW_SIZE, \
-				    (bit_off) % BITS_IN_DW, second_dw_mask); \
+				    (bit_off + BITS_IN_DW) % BITS_IN_DW, second_dw_mask); \
 		} else { \
 			_HWS_SET32(p, v, byte_off, (bit_off), (mask)); \
 		} \
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 29\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From be482f1d10da781db9445d2753c1e3f1fd82babf Mon Sep 17 00:00:00 2001\n|From: Yevgeny Kliteynik <kliteyn@nvidia.com>\n|Date: Thu, 2 Jan 2025 20:14:10 +0200\n|Subject: [PATCH] net/mlx5: HWS, fix definer's HWS_SET32 macro for negative\n| offset\n|\n|When bit offset for HWS_SET32 macro is negative,\n|UBSAN complains about the shift-out-of-bounds:\n|\n|  UBSAN: shift-out-of-bounds in\n|  drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c:177:2\n|  shift exponent -8 is negative\n|\n|Fixes: 74a778b4a63f (\""net/mlx5: HWS, added definers handling\"")\n|Signed-off-by: Yevgeny Kliteynik <kliteyn@nvidia.com>\n|Reviewed-by: Erez Shitrit <erezsh@nvidia.com>\n|Reviewed-by: Mark Bloch <mbloch@nvidia.com>\n|Signed-off-by: Tariq Toukan <tariqt@nvidia.com>\n|Link: https://patch.msgid.link/20250102181415.1477316-12-tariqt@nvidia.com\n|Signed-off-by: Jakub Kicinski <kuba@kernel.org>\n|---\n| drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c | 2 +-\n| 1 file changed, 1 insertion(+), 1 deletion(-)\n|\n|diff --git a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c\n|index 8fe96eb76baf..10ece7df1cfa 100644\n|--- a/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c\n|+++ b/drivers/net/ethernet/mellanox/mlx5/core/steering/hws/definer.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",3,0,0,40,1,OTHER,DELETE
CVE-2025-21811,367a9bffabe08c04f6d725032cce3d891b2b9e1a,"From 367a9bffabe08c04f6d725032cce3d891b2b9e1a Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:47 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 587251830897..58a598b548fa 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -734,7 +734,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                 if (!head)
                         head = create_empty_buffers(folio,
                                         i_blocksize(inode), 0);
-                folio_unlock(folio);
 
                 bh = head;
                 do {
@@ -744,11 +743,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                         list_add_tail(&bh->b_assoc_buffers, listp);
                         ndirties++;
                         if (unlikely(ndirties >= nlimit)) {
+                                folio_unlock(folio);
                                 folio_batch_release(&fbatch);
                                 cond_resched();
                                 return ndirties;
                         }
                 } while (bh = bh->b_this_page, bh != head);
+
+                folio_unlock(folio);
         }
         folio_batch_release(&fbatch);
         cond_resched();
-- 
2.39.5 (Apple Git-154)

",58c27fa7a610b6e8d44e6220e7dbddfbaccaf439,"From 58c27fa7a610b6e8d44e6220e7dbddfbaccaf439 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 03:14:32 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

commit 367a9bffabe08c04f6d725032cce3d891b2b9e1a upstream.

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 6bc8ad0d41f8..af4ab80443fe 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -732,7 +732,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                 }
                 if (!page_has_buffers(page))
                         create_empty_buffers(page, i_blocksize(inode), 0);
-                unlock_page(page);
 
                 bh = head = page_buffers(page);
                 do {
@@ -742,11 +741,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                         list_add_tail(&bh->b_assoc_buffers, listp);
                         ndirties++;
                         if (unlikely(ndirties >= nlimit)) {
+                                unlock_page(page);
                                 pagevec_release(&pvec);
                                 cond_resched();
                                 return ndirties;
                         }
                 } while (bh = bh->b_this_page, bh != head);
+
+                unlock_page(page);
         }
         pagevec_release(&pvec);
         cond_resched();
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/segment.c\nHunk #1 FAILED at 734.\nHunk #2 FAILED at 744.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/segment.c.rej,5,0,2,56,1,COMMENTS,COMMENTS
CVE-2025-21811,367a9bffabe08c04f6d725032cce3d891b2b9e1a,"From 367a9bffabe08c04f6d725032cce3d891b2b9e1a Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:47 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 587251830897..58a598b548fa 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -734,7 +734,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                 if (!head)
                         head = create_empty_buffers(folio,
                                         i_blocksize(inode), 0);
-                folio_unlock(folio);
 
                 bh = head;
                 do {
@@ -744,11 +743,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                         list_add_tail(&bh->b_assoc_buffers, listp);
                         ndirties++;
                         if (unlikely(ndirties >= nlimit)) {
+                                folio_unlock(folio);
                                 folio_batch_release(&fbatch);
                                 cond_resched();
                                 return ndirties;
                         }
                 } while (bh = bh->b_this_page, bh != head);
+
+                folio_unlock(folio);
         }
         folio_batch_release(&fbatch);
         cond_resched();
-- 
2.39.5 (Apple Git-154)

",e1fc4a90a90ea8514246c45435662531975937d9,"From e1fc4a90a90ea8514246c45435662531975937d9 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 23:23:49 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

commit 367a9bffabe08c04f6d725032cce3d891b2b9e1a upstream.

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 9a5dd4106c3d..ad28737122ca 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -732,7 +732,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 		}
 		if (!page_has_buffers(page))
 			create_empty_buffers(page, i_blocksize(inode), 0);
-		unlock_page(page);
 
 		bh = head = page_buffers(page);
 		do {
@@ -742,11 +741,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 			list_add_tail(&bh->b_assoc_buffers, listp);
 			ndirties++;
 			if (unlikely(ndirties >= nlimit)) {
+				unlock_page(page);
 				pagevec_release(&pvec);
 				cond_resched();
 				return ndirties;
 			}
 		} while (bh = bh->b_this_page, bh != head);
+
+		unlock_page(page);
 	}
 	pagevec_release(&pvec);
 	cond_resched();
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/segment.c\nHunk #1 FAILED at 734.\nHunk #2 FAILED at 744.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/segment.c.rej,5,0,2,56,1,COMMENTS,COMMENTS
CVE-2025-21811,367a9bffabe08c04f6d725032cce3d891b2b9e1a,"From 367a9bffabe08c04f6d725032cce3d891b2b9e1a Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:47 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 587251830897..58a598b548fa 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -734,7 +734,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 		if (!head)
 			head = create_empty_buffers(folio,
 					i_blocksize(inode), 0);
-		folio_unlock(folio);
 
 		bh = head;
 		do {
@@ -744,11 +743,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 			list_add_tail(&bh->b_assoc_buffers, listp);
 			ndirties++;
 			if (unlikely(ndirties >= nlimit)) {
+				folio_unlock(folio);
 				folio_batch_release(&fbatch);
 				cond_resched();
 				return ndirties;
 			}
 		} while (bh = bh->b_this_page, bh != head);
+
+		folio_unlock(folio);
 	}
 	folio_batch_release(&fbatch);
 	cond_resched();
-- 
2.39.5 (Apple Git-154)

",72cf688d0ce7e642b12ddc9b2a42524737ec1b4a,"From 72cf688d0ce7e642b12ddc9b2a42524737ec1b4a Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 23:23:49 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

commit 367a9bffabe08c04f6d725032cce3d891b2b9e1a upstream.

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 2213011afab7..5ac4ff2a065e 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -732,7 +732,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                 }
                 if (!page_has_buffers(page))
                         create_empty_buffers(page, i_blocksize(inode), 0);
-                unlock_page(page);
 
                 bh = head = page_buffers(page);
                 do {
@@ -742,11 +741,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
                         list_add_tail(&bh->b_assoc_buffers, listp);
                         ndirties++;
                         if (unlikely(ndirties >= nlimit)) {
+                                unlock_page(page);
                                 pagevec_release(&pvec);
                                 cond_resched();
                                 return ndirties;
                         }
                 } while (bh = bh->b_this_page, bh != head);
+
+                unlock_page(page);
         }
         pagevec_release(&pvec);
         cond_resched();
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/segment.c\nHunk #1 FAILED at 734.\nHunk #2 FAILED at 744.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/segment.c.rej,5,0,2,56,1,OTHER,COMMENTS
CVE-2025-21811,367a9bffabe08c04f6d725032cce3d891b2b9e1a,"From 367a9bffabe08c04f6d725032cce3d891b2b9e1a Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Wed, 8 Jan 2025 05:00:47 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 587251830897..58a598b548fa 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -734,7 +734,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 		if (!head)
 			head = create_empty_buffers(folio,
 					i_blocksize(inode), 0);
-		folio_unlock(folio);
 
 		bh = head;
 		do {
@@ -744,11 +743,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 			list_add_tail(&bh->b_assoc_buffers, listp);
 			ndirties++;
 			if (unlikely(ndirties >= nlimit)) {
+				folio_unlock(folio);
 				folio_batch_release(&fbatch);
 				cond_resched();
 				return ndirties;
 			}
 		} while (bh = bh->b_this_page, bh != head);
+
+		folio_unlock(folio);
 	}
 	folio_batch_release(&fbatch);
 	cond_resched();
-- 
2.39.5 (Apple Git-154)

",d8ff250e085a4c4cdda4ad1cdd234ed110393143,"From d8ff250e085a4c4cdda4ad1cdd234ed110393143 Mon Sep 17 00:00:00 2001
From: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Date: Fri, 7 Feb 2025 23:23:49 +0900
Subject: [PATCH] nilfs2: protect access to buffers with no active references

commit 367a9bffabe08c04f6d725032cce3d891b2b9e1a upstream.

nilfs_lookup_dirty_data_buffers(), which iterates through the buffers
attached to dirty data folios/pages, accesses the attached buffers without
locking the folios/pages.

For data cache, nilfs_clear_folio_dirty() may be called asynchronously
when the file system degenerates to read only, so
nilfs_lookup_dirty_data_buffers() still has the potential to cause use
after free issues when buffers lose the protection of their dirty state
midway due to this asynchronous clearing and are unintentionally freed by
try_to_free_buffers().

Eliminate this race issue by adjusting the lock section in this function.

[konishi.ryusuke@gmail.com: adjusted for page/folio conversion]
Link: https://lkml.kernel.org/r/20250107200202.6432-3-konishi.ryusuke@gmail.com
Signed-off-by: Ryusuke Konishi <konishi.ryusuke@gmail.com>
Fixes: 8c26c4e2694a (""nilfs2: fix issue with flush kernel thread after remount in RO mode because of driver's internal error or metadata corruption"")
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 fs/nilfs2/segment.c | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/fs/nilfs2/segment.c b/fs/nilfs2/segment.c
index 75fd6e86f18a..2f778234ecf2 100644
--- a/fs/nilfs2/segment.c
+++ b/fs/nilfs2/segment.c
@@ -732,7 +732,6 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 		}
 		if (!page_has_buffers(page))
 			create_empty_buffers(page, i_blocksize(inode), 0);
-		unlock_page(page);
 
 		bh = head = page_buffers(page);
 		do {
@@ -742,11 +741,14 @@ static size_t nilfs_lookup_dirty_data_buffers(struct inode *inode,
 			list_add_tail(&bh->b_assoc_buffers, listp);
 			ndirties++;
 			if (unlikely(ndirties >= nlimit)) {
+				unlock_page(page);
 				pagevec_release(&pvec);
 				cond_resched();
 				return ndirties;
 			}
 		} while (bh = bh->b_this_page, bh != head);
+
+		unlock_page(page);
 	}
 	pagevec_release(&pvec);
 	cond_resched();
-- 
2.39.5 (Apple Git-154)

",patching file fs/nilfs2/segment.c\nHunk #1 FAILED at 734.\nHunk #2 FAILED at 744.\n2 out of 2 hunks FAILED -- saving rejects to file fs/nilfs2/segment.c.rej,5,0,2,56,1,SEMANTIC,COMMENTS
CVE-2025-21820,b06f388994500297bb91be60ffaf6825ecfd2afe,"From b06f388994500297bb91be60ffaf6825ecfd2afe Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 16:38:22 -0500
Subject: [PATCH] tty: xilinx_uartps: split sysrq handling

lockdep detects the following circular locking dependency:

CPU 0                      CPU 1
========================== ============================
cdns_uart_isr()            printk()
  uart_port_lock(port)       console_lock()
			     cdns_uart_console_write()
                               if (!port->sysrq)
                                 uart_port_lock(port)
  uart_handle_break()
    port->sysrq = ...
  uart_handle_sysrq_char()
    printk()
      console_lock()

The fixed commit attempts to avoid this situation by only taking the
port lock in cdns_uart_console_write if port->sysrq unset. However, if
(as shown above) cdns_uart_console_write runs before port->sysrq is set,
then it will try to take the port lock anyway. This may result in a
deadlock.

Fix this by splitting sysrq handling into two parts. We use the prepare
helper under the port lock and defer handling until we release the lock.

Fixes: 74ea66d4ca06 (""tty: xuartps: Improve sysrq handling"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org # c980248179d: serial: xilinx_uartps: Use port lock wrappers
Acked-by: John Ogness <john.ogness@linutronix.de>
Link: https://lore.kernel.org/r/20250110213822.2107462-1-sean.anderson@linux.dev
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/serial/xilinx_uartps.c | 8 +++-----
 1 file changed, 3 insertions(+), 5 deletions(-)

diff --git a/drivers/tty/serial/xilinx_uartps.c b/drivers/tty/serial/xilinx_uartps.c
index beb151be4d32..92ec51870d1d 100644
--- a/drivers/tty/serial/xilinx_uartps.c
+++ b/drivers/tty/serial/xilinx_uartps.c
@@ -287,7 +287,7 @@ static void cdns_uart_handle_rx(void *dev_id, unsigned int isrstatus)
 				continue;
 		}
 
-		if (uart_handle_sysrq_char(port, data))
+		if (uart_prepare_sysrq_char(port, data))
 			continue;
 
 		if (is_rxbs_support) {
@@ -495,7 +495,7 @@ static irqreturn_t cdns_uart_isr(int irq, void *dev_id)
 	    !(readl(port->membase + CDNS_UART_CR) & CDNS_UART_CR_RX_DIS))
 		cdns_uart_handle_rx(dev_id, isrstatus);
 
-	uart_port_unlock(port);
+	uart_unlock_and_check_sysrq(port);
 	return IRQ_HANDLED;
 }
 
@@ -1380,9 +1380,7 @@ static void cdns_uart_console_write(struct console *co, const char *s,
 	unsigned int imr, ctrl;
 	int locked = 1;
 
-	if (port->sysrq)
-		locked = 0;
-	else if (oops_in_progress)
+	if (oops_in_progress)
 		locked = uart_port_trylock_irqsave(port, &flags);
 	else
 		uart_port_lock_irqsave(port, &flags);
-- 
2.39.5 (Apple Git-154)

",8ea0e7b3d7b8f2f0fc9db491ff22a0abe120801c,"From 8ea0e7b3d7b8f2f0fc9db491ff22a0abe120801c Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 16:38:22 -0500
Subject: [PATCH] tty: xilinx_uartps: split sysrq handling

commit b06f388994500297bb91be60ffaf6825ecfd2afe upstream.

lockdep detects the following circular locking dependency:

CPU 0                      CPU 1
========================== ============================
cdns_uart_isr()            printk()
  uart_port_lock(port)       console_lock()
			     cdns_uart_console_write()
                               if (!port->sysrq)
                                 uart_port_lock(port)
  uart_handle_break()
    port->sysrq = ...
  uart_handle_sysrq_char()
    printk()
      console_lock()

The fixed commit attempts to avoid this situation by only taking the
port lock in cdns_uart_console_write if port->sysrq unset. However, if
(as shown above) cdns_uart_console_write runs before port->sysrq is set,
then it will try to take the port lock anyway. This may result in a
deadlock.

Fix this by splitting sysrq handling into two parts. We use the prepare
helper under the port lock and defer handling until we release the lock.

Fixes: 74ea66d4ca06 (""tty: xuartps: Improve sysrq handling"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org # c980248179d: serial: xilinx_uartps: Use port lock wrappers
Acked-by: John Ogness <john.ogness@linutronix.de>
Link: https://lore.kernel.org/r/20250110213822.2107462-1-sean.anderson@linux.dev
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/serial/xilinx_uartps.c | 10 ++++------
 1 file changed, 4 insertions(+), 6 deletions(-)

diff --git a/drivers/tty/serial/xilinx_uartps.c b/drivers/tty/serial/xilinx_uartps.c
index 2e5e86a00a77..7f83d2780017 100644
--- a/drivers/tty/serial/xilinx_uartps.c
+++ b/drivers/tty/serial/xilinx_uartps.c
@@ -268,7 +268,7 @@ static void cdns_uart_handle_rx(void *dev_id, unsigned int isrstatus)
 				continue;
 		}
 
-		if (uart_handle_sysrq_char(port, data))
+		if (uart_prepare_sysrq_char(port, data))
 			continue;
 
 		if (is_rxbs_support) {
@@ -369,7 +369,7 @@ static irqreturn_t cdns_uart_isr(int irq, void *dev_id)
 	    !(readl(port->membase + CDNS_UART_CR) & CDNS_UART_CR_RX_DIS))
 		cdns_uart_handle_rx(dev_id, isrstatus);
 
-	spin_unlock(&port->lock);
+	uart_unlock_and_check_sysrq(port);
 	return IRQ_HANDLED;
 }
 
@@ -1229,10 +1229,8 @@ static void cdns_uart_console_write(struct console *co, const char *s,
 	unsigned int imr, ctrl;
 	int locked = 1;
 
-	if (port->sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock_irqsave(&port->lock, flags);
+	if (oops_in_progress)
+		locked = uart_port_trylock_irqsave(port, &flags);
 	else
 		spin_lock_irqsave(&port->lock, flags);
 
-- 
2.39.5 (Apple Git-154)

",patching file drivers/tty/serial/xilinx_uartps.c\nHunk #1 succeeded at 268 (offset -19 lines).\nHunk #2 FAILED at 495.\nHunk #3 FAILED at 1380.\n2 out of 3 hunks FAILED -- saving rejects to file drivers/tty/serial/xilinx_uartps.c.rej,9,0,1,75,1,OTHER,COMMENTS
CVE-2025-21820,b06f388994500297bb91be60ffaf6825ecfd2afe,"From b06f388994500297bb91be60ffaf6825ecfd2afe Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 16:38:22 -0500
Subject: [PATCH] tty: xilinx_uartps: split sysrq handling

lockdep detects the following circular locking dependency:

CPU 0                      CPU 1
========================== ============================
cdns_uart_isr()            printk()
  uart_port_lock(port)       console_lock()
			     cdns_uart_console_write()
                               if (!port->sysrq)
                                 uart_port_lock(port)
  uart_handle_break()
    port->sysrq = ...
  uart_handle_sysrq_char()
    printk()
      console_lock()

The fixed commit attempts to avoid this situation by only taking the
port lock in cdns_uart_console_write if port->sysrq unset. However, if
(as shown above) cdns_uart_console_write runs before port->sysrq is set,
then it will try to take the port lock anyway. This may result in a
deadlock.

Fix this by splitting sysrq handling into two parts. We use the prepare
helper under the port lock and defer handling until we release the lock.

Fixes: 74ea66d4ca06 (""tty: xuartps: Improve sysrq handling"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org # c980248179d: serial: xilinx_uartps: Use port lock wrappers
Acked-by: John Ogness <john.ogness@linutronix.de>
Link: https://lore.kernel.org/r/20250110213822.2107462-1-sean.anderson@linux.dev
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/serial/xilinx_uartps.c | 8 +++-----
 1 file changed, 3 insertions(+), 5 deletions(-)

diff --git a/drivers/tty/serial/xilinx_uartps.c b/drivers/tty/serial/xilinx_uartps.c
index beb151be4d32..92ec51870d1d 100644
--- a/drivers/tty/serial/xilinx_uartps.c
+++ b/drivers/tty/serial/xilinx_uartps.c
@@ -287,7 +287,7 @@ static void cdns_uart_handle_rx(void *dev_id, unsigned int isrstatus)
 				continue;
 		}
 
-		if (uart_handle_sysrq_char(port, data))
+		if (uart_prepare_sysrq_char(port, data))
 			continue;
 
 		if (is_rxbs_support) {
@@ -495,7 +495,7 @@ static irqreturn_t cdns_uart_isr(int irq, void *dev_id)
 	    !(readl(port->membase + CDNS_UART_CR) & CDNS_UART_CR_RX_DIS))
 		cdns_uart_handle_rx(dev_id, isrstatus);
 
-	uart_port_unlock(port);
+	uart_unlock_and_check_sysrq(port);
 	return IRQ_HANDLED;
 }
 
@@ -1380,9 +1380,7 @@ static void cdns_uart_console_write(struct console *co, const char *s,
 	unsigned int imr, ctrl;
 	int locked = 1;
 
-	if (port->sysrq)
-		locked = 0;
-	else if (oops_in_progress)
+	if (oops_in_progress)
 		locked = uart_port_trylock_irqsave(port, &flags);
 	else
 		uart_port_lock_irqsave(port, &flags);
-- 
2.39.5 (Apple Git-154)

",de5bd24197bd9ee37ec1e379a3d882bbd15c5065,"From de5bd24197bd9ee37ec1e379a3d882bbd15c5065 Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 16:38:22 -0500
Subject: [PATCH] tty: xilinx_uartps: split sysrq handling

commit b06f388994500297bb91be60ffaf6825ecfd2afe upstream.

lockdep detects the following circular locking dependency:

CPU 0                      CPU 1
========================== ============================
cdns_uart_isr()            printk()
  uart_port_lock(port)       console_lock()
			     cdns_uart_console_write()
                               if (!port->sysrq)
                                 uart_port_lock(port)
  uart_handle_break()
    port->sysrq = ...
  uart_handle_sysrq_char()
    printk()
      console_lock()

The fixed commit attempts to avoid this situation by only taking the
port lock in cdns_uart_console_write if port->sysrq unset. However, if
(as shown above) cdns_uart_console_write runs before port->sysrq is set,
then it will try to take the port lock anyway. This may result in a
deadlock.

Fix this by splitting sysrq handling into two parts. We use the prepare
helper under the port lock and defer handling until we release the lock.

Fixes: 74ea66d4ca06 (""tty: xuartps: Improve sysrq handling"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org # c980248179d: serial: xilinx_uartps: Use port lock wrappers
Acked-by: John Ogness <john.ogness@linutronix.de>
Link: https://lore.kernel.org/r/20250110213822.2107462-1-sean.anderson@linux.dev
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/serial/xilinx_uartps.c | 10 ++++------
 1 file changed, 4 insertions(+), 6 deletions(-)

diff --git a/drivers/tty/serial/xilinx_uartps.c b/drivers/tty/serial/xilinx_uartps.c
index 2eff7cff57c4..29afcc6d9bb7 100644
--- a/drivers/tty/serial/xilinx_uartps.c
+++ b/drivers/tty/serial/xilinx_uartps.c
@@ -268,7 +268,7 @@ static void cdns_uart_handle_rx(void *dev_id, unsigned int isrstatus)
 				continue;
 		}
 
-		if (uart_handle_sysrq_char(port, data))
+		if (uart_prepare_sysrq_char(port, data))
 			continue;
 
 		if (is_rxbs_support) {
@@ -371,7 +371,7 @@ static irqreturn_t cdns_uart_isr(int irq, void *dev_id)
 	    !(readl(port->membase + CDNS_UART_CR) & CDNS_UART_CR_RX_DIS))
 		cdns_uart_handle_rx(dev_id, isrstatus);
 
-	spin_unlock(&port->lock);
+	uart_unlock_and_check_sysrq(port);
 	return IRQ_HANDLED;
 }
 
@@ -1231,10 +1231,8 @@ static void cdns_uart_console_write(struct console *co, const char *s,
 	unsigned int imr, ctrl;
 	int locked = 1;
 
-	if (port->sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock_irqsave(&port->lock, flags);
+	if (oops_in_progress)
+		locked = uart_port_trylock_irqsave(port, &flags);
 	else
 		spin_lock_irqsave(&port->lock, flags);
 
-- 
2.39.5 (Apple Git-154)

",patching file drivers/tty/serial/xilinx_uartps.c\nHunk #1 succeeded at 268 (offset -19 lines).\nHunk #2 FAILED at 495.\nHunk #3 FAILED at 1380.\n2 out of 3 hunks FAILED -- saving rejects to file drivers/tty/serial/xilinx_uartps.c.rej,9,0,1,75,1,DISJOINT,COMMENTS
CVE-2025-21820,b06f388994500297bb91be60ffaf6825ecfd2afe,"From b06f388994500297bb91be60ffaf6825ecfd2afe Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 16:38:22 -0500
Subject: [PATCH] tty: xilinx_uartps: split sysrq handling

lockdep detects the following circular locking dependency:

CPU 0                      CPU 1
========================== ============================
cdns_uart_isr()            printk()
  uart_port_lock(port)       console_lock()
			     cdns_uart_console_write()
                               if (!port->sysrq)
                                 uart_port_lock(port)
  uart_handle_break()
    port->sysrq = ...
  uart_handle_sysrq_char()
    printk()
      console_lock()

The fixed commit attempts to avoid this situation by only taking the
port lock in cdns_uart_console_write if port->sysrq unset. However, if
(as shown above) cdns_uart_console_write runs before port->sysrq is set,
then it will try to take the port lock anyway. This may result in a
deadlock.

Fix this by splitting sysrq handling into two parts. We use the prepare
helper under the port lock and defer handling until we release the lock.

Fixes: 74ea66d4ca06 (""tty: xuartps: Improve sysrq handling"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org # c980248179d: serial: xilinx_uartps: Use port lock wrappers
Acked-by: John Ogness <john.ogness@linutronix.de>
Link: https://lore.kernel.org/r/20250110213822.2107462-1-sean.anderson@linux.dev
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/serial/xilinx_uartps.c | 8 +++-----
 1 file changed, 3 insertions(+), 5 deletions(-)

diff --git a/drivers/tty/serial/xilinx_uartps.c b/drivers/tty/serial/xilinx_uartps.c
index beb151be4d32..92ec51870d1d 100644
--- a/drivers/tty/serial/xilinx_uartps.c
+++ b/drivers/tty/serial/xilinx_uartps.c
@@ -287,7 +287,7 @@ static void cdns_uart_handle_rx(void *dev_id, unsigned int isrstatus)
 				continue;
 		}
 
-		if (uart_handle_sysrq_char(port, data))
+		if (uart_prepare_sysrq_char(port, data))
 			continue;
 
 		if (is_rxbs_support) {
@@ -495,7 +495,7 @@ static irqreturn_t cdns_uart_isr(int irq, void *dev_id)
 	    !(readl(port->membase + CDNS_UART_CR) & CDNS_UART_CR_RX_DIS))
 		cdns_uart_handle_rx(dev_id, isrstatus);
 
-	uart_port_unlock(port);
+	uart_unlock_and_check_sysrq(port);
 	return IRQ_HANDLED;
 }
 
@@ -1380,9 +1380,7 @@ static void cdns_uart_console_write(struct console *co, const char *s,
 	unsigned int imr, ctrl;
 	int locked = 1;
 
-	if (port->sysrq)
-		locked = 0;
-	else if (oops_in_progress)
+	if (oops_in_progress)
 		locked = uart_port_trylock_irqsave(port, &flags);
 	else
 		uart_port_lock_irqsave(port, &flags);
-- 
2.39.5 (Apple Git-154)

",e22a97700901ba5e8bf8db68056a0d50f9440cae,"From e22a97700901ba5e8bf8db68056a0d50f9440cae Mon Sep 17 00:00:00 2001
From: Sean Anderson <sean.anderson@linux.dev>
Date: Fri, 10 Jan 2025 16:38:22 -0500
Subject: [PATCH] tty: xilinx_uartps: split sysrq handling

commit b06f388994500297bb91be60ffaf6825ecfd2afe upstream.

lockdep detects the following circular locking dependency:

CPU 0                      CPU 1
========================== ============================
cdns_uart_isr()            printk()
  uart_port_lock(port)       console_lock()
			     cdns_uart_console_write()
                               if (!port->sysrq)
                                 uart_port_lock(port)
  uart_handle_break()
    port->sysrq = ...
  uart_handle_sysrq_char()
    printk()
      console_lock()

The fixed commit attempts to avoid this situation by only taking the
port lock in cdns_uart_console_write if port->sysrq unset. However, if
(as shown above) cdns_uart_console_write runs before port->sysrq is set,
then it will try to take the port lock anyway. This may result in a
deadlock.

Fix this by splitting sysrq handling into two parts. We use the prepare
helper under the port lock and defer handling until we release the lock.

Fixes: 74ea66d4ca06 (""tty: xuartps: Improve sysrq handling"")
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Cc: stable@vger.kernel.org # c980248179d: serial: xilinx_uartps: Use port lock wrappers
Acked-by: John Ogness <john.ogness@linutronix.de>
Link: https://lore.kernel.org/r/20250110213822.2107462-1-sean.anderson@linux.dev
Signed-off-by: Sean Anderson <sean.anderson@linux.dev>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/tty/serial/xilinx_uartps.c | 10 ++++------
 1 file changed, 4 insertions(+), 6 deletions(-)

diff --git a/drivers/tty/serial/xilinx_uartps.c b/drivers/tty/serial/xilinx_uartps.c
index 815e3e26ee20..4fb2f7bee91d 100644
--- a/drivers/tty/serial/xilinx_uartps.c
+++ b/drivers/tty/serial/xilinx_uartps.c
@@ -268,7 +268,7 @@ static void cdns_uart_handle_rx(void *dev_id, unsigned int isrstatus)
 				continue;
 		}
 
-		if (uart_handle_sysrq_char(port, data))
+		if (uart_prepare_sysrq_char(port, data))
 			continue;
 
 		if (is_rxbs_support) {
@@ -385,7 +385,7 @@ static irqreturn_t cdns_uart_isr(int irq, void *dev_id)
 	    !(readl(port->membase + CDNS_UART_CR) & CDNS_UART_CR_RX_DIS))
 		cdns_uart_handle_rx(dev_id, isrstatus);
 
-	spin_unlock(&port->lock);
+	uart_unlock_and_check_sysrq(port);
 	return IRQ_HANDLED;
 }
 
@@ -1217,10 +1217,8 @@ static void cdns_uart_console_write(struct console *co, const char *s,
 	unsigned int imr, ctrl;
 	int locked = 1;
 
-	if (port->sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock_irqsave(&port->lock, flags);
+	if (oops_in_progress)
+		locked = uart_port_trylock_irqsave(port, &flags);
 	else
 		spin_lock_irqsave(&port->lock, flags);
 
-- 
2.39.5 (Apple Git-154)

",patching file drivers/tty/serial/xilinx_uartps.c\nHunk #1 succeeded at 268 (offset -19 lines).\nHunk #2 FAILED at 495.\nHunk #3 FAILED at 1380.\n2 out of 3 hunks FAILED -- saving rejects to file drivers/tty/serial/xilinx_uartps.c.rej,9,0,1,75,1,COMMENTS,COMMENTS
CVE-2025-21823,8c8ecc98f5c65947b0070a24bac11e12e47cc65d,"From 8c8ecc98f5c65947b0070a24bac11e12e47cc65d Mon Sep 17 00:00:00 2001
From: Sven Eckelmann <sven@narfation.org>
Date: Mon, 20 Jan 2025 00:06:11 +0100
Subject: [PATCH] batman-adv: Drop unmanaged ELP metric worker

The ELP worker needs to calculate new metric values for all neighbors
""reachable"" over an interface. Some of the used metric sources require
locks which might need to sleep. This sleep is incompatible with the RCU
list iterator used for the recorded neighbors. The initial approach to work
around of this problem was to queue another work item per neighbor and then
run this in a new context.

Even when this solved the RCU vs might_sleep() conflict, it has a major
problems: Nothing was stopping the work item in case it is not needed
anymore - for example because one of the related interfaces was removed or
the batman-adv module was unloaded - resulting in potential invalid memory
accesses.

Directly canceling the metric worker also has various problems:

* cancel_work_sync for a to-be-deactivated interface is called with
  rtnl_lock held. But the code in the ELP metric worker also tries to use
  rtnl_lock() - which will never return in this case. This also means that
  cancel_work_sync would never return because it is waiting for the worker
  to finish.
* iterating over the neighbor list for the to-be-deactivated interface is
  currently done using the RCU specific methods. Which means that it is
  possible to miss items when iterating over it without the associated
  spinlock - a behaviour which is acceptable for a periodic metric check
  but not for a cleanup routine (which must ""stop"" all still running
  workers)

The better approch is to get rid of the per interface neighbor metric
worker and handle everything in the interface worker. The original problems
are solved by:

* creating a list of neighbors which require new metric information inside
  the RCU protected context, gathering the metric according to the new list
  outside the RCU protected context
* only use rcu_trylock inside metric gathering code to avoid a deadlock
  when the cancel_delayed_work_sync is called in the interface removal code
  (which is called with the rtnl_lock held)

Cc: stable@vger.kernel.org
Fixes: c833484e5f38 (""batman-adv: ELP - compute the metric based on the estimated throughput"")
Signed-off-by: Sven Eckelmann <sven@narfation.org>
Signed-off-by: Simon Wunderlich <sw@simonwunderlich.de>
---
 net/batman-adv/bat_v.c     |  2 --
 net/batman-adv/bat_v_elp.c | 71 ++++++++++++++++++++++++++------------
 net/batman-adv/bat_v_elp.h |  2 --
 net/batman-adv/types.h     |  3 --
 4 files changed, 48 insertions(+), 30 deletions(-)

diff --git a/net/batman-adv/bat_v.c b/net/batman-adv/bat_v.c
index ac11f1f08db0..d35479c465e2 100644
--- a/net/batman-adv/bat_v.c
+++ b/net/batman-adv/bat_v.c
@@ -113,8 +113,6 @@ static void
 batadv_v_hardif_neigh_init(struct batadv_hardif_neigh_node *hardif_neigh)
 {
         ewma_throughput_init(&hardif_neigh->bat_v.throughput);
-        INIT_WORK(&hardif_neigh->bat_v.metric_work,
-                  batadv_v_elp_throughput_metric_update);
 }
 
 /**
diff --git a/net/batman-adv/bat_v_elp.c b/net/batman-adv/bat_v_elp.c
index 65e52de52bcd..b065578b4436 100644
--- a/net/batman-adv/bat_v_elp.c
+++ b/net/batman-adv/bat_v_elp.c
@@ -18,6 +18,7 @@
 #include <linux/if_ether.h>
 #include <linux/jiffies.h>
 #include <linux/kref.h>
+#include <linux/list.h>
 #include <linux/minmax.h>
 #include <linux/netdevice.h>
 #include <linux/nl80211.h>
@@ -26,6 +27,7 @@
 #include <linux/rcupdate.h>
 #include <linux/rtnetlink.h>
 #include <linux/skbuff.h>
+#include <linux/slab.h>
 #include <linux/stddef.h>
 #include <linux/string.h>
 #include <linux/types.h>
@@ -41,6 +43,18 @@
 #include ""routing.h""
 #include ""send.h""
 
+/**
+ * struct batadv_v_metric_queue_entry - list of hardif neighbors which require
+ *  and metric update
+ */
+struct batadv_v_metric_queue_entry {
+        /** @hardif_neigh: hardif neighbor scheduled for metric update */
+        struct batadv_hardif_neigh_node *hardif_neigh;
+
+        /** @list: list node for metric_queue */
+        struct list_head list;
+};
+
 /**
  * batadv_v_elp_start_timer() - restart timer for ELP periodic work
  * @hard_iface: the interface for which the timer has to be reset
@@ -137,10 +151,17 @@ static bool batadv_v_elp_get_throughput(struct batadv_hardif_neigh_node *neigh,
                 goto default_throughput;
         }
 
+        /* only use rtnl_trylock because the elp worker will be cancelled while
+         * the rntl_lock is held. the cancel_delayed_work_sync() would otherwise
+         * wait forever when the elp work_item was started and it is then also
+         * trying to rtnl_lock
+         */
+        if (!rtnl_trylock())
+                return false;
+
         /* if not a wifi interface, check if this device provides data via
          * ethtool (e.g. an Ethernet adapter)
          */
-        rtnl_lock();
         ret = __ethtool_get_link_ksettings(hard_iface->net_dev, &link_settings);
         rtnl_unlock();
         if (ret == 0) {
@@ -175,31 +196,19 @@ static bool batadv_v_elp_get_throughput(struct batadv_hardif_neigh_node *neigh,
 /**
  * batadv_v_elp_throughput_metric_update() - worker updating the throughput
  *  metric of a single hop neighbour
- * @work: the work queue item
+ * @neigh: the neighbour to probe
  */
-void batadv_v_elp_throughput_metric_update(struct work_struct *work)
+static void
+batadv_v_elp_throughput_metric_update(struct batadv_hardif_neigh_node *neigh)
 {
-        struct batadv_hardif_neigh_node_bat_v *neigh_bat_v;
-        struct batadv_hardif_neigh_node *neigh;
         u32 throughput;
         bool valid;
 
-        neigh_bat_v = container_of(work, struct batadv_hardif_neigh_node_bat_v,
-                                   metric_work);
-        neigh = container_of(neigh_bat_v, struct batadv_hardif_neigh_node,
-                             bat_v);
-
         valid = batadv_v_elp_get_throughput(neigh, &throughput);
         if (!valid)
-                goto put_neigh;
+                return;
 
         ewma_throughput_add(&neigh->bat_v.throughput, throughput);
-
-put_neigh:
-        /* decrement refcounter to balance increment performed before scheduling
-         * this task
-         */
-        batadv_hardif_neigh_put(neigh);
 }
 
 /**
@@ -273,14 +282,16 @@ batadv_v_elp_wifi_neigh_probe(struct batadv_hardif_neigh_node *neigh)
  */
 static void batadv_v_elp_periodic_work(struct work_struct *work)
 {
+        struct batadv_v_metric_queue_entry *metric_entry;
+        struct batadv_v_metric_queue_entry *metric_safe;
         struct batadv_hardif_neigh_node *hardif_neigh;
         struct batadv_hard_iface *hard_iface;
         struct batadv_hard_iface_bat_v *bat_v;
         struct batadv_elp_packet *elp_packet;
+        struct list_head metric_queue;
         struct batadv_priv *bat_priv;
         struct sk_buff *skb;
         u32 elp_interval;
-        bool ret;
 
         bat_v = container_of(work, struct batadv_hard_iface_bat_v, elp_wq.work);
         hard_iface = container_of(bat_v, struct batadv_hard_iface, bat_v);
@@ -316,6 +327,8 @@ static void batadv_v_elp_periodic_work(struct work_struct *work)
 
         atomic_inc(&hard_iface->bat_v.elp_seqno);
 
+        INIT_LIST_HEAD(&metric_queue);
+
         /* The throughput metric is updated on each sent packet. This way, if a
          * node is dead and no longer sends packets, batman-adv is still able to
          * react timely to its death.
@@ -340,16 +353,28 @@ static void batadv_v_elp_periodic_work(struct work_struct *work)
 
                 /* Reading the estimated throughput from cfg80211 is a task that
                  * may sleep and that is not allowed in an rcu protected
-                 * context. Therefore schedule a task for that.
+                 * context. Therefore add it to metric_queue and process it
+                 * outside rcu protected context.
                  */
-                ret = queue_work(batadv_event_workqueue,
-                                 &hardif_neigh->bat_v.metric_work);
-
-                if (!ret)
+                metric_entry = kzalloc(sizeof(*metric_entry), GFP_ATOMIC);
+                if (!metric_entry) {
                         batadv_hardif_neigh_put(hardif_neigh);
+                        continue;
+                }
+
+                metric_entry->hardif_neigh = hardif_neigh;
+                list_add(&metric_entry->list, &metric_queue);
         }
         rcu_read_unlock();
 
+        list_for_each_entry_safe(metric_entry, metric_safe, &metric_queue, list) {
+                batadv_v_elp_throughput_metric_update(metric_entry->hardif_neigh);
+
+                batadv_hardif_neigh_put(metric_entry->hardif_neigh);
+                list_del(&metric_entry->list);
+                kfree(metric_entry);
+        }
+
 restart_timer:
         batadv_v_elp_start_timer(hard_iface);
 out:
diff --git a/net/batman-adv/bat_v_elp.h b/net/batman-adv/bat_v_elp.h
index 9e2740195fa2..c9cb0a307100 100644
--- a/net/batman-adv/bat_v_elp.h
+++ b/net/batman-adv/bat_v_elp.h
@@ -10,7 +10,6 @@
 #include ""main.h""
 
 #include <linux/skbuff.h>
-#include <linux/workqueue.h>
 
 int batadv_v_elp_iface_enable(struct batadv_hard_iface *hard_iface);
 void batadv_v_elp_iface_disable(struct batadv_hard_iface *hard_iface);
@@ -19,6 +18,5 @@ void batadv_v_elp_iface_activate(struct batadv_hard_iface *primary_iface,
 void batadv_v_elp_primary_iface_set(struct batadv_hard_iface *primary_iface);
 int batadv_v_elp_packet_recv(struct sk_buff *skb,
                              struct batadv_hard_iface *if_incoming);
-void batadv_v_elp_throughput_metric_update(struct work_struct *work);
 
 #endif /* _NET_BATMAN_ADV_BAT_V_ELP_H_ */
diff --git a/net/batman-adv/types.h b/net/batman-adv/types.h
index 04f6398b3a40..85a50096f5b2 100644
--- a/net/batman-adv/types.h
+++ b/net/batman-adv/types.h
@@ -596,9 +596,6 @@ struct batadv_hardif_neigh_node_bat_v {
          *  neighbor
          */
         unsigned long last_unicast_tx;
-
-        /** @metric_work: work queue callback item for metric update */
-        struct work_struct metric_work;
 };
 
 /**
-- 
2.39.5 (Apple Git-154)

",1c334629176c2d644befc31a20d4bf75542f7631,"From 1c334629176c2d644befc31a20d4bf75542f7631 Mon Sep 17 00:00:00 2001
From: Sven Eckelmann <sven@narfation.org>
Date: Mon, 20 Jan 2025 00:06:11 +0100
Subject: [PATCH] batman-adv: Drop unmanaged ELP metric worker

commit 8c8ecc98f5c65947b0070a24bac11e12e47cc65d upstream.

The ELP worker needs to calculate new metric values for all neighbors
""reachable"" over an interface. Some of the used metric sources require
locks which might need to sleep. This sleep is incompatible with the RCU
list iterator used for the recorded neighbors. The initial approach to work
around of this problem was to queue another work item per neighbor and then
run this in a new context.

Even when this solved the RCU vs might_sleep() conflict, it has a major
problems: Nothing was stopping the work item in case it is not needed
anymore - for example because one of the related interfaces was removed or
the batman-adv module was unloaded - resulting in potential invalid memory
accesses.

Directly canceling the metric worker also has various problems:

* cancel_work_sync for a to-be-deactivated interface is called with
  rtnl_lock held. But the code in the ELP metric worker also tries to use
  rtnl_lock() - which will never return in this case. This also means that
  cancel_work_sync would never return because it is waiting for the worker
  to finish.
* iterating over the neighbor list for the to-be-deactivated interface is
  currently done using the RCU specific methods. Which means that it is
  possible to miss items when iterating over it without the associated
  spinlock - a behaviour which is acceptable for a periodic metric check
  but not for a cleanup routine (which must ""stop"" all still running
  workers)

The better approch is to get rid of the per interface neighbor metric
worker and handle everything in the interface worker. The original problems
are solved by:

* creating a list of neighbors which require new metric information inside
  the RCU protected context, gathering the metric according to the new list
  outside the RCU protected context
* only use rcu_trylock inside metric gathering code to avoid a deadlock
  when the cancel_delayed_work_sync is called in the interface removal code
  (which is called with the rtnl_lock held)

Cc: stable@vger.kernel.org
Fixes: c833484e5f38 (""batman-adv: ELP - compute the metric based on the estimated throughput"")
Signed-off-by: Sven Eckelmann <sven@narfation.org>
Signed-off-by: Simon Wunderlich <sw@simonwunderlich.de>
Signed-off-by: Sven Eckelmann <sven@narfation.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 net/batman-adv/bat_v.c     |  2 --
 net/batman-adv/bat_v_elp.c | 74 +++++++++++++++++++++++++-------------
 net/batman-adv/bat_v_elp.h |  2 --
 net/batman-adv/types.h     |  3 --
 4 files changed, 50 insertions(+), 31 deletions(-)

diff --git a/net/batman-adv/bat_v.c b/net/batman-adv/bat_v.c
index 64054edc2e3c..8a68f68109bf 100644
--- a/net/batman-adv/bat_v.c
+++ b/net/batman-adv/bat_v.c
@@ -115,8 +115,6 @@ static void
 batadv_v_hardif_neigh_init(struct batadv_hardif_neigh_node *hardif_neigh)
 {
         ewma_throughput_init(&hardif_neigh->bat_v.throughput);
-        INIT_WORK(&hardif_neigh->bat_v.metric_work,
-                  batadv_v_elp_throughput_metric_update);
 }
 
 #ifdef CONFIG_BATMAN_ADV_DEBUGFS
diff --git a/net/batman-adv/bat_v_elp.c b/net/batman-adv/bat_v_elp.c
index aa98637fe83c..b6d580494b89 100644
--- a/net/batman-adv/bat_v_elp.c
+++ b/net/batman-adv/bat_v_elp.c
@@ -18,6 +18,7 @@
 #include <linux/jiffies.h>
 #include <linux/kernel.h>
 #include <linux/kref.h>
+#include <linux/list.h>
 #include <linux/netdevice.h>
 #include <linux/nl80211.h>
 #include <linux/random.h>
@@ -25,6 +26,7 @@
 #include <linux/rcupdate.h>
 #include <linux/rtnetlink.h>
 #include <linux/skbuff.h>
+#include <linux/slab.h>
 #include <linux/stddef.h>
 #include <linux/string.h>
 #include <linux/types.h>
@@ -40,6 +42,18 @@
 #include ""routing.h""
 #include ""send.h""
 
+/**
+ * struct batadv_v_metric_queue_entry - list of hardif neighbors which require
+ *  and metric update
+ */
+struct batadv_v_metric_queue_entry {
+        /** @hardif_neigh: hardif neighbor scheduled for metric update */
+        struct batadv_hardif_neigh_node *hardif_neigh;
+
+        /** @list: list node for metric_queue */
+        struct list_head list;
+};
+
 /**
  * batadv_v_elp_start_timer() - restart timer for ELP periodic work
  * @hard_iface: the interface for which the timer has to be reset
@@ -126,11 +140,19 @@ static bool batadv_v_elp_get_throughput(struct batadv_hardif_neigh_node *neigh,
                 return true;
         }
 
+        memset(&link_settings, 0, sizeof(link_settings));
+
+        /* only use rtnl_trylock because the elp worker will be cancelled while
+         * the rntl_lock is held. the cancel_delayed_work_sync() would otherwise
+         * wait forever when the elp work_item was started and it is then also
+         * trying to rtnl_lock
+         */
+        if (!rtnl_trylock())
+                return false;
+
         /* if not a wifi interface, check if this device provides data via
          * ethtool (e.g. an Ethernet adapter)
          */
-        memset(&link_settings, 0, sizeof(link_settings));
-        rtnl_lock();
         ret = __ethtool_get_link_ksettings(hard_iface->net_dev, &link_settings);
         rtnl_unlock();
         if (ret == 0) {
@@ -165,31 +187,19 @@ default_throughput:
 /**
  * batadv_v_elp_throughput_metric_update() - worker updating the throughput
  *  metric of a single hop neighbour
- * @work: the work queue item
+ * @neigh: the neighbour to probe
  */
-void batadv_v_elp_throughput_metric_update(struct work_struct *work)
+static void
+batadv_v_elp_throughput_metric_update(struct batadv_hardif_neigh_node *neigh)
 {
-        struct batadv_hardif_neigh_node_bat_v *neigh_bat_v;
-        struct batadv_hardif_neigh_node *neigh;
         u32 throughput;
         bool valid;
 
-        neigh_bat_v = container_of(work, struct batadv_hardif_neigh_node_bat_v,
-                                   metric_work);
-        neigh = container_of(neigh_bat_v, struct batadv_hardif_neigh_node,
-                             bat_v);
-
         valid = batadv_v_elp_get_throughput(neigh, &throughput);
         if (!valid)
-                goto put_neigh;
+                return;
 
         ewma_throughput_add(&neigh->bat_v.throughput, throughput);
-
-put_neigh:
-        /* decrement refcounter to balance increment performed before scheduling
-         * this task
-         */
-        batadv_hardif_neigh_put(neigh);
 }
 
 /**
@@ -263,14 +273,16 @@ batadv_v_elp_wifi_neigh_probe(struct batadv_hardif_neigh_node *neigh)
  */
 static void batadv_v_elp_periodic_work(struct work_struct *work)
 {
+        struct batadv_v_metric_queue_entry *metric_entry;
+        struct batadv_v_metric_queue_entry *metric_safe;
         struct batadv_hardif_neigh_node *hardif_neigh;
         struct batadv_hard_iface *hard_iface;
         struct batadv_hard_iface_bat_v *bat_v;
         struct batadv_elp_packet *elp_packet;
+        struct list_head metric_queue;
         struct batadv_priv *bat_priv;
         struct sk_buff *skb;
         u32 elp_interval;
-        bool ret;
 
         bat_v = container_of(work, struct batadv_hard_iface_bat_v, elp_wq.work);
         hard_iface = container_of(bat_v, struct batadv_hard_iface, bat_v);
@@ -306,6 +318,8 @@ static void batadv_v_elp_periodic_work(struct work_struct *work)
 
         atomic_inc(&hard_iface->bat_v.elp_seqno);
 
+        INIT_LIST_HEAD(&metric_queue);
+
         /* The throughput metric is updated on each sent packet. This way, if a
          * node is dead and no longer sends packets, batman-adv is still able to
          * react timely to its death.
@@ -330,16 +344,28 @@ static void batadv_v_elp_periodic_work(struct work_struct *work)
 
                 /* Reading the estimated throughput from cfg80211 is a task that
                  * may sleep and that is not allowed in an rcu protected
-                 * context. Therefore schedule a task for that.
+                 * context. Therefore add it to metric_queue and process it
+                 * outside rcu protected context.
                  */
-                ret = queue_work(batadv_event_workqueue,
-                                 &hardif_neigh->bat_v.metric_work);
-
-                if (!ret)
+                metric_entry = kzalloc(sizeof(*metric_entry), GFP_ATOMIC);
+                if (!metric_entry) {
                         batadv_hardif_neigh_put(hardif_neigh);
+                        continue;
+                }
+
+                metric_entry->hardif_neigh = hardif_neigh;
+                list_add(&metric_entry->list, &metric_queue);
         }
         rcu_read_unlock();
 
+        list_for_each_entry_safe(metric_entry, metric_safe, &metric_queue, list) {
+                batadv_v_elp_throughput_metric_update(metric_entry->hardif_neigh);
+
+                batadv_hardif_neigh_put(metric_entry->hardif_neigh);
+                list_del(&metric_entry->list);
+                kfree(metric_entry);
+        }
+
 restart_timer:
         batadv_v_elp_start_timer(hard_iface);
 out:
diff --git a/net/batman-adv/bat_v_elp.h b/net/batman-adv/bat_v_elp.h
index 1a29505f4f66..5595fa531d7c 100644
--- a/net/batman-adv/bat_v_elp.h
+++ b/net/batman-adv/bat_v_elp.h
@@ -10,7 +10,6 @@
 #include ""main.h""
 
 #include <linux/skbuff.h>
-#include <linux/workqueue.h>
 
 int batadv_v_elp_iface_enable(struct batadv_hard_iface *hard_iface);
 void batadv_v_elp_iface_disable(struct batadv_hard_iface *hard_iface);
@@ -19,6 +18,5 @@ void batadv_v_elp_iface_activate(struct batadv_hard_iface *primary_iface,
 void batadv_v_elp_primary_iface_set(struct batadv_hard_iface *primary_iface);
 int batadv_v_elp_packet_recv(struct sk_buff *skb,
                              struct batadv_hard_iface *if_incoming);
-void batadv_v_elp_throughput_metric_update(struct work_struct *work);
 
 #endif /* _NET_BATMAN_ADV_BAT_V_ELP_H_ */
diff --git a/net/batman-adv/types.h b/net/batman-adv/types.h
index 9fdf1be9b99b..a4a4c0c0c887 100644
--- a/net/batman-adv/types.h
+++ b/net/batman-adv/types.h
@@ -603,9 +603,6 @@ struct batadv_hardif_neigh_node_bat_v {
          *  neighbor
          */
         unsigned long last_unicast_tx;
-
-        /** @metric_work: work queue callback item for metric update */
-        struct work_struct metric_work;
 };
 
 /**
-- 
2.39.5 (Apple Git-154)

",patching file net/batman-adv/bat_v.c\nHunk #1 succeeded at 115 with fuzz 1 (offset 2 lines).\npatching file net/batman-adv/bat_v_elp.c\nHunk #1 FAILED at 18.\nHunk #2 succeeded at 25 (offset -1 lines).\nHunk #3 succeeded at 41 (offset -1 lines).\nHunk #4 FAILED at 150.\nHunk #5 succeeded at 178 (offset -10 lines).\nHunk #6 succeeded at 264 (offset -10 lines).\nHunk #7 succeeded at 309 (offset -10 lines).\nHunk #8 succeeded at 335 (offset -10 lines).\n2 out of 8 hunks FAILED -- saving rejects to file net/batman-adv/bat_v_elp.c.rej\npatching file net/batman-adv/bat_v_elp.h\npatching file net/batman-adv/types.h\nHunk #1 succeeded at 603 (offset 7 lines).,79,2,37,258,1,FORMATTING,FORMATTING
CVE-2025-21844,860ca5e50f73c2a1cef7eefc9d39d04e275417f7,"From 860ca5e50f73c2a1cef7eefc9d39d04e275417f7 Mon Sep 17 00:00:00 2001
From: Haoxiang Li <haoxiang_li2024@163.com>
Date: Mon, 17 Feb 2025 15:20:38 +0800
Subject: [PATCH] smb: client: Add check for next_buffer in
 receive_encrypted_standard()

Add check for the return value of cifs_buf_get() and cifs_small_buf_get()
in receive_encrypted_standard() to prevent null pointer dereference.

Fixes: eec04ea11969 (""smb: client: fix OOB in receive_encrypted_standard()"")
Cc: stable@vger.kernel.org
Signed-off-by: Haoxiang Li <haoxiang_li2024@163.com>
Signed-off-by: Steve French <stfrench@microsoft.com>
---
 fs/smb/client/smb2ops.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/fs/smb/client/smb2ops.c b/fs/smb/client/smb2ops.c
index 23e0c8be7fb5..4dd11eafb69d 100644
--- a/fs/smb/client/smb2ops.c
+++ b/fs/smb/client/smb2ops.c
@@ -4965,6 +4965,10 @@ receive_encrypted_standard(struct TCP_Server_Info *server,
 			next_buffer = (char *)cifs_buf_get();
 		else
 			next_buffer = (char *)cifs_small_buf_get();
+		if (!next_buffer) {
+			cifs_server_dbg(VFS, ""No memory for (large) SMB response\n"");
+			return -1;
+		}
 		memcpy(next_buffer, buf + next_cmd, pdu_length - next_cmd);
 	}
 
-- 
2.39.5 (Apple Git-154)
",f277e479eea3d1aa18bc712abe1d2bf3dece2e30,"From f277e479eea3d1aa18bc712abe1d2bf3dece2e30 Mon Sep 17 00:00:00 2001
From: Haoxiang Li <haoxiang_li2024@163.com>
Date: Mon, 17 Feb 2025 15:20:38 +0800
Subject: [PATCH] smb: client: Add check for next_buffer in
 receive_encrypted_standard()

[ Upstream commit 860ca5e50f73c2a1cef7eefc9d39d04e275417f7 ]

Add check for the return value of cifs_buf_get() and cifs_small_buf_get()
in receive_encrypted_standard() to prevent null pointer dereference.

Fixes: eec04ea11969 (""smb: client: fix OOB in receive_encrypted_standard()"")
Cc: stable@vger.kernel.org
Signed-off-by: Haoxiang Li <haoxiang_li2024@163.com>
Signed-off-by: Steve French <stfrench@microsoft.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 fs/cifs/smb2ops.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/fs/cifs/smb2ops.c b/fs/cifs/smb2ops.c
index 68f93de2b152..70a4d101b542 100644
--- a/fs/cifs/smb2ops.c
+++ b/fs/cifs/smb2ops.c
@@ -4938,6 +4938,10 @@ receive_encrypted_standard(struct TCP_Server_Info *server,
                         next_buffer = (char *)cifs_buf_get();
                 else
                         next_buffer = (char *)cifs_small_buf_get();
+                if (!next_buffer) {
+                        cifs_server_dbg(VFS, ""No memory for (large) SMB response\n"");
+                        return -1;
+                }
                 memcpy(next_buffer, buf + next_cmd, pdu_length - next_cmd);
         }
 
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 22\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 860ca5e50f73c2a1cef7eefc9d39d04e275417f7 Mon Sep 17 00:00:00 2001\n|From: Haoxiang Li <haoxiang_li2024@163.com>\n|Date: Mon, 17 Feb 2025 15:20:38 +0800\n|Subject: [PATCH] smb: client: Add check for next_buffer in\n| receive_encrypted_standard()\n|\n|Add check for the return value of cifs_buf_get() and cifs_small_buf_get()\n|in receive_encrypted_standard() to prevent null pointer dereference.\n|\n|Fixes: eec04ea11969 (\""smb: client: fix OOB in receive_encrypted_standard()\"")\n|Cc: stable@vger.kernel.org\n|Signed-off-by: Haoxiang Li <haoxiang_li2024@163.com>\n|Signed-off-by: Steve French <stfrench@microsoft.com>\n|---\n| fs/smb/client/smb2ops.c | 4 ++++\n| 1 file changed, 4 insertions(+)\n|\n|diff --git a/fs/smb/client/smb2ops.c b/fs/smb/client/smb2ops.c\n|index 23e0c8be7fb5..4dd11eafb69d 100644\n|--- a/fs/smb/client/smb2ops.c\n|+++ b/fs/smb/client/smb2ops.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,1,34,1,COMMENTS,COMMENTS
CVE-2025-21844,860ca5e50f73c2a1cef7eefc9d39d04e275417f7,"From 860ca5e50f73c2a1cef7eefc9d39d04e275417f7 Mon Sep 17 00:00:00 2001
From: Haoxiang Li <haoxiang_li2024@163.com>
Date: Mon, 17 Feb 2025 15:20:38 +0800
Subject: [PATCH] smb: client: Add check for next_buffer in
 receive_encrypted_standard()

Add check for the return value of cifs_buf_get() and cifs_small_buf_get()
in receive_encrypted_standard() to prevent null pointer dereference.

Fixes: eec04ea11969 (""smb: client: fix OOB in receive_encrypted_standard()"")
Cc: stable@vger.kernel.org
Signed-off-by: Haoxiang Li <haoxiang_li2024@163.com>
Signed-off-by: Steve French <stfrench@microsoft.com>
---
 fs/smb/client/smb2ops.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/fs/smb/client/smb2ops.c b/fs/smb/client/smb2ops.c
index 23e0c8be7fb5..4dd11eafb69d 100644
--- a/fs/smb/client/smb2ops.c
+++ b/fs/smb/client/smb2ops.c
@@ -4965,6 +4965,10 @@ receive_encrypted_standard(struct TCP_Server_Info *server,
 			next_buffer = (char *)cifs_buf_get();
 		else
 			next_buffer = (char *)cifs_small_buf_get();
+		if (!next_buffer) {
+			cifs_server_dbg(VFS, ""No memory for (large) SMB response\n"");
+			return -1;
+		}
 		memcpy(next_buffer, buf + next_cmd, pdu_length - next_cmd);
 	}
 
-- 
2.39.5 (Apple Git-154)

",f618aeb6cad2307e48a641379db610abcf593edf,"From f618aeb6cad2307e48a641379db610abcf593edf Mon Sep 17 00:00:00 2001
From: Haoxiang Li <haoxiang_li2024@163.com>
Date: Mon, 17 Feb 2025 15:20:38 +0800
Subject: [PATCH] smb: client: Add check for next_buffer in
 receive_encrypted_standard()

[ Upstream commit 860ca5e50f73c2a1cef7eefc9d39d04e275417f7 ]

Add check for the return value of cifs_buf_get() and cifs_small_buf_get()
in receive_encrypted_standard() to prevent null pointer dereference.

Fixes: eec04ea11969 (""smb: client: fix OOB in receive_encrypted_standard()"")
Cc: stable@vger.kernel.org
Signed-off-by: Haoxiang Li <haoxiang_li2024@163.com>
Signed-off-by: Steve French <stfrench@microsoft.com>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 fs/cifs/smb2ops.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/fs/cifs/smb2ops.c b/fs/cifs/smb2ops.c
index ee9a1e6550e3..7bce1ab86c4d 100644
--- a/fs/cifs/smb2ops.c
+++ b/fs/cifs/smb2ops.c
@@ -5198,6 +5198,10 @@ receive_encrypted_standard(struct TCP_Server_Info *server,
 			next_buffer = (char *)cifs_buf_get();
 		else
 			next_buffer = (char *)cifs_small_buf_get();
+		if (!next_buffer) {
+			cifs_server_dbg(VFS, ""No memory for (large) SMB response\n"");
+			return -1;
+		}
 		memcpy(next_buffer, buf + next_cmd, pdu_length - next_cmd);
 	}
 
-- 
2.39.5 (Apple Git-154)

","can't find file to patch at input line 22\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n|From 860ca5e50f73c2a1cef7eefc9d39d04e275417f7 Mon Sep 17 00:00:00 2001\n|From: Haoxiang Li <haoxiang_li2024@163.com>\n|Date: Mon, 17 Feb 2025 15:20:38 +0800\n|Subject: [PATCH] smb: client: Add check for next_buffer in\n| receive_encrypted_standard()\n|\n|Add check for the return value of cifs_buf_get() and cifs_small_buf_get()\n|in receive_encrypted_standard() to prevent null pointer dereference.\n|\n|Fixes: eec04ea11969 (\""smb: client: fix OOB in receive_encrypted_standard()\"")\n|Cc: stable@vger.kernel.org\n|Signed-off-by: Haoxiang Li <haoxiang_li2024@163.com>\n|Signed-off-by: Steve French <stfrench@microsoft.com>\n|---\n| fs/smb/client/smb2ops.c | 4 ++++\n| 1 file changed, 4 insertions(+)\n|\n|diff --git a/fs/smb/client/smb2ops.c b/fs/smb/client/smb2ops.c\n|index 23e0c8be7fb5..4dd11eafb69d 100644\n|--- a/fs/smb/client/smb2ops.c\n|+++ b/fs/smb/client/smb2ops.c\n--------------------------\nFile to patch: \nSkip this patch? [y] \nSkipping patch.\n1 out of 1 hunk ignored",5,0,1,35,1,OTHER,COMMENTS
CVE-2025-21846,56d5f3eba3f5de0efdd556de4ef381e109b973a9,"From 56d5f3eba3f5de0efdd556de4ef381e109b973a9 Mon Sep 17 00:00:00 2001
From: Christian Brauner <brauner@kernel.org>
Date: Tue, 11 Feb 2025 18:15:59 +0100
Subject: [PATCH] acct: perform last write from workqueue

In [1] it was reported that the acct(2) system call can be used to
trigger NULL deref in cases where it is set to write to a file that
triggers an internal lookup. This can e.g., happen when pointing acc(2)
to /sys/power/resume. At the point the where the write to this file
happens the calling task has already exited and called exit_fs(). A
lookup will thus trigger a NULL-deref when accessing current->fs.

Reorganize the code so that the the final write happens from the
workqueue but with the caller's credentials. This preserves the
(strange) permission model and has almost no regression risk.

This api should stop to exist though.

Link: https://lore.kernel.org/r/20250127091811.3183623-1-quzicheng@huawei.com [1]
Link: https://lore.kernel.org/r/20250211-work-acct-v1-1-1c16aecab8b3@kernel.org
Fixes: 1da177e4c3f4 (""Linux-2.6.12-rc2"")
Reported-by: Zicheng Qu <quzicheng@huawei.com>
Cc: stable@vger.kernel.org
Signed-off-by: Christian Brauner <brauner@kernel.org>
---
 kernel/acct.c | 120 +++++++++++++++++++++++++++++---------------------
 1 file changed, 70 insertions(+), 50 deletions(-)

diff --git a/kernel/acct.c b/kernel/acct.c
index 31222e8cd534..48283efe8a12 100644
--- a/kernel/acct.c
+++ b/kernel/acct.c
@@ -103,48 +103,50 @@ struct bsd_acct_struct {
         atomic_long_t                count;
         struct rcu_head                rcu;
         struct mutex                lock;
-        int                        active;
+        bool                        active;
+        bool                        check_space;
         unsigned long                needcheck;
         struct file                *file;
         struct pid_namespace        *ns;
         struct work_struct        work;
         struct completion        done;
+        acct_t                        ac;
 };
 
-static void do_acct_process(struct bsd_acct_struct *acct);
+static void fill_ac(struct bsd_acct_struct *acct);
+static void acct_write_process(struct bsd_acct_struct *acct);
 
 /*
  * Check the amount of free space and suspend/resume accordingly.
  */
-static int check_free_space(struct bsd_acct_struct *acct)
+static bool check_free_space(struct bsd_acct_struct *acct)
 {
         struct kstatfs sbuf;
 
-        if (time_is_after_jiffies(acct->needcheck))
-                goto out;
+        if (!acct->check_space)
+                return acct->active;
 
         /* May block */
         if (vfs_statfs(&acct->file->f_path, &sbuf))
-                goto out;
+                return acct->active;
 
         if (acct->active) {
                 u64 suspend = sbuf.f_blocks * SUSPEND;
                 do_div(suspend, 100);
                 if (sbuf.f_bavail <= suspend) {
-                        acct->active = 0;
+                        acct->active = false;
                         pr_info(""Process accounting paused\n"");
                 }
         } else {
                 u64 resume = sbuf.f_blocks * RESUME;
                 do_div(resume, 100);
                 if (sbuf.f_bavail >= resume) {
-                        acct->active = 1;
+                        acct->active = true;
                         pr_info(""Process accounting resumed\n"");
                 }
         }
 
         acct->needcheck = jiffies + ACCT_TIMEOUT*HZ;
-out:
         return acct->active;
 }
 
@@ -189,7 +191,11 @@ static void acct_pin_kill(struct fs_pin *pin)
 {
         struct bsd_acct_struct *acct = to_acct(pin);
         mutex_lock(&acct->lock);
-        do_acct_process(acct);
+        /*
+         * Fill the accounting struct with the exiting task's info
+         * before punting to the workqueue.
+         */
+        fill_ac(acct);
         schedule_work(&acct->work);
         wait_for_completion(&acct->done);
         cmpxchg(&acct->ns->bacct, pin, NULL);
@@ -202,6 +208,9 @@ static void close_work(struct work_struct *work)
 {
         struct bsd_acct_struct *acct = container_of(work, struct bsd_acct_struct, work);
         struct file *file = acct->file;
+
+        /* We were fired by acct_pin_kill() which holds acct->lock. */
+        acct_write_process(acct);
         if (file->f_op->flush)
                 file->f_op->flush(file, NULL);
         __fput_sync(file);
@@ -430,13 +439,27 @@ static u32 encode_float(u64 value)
  *  do_exit() or when switching to a different output file.
  */
 
-static void fill_ac(acct_t *ac)
+static void fill_ac(struct bsd_acct_struct *acct)
 {
         struct pacct_struct *pacct = &current->signal->pacct;
+        struct file *file = acct->file;
+        acct_t *ac = &acct->ac;
         u64 elapsed, run_time;
         time64_t btime;
         struct tty_struct *tty;
 
+        lockdep_assert_held(&acct->lock);
+
+        if (time_is_after_jiffies(acct->needcheck)) {
+                acct->check_space = false;
+
+                /* Don't fill in @ac if nothing will be written. */
+                if (!acct->active)
+                        return;
+        } else {
+                acct->check_space = true;
+        }
+
         /*
          * Fill the accounting struct with the needed info as recorded
          * by the different kernel functions.
@@ -484,64 +507,61 @@ static void fill_ac(acct_t *ac)
         ac->ac_majflt = encode_comp_t(pacct->ac_majflt);
         ac->ac_exitcode = pacct->ac_exitcode;
         spin_unlock_irq(&current->sighand->siglock);
-}
-/*
- *  do_acct_process does all actual work. Caller holds the reference to file.
- */
-static void do_acct_process(struct bsd_acct_struct *acct)
-{
-        acct_t ac;
-        unsigned long flim;
-        const struct cred *orig_cred;
-        struct file *file = acct->file;
-
-        /*
-         * Accounting records are not subject to resource limits.
-         */
-        flim = rlimit(RLIMIT_FSIZE);
-        current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
-        /* Perform file operations on behalf of whoever enabled accounting */
-        orig_cred = override_creds(file->f_cred);
 
-        /*
-         * First check to see if there is enough free_space to continue
-         * the process accounting system.
-         */
-        if (!check_free_space(acct))
-                goto out;
-
-        fill_ac(&ac);
         /* we really need to bite the bullet and change layout */
-        ac.ac_uid = from_kuid_munged(file->f_cred->user_ns, orig_cred->uid);
-        ac.ac_gid = from_kgid_munged(file->f_cred->user_ns, orig_cred->gid);
+        ac->ac_uid = from_kuid_munged(file->f_cred->user_ns, current_uid());
+        ac->ac_gid = from_kgid_munged(file->f_cred->user_ns, current_gid());
 #if ACCT_VERSION == 1 || ACCT_VERSION == 2
         /* backward-compatible 16 bit fields */
-        ac.ac_uid16 = ac.ac_uid;
-        ac.ac_gid16 = ac.ac_gid;
+        ac->ac_uid16 = ac->ac_uid;
+        ac->ac_gid16 = ac->ac_gid;
 #elif ACCT_VERSION == 3
         {
                 struct pid_namespace *ns = acct->ns;
 
-                ac.ac_pid = task_tgid_nr_ns(current, ns);
+                ac->ac_pid = task_tgid_nr_ns(current, ns);
                 rcu_read_lock();
-                ac.ac_ppid = task_tgid_nr_ns(rcu_dereference(current->real_parent),
-                                             ns);
+                ac->ac_ppid = task_tgid_nr_ns(rcu_dereference(current->real_parent), ns);
                 rcu_read_unlock();
         }
 #endif
+}
+
+static void acct_write_process(struct bsd_acct_struct *acct)
+{
+        struct file *file = acct->file;
+        const struct cred *cred;
+        acct_t *ac = &acct->ac;
+
+        /* Perform file operations on behalf of whoever enabled accounting */
+        cred = override_creds(file->f_cred);
+
         /*
-         * Get freeze protection. If the fs is frozen, just skip the write
-         * as we could deadlock the system otherwise.
+         * First check to see if there is enough free_space to continue
+         * the process accounting system. Then get freeze protection. If
+         * the fs is frozen, just skip the write as we could deadlock
+         * the system otherwise.
          */
-        if (file_start_write_trylock(file)) {
+        if (check_free_space(acct) && file_start_write_trylock(file)) {
                 /* it's been opened O_APPEND, so position is irrelevant */
                 loff_t pos = 0;
-                __kernel_write(file, &ac, sizeof(acct_t), &pos);
+                __kernel_write(file, ac, sizeof(acct_t), &pos);
                 file_end_write(file);
         }
-out:
+
+        revert_creds(cred);
+}
+
+static void do_acct_process(struct bsd_acct_struct *acct)
+{
+        unsigned long flim;
+
+        /* Accounting records are not subject to resource limits. */
+        flim = rlimit(RLIMIT_FSIZE);
+        current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
+        fill_ac(acct);
+        acct_write_process(acct);
         current->signal->rlim[RLIMIT_FSIZE].rlim_cur = flim;
-        revert_creds(orig_cred);
 }
 
 /**
-- 
2.39.5 (Apple Git-154)

",8acbf4a88c6a98c8ed00afd1a7d1abcca9b4735e,"From 8acbf4a88c6a98c8ed00afd1a7d1abcca9b4735e Mon Sep 17 00:00:00 2001
From: Christian Brauner <brauner@kernel.org>
Date: Tue, 11 Feb 2025 18:15:59 +0100
Subject: [PATCH] acct: perform last write from workqueue

[ Upstream commit 56d5f3eba3f5de0efdd556de4ef381e109b973a9 ]

In [1] it was reported that the acct(2) system call can be used to
trigger NULL deref in cases where it is set to write to a file that
triggers an internal lookup. This can e.g., happen when pointing acc(2)
to /sys/power/resume. At the point the where the write to this file
happens the calling task has already exited and called exit_fs(). A
lookup will thus trigger a NULL-deref when accessing current->fs.

Reorganize the code so that the the final write happens from the
workqueue but with the caller's credentials. This preserves the
(strange) permission model and has almost no regression risk.

This api should stop to exist though.

Link: https://lore.kernel.org/r/20250127091811.3183623-1-quzicheng@huawei.com [1]
Link: https://lore.kernel.org/r/20250211-work-acct-v1-1-1c16aecab8b3@kernel.org
Fixes: 1da177e4c3f4 (""Linux-2.6.12-rc2"")
Reported-by: Zicheng Qu <quzicheng@huawei.com>
Cc: stable@vger.kernel.org
Signed-off-by: Christian Brauner <brauner@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 kernel/acct.c | 120 +++++++++++++++++++++++++++++---------------------
 1 file changed, 70 insertions(+), 50 deletions(-)

diff --git a/kernel/acct.c b/kernel/acct.c
index cdfe1b0ce0e3..bddb31472f9e 100644
--- a/kernel/acct.c
+++ b/kernel/acct.c
@@ -85,48 +85,50 @@ struct bsd_acct_struct {
         atomic_long_t                count;
         struct rcu_head                rcu;
         struct mutex                lock;
-        int                        active;
+        bool                        active;
+        bool                        check_space;
         unsigned long                needcheck;
         struct file                *file;
         struct pid_namespace        *ns;
         struct work_struct        work;
         struct completion        done;
+        acct_t                        ac;
 };
 
-static void do_acct_process(struct bsd_acct_struct *acct);
+static void fill_ac(struct bsd_acct_struct *acct);
+static void acct_write_process(struct bsd_acct_struct *acct);
 
 /*
  * Check the amount of free space and suspend/resume accordingly.
  */
-static int check_free_space(struct bsd_acct_struct *acct)
+static bool check_free_space(struct bsd_acct_struct *acct)
 {
         struct kstatfs sbuf;
 
-        if (time_is_after_jiffies(acct->needcheck))
-                goto out;
+        if (!acct->check_space)
+                return acct->active;
 
         /* May block */
         if (vfs_statfs(&acct->file->f_path, &sbuf))
-                goto out;
+                return acct->active;
 
         if (acct->active) {
                 u64 suspend = sbuf.f_blocks * SUSPEND;
                 do_div(suspend, 100);
                 if (sbuf.f_bavail <= suspend) {
-                        acct->active = 0;
+                        acct->active = false;
                         pr_info(""Process accounting paused\n"");
                 }
         } else {
                 u64 resume = sbuf.f_blocks * RESUME;
                 do_div(resume, 100);
                 if (sbuf.f_bavail >= resume) {
-                        acct->active = 1;
+                        acct->active = true;
                         pr_info(""Process accounting resumed\n"");
                 }
         }
 
         acct->needcheck = jiffies + ACCT_TIMEOUT*HZ;
-out:
         return acct->active;
 }
 
@@ -171,7 +173,11 @@ static void acct_pin_kill(struct fs_pin *pin)
 {
         struct bsd_acct_struct *acct = to_acct(pin);
         mutex_lock(&acct->lock);
-        do_acct_process(acct);
+        /*
+         * Fill the accounting struct with the exiting task's info
+         * before punting to the workqueue.
+         */
+        fill_ac(acct);
         schedule_work(&acct->work);
         wait_for_completion(&acct->done);
         cmpxchg(&acct->ns->bacct, pin, NULL);
@@ -184,6 +190,9 @@ static void close_work(struct work_struct *work)
 {
         struct bsd_acct_struct *acct = container_of(work, struct bsd_acct_struct, work);
         struct file *file = acct->file;
+
+        /* We were fired by acct_pin_kill() which holds acct->lock. */
+        acct_write_process(acct);
         if (file->f_op->flush)
                 file->f_op->flush(file, NULL);
         __fput_sync(file);
@@ -426,12 +435,26 @@ static u32 encode_float(u64 value)
  *  do_exit() or when switching to a different output file.
  */
 
-static void fill_ac(acct_t *ac)
+static void fill_ac(struct bsd_acct_struct *acct)
 {
         struct pacct_struct *pacct = &current->signal->pacct;
+        struct file *file = acct->file;
+        acct_t *ac = &acct->ac;
         u64 elapsed, run_time;
         struct tty_struct *tty;
 
+        lockdep_assert_held(&acct->lock);
+
+        if (time_is_after_jiffies(acct->needcheck)) {
+                acct->check_space = false;
+
+                /* Don't fill in @ac if nothing will be written. */
+                if (!acct->active)
+                        return;
+        } else {
+                acct->check_space = true;
+        }
+
         /*
          * Fill the accounting struct with the needed info as recorded
          * by the different kernel functions.
@@ -478,64 +501,61 @@ static void fill_ac(acct_t *ac)
         ac->ac_majflt = encode_comp_t(pacct->ac_majflt);
         ac->ac_exitcode = pacct->ac_exitcode;
         spin_unlock_irq(&current->sighand->siglock);
-}
-/*
- *  do_acct_process does all actual work. Caller holds the reference to file.
- */
-static void do_acct_process(struct bsd_acct_struct *acct)
-{
-        acct_t ac;
-        unsigned long flim;
-        const struct cred *orig_cred;
-        struct file *file = acct->file;
-
-        /*
-         * Accounting records are not subject to resource limits.
-         */
-        flim = rlimit(RLIMIT_FSIZE);
-        current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
-        /* Perform file operations on behalf of whoever enabled accounting */
-        orig_cred = override_creds(file->f_cred);
 
-        /*
-         * First check to see if there is enough free_space to continue
-         * the process accounting system.
-         */
-        if (!check_free_space(acct))
-                goto out;
-
-        fill_ac(&ac);
         /* we really need to bite the bullet and change layout */
-        ac.ac_uid = from_kuid_munged(file->f_cred->user_ns, orig_cred->uid);
-        ac.ac_gid = from_kgid_munged(file->f_cred->user_ns, orig_cred->gid);
+        ac->ac_uid = from_kuid_munged(file->f_cred->user_ns, current_uid());
+        ac->ac_gid = from_kgid_munged(file->f_cred->user_ns, current_gid());
 #if ACCT_VERSION == 1 || ACCT_VERSION == 2
         /* backward-compatible 16 bit fields */
-        ac.ac_uid16 = ac.ac_uid;
-        ac.ac_gid16 = ac.ac_gid;
+        ac->ac_uid16 = ac->ac_uid;
+        ac->ac_gid16 = ac->ac_gid;
 #elif ACCT_VERSION == 3
         {
                 struct pid_namespace *ns = acct->ns;
 
-                ac.ac_pid = task_tgid_nr_ns(current, ns);
+                ac->ac_pid = task_tgid_nr_ns(current, ns);
                 rcu_read_lock();
-                ac.ac_ppid = task_tgid_nr_ns(rcu_dereference(current->real_parent),
-                                             ns);
+                ac->ac_ppid = task_tgid_nr_ns(rcu_dereference(current->real_parent), ns);
                 rcu_read_unlock();
         }
 #endif
+}
+
+static void acct_write_process(struct bsd_acct_struct *acct)
+{
+        struct file *file = acct->file;
+        const struct cred *cred;
+        acct_t *ac = &acct->ac;
+
+        /* Perform file operations on behalf of whoever enabled accounting */
+        cred = override_creds(file->f_cred);
+
         /*
-         * Get freeze protection. If the fs is frozen, just skip the write
-         * as we could deadlock the system otherwise.
+         * First check to see if there is enough free_space to continue
+         * the process accounting system. Then get freeze protection. If
+         * the fs is frozen, just skip the write as we could deadlock
+         * the system otherwise.
          */
-        if (file_start_write_trylock(file)) {
+        if (check_free_space(acct) && file_start_write_trylock(file)) {
                 /* it's been opened O_APPEND, so position is irrelevant */
                 loff_t pos = 0;
-                __kernel_write(file, &ac, sizeof(acct_t), &pos);
+                __kernel_write(file, ac, sizeof(acct_t), &pos);
                 file_end_write(file);
         }
-out:
+
+        revert_creds(cred);
+}
+
+static void do_acct_process(struct bsd_acct_struct *acct)
+{
+        unsigned long flim;
+
+        /* Accounting records are not subject to resource limits. */
+        flim = rlimit(RLIMIT_FSIZE);
+        current->signal->rlim[RLIMIT_FSIZE].rlim_cur = RLIM_INFINITY;
+        fill_ac(acct);
+        acct_write_process(acct);
         current->signal->rlim[RLIMIT_FSIZE].rlim_cur = flim;
-        revert_creds(orig_cred);
 }
 
 /**
-- 
2.39.5 (Apple Git-154)

",patching file kernel/acct.c\nHunk #1 succeeded at 85 (offset -18 lines).\nHunk #2 succeeded at 173 (offset -18 lines).\nHunk #3 succeeded at 190 (offset -18 lines).\nHunk #4 FAILED at 439.\nHunk #5 succeeded at 487 (offset -6 lines).\n1 out of 5 hunks FAILED -- saving rejects to file kernel/acct.c.rej,121,4,33,248,1,SEMANTIC,FORMATTING
CVE-2025-21862,07b598c0e6f06a0f254c88dafb4ad50f8a8c6eea,"From 07b598c0e6f06a0f254c88dafb4ad50f8a8c6eea Mon Sep 17 00:00:00 2001
From: Gavrilov Ilia <Ilia.Gavrilov@infotecs.ru>
Date: Thu, 13 Feb 2025 15:20:55 +0000
Subject: [PATCH] drop_monitor: fix incorrect initialization order

Syzkaller reports the following bug:

BUG: spinlock bad magic on CPU#1, syz-executor.0/7995
 lock: 0xffff88805303f3e0, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
CPU: 1 PID: 7995 Comm: syz-executor.0 Tainted: G            E     5.10.209+ #1
Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 11/12/2020
Call Trace:
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x119/0x179 lib/dump_stack.c:118
 debug_spin_lock_before kernel/locking/spinlock_debug.c:83 [inline]
 do_raw_spin_lock+0x1f6/0x270 kernel/locking/spinlock_debug.c:112
 __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:117 [inline]
 _raw_spin_lock_irqsave+0x50/0x70 kernel/locking/spinlock.c:159
 reset_per_cpu_data+0xe6/0x240 [drop_monitor]
 net_dm_cmd_trace+0x43d/0x17a0 [drop_monitor]
 genl_family_rcv_msg_doit+0x22f/0x330 net/netlink/genetlink.c:739
 genl_family_rcv_msg net/netlink/genetlink.c:783 [inline]
 genl_rcv_msg+0x341/0x5a0 net/netlink/genetlink.c:800
 netlink_rcv_skb+0x14d/0x440 net/netlink/af_netlink.c:2497
 genl_rcv+0x29/0x40 net/netlink/genetlink.c:811
 netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
 netlink_unicast+0x54b/0x800 net/netlink/af_netlink.c:1348
 netlink_sendmsg+0x914/0xe00 net/netlink/af_netlink.c:1916
 sock_sendmsg_nosec net/socket.c:651 [inline]
 __sock_sendmsg+0x157/0x190 net/socket.c:663
 ____sys_sendmsg+0x712/0x870 net/socket.c:2378
 ___sys_sendmsg+0xf8/0x170 net/socket.c:2432
 __sys_sendmsg+0xea/0x1b0 net/socket.c:2461
 do_syscall_64+0x30/0x40 arch/x86/entry/common.c:46
 entry_SYSCALL_64_after_hwframe+0x62/0xc7
RIP: 0033:0x7f3f9815aee9
Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b0 ff ff ff f7 d8 64 89 01 48
RSP: 002b:00007f3f972bf0c8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
RAX: ffffffffffffffda RBX: 00007f3f9826d050 RCX: 00007f3f9815aee9
RDX: 0000000020000000 RSI: 0000000020001300 RDI: 0000000000000007
RBP: 00007f3f981b63bd R08: 0000000000000000 R09: 0000000000000000
R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
R13: 000000000000006e R14: 00007f3f9826d050 R15: 00007ffe01ee6768

If drop_monitor is built as a kernel module, syzkaller may have time
to send a netlink NET_DM_CMD_START message during the module loading.
This will call the net_dm_monitor_start() function that uses
a spinlock that has not yet been initialized.

To fix this, let's place resource initialization above the registration
of a generic netlink family.

Found by InfoTeCS on behalf of Linux Verification Center
(linuxtesting.org) with Syzkaller.

Fixes: 9a8afc8d3962 (""Network Drop Monitor: Adding drop monitor implementation & Netlink protocol"")
Cc: stable@vger.kernel.org
Signed-off-by: Ilia Gavrilov <Ilia.Gavrilov@infotecs.ru>
Reviewed-by: Ido Schimmel <idosch@nvidia.com>
Link: https://patch.msgid.link/20250213152054.2785669-1-Ilia.Gavrilov@infotecs.ru
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/core/drop_monitor.c | 29 ++++++++++++++---------------
 1 file changed, 14 insertions(+), 15 deletions(-)

diff --git a/net/core/drop_monitor.c b/net/core/drop_monitor.c
index 6efd4cccc9dd..212f0a048cab 100644
--- a/net/core/drop_monitor.c
+++ b/net/core/drop_monitor.c
@@ -1734,30 +1734,30 @@ static int __init init_net_drop_monitor(void)
                 return -ENOSPC;
         }
 
-        rc = genl_register_family(&net_drop_monitor_family);
-        if (rc) {
-                pr_err(""Could not create drop monitor netlink family\n"");
-                return rc;
+        for_each_possible_cpu(cpu) {
+                net_dm_cpu_data_init(cpu);
+                net_dm_hw_cpu_data_init(cpu);
         }
-        WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
         rc = register_netdevice_notifier(&dropmon_net_notifier);
         if (rc < 0) {
                 pr_crit(""Failed to register netdevice notifier\n"");
+                return rc;
+        }
+
+        rc = genl_register_family(&net_drop_monitor_family);
+        if (rc) {
+                pr_err(""Could not create drop monitor netlink family\n"");
                 goto out_unreg;
         }
+        WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
         rc = 0;
 
-        for_each_possible_cpu(cpu) {
-                net_dm_cpu_data_init(cpu);
-                net_dm_hw_cpu_data_init(cpu);
-        }
-
         goto out;
 
 out_unreg:
-        genl_unregister_family(&net_drop_monitor_family);
+        WARN_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 out:
         return rc;
 }
@@ -1766,19 +1766,18 @@ static void exit_net_drop_monitor(void)
 {
         int cpu;
 
-        BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
-
         /*
          * Because of the module_get/put we do in the trace state change path
          * we are guaranteed not to have any current users when we get here
          */
+        BUG_ON(genl_unregister_family(&net_drop_monitor_family));
+
+        BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 
         for_each_possible_cpu(cpu) {
                 net_dm_hw_cpu_data_fini(cpu);
                 net_dm_cpu_data_fini(cpu);
         }
-
-        BUG_ON(genl_unregister_family(&net_drop_monitor_family));
 }
 
 module_init(init_net_drop_monitor);
-- 
2.39.5 (Apple Git-154)

",6e9e0f224ffd8b819da3ea247dda404795fdd182,"From 6e9e0f224ffd8b819da3ea247dda404795fdd182 Mon Sep 17 00:00:00 2001
From: Gavrilov Ilia <Ilia.Gavrilov@infotecs.ru>
Date: Thu, 13 Feb 2025 15:20:55 +0000
Subject: [PATCH] drop_monitor: fix incorrect initialization order

[ Upstream commit 07b598c0e6f06a0f254c88dafb4ad50f8a8c6eea ]

Syzkaller reports the following bug:

BUG: spinlock bad magic on CPU#1, syz-executor.0/7995
 lock: 0xffff88805303f3e0, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
CPU: 1 PID: 7995 Comm: syz-executor.0 Tainted: G            E     5.10.209+ #1
Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 11/12/2020
Call Trace:
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x119/0x179 lib/dump_stack.c:118
 debug_spin_lock_before kernel/locking/spinlock_debug.c:83 [inline]
 do_raw_spin_lock+0x1f6/0x270 kernel/locking/spinlock_debug.c:112
 __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:117 [inline]
 _raw_spin_lock_irqsave+0x50/0x70 kernel/locking/spinlock.c:159
 reset_per_cpu_data+0xe6/0x240 [drop_monitor]
 net_dm_cmd_trace+0x43d/0x17a0 [drop_monitor]
 genl_family_rcv_msg_doit+0x22f/0x330 net/netlink/genetlink.c:739
 genl_family_rcv_msg net/netlink/genetlink.c:783 [inline]
 genl_rcv_msg+0x341/0x5a0 net/netlink/genetlink.c:800
 netlink_rcv_skb+0x14d/0x440 net/netlink/af_netlink.c:2497
 genl_rcv+0x29/0x40 net/netlink/genetlink.c:811
 netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
 netlink_unicast+0x54b/0x800 net/netlink/af_netlink.c:1348
 netlink_sendmsg+0x914/0xe00 net/netlink/af_netlink.c:1916
 sock_sendmsg_nosec net/socket.c:651 [inline]
 __sock_sendmsg+0x157/0x190 net/socket.c:663
 ____sys_sendmsg+0x712/0x870 net/socket.c:2378
 ___sys_sendmsg+0xf8/0x170 net/socket.c:2432
 __sys_sendmsg+0xea/0x1b0 net/socket.c:2461
 do_syscall_64+0x30/0x40 arch/x86/entry/common.c:46
 entry_SYSCALL_64_after_hwframe+0x62/0xc7
RIP: 0033:0x7f3f9815aee9
Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b0 ff ff ff f7 d8 64 89 01 48
RSP: 002b:00007f3f972bf0c8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
RAX: ffffffffffffffda RBX: 00007f3f9826d050 RCX: 00007f3f9815aee9
RDX: 0000000020000000 RSI: 0000000020001300 RDI: 0000000000000007
RBP: 00007f3f981b63bd R08: 0000000000000000 R09: 0000000000000000
R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
R13: 000000000000006e R14: 00007f3f9826d050 R15: 00007ffe01ee6768

If drop_monitor is built as a kernel module, syzkaller may have time
to send a netlink NET_DM_CMD_START message during the module loading.
This will call the net_dm_monitor_start() function that uses
a spinlock that has not yet been initialized.

To fix this, let's place resource initialization above the registration
of a generic netlink family.

Found by InfoTeCS on behalf of Linux Verification Center
(linuxtesting.org) with Syzkaller.

Fixes: 9a8afc8d3962 (""Network Drop Monitor: Adding drop monitor implementation & Netlink protocol"")
Cc: stable@vger.kernel.org
Signed-off-by: Ilia Gavrilov <Ilia.Gavrilov@infotecs.ru>
Reviewed-by: Ido Schimmel <idosch@nvidia.com>
Link: https://patch.msgid.link/20250213152054.2785669-1-Ilia.Gavrilov@infotecs.ru
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 net/core/drop_monitor.c | 29 ++++++++++++++---------------
 1 file changed, 14 insertions(+), 15 deletions(-)

diff --git a/net/core/drop_monitor.c b/net/core/drop_monitor.c
index b37465af47e4..0c8afafcce43 100644
--- a/net/core/drop_monitor.c
+++ b/net/core/drop_monitor.c
@@ -1650,30 +1650,30 @@ static int __init init_net_drop_monitor(void)
                 return -ENOSPC;
         }
 
-        rc = genl_register_family(&net_drop_monitor_family);
-        if (rc) {
-                pr_err(""Could not create drop monitor netlink family\n"");
-                return rc;
+        for_each_possible_cpu(cpu) {
+                net_dm_cpu_data_init(cpu);
+                net_dm_hw_cpu_data_init(cpu);
         }
-        WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
         rc = register_netdevice_notifier(&dropmon_net_notifier);
         if (rc < 0) {
                 pr_crit(""Failed to register netdevice notifier\n"");
+                return rc;
+        }
+
+        rc = genl_register_family(&net_drop_monitor_family);
+        if (rc) {
+                pr_err(""Could not create drop monitor netlink family\n"");
                 goto out_unreg;
         }
+        WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
         rc = 0;
 
-        for_each_possible_cpu(cpu) {
-                net_dm_cpu_data_init(cpu);
-                net_dm_hw_cpu_data_init(cpu);
-        }
-
         goto out;
 
 out_unreg:
-        genl_unregister_family(&net_drop_monitor_family);
+        WARN_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 out:
         return rc;
 }
@@ -1682,19 +1682,18 @@ static void exit_net_drop_monitor(void)
 {
         int cpu;
 
-        BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
-
         /*
          * Because of the module_get/put we do in the trace state change path
          * we are guarnateed not to have any current users when we get here
          */
+        BUG_ON(genl_unregister_family(&net_drop_monitor_family));
+
+        BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 
         for_each_possible_cpu(cpu) {
                 net_dm_hw_cpu_data_fini(cpu);
                 net_dm_cpu_data_fini(cpu);
         }
-
-        BUG_ON(genl_unregister_family(&net_drop_monitor_family));
 }
 
 module_init(init_net_drop_monitor);
-- 
2.39.5 (Apple Git-154)

",patching file net/core/drop_monitor.c\nHunk #1 succeeded at 1650 (offset -84 lines).\nHunk #2 FAILED at 1766.\n1 out of 2 hunks FAILED -- saving rejects to file net/core/drop_monitor.c.rej,30,1,0,137,1,COMMENTS,COMMENTS
CVE-2025-21862,07b598c0e6f06a0f254c88dafb4ad50f8a8c6eea,"From 07b598c0e6f06a0f254c88dafb4ad50f8a8c6eea Mon Sep 17 00:00:00 2001
From: Gavrilov Ilia <Ilia.Gavrilov@infotecs.ru>
Date: Thu, 13 Feb 2025 15:20:55 +0000
Subject: [PATCH] drop_monitor: fix incorrect initialization order

Syzkaller reports the following bug:

BUG: spinlock bad magic on CPU#1, syz-executor.0/7995
 lock: 0xffff88805303f3e0, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
CPU: 1 PID: 7995 Comm: syz-executor.0 Tainted: G            E     5.10.209+ #1
Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 11/12/2020
Call Trace:
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x119/0x179 lib/dump_stack.c:118
 debug_spin_lock_before kernel/locking/spinlock_debug.c:83 [inline]
 do_raw_spin_lock+0x1f6/0x270 kernel/locking/spinlock_debug.c:112
 __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:117 [inline]
 _raw_spin_lock_irqsave+0x50/0x70 kernel/locking/spinlock.c:159
 reset_per_cpu_data+0xe6/0x240 [drop_monitor]
 net_dm_cmd_trace+0x43d/0x17a0 [drop_monitor]
 genl_family_rcv_msg_doit+0x22f/0x330 net/netlink/genetlink.c:739
 genl_family_rcv_msg net/netlink/genetlink.c:783 [inline]
 genl_rcv_msg+0x341/0x5a0 net/netlink/genetlink.c:800
 netlink_rcv_skb+0x14d/0x440 net/netlink/af_netlink.c:2497
 genl_rcv+0x29/0x40 net/netlink/genetlink.c:811
 netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
 netlink_unicast+0x54b/0x800 net/netlink/af_netlink.c:1348
 netlink_sendmsg+0x914/0xe00 net/netlink/af_netlink.c:1916
 sock_sendmsg_nosec net/socket.c:651 [inline]
 __sock_sendmsg+0x157/0x190 net/socket.c:663
 ____sys_sendmsg+0x712/0x870 net/socket.c:2378
 ___sys_sendmsg+0xf8/0x170 net/socket.c:2432
 __sys_sendmsg+0xea/0x1b0 net/socket.c:2461
 do_syscall_64+0x30/0x40 arch/x86/entry/common.c:46
 entry_SYSCALL_64_after_hwframe+0x62/0xc7
RIP: 0033:0x7f3f9815aee9
Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b0 ff ff ff f7 d8 64 89 01 48
RSP: 002b:00007f3f972bf0c8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
RAX: ffffffffffffffda RBX: 00007f3f9826d050 RCX: 00007f3f9815aee9
RDX: 0000000020000000 RSI: 0000000020001300 RDI: 0000000000000007
RBP: 00007f3f981b63bd R08: 0000000000000000 R09: 0000000000000000
R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
R13: 000000000000006e R14: 00007f3f9826d050 R15: 00007ffe01ee6768

If drop_monitor is built as a kernel module, syzkaller may have time
to send a netlink NET_DM_CMD_START message during the module loading.
This will call the net_dm_monitor_start() function that uses
a spinlock that has not yet been initialized.

To fix this, let's place resource initialization above the registration
of a generic netlink family.

Found by InfoTeCS on behalf of Linux Verification Center
(linuxtesting.org) with Syzkaller.

Fixes: 9a8afc8d3962 (""Network Drop Monitor: Adding drop monitor implementation & Netlink protocol"")
Cc: stable@vger.kernel.org
Signed-off-by: Ilia Gavrilov <Ilia.Gavrilov@infotecs.ru>
Reviewed-by: Ido Schimmel <idosch@nvidia.com>
Link: https://patch.msgid.link/20250213152054.2785669-1-Ilia.Gavrilov@infotecs.ru
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
---
 net/core/drop_monitor.c | 29 ++++++++++++++---------------
 1 file changed, 14 insertions(+), 15 deletions(-)

diff --git a/net/core/drop_monitor.c b/net/core/drop_monitor.c
index 6efd4cccc9dd..212f0a048cab 100644
--- a/net/core/drop_monitor.c
+++ b/net/core/drop_monitor.c
@@ -1734,30 +1734,30 @@ static int __init init_net_drop_monitor(void)
 		return -ENOSPC;
 	}
 
-	rc = genl_register_family(&net_drop_monitor_family);
-	if (rc) {
-		pr_err(""Could not create drop monitor netlink family\n"");
-		return rc;
+	for_each_possible_cpu(cpu) {
+		net_dm_cpu_data_init(cpu);
+		net_dm_hw_cpu_data_init(cpu);
 	}
-	WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
 	rc = register_netdevice_notifier(&dropmon_net_notifier);
 	if (rc < 0) {
 		pr_crit(""Failed to register netdevice notifier\n"");
+		return rc;
+	}
+
+	rc = genl_register_family(&net_drop_monitor_family);
+	if (rc) {
+		pr_err(""Could not create drop monitor netlink family\n"");
 		goto out_unreg;
 	}
+	WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
 	rc = 0;
 
-	for_each_possible_cpu(cpu) {
-		net_dm_cpu_data_init(cpu);
-		net_dm_hw_cpu_data_init(cpu);
-	}
-
 	goto out;
 
 out_unreg:
-	genl_unregister_family(&net_drop_monitor_family);
+	WARN_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 out:
 	return rc;
 }
@@ -1766,19 +1766,18 @@ static void exit_net_drop_monitor(void)
 {
 	int cpu;
 
-	BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
-
 	/*
 	 * Because of the module_get/put we do in the trace state change path
 	 * we are guaranteed not to have any current users when we get here
 	 */
+	BUG_ON(genl_unregister_family(&net_drop_monitor_family));
+
+	BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 
 	for_each_possible_cpu(cpu) {
 		net_dm_hw_cpu_data_fini(cpu);
 		net_dm_cpu_data_fini(cpu);
 	}
-
-	BUG_ON(genl_unregister_family(&net_drop_monitor_family));
 }
 
 module_init(init_net_drop_monitor);
-- 
2.39.5 (Apple Git-154)

",29f9cdcab3d96d5207a5c92b52c40ad75e5915d8,"From 29f9cdcab3d96d5207a5c92b52c40ad75e5915d8 Mon Sep 17 00:00:00 2001
From: Gavrilov Ilia <Ilia.Gavrilov@infotecs.ru>
Date: Thu, 13 Feb 2025 15:20:55 +0000
Subject: [PATCH] drop_monitor: fix incorrect initialization order

[ Upstream commit 07b598c0e6f06a0f254c88dafb4ad50f8a8c6eea ]

Syzkaller reports the following bug:

BUG: spinlock bad magic on CPU#1, syz-executor.0/7995
 lock: 0xffff88805303f3e0, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
CPU: 1 PID: 7995 Comm: syz-executor.0 Tainted: G            E     5.10.209+ #1
Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 11/12/2020
Call Trace:
 __dump_stack lib/dump_stack.c:77 [inline]
 dump_stack+0x119/0x179 lib/dump_stack.c:118
 debug_spin_lock_before kernel/locking/spinlock_debug.c:83 [inline]
 do_raw_spin_lock+0x1f6/0x270 kernel/locking/spinlock_debug.c:112
 __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:117 [inline]
 _raw_spin_lock_irqsave+0x50/0x70 kernel/locking/spinlock.c:159
 reset_per_cpu_data+0xe6/0x240 [drop_monitor]
 net_dm_cmd_trace+0x43d/0x17a0 [drop_monitor]
 genl_family_rcv_msg_doit+0x22f/0x330 net/netlink/genetlink.c:739
 genl_family_rcv_msg net/netlink/genetlink.c:783 [inline]
 genl_rcv_msg+0x341/0x5a0 net/netlink/genetlink.c:800
 netlink_rcv_skb+0x14d/0x440 net/netlink/af_netlink.c:2497
 genl_rcv+0x29/0x40 net/netlink/genetlink.c:811
 netlink_unicast_kernel net/netlink/af_netlink.c:1322 [inline]
 netlink_unicast+0x54b/0x800 net/netlink/af_netlink.c:1348
 netlink_sendmsg+0x914/0xe00 net/netlink/af_netlink.c:1916
 sock_sendmsg_nosec net/socket.c:651 [inline]
 __sock_sendmsg+0x157/0x190 net/socket.c:663
 ____sys_sendmsg+0x712/0x870 net/socket.c:2378
 ___sys_sendmsg+0xf8/0x170 net/socket.c:2432
 __sys_sendmsg+0xea/0x1b0 net/socket.c:2461
 do_syscall_64+0x30/0x40 arch/x86/entry/common.c:46
 entry_SYSCALL_64_after_hwframe+0x62/0xc7
RIP: 0033:0x7f3f9815aee9
Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b0 ff ff ff f7 d8 64 89 01 48
RSP: 002b:00007f3f972bf0c8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
RAX: ffffffffffffffda RBX: 00007f3f9826d050 RCX: 00007f3f9815aee9
RDX: 0000000020000000 RSI: 0000000020001300 RDI: 0000000000000007
RBP: 00007f3f981b63bd R08: 0000000000000000 R09: 0000000000000000
R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
R13: 000000000000006e R14: 00007f3f9826d050 R15: 00007ffe01ee6768

If drop_monitor is built as a kernel module, syzkaller may have time
to send a netlink NET_DM_CMD_START message during the module loading.
This will call the net_dm_monitor_start() function that uses
a spinlock that has not yet been initialized.

To fix this, let's place resource initialization above the registration
of a generic netlink family.

Found by InfoTeCS on behalf of Linux Verification Center
(linuxtesting.org) with Syzkaller.

Fixes: 9a8afc8d3962 (""Network Drop Monitor: Adding drop monitor implementation & Netlink protocol"")
Cc: stable@vger.kernel.org
Signed-off-by: Ilia Gavrilov <Ilia.Gavrilov@infotecs.ru>
Reviewed-by: Ido Schimmel <idosch@nvidia.com>
Link: https://patch.msgid.link/20250213152054.2785669-1-Ilia.Gavrilov@infotecs.ru
Signed-off-by: Jakub Kicinski <kuba@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 net/core/drop_monitor.c | 29 ++++++++++++++---------------
 1 file changed, 14 insertions(+), 15 deletions(-)

diff --git a/net/core/drop_monitor.c b/net/core/drop_monitor.c
index 009b9e22c4e7..c8a3d6056365 100644
--- a/net/core/drop_monitor.c
+++ b/net/core/drop_monitor.c
@@ -1727,30 +1727,30 @@ static int __init init_net_drop_monitor(void)
 		return -ENOSPC;
 	}
 
-	rc = genl_register_family(&net_drop_monitor_family);
-	if (rc) {
-		pr_err(""Could not create drop monitor netlink family\n"");
-		return rc;
+	for_each_possible_cpu(cpu) {
+		net_dm_cpu_data_init(cpu);
+		net_dm_hw_cpu_data_init(cpu);
 	}
-	WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
 	rc = register_netdevice_notifier(&dropmon_net_notifier);
 	if (rc < 0) {
 		pr_crit(""Failed to register netdevice notifier\n"");
+		return rc;
+	}
+
+	rc = genl_register_family(&net_drop_monitor_family);
+	if (rc) {
+		pr_err(""Could not create drop monitor netlink family\n"");
 		goto out_unreg;
 	}
+	WARN_ON(net_drop_monitor_family.mcgrp_offset != NET_DM_GRP_ALERT);
 
 	rc = 0;
 
-	for_each_possible_cpu(cpu) {
-		net_dm_cpu_data_init(cpu);
-		net_dm_hw_cpu_data_init(cpu);
-	}
-
 	goto out;
 
 out_unreg:
-	genl_unregister_family(&net_drop_monitor_family);
+	WARN_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 out:
 	return rc;
 }
@@ -1759,19 +1759,18 @@ static void exit_net_drop_monitor(void)
 {
 	int cpu;
 
-	BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
-
 	/*
 	 * Because of the module_get/put we do in the trace state change path
 	 * we are guarnateed not to have any current users when we get here
 	 */
+	BUG_ON(genl_unregister_family(&net_drop_monitor_family));
+
+	BUG_ON(unregister_netdevice_notifier(&dropmon_net_notifier));
 
 	for_each_possible_cpu(cpu) {
 		net_dm_hw_cpu_data_fini(cpu);
 		net_dm_cpu_data_fini(cpu);
 	}
-
-	BUG_ON(genl_unregister_family(&net_drop_monitor_family));
 }
 
 module_init(init_net_drop_monitor);
-- 
2.39.5 (Apple Git-154)

",patching file net/core/drop_monitor.c\nHunk #1 succeeded at 1727 (offset -7 lines).\nHunk #2 FAILED at 1766.\n1 out of 2 hunks FAILED -- saving rejects to file net/core/drop_monitor.c.rej,30,1,0,137,1,DISJOINT,COMMENTS
