r"""Fetches and filters patches from Git repositories.

This module is the second step in the patch management pipeline. It reads the
structured `parsed_report.json` file generated by the report parser, downloads
the specified patch for each entry from its Git repository (Googlesource or
CodeLinaro), filters it to include only relevant files, and saves the final
diff file to disk.

It includes functionality to handle network rate limiting with exponential
backoff and decodes Base64-encoded patches from Googlesource.

Usage:
  python patch_fetcher.py
"""

import json
import sys
import os
import requests
import re
import base64
import time
from pathlib import Path
from urllib.parse import urlparse

# --- UTILITY FUNCTIONS ---

def _extract_commit_hash(commit_url: str) -> str | None:
    """Extracts the commit hash from a Googlesource or CodeLinaro URL.

    Args:
        commit_url: The URL to the commit in a Git repository.

    Returns:
        The commit hash extracted from the URL, or None if not found.
    """
    path = urlparse(commit_url).path
    # The hash is the last part of the path, after '/+/'
    try:
        return path.split('/+/')[-1].rstrip('^!')
    except IndexError:
        return None

def _get_with_backoff(url: str, retries: int = 5) -> requests.Response:
    """Makes an HTTP GET request with exponential backoff for rate limiting.

    If a request fails with a 429 status (Too Many Requests), the function
    waits for an exponentially increasing amount of time before retrying.

    Args:
        url: The URL to request.
        retries: The maximum number of retry attempts.

    Returns:
        The final HTTP response from the server.
    """
    for i in range(retries):
        response = requests.get(url)
        if response.status_code != 429:
            return response
        # Exponential backoff: 1, 2, 4, 8, 16 seconds
        backoff_time = 2**i
        print(f"⚠️ Received status 429 (Too Many Requests). "
              f"Waiting {backoff_time}s before retrying...")
        time.sleep(backoff_time)
    return response  # Return the last response after all retries

# --- CORE PATCH FETCHING FUNCTIONALITY ---

def fetch_and_filter_patch(commit_url: str, files_to_include: list[str]) -> str | None:
    """Downloads, filters, and saves the diff for a given commit URL.

    This function performs the following steps:
    1. Extracts the commit hash and constructs the correct raw diff URL.
    2. Fetches the patch content with rate-limiting backoff.
    3. Decodes the patch if it is Base64 encoded (from Googlesource).
    4. Filters the diff content to include only changes for specified files.
    5. Saves the filtered patch to a local file.

    Args:
        commit_url: The URL to the commit in a Git repository.
        files_to_include: A list of file paths to include in the filtered patch.

    Returns:
        The path to the saved patch file, or None if the operation failed.
    """
    project_root = Path(__file__).resolve().parent.parent
    commit_hash = _extract_commit_hash(commit_url)
    if not commit_hash:
        print(f"❌ Could not extract commit hash from URL: {commit_url}")
        return None

    # Determine the repository type and construct the appropriate download URL.
    if "android.googlesource.com" in commit_url:
        # Googlesource uses a specific format for raw, Base64-encoded text diffs.
        diff_url = commit_url + "^!/?format=TEXT"
    elif "git.codelinaro.org" in commit_url:
        diff_url = commit_url + ".diff"
    else:
        print(f"❌ Unsupported repository host in URL: {commit_url}")
        return None

    print(f"  -> Fetching diff from: {diff_url}")

    # Create the output directory if it doesn't exist.
    output_dir = project_root / "fetch_patch_output" / "diff_output"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_filename = output_dir / f"{commit_hash}.diff"

    response = _get_with_backoff(diff_url)
    if response.status_code != 200:
        print(f"❌ Failed to fetch diff for {commit_hash}. "
              f"HTTP Status: {response.status_code}")
        return None

    # Googlesource encodes the patch in Base64, while others are plain text.
    if "android.googlesource.com" in commit_url:
        try:
            raw_diff = base64.b64decode(response.text)
            diff_text = raw_diff.decode("utf-8")
        except (ValueError, TypeError) as e:
            print(f"❌ Failed to decode Base64 content for {commit_hash}: {e}")
            return None
    else:
        diff_text = response.text

    # Filter the full diff to include only the specified files.
    filtered_diff_lines = []
    current_file_being_processed = None
    should_capture_lines = False

    for line in diff_text.splitlines():
        if line.startswith("diff --git"):
            # A new file section has started; reset capture flag.
            should_capture_lines = False
            match = re.search(r" a/(.+) b/(.+)", line)
            if match:
                # Check if this new file path is in our list of files to include.
                current_file_being_processed = match.group(1)
                if any(
                    file_path.endswith(current_file_being_processed)
                    for file_path in files_to_include
                ):
                    should_capture_lines = True

        if should_capture_lines:
            filtered_diff_lines.append(line)

    if not filtered_diff_lines:
        print(f"❌ No matching diff content found for files: {files_to_include}")
        return None

    # Save the filtered patch to disk.
    with open(output_filename, "w", encoding="utf-8") as f:
        f.write("\n".join(filtered_diff_lines).strip() + "\n")

    return str(output_filename)

# --- BATCH PROCESSING FUNCTIONALITY ---

def process_patches_from_report(report_path: str = None) -> dict:
    """Processes all patches listed in a parsed report JSON file.

    This function orchestrates the patch fetching process by iterating through
    the report, calling the fetch function for each entry, and tracking the
    results.

    Args:
        report_path: The path to the parsed report JSON. Defaults to the
          standard location within the 'reports' directory.

    Returns:
        A dictionary summarizing the successful and failed fetch operations.
    """
    vidar_dir = Path(__file__).resolve().parent

    # Define the default path to the parsed report if not provided.
    if not report_path:
        report_path = vidar_dir / "reports" / "parsed_report.json"

    results = {"successful": [], "failed": []}

    try:
        with open(report_path, "r", encoding="utf-8") as f:
            parsed_report = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"❌ Error loading parsed report from '{report_path}': {e}")
        return results

    patches_to_process = parsed_report.get("patches", [])
    print(f"Processing {len(patches_to_process)} patches from report: {report_path.name}")

    for i, patch in enumerate(patches_to_process):
        patch_url = patch.get("patch_url")
        if not patch_url:
            print(f"⚠️ Skipping entry {i+1} due to missing 'patch_url'.")
            continue

        files_to_include = list(patch.get("files", {}).keys())
        print(f"\n[{i+1}/{len(patches_to_process)}] Processing patch: {patch_url}")
        print(f"  -> Filtering for files: {files_to_include}")

        try:
            diff_file_path = fetch_and_filter_patch(patch_url, files_to_include)

            if diff_file_path:
                print(f"✅ Successfully saved patch to: {Path(diff_file_path).name}")
                results["successful"].append({
                    "url": patch_url,
                    "file": diff_file_path,
                })
            else:
                print(f"❌ Failed to fetch or filter patch: {patch_url}")
                results["failed"].append({
                    "url": patch_url,
                    "reason": "Fetch, decode, or filter operation failed.",
                })

        except Exception as e:
            # Catch any unexpected errors during processing.
            print(f"❌ An unexpected error occurred while processing {patch_url}: {e}")
            results["failed"].append({"url": patch_url, "reason": str(e)})

    # Print a final summary of the operations.
    print("\n" + "="*40)
    print("Patch Fetching Summary")
    print("="*40)
    print(f"Successfully fetched: {len(results['successful'])}")
    print(f"Failed to fetch: {len(results['failed'])}")
    print("="*40)

    return results

# --- MAIN ENTRY POINT ---

def main():
    """Main entry point for the patch fetcher script."""
    results = process_patches_from_report()

    # Exit with a non-zero status code if any patches failed to download.
    if results["failed"]:
        print("\nSome patches could not be fetched. Please review the errors above.")
        sys.exit(1)

    sys.exit(0)

if __name__ == "__main__":
    main()